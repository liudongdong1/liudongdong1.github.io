<!DOCTYPE html>
<html itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">
  <head>
    
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
    <meta name="robots" content="noodp" />
    <title>Hand Analyse Record - DAY By DAY</title><meta name="author" content="LiuDongdong">
<meta name="author-link" content="https://liudongdong1.github.io/">
<meta name="description" content="level: CVPR CCF_A author: Tomas Simon Carnegie Mellon University date: 2017 keyword: hand pose Paper: OpenPose HandKeypoint Summary present an approach that uses a multi-camera system to train fine-grained detectors for keypoints. Research Objective Application Area: hand based HCI and robotics Purpose: to extract hand point coordinate from single RGB images. Proble Statement self-occlusion due to articulation, view-point, grasped object. previous work: many approaches to image-based face and body keypoint" /><meta name="keywords" content='CV' /><meta itemprop="name" content="Hand Analyse Record">
<meta itemprop="description" content="level: CVPR CCF_A author: Tomas Simon Carnegie Mellon University date: 2017 keyword: hand pose Paper: OpenPose HandKeypoint Summary present an approach that uses a multi-camera system to train fine-grained detectors for keypoints. Research Objective Application Area: hand based HCI and robotics Purpose: to extract hand point coordinate from single RGB images. Proble Statement self-occlusion due to articulation, view-point, grasped object. previous work: many approaches to image-based face and body keypoint"><meta itemprop="datePublished" content="2020-06-20T11:32:44+00:00" />
<meta itemprop="dateModified" content="2023-09-28T23:21:36+08:00" />
<meta itemprop="wordCount" content="7736"><meta itemprop="image" content="https://liudongdong1.github.io/logo.png"/>
<meta itemprop="keywords" content="CV," /><meta property="og:title" content="Hand Analyse Record" />
<meta property="og:description" content="level: CVPR CCF_A author: Tomas Simon Carnegie Mellon University date: 2017 keyword: hand pose Paper: OpenPose HandKeypoint Summary present an approach that uses a multi-camera system to train fine-grained detectors for keypoints. Research Objective Application Area: hand based HCI and robotics Purpose: to extract hand point coordinate from single RGB images. Proble Statement self-occlusion due to articulation, view-point, grasped object. previous work: many approaches to image-based face and body keypoint" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://liudongdong1.github.io/hand-analyse-record/" /><meta property="og:image" content="https://liudongdong1.github.io/logo.png"/><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2020-06-20T11:32:44+00:00" />
<meta property="article:modified_time" content="2023-09-28T23:21:36+08:00" />
<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://liudongdong1.github.io/logo.png"/>

<meta name="twitter:title" content="Hand Analyse Record"/>
<meta name="twitter:description" content="level: CVPR CCF_A author: Tomas Simon Carnegie Mellon University date: 2017 keyword: hand pose Paper: OpenPose HandKeypoint Summary present an approach that uses a multi-camera system to train fine-grained detectors for keypoints. Research Objective Application Area: hand based HCI and robotics Purpose: to extract hand point coordinate from single RGB images. Proble Statement self-occlusion due to articulation, view-point, grasped object. previous work: many approaches to image-based face and body keypoint"/>
<meta name="application-name" content="DAY By DAY">
<meta name="apple-mobile-web-app-title" content="DAY By DAY"><meta name="theme-color" data-light="#f8f8f8" data-dark="#252627" content="#f8f8f8"><meta name="msapplication-TileColor" content="#da532c"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" />
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"><link rel="manifest" href="/site.webmanifest"><link rel="canonical" href="https://liudongdong1.github.io/hand-analyse-record/" /><link rel="prev" href="https://liudongdong1.github.io/rfid-actionrecognition/" /><link rel="next" href="https://liudongdong1.github.io/bluepaperrecord/" /><link rel="stylesheet" href="/css/style.min.css"><link rel="stylesheet" href="/lib/fontawesome-free/all.min.css"><link rel="stylesheet" href="/lib/animate/animate.min.css"><script type="application/ld+json">
  {
    "@context": "http://schema.org",
    "@type": "BlogPosting",
    "headline": "Hand Analyse Record",
    "inLanguage": "zh-CN",
    "mainEntityOfPage": {
      "@type": "WebPage",
      "@id": "https:\/\/liudongdong1.github.io\/hand-analyse-record\/"
    },"genre": "posts","keywords": "CV","wordcount":  7736 ,
    "url": "https:\/\/liudongdong1.github.io\/hand-analyse-record\/","datePublished": "2020-06-20T11:32:44+00:00","dateModified": "2023-09-28T23:21:36+08:00","license": "This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.","publisher": {
      "@type": "Organization",
      "name": "LiuDongdong","logo": "https:\/\/liudongdong1.github.io\/images\/person.png"},"author": {
        "@type": "Person",
        "name": "liudongdong1"
      },"description": ""
  }
  </script></head>
  <body data-header-desktop="auto" data-header-mobile="auto"><script>(window.localStorage?.getItem('theme') ? localStorage.getItem('theme') === 'dark' : ('auto' === 'auto' ? window.matchMedia('(prefers-color-scheme: dark)').matches : 'auto' === 'dark')) && document.body.setAttribute('data-theme', 'dark');</script><div class="wrapper"><header class="desktop animate__faster" id="header-desktop">
  <div class="header-wrapper" data-github-corner="right">
    <div class="header-title">
      <a href="/" title="DAY By DAY"><img
    class="lazyload logo"
    src="/svg/loading.min.svg"
    data-src="/fixit.min.svg"
    data-srcset="/fixit.min.svg, /fixit.min.svg 1.5x, /fixit.min.svg 2x"
    data-sizes="auto"
    alt="DAY By DAY"
    title="DAY By DAY"/><span class="header-title-text"></span></a><span id="typeit-header-subtitle-desktop" class="typeit header-subtitle"></span></div>
    <nav>
      <ul class="menu"><li class="menu-item">
              <a
                class="menu-link"
                href="/posts/"
                
                
              ><i class="fa-solid fa-archive fa-fw fa-sm" aria-hidden="true"></i> 所有文章</a></li><li class="menu-item">
              <a
                class="menu-link"
                href="/categories/"
                
                
              ><i class="fa-solid fa-th fa-fw fa-sm" aria-hidden="true"></i> 分类</a></li><li class="menu-item">
              <a
                class="menu-link"
                href="/tags/"
                
                
              ><i class="fa-solid fa-tags fa-fw fa-sm" aria-hidden="true"></i> 标签</a></li><li class="menu-item">
              <a
                class="menu-link"
                href="/friends/"
                title="友情链接"
                
              ><i class="fa-solid fa-users fa-fw fa-sm" aria-hidden="true"></i> 友链</a></li><li class="menu-item">
              <a
                class="menu-link"
                href="/about/"
                
                
              ><i class="fa-solid fa-info-circle fa-fw fa-sm" aria-hidden="true"></i> 关于</a></li><li class="menu-item delimiter"></li><li class="menu-item language">
            <span role="button" aria-label="选择语言" title="选择语言">简体中文<i class="dropdown-icon fa-solid fa-chevron-down" aria-hidden="true"></i>
            </span>
            <ul class="sub-menu"><li class="menu-item">没有更多翻译</li></ul>
          </li><li class="menu-item search" id="search-desktop">
            <input type="text" placeholder="搜索文章标题或内容 ..." id="search-input-desktop">
            <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-desktop" title="搜索">
              <i class="fa-solid fa-search fa-fw" aria-hidden="true"></i>
            </a>
            <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-desktop" title="清空">
              <i class="fa-solid fa-times-circle fa-fw" aria-hidden="true"></i>
            </a>
            <span class="search-button search-loading" id="search-loading-desktop">
              <i class="fa-solid fa-spinner fa-fw fa-spin" aria-hidden="true"></i>
            </span>
          </li><li class="menu-item theme-switch" title="切换主题">
          <i class="fa-solid fa-adjust fa-fw" aria-hidden="true"></i>
        </li>
      </ul>
    </nav>
  </div>
</header><header class="mobile animate__faster" id="header-mobile">
  <div class="header-container">
    <div class="header-wrapper">
      <div class="header-title">
        <a href="/" title="DAY By DAY"><img
    class="lazyload logo"
    src="/svg/loading.min.svg"
    data-src="/fixit.min.svg"
    data-srcset="/fixit.min.svg, /fixit.min.svg 1.5x, /fixit.min.svg 2x"
    data-sizes="auto"
    alt="/fixit.min.svg"
    title="/fixit.min.svg"/><span class="header-title-text"></span></a><span id="typeit-header-subtitle-mobile" class="typeit header-subtitle"></span></div>
      <div class="menu-toggle" id="menu-toggle-mobile">
        <span></span><span></span><span></span>
      </div>
    </div>
    <nav>
      <ul class="menu" id="menu-mobile"><li class="search-wrapper">
            <div class="search mobile" id="search-mobile">
              <input type="text" placeholder="搜索文章标题或内容 ..." id="search-input-mobile">
              <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-mobile" title="搜索">
                <i class="fa-solid fa-search fa-fw" aria-hidden="true"></i>
              </a>
              <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-mobile" title="清空">
                <i class="fa-solid fa-times-circle fa-fw" aria-hidden="true"></i>
              </a>
              <span class="search-button search-loading" id="search-loading-mobile">
                <i class="fa-solid fa-spinner fa-fw fa-spin" aria-hidden="true"></i>
              </span>
            </div>
            <a href="javascript:void(0);" class="search-cancel" id="search-cancel-mobile">
              取消
            </a>
          </li><li
              class="menu-item"
            ><a
                  class="menu-link"
                  href="/posts/"
                  
                  
                ><i class="fa-solid fa-archive fa-fw fa-sm" aria-hidden="true"></i> 所有文章</a></li><li
              class="menu-item"
            ><a
                  class="menu-link"
                  href="/categories/"
                  
                  
                ><i class="fa-solid fa-th fa-fw fa-sm" aria-hidden="true"></i> 分类</a></li><li
              class="menu-item"
            ><a
                  class="menu-link"
                  href="/tags/"
                  
                  
                ><i class="fa-solid fa-tags fa-fw fa-sm" aria-hidden="true"></i> 标签</a></li><li
              class="menu-item"
            ><a
                  class="menu-link"
                  href="/friends/"
                  title="友情链接"
                  
                ><i class="fa-solid fa-users fa-fw fa-sm" aria-hidden="true"></i> 友链</a></li><li
              class="menu-item"
            ><a
                  class="menu-link"
                  href="/about/"
                  
                  
                ><i class="fa-solid fa-info-circle fa-fw fa-sm" aria-hidden="true"></i> 关于</a></li><li
              class="menu-item text-center"
            ><a
                  class="menu-link"
                  href="/"
                  title="GitHub"
                  
                ><i class='fa-brands fa-github fa-fw' aria-hidden='true'></i> </a></li><li class="menu-item theme-switch" title="切换主题">
          <i class="fa-solid fa-adjust fa-fw" aria-hidden="true"></i>
        </li><li class="menu-item language">
            <span role="button" aria-label="选择语言" title="选择语言">简体中文<i class="dropdown-icon fa-solid fa-chevron-down" aria-hidden="true"></i>
            </span>
            <select class="language-select" onchange="location = this.value;"><option disabled>没有更多翻译</option></select>
          </li></ul>
    </nav>
  </div>
</header><div class="search-dropdown desktop">
    <div id="search-dropdown-desktop"></div>
  </div>
  <div class="search-dropdown mobile">
    <div id="search-dropdown-mobile"></div>
  </div><main class="container" data-page-style="normal"><aside class="toc" id="toc-auto"><h2 class="toc-title">目录 <i class="toc-icon fa-solid fa-angle-down fa-fw"></i></h2>
      <div class="toc-content" id="toc-content-auto"></div></aside>

  <aside class="aside-custom" id="aside-sakana">
    

<div class="sakana-widget">
  <div class="sakana-item" id="takina-widget"></div>
  <div class="sakana-item" id="chisato-widget"></div>
</div>
<script>
  function initSakanaWidget() {
    const takina = SakanaWidget.getCharacter('takina')
    SakanaWidget.registerCharacter('takina-slow', takina);
    new SakanaWidget({
      character: 'takina-slow',
      controls: false,
      autoFit: true,
      stroke: {
        color: "#b4b4b4",
        width: 2
      }
    }).mount('#takina-widget');

    const chisato = SakanaWidget.getCharacter('chisato')
    SakanaWidget.registerCharacter('chisato-slow', chisato);
    new SakanaWidget({
      character: 'chisato-slow',
      controls: false,
      autoFit: true,
      stroke: {
        color: "#b4b4b4",
        width: 2
      }
    }).mount('#chisato-widget');
  }
</script>
<script async onload="initSakanaWidget()" src="https://cdn.jsdelivr.net/npm/sakana-widget@2.3.0/lib/sakana.min.js">
</script></aside>

  <article class="page single">
    <div class="header"><h1 class="single-title animate__animated animate__flipInX">
        <span>Hand Analyse Record</span>
      </h1></div><div class="post-meta">
      <div class="post-meta-line"><span class="post-author"><span class="author"><i class="fa-solid fa-user-circle" aria-hidden="true"></i>
      liudongdong1</span></span>
          <span class="post-category">收录于 <a href="/categories/"><i class="fa-regular fa-folder fa-fw"></i>&nbsp;Categories</a>&ensp;<a href="/categories/aiot/"><i class="fa-regular fa-folder fa-fw"></i>&nbsp;AIOT</a></span></div>
      <div class="post-meta-line"><span title=2020-06-20&#32;11:32:44>
            <i class="fa-regular fa-calendar-alt fa-fw"></i>&nbsp;<time datetime="2020-06-20" >2020-06-20</time>
          </span>&nbsp;<i class="fa-solid fa-pencil-alt fa-fw"></i>&nbsp;约 7736 字&nbsp;
        <i class="fa-regular fa-clock fa-fw"></i>&nbsp;预计阅读 16 分钟&nbsp;<span id="busuanzi_container_page_pv" class="busuanzi_visitors comment-visitors" data-flag-title="Hand Analyse Record">
            <i class="fa-regular fa-eye fa-fw"></i>&nbsp;<span id="busuanzi_value_page_pv">-</span>&nbsp;次阅读
          </span>&nbsp;</div>
    </div><div class="featured-image"><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/dataglove.jpg"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/dataglove.jpg, https://gitee.com/github-25970295/blogImage/raw/master/img/dataglove.jpg 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/dataglove.jpg 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/dataglove.jpg"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/dataglove.jpg"/></div><div class="details toc" id="toc-static" kept="true">
        <div class="details-summary toc-title">
          <span>目录</span>
          <span><i class="details-icon fa-solid fa-angle-right"></i></span>
        </div>
        <div class="details-content toc-content" id="toc-content-static"><nav id="TableOfContents">
  <ul>
    <li><a href="#paper-openpose-handkeypoint">Paper: OpenPose HandKeypoint</a>
      <ul>
        <li></li>
      </ul>
    </li>
    <li><a href="#paper-crossinfonet">Paper: CrossInfoNet</a>
      <ul>
        <li></li>
      </ul>
    </li>
    <li><a href="#paper-emotion-identification">Paper: Emotion Identification</a>
      <ul>
        <li></li>
      </ul>
    </li>
    <li><a href="#paper-gesture-to-speech">Paper: Gesture To Speech</a>
      <ul>
        <li></li>
      </ul>
    </li>
    <li><a href="#paper-flex">Paper: Flex</a></li>
    <li><a href="#paper-survey-on-hand-pose-estimation">Paper: Survey on Hand Pose Estimation</a></li>
    <li><a href="#paper-flexgyroscopes">Paper: Flex&amp;Gyroscopes</a></li>
  </ul>

  <ul>
    <li>
      <ul>
        <li></li>
      </ul>
    </li>
    <li><a href="#paper--a-mobile-robot-hand-arm">Paper:  A Mobile Robot Hand-arm</a>
      <ul>
        <li></li>
      </ul>
    </li>
    <li><a href="#paper-human-robot">Paper: Human-Robot</a>
      <ul>
        <li></li>
      </ul>
    </li>
    <li><a href="#paper-mktbgrb">Paper: MKTB&amp;GRB</a>
      <ul>
        <li></li>
      </ul>
    </li>
    <li><a href="#paper-hand-pointnet">Paper: Hand PointNet</a>
      <ul>
        <li></li>
      </ul>
    </li>
    <li><a href="#paper-r3dcnn-dynamic-hand">Paper: R3DCNN Dynamic Hand</a>
      <ul>
        <li></li>
      </ul>
    </li>
  </ul>
</nav></div>
      </div><div
      class="content"
      id="content"
      
      
    ><p><strong>level</strong>:  CVPR  CCF_A
<strong>author</strong>: Tomas Simon   Carnegie Mellon University
<strong>date</strong>: 2017
<strong>keyword</strong>:</p>
<ul>
<li>hand pose</li>
</ul>
<hr>
<h2 id="paper-openpose-handkeypoint">Paper: OpenPose HandKeypoint</h2>
<!-- raw HTML omitted -->
<h4 id="summary">Summary</h4>
<ol>
<li>present an approach that uses a multi-camera system to train fine-grained detectors for keypoints.</li>
</ol>
<h4 id="research-objective">Research Objective</h4>
<ul>
<li><strong>Application Area</strong>: hand based HCI and robotics</li>
<li><strong>Purpose</strong>:  to extract hand point coordinate from single RGB images.</li>
</ul>
<h4 id="proble-statement">Proble Statement</h4>
<ul>
<li>self-occlusion due to articulation, view-point, grasped object.<img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200313092350789.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200313092350789.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200313092350789.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200313092350789.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200313092350789.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200313092350789.png"/></li>
</ul>
<p>previous work:</p>
<ul>
<li>many approaches to image-based face and body keypoint localization exist, there are no markerless hand keypoint detectors that work on RGB images in the wild.</li>
</ul>
<h4 id="methods">Methods</h4>
<ul>
<li><strong>Problem Formulation</strong>:</li>
</ul>
<p>input:   a crop image patch $I\epsilon R^{w<em>h</em>3}$</p>
<p>output:  P keypoint location, $X_p\epsilon R^2$,with associated confidence $C_p$.
$$
Keypoint;detector:  ;;;;d(I)-&gt;[ (X_p,c_p) ;for ; p \epsilon[1&hellip;.P]]
$$</p>
<ul>
<li><strong>system overview</strong>:</li>
</ul>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200307172658147.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200307172658147.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200307172658147.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200307172658147.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200307172658147.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200307172658147.png"/></p>
<p><strong>【Multiview Bootstrapped Training】</strong></p>
<ul>
<li>$Initial;trainingset: ;;;;T_0:=[ (I_f,y_p^f) ;for;f\epsilon[1&hellip;N_0]]$, $f$ denote the particular image frame,set$[y_p^f\epsilon R^2]$ include all labeled keypoints for image $I^f$ .</li>
</ul>
<p>Multiview Bootrstrap:</p>
<ul>
<li>Inputs:
<ul>
<li>calibrated cameras configuration</li>
<li>unlabled images: $[ I_v^f ; for ; v\epsilon views,; f\epsilon frames]$</li>
<li>keypoint detector: $d_0(I)-&gt;[(x_p,c_p);for;p\epsilon points]$</li>
<li>labeled training data: $T_0$</li>
</ul>
</li>
<li>Output: improved detector $d_K(.)$ and training set $T_k$</li>
<li>for iteration $i$ in 0  to K:
<ol>
<li><!-- raw HTML omitted -->Triangulate keypoint from weak detections<!-- raw HTML omitted -->
<ul>
<li>for every frame $f$:
<ul>
<li>run detector $d_i(I_v^f)$ on all views $v$ , $D&lt;-{d_i(I_v^f) ; for ; v\epsilon [1&hellip;V] }$                 (1)</li>
<li>robustly triangulate keypoints,   $X_p^f=argmin_X \sum_{v\epsilon I_p^f}{||P_v(X)-x_p^v||_2^2}$      (2)</li>
</ul>
</li>
</ul>
</li>
<li><!-- raw HTML omitted -->score and sort triangulated frames<!-- raw HTML omitted -->  ,         $score({X_p^f})=\sum_{p\epsilon [1&hellip;P]}\sum_{v\epsilon I_p^f}C_p^v$           (3)</li>
<li><!-- raw HTML omitted -->retrain with N-best reprojections<!-- raw HTML omitted -->. $d_{i+1}&lt;-train(T_0;U;T_{i+1})$       (4)</li>
</ol>
</li>
</ul>
<p><strong>supplement for the mathmatic:</strong></p>
<ul>
<li>
<p>for (1):for one frame,  for each keypoint p, we have V detections $(x_p^v,c_p^v)$ , robustly triangulate each point p into a 3D location, use RANSAC on point D with confidence above a detection threshold $\lambda$.</p>
</li>
<li>
<p>for (2): $I_p^f$ is the inlier set, $X_p^f \epsilon R^3$   is the 3D triangulated keypoint p in frame f,  $P_v(X) \epsilon R^2$   denotes projection of 3D point $X$ into view $v$.  triangulate all landmarks of each finger(4 points ) at a time.</p>
</li>
<li>
<p>for(3): pick the best frame for every window of $W$ frames. Sort the frame in descending order according to their score, to obtain an ordered sequence of frames,$[s_1,s_2,&hellip;s_F^‘]$, $F^‘$ is the number of subsampled frames, $s_i$ is the ordered frame index.</p>
<ul>
<li>while verigy the good labled frame, using some strategies to automatically removing bad frame:
<ul>
<li>average number of inliews</li>
<li>average detection detection confidence</li>
<li>difference of per-point velocity with medium velocity between two video frames</li>
<li>anthropomorphic limits on joint lengths</li>
<li>complete occlusion as determined by camera ray intersection with body joints</li>
</ul>
</li>
</ul>
</li>
<li>
<p>for(4):           $T_{i+1}={(I_v^{s_n},{P_v(X_p^{s_n}):;v\epsilon [1&hellip;V],; p\epsilon [1&hellip;P]}); for; n\epsilon[1&hellip;N]}$</p>
</li>
</ul>
<p><strong>【Detection Architecture】</strong></p>
<ul>
<li><strong>Hand Bounding Box Detection:</strong> directly use the body pose estimation models from [29], and [4] and use wrist and elbow position to approximate the hand location, assuming the hand extends 0.15 times the length of the forearm(前臂) in the same direction.</li>
<li>using architecture of CPMs with some modification.  <!-- raw HTML omitted -->CPMs predict a confidence map for each keypoint, representing the keypoint&rsquo;s location as a Gaussian centered at the true position<!-- raw HTML omitted --></li>
<li>using pre-trained VGG-19 network</li>
</ul>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200307174424118.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200307174424118.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200307174424118.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200307174424118.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200307174424118.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200307174424118.png"/></p>
<h4 id="evaluation">Evaluation</h4>
<ul>
<li><strong>Environment</strong>:
<ul>
<li>Dataset:
<ul>
<li>the MPII human pose dataset[2] <!-- raw HTML omitted -->reflect every-day human activities<!-- raw HTML omitted --></li>
<li>Images from the New Zealand Sign Language Exercised os the Victoria University of Wellington <!-- raw HTML omitted -->contains a variety of hand poses found in conversation<!-- raw HTML omitted --></li>
</ul>
</li>
</ul>
</li>
</ul>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200307171832277.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200307171832277.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200307171832277.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200307171832277.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200307171832277.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200307171832277.png"/></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200307175621323.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200307175621323.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200307175621323.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200307175621323.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200307175621323.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200307175621323.png"/></p>
<h4 id="conclusion">Conclusion</h4>
<ul>
<li>the first real-time hand keypoint detector showing practical applicability to in-the-wild RGB videos</li>
<li>the first markerless 3D hand motion capture system capable of reconstructing challenging hand-object interactions and musical performances without manual intervention</li>
<li>using multi-view bootstrapping, improving both the quality and quantity of the annotations</li>
</ul>
<h4 id="notes">Notes</h4>
<ul>
<li>
<p><strong>Bootstrap步骤：</strong></p>
<ul>
<li>
<p>在原有的样本中通过重抽样抽取一定数量（比如100）的新样本。</p>
</li>
<li>
<p>基于产生的新样本，计算我们需要估计的统计量$\alpha_i$。</p>
</li>
<li>
<p>重复上述步骤n次（一般是n&gt;1000次）。计算被估计量的均值和方差。</p>
</li>
<li>
<p>$$
\vec{\alpha}=Mean(\alpha_i&hellip;)
$$</p>
</li>
<li>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200607110758736.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200607110758736.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200607110758736.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200607110758736.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200607110758736.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200607110758736.png"/></p>
</li>
</ul>
</li>
<li>
<p><strong><a href="http://www.cse.yorku.ca/~kosta/CompVis_Notes/ransac.pdf"target="_blank" rel="external nofollow noopener noreferrer">RANSAC:<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></strong> robust estimation techniques such as M-estimators and least-median squares that have been adopted by the computer vision community from the statistics literature, RANSAC was developed from within the computer vision community</p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200307183024378.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200307183024378.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200307183024378.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200307183024378.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200307183024378.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200307183024378.png"/></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200307183041127.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200307183041127.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200307183041127.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200307183041127.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200307183041127.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200307183041127.png"/></p>
</li>
</ul>
<p><strong>level</strong>: CVPR
<strong>author</strong>: Kuo Du1
<strong>date</strong>: 2019
<strong>keyword</strong>:</p>
<ul>
<li>hand skeleton</li>
</ul>
<hr>
<h2 id="paper-crossinfonet">Paper: CrossInfoNet</h2>
<!-- raw HTML omitted -->
<h4 id="summary-1">Summary</h4>
<ol>
<li>proposed CrossInfoNet decomposes hand pose estimation task into palm pose estimation sub-task and finger pose estimation sub-task, and adopts two-branch cross-connection structure to share the beneficial complementary information between the sub-tasks.</li>
<li>propose a heat-map guided feature extraction structure to get better feature maps, and train the complete network end-to-end.</li>
</ol>
<h4 id="proble-statement-1">Proble Statement</h4>
<p>previous work:</p>
<ul>
<li>treating depth maps as 2D images and regressing 3D joint coordinates directly is a commonly used hand pose estimation pipeline.</li>
<li>designing effective networks receives the most attentions. Learning multiple tasks simultaneously will be helpful to enforce a model with better generalizing ability.</li>
<li>the output representations can be classified into the probability density map or the 3D coordinates for each joint.  <!-- raw HTML omitted -->heat-map based method outperforms direct coordinate regression method, and the final joint coordinates have usually to be inferred by maximum operation on the heat-maps<!-- raw HTML omitted --></li>
</ul>
<h4 id="methods-1">Methods</h4>
<ul>
<li><strong>system overview</strong>:</li>
</ul>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200309194734899.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200309194734899.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200309194734899.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200309194734899.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200309194734899.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200309194734899.png"/></p>
<p><strong>【Heat-map guided feature extraction】</strong></p>
<ul>
<li>ResNet-50 [15] backbone network with four residual modules</li>
<li>apply the feature pyramid structure to merge different feature layers.</li>
<li>the heat maps are only used as the constraints to guide the feature extraction and will not be passed to the subsequent module.</li>
</ul>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200309194817022.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200309194817022.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200309194817022.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200309194817022.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200309194817022.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200309194817022.png"/></p>
<p><strong>【Baseline feature refinement architecture】</strong></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200309195036333.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200309195036333.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200309195036333.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200309195036333.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200309195036333.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200309195036333.png"/></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200309195052036.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200309195052036.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200309195052036.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200309195052036.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200309195052036.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200309195052036.png"/></p>
<p><strong>【New Feature refinement architecture】</strong></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200309195142885.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200309195142885.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200309195142885.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200309195142885.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200309195142885.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200309195142885.png"/></p>
<p><strong>【Loss Functions Defines】</strong></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200309195243947.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200309195243947.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200309195243947.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200309195243947.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200309195243947.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200309195243947.png"/></p>
<h4 id="evaluation-1">Evaluation</h4>
<ul>
<li><strong>Environment</strong>:
<ul>
<li>Dataset:  ICVL datasets, NYU datasets, MSRA datasets, Hands 2017 Challenge Frame-based Dataset.</li>
</ul>
</li>
</ul>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200309195412451.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200309195412451.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200309195412451.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200309195412451.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200309195412451.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200309195412451.png"/></p>
<h4 id="conclusion-1">Conclusion</h4>
<ul>
<li>use hierarchical model to decompose the final task into palm joint regression sub-task and finger joint regression sub-task.</li>
<li>a heat-map guided feature extraction structure is proposed.</li>
</ul>
<h4 id="notes-font-colororange去加强了解font">Notes <!-- raw HTML omitted -->去加强了解<!-- raw HTML omitted --></h4>
<ul>
<li><a href="https://github.com/dumyy/handpose"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/dumyy/handpose<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></li>
</ul>
<h2 id="paper-emotion-identification">Paper: Emotion Identification</h2>
<!-- raw HTML omitted -->
<h4 id="summary-2">Summary</h4>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200314105007698.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200314105007698.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200314105007698.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200314105007698.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200314105007698.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200314105007698.png"/></p>
<h2 id="paper-gesture-to-speech">Paper: Gesture To Speech</h2>
<!-- raw HTML omitted -->
<h4 id="summary-3">Summary</h4>
<ul>
<li>Arduino Uno, Flex Sensors, MPU6050 an accelerometer gyroscope sensor which is used to detect the alignment of an object.</li>
<li>To recognise the ALS Sign Language <img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200314105349781.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200314105349781.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200314105349781.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200314105349781.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200314105349781.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200314105349781.png"/></li>
<li><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200314105446162.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200314105446162.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200314105446162.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200314105446162.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200314105446162.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200314105446162.png"/></li>
</ul>
<h2 id="paper-flex">Paper: Flex</h2>
<!-- raw HTML omitted -->
<ul>
<li>
<p>Flex Sensors from Spectra-Symbol for angle displacement measuremetns.</p>
</li>
<li>
<p>apply a linear response delay filter to the raw sensors output for noise reduction and signal smoothing.</p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200314110009115.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200314110009115.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200314110009115.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200314110009115.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200314110009115.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200314110009115.png"/></p>
</li>
</ul>
<h2 id="paper-survey-on-hand-pose-estimation">Paper: Survey on Hand Pose Estimation</h2>
<!-- raw HTML omitted -->
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200314110451195.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200314110451195.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200314110451195.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200314110451195.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200314110451195.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200314110451195.png"/></p>
<ul>
<li>
<p>详细介绍了基于视觉基于传感器方法</p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200314110640684.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200314110640684.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200314110640684.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200314110640684.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200314110640684.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200314110640684.png"/></p>
</li>
</ul>
<h2 id="paper-flexgyroscopes">Paper: Flex&amp;Gyroscopes</h2>
<!-- raw HTML omitted -->
<ul>
<li>
<p>some sensors</p>
<ul>
<li>contact sensors for detecting fingers touching each other</li>
<li>accelerometers for measuring the acceleration of the hand in different direction</li>
<li>gyro-scopes for measuring the hand orientation and angular movement</li>
<li>magnetoresistive sensors for measuring the magnetic field for deriving the hand orientation</li>
</ul>
</li>
<li>
<p>presents a Thai sign language recognition framework using  a glove-based device with flex sensors and gyro-scops.</p>
</li>
<li>
<p>the measurements from the sensors are processed using finite Legendre and Linear Discriminant Analysis, then classified using k-nearest neighbors.</p>
</li>
<li>
<p>Handware design:<img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200314111857761.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200314111857761.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200314111857761.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200314111857761.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200314111857761.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200314111857761.png"/></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200314112748401.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200314112748401.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200314112748401.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200314112748401.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200314112748401.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200314112748401.png"/></p>
</li>
<li>
<p>the gyroscopes can return values in three different types of measurement</p>
<ul>
<li>the quaternions are the raw data returned from the sensor. This measurement yields a four-dimensional output.</li>
<li>Euler angles are data converted from the four quaternion values. The Euler angles consist of three values, matching x, y, and z axis.</li>
<li>YPR measures the angle but with respect to the direction of the ground. It has three elements like the Euler angles. However it also requires gravity values from the accelerometer in order to calibrate.<!-- raw HTML omitted --> to calculate YPR, four quaternion elements and three gravity values are needed<!-- raw HTML omitted --></li>
</ul>
</li>
<li>
<p>Date processing</p>
<ul>
<li>
<p>segment and normalize the data  ???how to segment data unclear??</p>
</li>
<li>
<p>the value from flex sensors differ greatly depend on person, by requiring a calibration phase which the user clenches and releases his hands at least 3 times to determine th e maximum and minimum values of each flex sensor, and quantize the data to 3 possible values(0,1,2)</p>
</li>
<li>
<p>这部分不理解：<img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200314140333583.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200314140333583.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200314140333583.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200314140333583.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200314140333583.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200314140333583.png"/></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200314140348967.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200314140348967.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200314140348967.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200314140348967.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200314140348967.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200314140348967.png"/></p>
</li>
</ul>
</li>
</ul>
<h1 id="human-machine-interaction">Human-Machine-Interaction</h1>
<blockquote>
<p>Taheri, Omid, et al. &ldquo;GRAB: A dataset of whole-body human grasping of objects.&rdquo; <em>European Conference on Computer Vision</em>. Springer, Cham, 2020.</p>
</blockquote>
<hr>
<h1 id="paper-grab">Paper: GRAB</h1>
<!-- raw HTML omitted -->
<h4 id="summary-4">Summary</h4>
<ol>
<li>collect a new dataset, GRAB of whole-body grasps, containing full 3D shape and pose sequences of 10 subjects interacting with 51 every day objects of varying shape and size.</li>
<li>using MoCap markers to fit the full 3D body shape and pose, including the articulated face and hands, as well as the 3D object pose.</li>
<li>adapt MoSh++ to solve for the body, face, and hands of SMPL-X to obtain detailed moving 3D meshes, and according to the meshes and tracked 3D objects, we compute plausible contact on the object and the human and provide an analysis of observed patterns.</li>
</ol>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210224170220204.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210224170220204.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210224170220204.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210224170220204.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210224170220204.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210224170220204.png"/></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210224170341797.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210224170341797.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210224170341797.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210224170341797.png 2x"
    data-sizes="auto"
    alt="Contact Annotation"
    title="Contact Annotation"/></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210224170455822.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210224170455822.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210224170455822.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210224170455822.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210224170455822.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210224170455822.png"/></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210224170556522.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210224170556522.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210224170556522.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210224170556522.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210224170556522.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210224170556522.png"/></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210224170611285.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210224170611285.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210224170611285.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210224170611285.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210224170611285.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210224170611285.png"/></p>
<h4 id="relative">Relative</h4>
<ul>
<li>require complex 3D object shapes, detailed contact information, hand pose and shape, and the 3D body motion over time;</li>
<li>MoCap: <a href="https://mocap.reallusion.com/iClone-motion-live-mocap/"target="_blank" rel="external nofollow noopener noreferrer">https://mocap.reallusion.com/iClone-motion-live-mocap/<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></li>
</ul>
<hr>
<h2 id="paper--a-mobile-robot-hand-arm">Paper:  A Mobile Robot Hand-arm</h2>
<!-- raw HTML omitted -->
<h4 id="summary-5">Summary</h4>
<blockquote>
<ol>
<li>present a multi-modal mobile teleoperation system that consists of a novel vision-based hand pose regression network and IMU-based arm tracking methods.</li>
<li>observe the human hand through a  depth camera and generates joint angles and depth images of paired robot hand poses through an image-to-image translation process.</li>
<li>Transteleop takes the depth image of the human hand as input, then estimates the joint angles of the robot hand, and also generates the reconstructed image of the robot hand.</li>
<li>design a keypoint-based reconstruction loss to focus on the local reconstruction quality around the keypoints of the hand.</li>
</ol>
</blockquote>
<h4 id="research-objective-1">Research Objective</h4>
<ul>
<li><strong>Application Area</strong>: space, rescue, medical, surgery, imitation learning.</li>
<li><strong>Purpose</strong>:  implement different manipulation tasks such as pick and place, cup insertion, object pushing, and dual-arm handover tasks</li>
</ul>
<h4 id="proble-statement-2">Proble Statement</h4>
<ul>
<li>the robot hand and human hand occupy two different domains, how to compensate for kinematic differences between them plays an essential role in markerless vision-based teleoperation</li>
</ul>
<p>previous work:</p>
<ul>
<li><strong>Image-to-Image translation:</strong>  aims to map representation of a scene into another, used in collection of style transfer, object transfiguration, and imitation learning.</li>
</ul>
<h4 id="methodsj">Methodsj</h4>
<ul>
<li><strong>system overview</strong>:</li>
</ul>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200612092238443.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200612092238443.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200612092238443.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200612092238443.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200612092238443.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200612092238443.png"/></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200612092928609.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200612092928609.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200612092928609.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200612092928609.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200612092928609.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200612092928609.png"/></p>
<p>【Question 1】how to discover the latent feature embedding the Zpose between the human hand and robot hand?</p>
<blockquote>
<p>using Encoder-decoder module</p>
</blockquote>
<p>【Question 2】how to get more accuracy of local features such as the position of fingertips instead of global features such as image style?</p>
<blockquote>
<p>design a keypoint-based reconstruction loss to capture the overall structure of the hand and concentrate on the pixels around the 15 keypoints of the hand.</p>
<p>using mean squared error(MSE) loss to  calculate the joint from $Z_R$ (robot feature)</p>
</blockquote>
<p>【Question 3】the poses of the human hand vary considerably in their global orientations?</p>
<blockquote>
<p>applied spatial transformation network(STN) provides spatial transformation capabilities of input images before the encoder module.</p>
</blockquote>
<p>【Question 4】the hand easily disappears from the field of view of the camera, and the camera position is uncertain ?</p>
<blockquote>
<p>using a cheap 3D-printed camera holder</p>
<p>using Perception Neuron device to control the arm of the robot.</p>
</blockquote>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200612095958385.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200612095958385.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200612095958385.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200612095958385.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200612095958385.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200612095958385.png"/></p>
<h4 id="evaluation-2">Evaluation</h4>
<ul>
<li>
<p><strong>Environment</strong>:</p>
<ul>
<li>Dataset:  dataset of paired human-robot images, contains 400k pairs of simulated robot depth images and human hand depth images, the ground trush are 19 joint angles of the robot hand, record the 9 depth images of the robot hand from different viewpoints simultaneously corresponding to one human pose.</li>
</ul>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200612100328537.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200612100328537.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200612100328537.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200612100328537.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200612100328537.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200612100328537.png"/></p>
</li>
</ul>
<h4 id="notes-font-colororange去加强了解font-1">Notes <!-- raw HTML omitted -->去加强了解<!-- raw HTML omitted --></h4>
<ul>
<li><a href="https://Smilels.github.io/multimodal-translation-teleop"target="_blank" rel="external nofollow noopener noreferrer">https://Smilels.github.io/multimodal-translation-teleop<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></li>
<li>可能有什么问题，</li>
</ul>
<p><strong>level</strong>: PerDial'19
<strong>author</strong>:
<strong>date</strong>:  2019
<strong>keyword</strong>:</p>
<ul>
<li>robot, ASL,</li>
</ul>
<hr>
<h2 id="paper-human-robot">Paper: Human-Robot</h2>
<!-- raw HTML omitted -->
<ol>
<li>presents a concept of smart robotic trolley for supermarkets with multi-modal user interface, including sign language and acoustic speech recognition, and equipped with a touch screen.</li>
</ol>
<h4 id="proble-statement-3">Proble Statement</h4>
<ul>
<li>continuous or dynamic sign language recognition remains an unresolved challenge.</li>
<li>sensitivity to size and speed variations, poor performance under varying lighting conditions and complex background have limited the use of SLR in modern dialogue systems.</li>
</ul>
<p>previous work:</p>
<ul>
<li>the level of voiced speech and isolated/static hand gesture automatic recognition quality is quite high.</li>
<li>EffiBot[1] takes goods and automatically goes with them to the point of discharge, and follow the user when the corresponding mode is activated.</li>
<li>The Dash Robotic Shopping Cart[2] :
<ul>
<li>a supermarket trolley that facilitates shopping and navigation in the store, the car is equipped with a touchscreen for entering a list of products of interest to the client.</li>
</ul>
</li>
<li>Gita by Piaggio[3]: a robotic trolley that follows the owner.</li>
<li><!-- raw HTML omitted -->none of the interfaces of the aforementioned robotic carts are multimodal.<!-- raw HTML omitted --></li>
</ul>
<h4 id="methods-2">Methods</h4>
<ul>
<li>
<p><strong>system overview</strong>:</p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200407165820138.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200407165820138.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200407165820138.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200407165820138.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200407165820138.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200407165820138.png"/></p>
</li>
</ul>
<ol>
<li>speaker-independent system of automatic continuous Russian speech recognition</li>
<li>speaker-independent system of Russian sign language recognition with video processing using Kinect2.0 device</li>
<li>interactive graphical user interface with touchscreen</li>
<li>dialogue and data manager that access an application database, generates multi modal output and synchronizes input modalities fusion and output modalities fission</li>
<li>modules for audio-visual speech synthesis to be applied for a talking avatar</li>
</ol>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200407171743516.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200407171743516.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200407171743516.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200407171743516.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200407171743516.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200407171743516.png"/></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200407171910822.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200407171910822.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200407171910822.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200407171910822.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200407171910822.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200407171910822.png"/></p>
<h4 id="conclusion-2">Conclusion</h4>
<ul>
<li>understanding voice commands</li>
<li>understanding Russian sign language commands</li>
<li>escort the user to a certain place in the store</li>
<li>speech synthesis, synthesis of answers in Russian sign language using a 3D avatar.</li>
</ul>
<h4 id="notes-1">Notes</h4>
<ul>
<li>介绍了一些手语 数据集</li>
<li>【30】，32 ，7 ， 34</li>
<li>机器人：https://www.effidence.com/effibot
<ul>
<li>​		https://mygita.com/#/how-does-gita-work</li>
</ul>
</li>
</ul>
<hr>
<p><strong>level</strong>:   IJCAI
<strong>author</strong>: YangYi (MediaLab,Tencent)    FengNi(PekingUniversity)
<strong>date</strong>: 2019
<strong>keyword</strong>:</p>
<ul>
<li>hand gesture understand</li>
</ul>
<hr>
<h2 id="paper-mktbgrb">Paper: MKTB&amp;GRB</h2>
<!-- raw HTML omitted -->
<h4 id="research-objective-2">Research Objective</h4>
<ul>
<li><strong>Purpose</strong>:  hand gesture recognition instead of human-human or human-object relationships.</li>
</ul>
<h4 id="proble-statement-4">Proble Statement</h4>
<ul>
<li>hand gesture recognition methods based on spatio-temporal features using 3DCNNs or ConvLSTM suffer from the inefficiency due to high computational complexity of their structure.</li>
</ul>
<p>previous work:</p>
<ul>
<li>
<p>Temporal Modeling for Action Recognition</p>
<ul>
<li>2DCNN by Narayana et al., 2018</li>
<li>3DCNNs by  Miao et al., 2017</li>
<li>ConvLSTM by Zhang et al., 2017</li>
<li>TSN by Wang et al.2016 models long-range temporal structures with segment-based sampling and aggregation module.</li>
<li>C3D by Li et al.2016 designs a 3DCNN with small 3<em>3</em>3 convolution kernels to learn spatiotemporal features.</li>
<li>I3D by Carreira 2017 inflates convolutional filters and pooling kernels into 3D structures.</li>
<li>R(2+1)D by Wang et al. 2018 present non-local operations to capture long-reange dependencies</li>
</ul>
</li>
<li>
<p>Gesture Recognition:</p>
<ul>
<li>2DCNN by Narayana et al.2018  (学习下，多模态的,只了解多模态部分) fuses multi-channels(global/left-hand/right-hand/for RGB/depth/RGB-flow/Depth-flow modalities)</li>
<li>combines 3DCNN， bidirectional ConvLSTM and 2DCNN into a unified framework. ( 学习下如何整合到一个框架中)</li>
</ul>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200313162234686.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200313162234686.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200313162234686.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200313162234686.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200313162234686.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200313162234686.png"/></p>
</li>
</ul>
<h4 id="methods-3">Methods</h4>
<ul>
<li>
<p><strong>system overview</strong>:</p>
<ul>
<li>the model builds upon TSN, for TSN lacks of capability of modeling the temporal information from feature-space, the proposed MKTB and GRB are effective temporal modeling modules in feature-space.</li>
</ul>
</li>
</ul>
<p>【Multi-Kernel Temporal Block】</p>
<ul>
<li>unlike 3DCNNs, performing convolutional operation for both spatial and temporal dimension jointly, the MTKB decouples the joint spatial-temporal modeling process and focuses on learning the temporal information.</li>
<li>the design of multi-kernel works well on shaping the pyramidal and discriminative temporal features.</li>
</ul>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200313162921564.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200313162921564.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200313162921564.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200313162921564.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200313162921564.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200313162921564.png"/></p>
<ul>
<li>define feature maps from layer $l$ of 2DCNN(ResNet-50) as $F_s\epsilon R^{(B<em>T)<em>C</em>H</em>W}$</li>
<li>reduce the channels of $F_s$  via convolution layer with kernel size of 1*1, denoted as  $F_s^‘ \epsilon R^{ ( B * T) * C^‘ * H * W}$</li>
<li><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200313163941544.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200313163941544.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200313163941544.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200313163941544.png 2x"
    data-sizes="auto"
    alt="image-20200313163941544"
    title="image-20200313163941544"/> using depthwise temporal conv [Chollet,2017]</li>
</ul>
<p>【Global Refinement Block】</p>
<ul>
<li>
<p>MKTB mainly focuses on the local neighborhoods,but the global temporal features across channels are not sufficiently attended.</p>
</li>
<li>
<p>GRB is designed to perform the weighted temporal aggregation, in which it allows distant temporal features to contribute to the filtered temporal features according to the cross-similarity.  <!-- raw HTML omitted -->遗留问题，如何计算similarity， MKTB 中如何sum<!-- raw HTML omitted --></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200313164405173.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200313164405173.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200313164405173.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200313164405173.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200313164405173.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200313164405173.png"/></p>
</li>
</ul>
<h4 id="evaluation-3">Evaluation</h4>
<ul>
<li><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200313164608764.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200313164608764.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200313164608764.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200313164608764.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200313164608764.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200313164608764.png"/></li>
</ul>
<h4 id="conclusion-3">Conclusion</h4>
<ul>
<li>MKTB captures both short-term and long-term temporal information by using the multiple 1D depthwise convolutions.</li>
<li>MKTB and GRB maintain the same size between input and output, and can be easily deployed everywhere.</li>
</ul>
<h4 id="notes-font-colororange去加强了解font-2">Notes <!-- raw HTML omitted -->去加强了解<!-- raw HTML omitted --></h4>
<ul>
<li><i class="fa-regular fa-square fa-fw" aria-hidden="true"></i> <a href="https://github.com/nemonameless/Gesture-Recognition"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/nemonameless/Gesture-Recognition<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></li>
</ul>
<p><strong>level</strong>: CCF_A  CVPR
<strong>author</strong>: Liuhao Ge, Nanyang Technological University
<strong>date</strong>:  2018
<strong>keyword</strong>:</p>
<ul>
<li>hand pose</li>
</ul>
<hr>
<h2 id="paper-hand-pointnet">Paper: Hand PointNet</h2>
<!-- raw HTML omitted -->
<h4 id="summary-6">Summary</h4>
<ol>
<li>propose HandPointNet model, that directly processes the 3D point cloud that models the visible surface of the hand for pose regression,Taking the normalized point cloud as the input, the regression network capture complex hand structures and accurately regress a low dimensional representation of the 3D hand pose.</li>
<li>design a fingertip refinement network that directly takes the neighboring points of the estimated fingertip location as input to refine the fingertip location.</li>
</ol>
<h4 id="research-objective-3">Research Objective</h4>
<ul>
<li><strong>Application Area</strong>: hand based interaction</li>
<li><strong>Purpose</strong>:   exact hand skeleton</li>
</ul>
<h4 id="proble-statement-5">Proble Statement</h4>
<ul>
<li>high dimensionality of 3D hand pose, large variations in hand orientations, high self-similarity of fingers and servere self-occlusion</li>
</ul>
<p>previous work:</p>
<ul>
<li>large hand pose datasets[38, 34, 33, 49, 48]</li>
<li><strong>CNN model:</strong>
<ul>
<li>the time and space complexities of the 3D CNN grow cubically with the resolution of the input 3D volume, using low resolution may lose useful details of the hand</li>
<li><!-- raw HTML omitted -->PointNet<!-- raw HTML omitted -->: perform 3D object classification and segmentation on point sets directly</li>
<li>using multi-view CNNs-based method and 3D CNN-based method</li>
</ul>
</li>
<li><strong>Hand Pose Estimation:</strong> Discriminative approaches,  generative approaches, hybrid approaches
<ul>
<li>feedback loop model[21]</li>
<li>spatial attention network[47]</li>
<li>deep generative models[41]</li>
</ul>
</li>
<li><strong>3D Deep Learning</strong>:
<ul>
<li>Multi-view CNNs-based approaches[32, 24, 7, 2] project 3D points into 2D images and use 2D CNNs to process them.</li>
<li>3D CNNs based on octrees[27, 43] are proposed for efficient computation on high resolution volumes.</li>
</ul>
</li>
</ul>
<h4 id="methods-4">Methods</h4>
<ul>
<li><strong>Problem Formulation</strong>:</li>
</ul>
<blockquote>
<ul>
<li>
<p>Input: depth image containing a hand;</p>
</li>
<li>
<p>outputs: a set of 3D hand joint locations in the amera coordinate system.</p>
</li>
</ul>
</blockquote>
<ul>
<li><strong>system overview</strong>:</li>
</ul>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200301140035643.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200301140035643.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200301140035643.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200301140035643.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200301140035643.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200301140035643.png"/></p>
<p>【Basic PointNet】 [23] directly takes a set of points as the input and is able to extract discriminative features of the point cloud.   <!-- raw HTML omitted -->cannot capture local structures of the point cloud in a hierarchical way<!-- raw HTML omitted -->.</p>
<blockquote>
<p>basic architecture of PointNet takes N points as the input, Each D-dim input point is mapped into a C-dim feature through MLP. Per-point features are aggregated into a global feature by max-pooling, and mapped into F-dim output vector.</p>
</blockquote>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211002200807400.png"
    data-srcset="https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211002200807400.png, https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211002200807400.png 1.5x, https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211002200807400.png 2x"
    data-sizes="auto"
    alt="pointnet classification"
    title="pointnet classification"/></p>
<p><strong>【Hierarchical PointNet】</strong>[25]:</p>
<blockquote>
<p>The hierarchical structure is composed by a number of set abstraction levels, at each level, a  set of points is processed and abstracted to produce a new set with fewer elements. The set abstraction level is made of three key layers: 点云采样+成组+提取局部特征（S+G+P）的方式，包含这三部分的机构称为 Set Abstraction</p>
<ul>
<li><strong>sampling layer</strong>: selects a set of points from input points, which defines the <code>centroids of lcoal regions</code>. use interative farthest point sampling(FPS) to choose the subset of points.</li>
<li><strong>grouping layer</strong>: constructs local region sets by fining &ldquo;neighboring&rdquo; points around the centroids. N`<em>K</em>(d+C): d-dim coordinates, and C-dim point feature, K is the number of points in the neighborhood of centroid points. Ball query finds all points that are within a radius to the query point.</li>
<li><strong>PointNet alyer:</strong>  uses a mini-PointNet to encode local region patterns into feaature vectors.</li>
</ul>
</blockquote>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211002105943638.png"
    data-srcset="https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211002105943638.png, https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211002105943638.png 1.5x, https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211002105943638.png 2x"
    data-sizes="auto"
    alt="PointNet&#43;&#43;"
    title="PointNet&#43;&#43;"/></p>
<blockquote>
<ul>
<li>分类网络是逐层提取特征，最后总结出全局特征。</li>
<li>分割网络先将点云提取一个全局特征，在通过这个全局特征逐步上采样。每层新的中心点都是从上一层抽取的特征子集，中心点的个数就是成组的点集数，随着层数增加，中心点的个数也会逐渐降低，抽取到点云的局部结构特征。<code>当点云不均匀时</code>，每个子区域中如果在分区的时候使用相同的球半径，会导致部分稀疏区域采样点过小。多尺度成组 (MSG)<strong>和</strong>多分辨率成组 (MRG)
<ul>
<li>**多尺度成组（MSG）：**对于选取的一个中心点设置多个半径进行成组，并将经过PointNet对每个区域抽取后的特征进行拼接（concat）来当做该中心点的特征.</li>
<li>**多分辨率成组（MRG）：**对不同特征层上（分辨率）提取的特征再进行concat，以上图右图为例，最后的concat包含左右两个部分特征，分别来自底层和高层的特征抽取，对于low level点云成组后经过一个pointnet和high level的进行concat，思想是特征的抽取中的跳层连接。当局部点云区域较稀疏时，上层提取到的特征可靠性可能比底层更差，因此考虑对底层特征提升权重。当然，点云密度较高时能够提取到的特征也会更多。这种方法优化了直接在稀疏点云上进行特征抽取产生的问题，且相对于MSG的效率也较高。</li>
</ul>
</li>
</ul>
</blockquote>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211002201356244.png"
    data-srcset="https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211002201356244.png, https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211002201356244.png 1.5x, https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211002201356244.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211002201356244.png"
    title="https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211002201356244.png"/></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211002201115350.png"
    data-srcset="https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211002201115350.png, https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211002201115350.png 1.5x, https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211002201115350.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211002201115350.png"
    title="https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211002201115350.png"/></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211002200643224.png"
    data-srcset="https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211002200643224.png, https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211002200643224.png 1.5x, https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211002200643224.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211002200643224.png"
    title="https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211002200643224.png"/></p>
<p>【<strong>OBB-based Point Cloud Normalization</strong>】to deal with large variation in global orientation of the hand. normalization the hand point cloud into a canonical coordinate system in which the global orientations of the transformed hand point clouds are as consistent as possible. <!-- raw HTML omitted -->normalization step ensures that our method is robust to variations in hand global orientations<!-- raw HTML omitted --></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211002170627689.png"
    data-srcset="https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211002170627689.png, https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211002170627689.png 1.5x, https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211002170627689.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211002170627689.png"
    title="https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211002170627689.png"/></p>
<blockquote>
<p><code>each column corresponds to the same local region</code>, and <code>each row correspnd to the same filter. </code>Following pictures show the sensitivity of points in three loacl regions to two fitlers at each of the first two levels.</p>
</blockquote>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211002170941689.png"
    data-srcset="https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211002170941689.png, https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211002170941689.png 1.5x, https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211002170941689.png 2x"
    data-sizes="auto"
    alt="image-20211002170941689"
    title="image-20211002170941689"/></p>
<p>【<strong>Refine the Fingertip</strong>】</p>
<p>Based on the obervation: the fingertip location of straightened finger is usually easy to be fined, since K nearest neighboring points of the fingertip will not change a lot even if the estimated location deviates from the ground truth location to some extent when <!-- raw HTML omitted -->K is relatively large<!-- raw HTML omitted --></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211002171012560.png"
    data-srcset="https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211002171012560.png, https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211002171012560.png 1.5x, https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211002171012560.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211002171012560.png"
    title="https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211002171012560.png"/></p>
<h4 id="conclusion-4">Conclusion</h4>
<ul>
<li>estimate 3D hand joint locations directly from 3D point cloud base on the netword architecture of PointNet. better expoit the 3D spatial information in the depth image</li>
<li>robust to variations in hand global orientations, normalize the sampled 3D points in an oriented bounding box without applying any additional network to transform the hand piont cloud.</li>
<li>refine the fingertip locations with a basic PointNet that takes the Neighboring points of the estimation fingertip location as input to regress the refined fingertip location.</li>
</ul>
<h4 id="notes-font-colororange去加强了解font-3">Notes <!-- raw HTML omitted -->去加强了解<!-- raw HTML omitted --></h4>
<h5 id="1-最远点采样">1. 最远点采样</h5>
<blockquote>
<p>最远点采样(Farthest Point Sampling)是一种非常常用的采样算法，由于能够保证对样本的均匀采样，被广泛使用，像3D点云深度学习框架中的PointNet++对样本点进行FPS采样再聚类作为感受野，3D目标检测网络VoteNet对投票得到的散乱点进行FPS采样再进行聚类，6D位姿估计算法PVN3D中用于选择物体的8个特征点进行投票并计算位姿。</p>
<ol>
<li>输入点云有N个点，从点云中选取一个点P0作为起始点，得到采样点集合S={P0}；</li>
<li>计算所有点到P0的距离，构成N维数组L，从中选择最大值对应的点作为P1，更新采样点集合S={P0，P1}；</li>
<li>计算所有点到P1的距离，对于每一个点Pi，其距离P1的距离如果小于L[i]，则更新L[i] = d(Pi, P1)，因此，数组L中存储的一直是每一个点到采样点集合S的最近距离；</li>
<li>选取L中最大值对应的点作为P2，更新采样点集合S={P0，P1，P2}；</li>
<li>重复2-4步，一直采样到N’个目标采样点为止。</li>
</ol>
</blockquote>
<blockquote>
<ul>
<li>初始点选择：
<ul>
<li>随机选择一个点，每次结果不同；</li>
<li>选择距离点云重心的最远点，每次结果相同，一般位于局部极值点，具有刻画能力；</li>
</ul>
</li>
<li>距离度量
<ul>
<li>欧氏距离：主要对于点云，在3D体空间均匀采样；</li>
<li>测地距离：主要对于三角网格，在三角网格面上进行均匀采样；</li>
</ul>
</li>
</ul>
</blockquote>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> __future__ <span style="color:#f92672">import</span> print_function
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> torch
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> torch.autograd <span style="color:#f92672">import</span> Variable
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">farthest_point_sample</span>(xyz, npoint): 
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    Input:
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        xyz: pointcloud data, [B, N, 3]
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        npoint: number of samples
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    Return:
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        centroids: sampled pointcloud index, [B, npoint]
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    xyz <span style="color:#f92672">=</span> xyz<span style="color:#f92672">.</span>transpose(<span style="color:#ae81ff">2</span>,<span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>    device <span style="color:#f92672">=</span> xyz<span style="color:#f92672">.</span>device
</span></span><span style="display:flex;"><span>    B, N, C <span style="color:#f92672">=</span> xyz<span style="color:#f92672">.</span>shape
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    centroids <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>zeros(B, npoint, dtype<span style="color:#f92672">=</span>torch<span style="color:#f92672">.</span>long)<span style="color:#f92672">.</span>to(device)     <span style="color:#75715e"># 采样点矩阵（B, npoint）</span>
</span></span><span style="display:flex;"><span>    distance <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>ones(B, N)<span style="color:#f92672">.</span>to(device) <span style="color:#f92672">*</span> <span style="color:#ae81ff">1e10</span>                       <span style="color:#75715e"># 采样点到所有点距离（B, N）</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    batch_indices <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>arange(B, dtype<span style="color:#f92672">=</span>torch<span style="color:#f92672">.</span>long)<span style="color:#f92672">.</span>to(device)        <span style="color:#75715e"># batch_size 数组</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e">#farthest = torch.randint(0, N, (B,), dtype=torch.long).to(device)  # 初始时随机选择一点</span>
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    barycenter <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>sum((xyz), <span style="color:#ae81ff">1</span>)                                    <span style="color:#75715e">#计算重心坐标 及 距离重心最远的点</span>
</span></span><span style="display:flex;"><span>    barycenter <span style="color:#f92672">=</span> barycenter<span style="color:#f92672">/</span>xyz<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">1</span>]
</span></span><span style="display:flex;"><span>    barycenter <span style="color:#f92672">=</span> barycenter<span style="color:#f92672">.</span>view(B, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">3</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    dist <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>sum((xyz <span style="color:#f92672">-</span> barycenter) <span style="color:#f92672">**</span> <span style="color:#ae81ff">2</span>, <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>    farthest <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>max(dist,<span style="color:#ae81ff">1</span>)[<span style="color:#ae81ff">1</span>]                                     <span style="color:#75715e">#将距离重心最远的点作为第一个点</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(npoint):
</span></span><span style="display:flex;"><span>        print(<span style="color:#e6db74">&#34;-------------------------------------------------------&#34;</span>)
</span></span><span style="display:flex;"><span>        print(<span style="color:#e6db74">&#34;The </span><span style="color:#e6db74">%d</span><span style="color:#e6db74"> farthest pts </span><span style="color:#e6db74">%s</span><span style="color:#e6db74"> &#34;</span> <span style="color:#f92672">%</span> (i, farthest))
</span></span><span style="display:flex;"><span>        centroids[:, i] <span style="color:#f92672">=</span> farthest                                      <span style="color:#75715e"># 更新第i个最远点</span>
</span></span><span style="display:flex;"><span>        centroid <span style="color:#f92672">=</span> xyz[batch_indices, farthest, :]<span style="color:#f92672">.</span>view(B, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">3</span>)        <span style="color:#75715e"># 取出这个最远点的xyz坐标</span>
</span></span><span style="display:flex;"><span>        dist <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>sum((xyz <span style="color:#f92672">-</span> centroid) <span style="color:#f92672">**</span> <span style="color:#ae81ff">2</span>, <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>)                     <span style="color:#75715e"># 计算点集中的所有点到这个最远点的欧式距离</span>
</span></span><span style="display:flex;"><span>        print(<span style="color:#e6db74">&#34;dist    : &#34;</span>, dist)
</span></span><span style="display:flex;"><span>        mask <span style="color:#f92672">=</span> dist <span style="color:#f92672">&lt;</span> distance
</span></span><span style="display:flex;"><span>        print(<span style="color:#e6db74">&#34;mask </span><span style="color:#e6db74">%i</span><span style="color:#e6db74"> : </span><span style="color:#e6db74">%s</span><span style="color:#e6db74">&#34;</span> <span style="color:#f92672">%</span> (i,mask))
</span></span><span style="display:flex;"><span>        distance[mask] <span style="color:#f92672">=</span> dist[mask]                                     <span style="color:#75715e"># 更新distance，记录样本中每个点距离所有已出现的采样点的最小距离</span>
</span></span><span style="display:flex;"><span>        print(<span style="color:#e6db74">&#34;distance: &#34;</span>, distance)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        farthest <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>max(distance, <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>)[<span style="color:#ae81ff">1</span>]                           <span style="color:#75715e"># 返回最远点索引</span>
</span></span><span style="display:flex;"><span> 
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> centroids
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">if</span> __name__ <span style="color:#f92672">==</span> <span style="color:#e6db74">&#39;__main__&#39;</span>:
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    sim_data <span style="color:#f92672">=</span> Variable(torch<span style="color:#f92672">.</span>rand(<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">3</span>,<span style="color:#ae81ff">8</span>))
</span></span><span style="display:flex;"><span>    print(sim_data)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    centroids <span style="color:#f92672">=</span> farthest_point_sample(sim_data, <span style="color:#ae81ff">4</span>)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">&#34;Sampled pts: &#34;</span>, centroids)
</span></span></code></pre></div><h5 id="2-pointnet网络结构-httpsgithubcomyanx27pointnet_pointnet2_pytorch">2. <a href="https://github.com/yanx27/Pointnet_Pointnet2_pytorch"target="_blank" rel="external nofollow noopener noreferrer">PointNet网络结构<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></h5>
<blockquote>
<p>数据集中每一行是六个点，及每一点有六个特征（3d坐标，法向量）normal意思是法向量，可以自己设置，要不要使用法向量，使用的话初始输入的点云数据除了3个位置信息x，y，z以外还有三个法向量Nx，Ny，Nz，每个点一共是6个特征。</p>
</blockquote>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">PointNetEncoder</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, global_feat<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>, feature_transform<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>, channel<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>):
</span></span><span style="display:flex;"><span>        super(PointNetEncoder, self)<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>stn <span style="color:#f92672">=</span> STN3d(channel)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>conv1 <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>Conv1d(channel, <span style="color:#ae81ff">64</span>, <span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>conv2 <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>Conv1d(<span style="color:#ae81ff">64</span>, <span style="color:#ae81ff">128</span>, <span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>conv3 <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>Conv1d(<span style="color:#ae81ff">128</span>, <span style="color:#ae81ff">1024</span>, <span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>bn1 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>BatchNorm1d(<span style="color:#ae81ff">64</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>bn2 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>BatchNorm1d(<span style="color:#ae81ff">128</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>bn3 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>BatchNorm1d(<span style="color:#ae81ff">1024</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>global_feat <span style="color:#f92672">=</span> global_feat
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>feature_transform <span style="color:#f92672">=</span> feature_transform
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> self<span style="color:#f92672">.</span>feature_transform:
</span></span><span style="display:flex;"><span>            self<span style="color:#f92672">.</span>fstn <span style="color:#f92672">=</span> STNkd(k<span style="color:#f92672">=</span><span style="color:#ae81ff">64</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x):
</span></span><span style="display:flex;"><span>        B, D, N <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>size()
</span></span><span style="display:flex;"><span>        trans <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>stn(x)
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>transpose(<span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> D <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">3</span>:
</span></span><span style="display:flex;"><span>            feature <span style="color:#f92672">=</span> x[:, :, <span style="color:#ae81ff">3</span>:]
</span></span><span style="display:flex;"><span>            x <span style="color:#f92672">=</span> x[:, :, :<span style="color:#ae81ff">3</span>]
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>bmm(x, trans)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> D <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">3</span>:
</span></span><span style="display:flex;"><span>            x <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>cat([x, feature], dim<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>transpose(<span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>relu(self<span style="color:#f92672">.</span>bn1(self<span style="color:#f92672">.</span>conv1(x)))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> self<span style="color:#f92672">.</span>feature_transform:
</span></span><span style="display:flex;"><span>            trans_feat <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>fstn(x)
</span></span><span style="display:flex;"><span>            x <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>transpose(<span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>            x <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>bmm(x, trans_feat)
</span></span><span style="display:flex;"><span>            x <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>transpose(<span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>            trans_feat <span style="color:#f92672">=</span> <span style="color:#66d9ef">None</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        pointfeat <span style="color:#f92672">=</span> x
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>relu(self<span style="color:#f92672">.</span>bn2(self<span style="color:#f92672">.</span>conv2(x)))
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>bn3(self<span style="color:#f92672">.</span>conv3(x))
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>max(x, <span style="color:#ae81ff">2</span>, keepdim<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)[<span style="color:#ae81ff">0</span>]
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>view(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1024</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> self<span style="color:#f92672">.</span>global_feat:
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">return</span> x, trans, trans_feat
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>            x <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>view(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1024</span>, <span style="color:#ae81ff">1</span>)<span style="color:#f92672">.</span>repeat(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>, N)
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">return</span> torch<span style="color:#f92672">.</span>cat([x, pointfeat], <span style="color:#ae81ff">1</span>), trans, trans_feat
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">PointNetCls</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, k <span style="color:#f92672">=</span> <span style="color:#ae81ff">2</span>):
</span></span><span style="display:flex;"><span>        super(PointNetCls, self)<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>k <span style="color:#f92672">=</span> k
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>feat <span style="color:#f92672">=</span> PointNetEncoder(global_feat<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>conv1 <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>Conv1d(<span style="color:#ae81ff">1088</span>, <span style="color:#ae81ff">512</span>, <span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>conv2 <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>Conv1d(<span style="color:#ae81ff">512</span>, <span style="color:#ae81ff">256</span>, <span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>conv3 <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>Conv1d(<span style="color:#ae81ff">256</span>, <span style="color:#ae81ff">128</span>, <span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>conv4 <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>Conv1d(<span style="color:#ae81ff">128</span>, self<span style="color:#f92672">.</span>k, <span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>bn1 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>BatchNorm1d(<span style="color:#ae81ff">512</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>bn2 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>BatchNorm1d(<span style="color:#ae81ff">256</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>bn3 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>BatchNorm1d(<span style="color:#ae81ff">128</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x):
</span></span><span style="display:flex;"><span>    	<span style="color:#e6db74">&#39;&#39;&#39;分类网络&#39;&#39;&#39;</span>
</span></span><span style="display:flex;"><span>        batchsize <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>size()[<span style="color:#ae81ff">0</span>]
</span></span><span style="display:flex;"><span>        n_pts <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>size()[<span style="color:#ae81ff">2</span>]
</span></span><span style="display:flex;"><span>        x, trans <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>feat(x)
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>relu(self<span style="color:#f92672">.</span>bn1(self<span style="color:#f92672">.</span>conv1(x)))
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>relu(self<span style="color:#f92672">.</span>bn2(self<span style="color:#f92672">.</span>conv2(x)))
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>relu(self<span style="color:#f92672">.</span>bn3(self<span style="color:#f92672">.</span>conv3(x)))
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>conv4(x)
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>transpose(<span style="color:#ae81ff">2</span>,<span style="color:#ae81ff">1</span>)<span style="color:#f92672">.</span>contiguous()
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>log_softmax(x<span style="color:#f92672">.</span>view(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,self<span style="color:#f92672">.</span>k), dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>view(batchsize, n_pts, self<span style="color:#f92672">.</span>k)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> x
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">PointNetPartSeg</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self,num_class):
</span></span><span style="display:flex;"><span>        super(PointNetPartSeg, self)<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>k <span style="color:#f92672">=</span> num_class
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>feat <span style="color:#f92672">=</span> PointNetEncoder(global_feat<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>conv1 <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>Conv1d(<span style="color:#ae81ff">1088</span>, <span style="color:#ae81ff">512</span>, <span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>conv2 <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>Conv1d(<span style="color:#ae81ff">512</span>, <span style="color:#ae81ff">256</span>, <span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>conv3 <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>Conv1d(<span style="color:#ae81ff">256</span>, <span style="color:#ae81ff">128</span>, <span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>conv4 <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>Conv1d(<span style="color:#ae81ff">128</span>, self<span style="color:#f92672">.</span>k, <span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>bn1 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>BatchNorm1d(<span style="color:#ae81ff">512</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>bn1_1 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>BatchNorm1d(<span style="color:#ae81ff">1024</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>bn2 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>BatchNorm1d(<span style="color:#ae81ff">256</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>bn3 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>BatchNorm1d(<span style="color:#ae81ff">128</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x):
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#39;&#39;&#39;分割网络&#39;&#39;&#39;</span>
</span></span><span style="display:flex;"><span>        batchsize <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>size()[<span style="color:#ae81ff">0</span>]
</span></span><span style="display:flex;"><span>        n_pts <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>size()[<span style="color:#ae81ff">2</span>]
</span></span><span style="display:flex;"><span>        x, trans <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>feat(x)
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>relu(self<span style="color:#f92672">.</span>bn1(self<span style="color:#f92672">.</span>conv1(x)))
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>relu(self<span style="color:#f92672">.</span>bn2(self<span style="color:#f92672">.</span>conv2(x)))
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>relu(self<span style="color:#f92672">.</span>bn3(self<span style="color:#f92672">.</span>conv3(x)))
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>conv4(x)
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>transpose(<span style="color:#ae81ff">2</span>,<span style="color:#ae81ff">1</span>)<span style="color:#f92672">.</span>contiguous()
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>log_softmax(x<span style="color:#f92672">.</span>view(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,self<span style="color:#f92672">.</span>k), dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>view(batchsize, n_pts, self<span style="color:#f92672">.</span>k)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> x, trans
</span></span></code></pre></div><blockquote>
<p>通过引入了<strong>不同分辨率/尺度的Grouping</strong>去对局部做PointNet求局部的全局特征，最后再将不同尺度的特征拼接起来；同时也通过<strong>在训练的时候随机删除一部分的点</strong>来增加模型的缺失鲁棒性。 &ndash;&gt;解决点稀疏问题</p>
</blockquote>
<h5 id="3-pointnet-网络结构-httpsgithubcomyanx27pointnet_pointnet2_pytorch">3. <a href="https://github.com/yanx27/Pointnet_Pointnet2_pytorch"target="_blank" rel="external nofollow noopener noreferrer">PointNet++ 网络结构<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></h5>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> torch.nn <span style="color:#66d9ef">as</span> nn
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> torch.nn.functional <span style="color:#66d9ef">as</span> F
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> pointnet2_utils <span style="color:#f92672">import</span> PointNetSetAbstraction
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> torch
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> numpy <span style="color:#66d9ef">as</span> np
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">get_model</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self,num_class,normal_channel<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>):
</span></span><span style="display:flex;"><span>        super(get_model, self)<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>        in_channel <span style="color:#f92672">=</span> <span style="color:#ae81ff">6</span> <span style="color:#66d9ef">if</span> normal_channel <span style="color:#66d9ef">else</span> <span style="color:#ae81ff">3</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>normal_channel <span style="color:#f92672">=</span> normal_channel
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>sa1 <span style="color:#f92672">=</span> PointNetSetAbstraction(npoint<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, radius<span style="color:#f92672">=</span><span style="color:#ae81ff">0.2</span>, nsample<span style="color:#f92672">=</span><span style="color:#ae81ff">32</span>, in_channel<span style="color:#f92672">=</span>in_channel, mlp<span style="color:#f92672">=</span>[<span style="color:#ae81ff">64</span>, <span style="color:#ae81ff">64</span>, <span style="color:#ae81ff">128</span>], group_all<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>sa2 <span style="color:#f92672">=</span> PointNetSetAbstraction(npoint<span style="color:#f92672">=</span><span style="color:#ae81ff">128</span>, radius<span style="color:#f92672">=</span><span style="color:#ae81ff">0.4</span>, nsample<span style="color:#f92672">=</span><span style="color:#ae81ff">64</span>, in_channel<span style="color:#f92672">=</span><span style="color:#ae81ff">128</span> <span style="color:#f92672">+</span> <span style="color:#ae81ff">3</span>, mlp<span style="color:#f92672">=</span>[<span style="color:#ae81ff">128</span>, <span style="color:#ae81ff">128</span>, <span style="color:#ae81ff">256</span>], group_all<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>sa3 <span style="color:#f92672">=</span> PointNetSetAbstraction(npoint<span style="color:#f92672">=</span><span style="color:#66d9ef">None</span>, radius<span style="color:#f92672">=</span><span style="color:#66d9ef">None</span>, nsample<span style="color:#f92672">=</span><span style="color:#66d9ef">None</span>, in_channel<span style="color:#f92672">=</span><span style="color:#ae81ff">256</span> <span style="color:#f92672">+</span> <span style="color:#ae81ff">3</span>, mlp<span style="color:#f92672">=</span>[<span style="color:#ae81ff">256</span>, <span style="color:#ae81ff">512</span>, <span style="color:#ae81ff">1024</span>], group_all<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>fc1 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(<span style="color:#ae81ff">1024</span>, <span style="color:#ae81ff">512</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>bn1 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>BatchNorm1d(<span style="color:#ae81ff">512</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>drop1 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Dropout(<span style="color:#ae81ff">0.4</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>fc2 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(<span style="color:#ae81ff">512</span>, <span style="color:#ae81ff">256</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>bn2 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>BatchNorm1d(<span style="color:#ae81ff">256</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>drop2 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Dropout(<span style="color:#ae81ff">0.4</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>fc3 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(<span style="color:#ae81ff">256</span>, num_class)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, xyz):
</span></span><span style="display:flex;"><span>        B, _, _ <span style="color:#f92672">=</span> xyz<span style="color:#f92672">.</span>shape
</span></span><span style="display:flex;"><span>        print(<span style="color:#e6db74">&#34;xyz.shape&#34;</span>,xyz<span style="color:#f92672">.</span>shape)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> self<span style="color:#f92672">.</span>normal_channel:
</span></span><span style="display:flex;"><span>            norm <span style="color:#f92672">=</span> xyz[:, <span style="color:#ae81ff">3</span>:, :]
</span></span><span style="display:flex;"><span>            xyz <span style="color:#f92672">=</span> xyz[:, :<span style="color:#ae81ff">3</span>, :]
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>            norm <span style="color:#f92672">=</span> <span style="color:#66d9ef">None</span>
</span></span><span style="display:flex;"><span>        l1_xyz, l1_points <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>sa1(xyz, norm)
</span></span><span style="display:flex;"><span>        l2_xyz, l2_points <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>sa2(l1_xyz, l1_points)
</span></span><span style="display:flex;"><span>        l3_xyz, l3_points <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>sa3(l2_xyz, l2_points)
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> l3_points<span style="color:#f92672">.</span>view(B, <span style="color:#ae81ff">1024</span>)
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>drop1(F<span style="color:#f92672">.</span>relu(self<span style="color:#f92672">.</span>bn1(self<span style="color:#f92672">.</span>fc1(x))))
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>drop2(F<span style="color:#f92672">.</span>relu(self<span style="color:#f92672">.</span>bn2(self<span style="color:#f92672">.</span>fc2(x))))
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>fc3(x)
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>log_softmax(x, <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> x, l3_points
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">get_loss</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self):
</span></span><span style="display:flex;"><span>        super(get_loss, self)<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, pred, target, trans_feat):
</span></span><span style="display:flex;"><span>        total_loss <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>nll_loss(pred, target)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> total_loss
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">if</span> __name__ <span style="color:#f92672">==</span> <span style="color:#e6db74">&#34;__main__&#34;</span>:
</span></span><span style="display:flex;"><span>    data <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>ones([<span style="color:#ae81ff">24</span>,<span style="color:#ae81ff">3</span>,<span style="color:#ae81ff">1024</span>])
</span></span><span style="display:flex;"><span>    print(data<span style="color:#f92672">.</span>shape)
</span></span><span style="display:flex;"><span>    model <span style="color:#f92672">=</span> get_model(num_class<span style="color:#f92672">=</span><span style="color:#ae81ff">40</span>,normal_channel<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>    print(model)
</span></span><span style="display:flex;"><span>    parameters <span style="color:#f92672">=</span> filter(<span style="color:#66d9ef">lambda</span> p: p<span style="color:#f92672">.</span>requires_grad, model<span style="color:#f92672">.</span>parameters())
</span></span><span style="display:flex;"><span>    parameters <span style="color:#f92672">=</span> sum([np<span style="color:#f92672">.</span>prod(p<span style="color:#f92672">.</span>size()) <span style="color:#66d9ef">for</span> p <span style="color:#f92672">in</span> parameters]) <span style="color:#f92672">/</span> <span style="color:#ae81ff">1_000_000</span>
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">&#39;Trainable Parameters: </span><span style="color:#e6db74">%.3f</span><span style="color:#e6db74">M&#39;</span> <span style="color:#f92672">%</span> parameters)
</span></span><span style="display:flex;"><span>    pred, trans_feat  <span style="color:#f92672">=</span> model(data)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">&#34;Shape of out :&#34;</span>, pred<span style="color:#f92672">.</span>shape)  <span style="color:#75715e"># [10,30,10]</span>
</span></span></code></pre></div><pre tabindex="0"><code>get_model(
  (sa1): PointNetSetAbstraction(
    (mlp_convs): ModuleList(
      (0): Conv2d(3, 64, kernel_size=(1, 1), stride=(1, 1))
      (1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
      (2): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns): ModuleList(
      (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (sa2): PointNetSetAbstraction(
    (mlp_convs): ModuleList(
      (0): Conv2d(131, 128, kernel_size=(1, 1), stride=(1, 1))
      (1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))
      (2): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns): ModuleList(
      (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (sa3): PointNetSetAbstraction(
    (mlp_convs): ModuleList(
      (0): Conv2d(259, 256, kernel_size=(1, 1), stride=(1, 1))
      (1): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))
      (2): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns): ModuleList(
      (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (fc1): Linear(in_features=1024, out_features=512, bias=True)
  (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (drop1): Dropout(p=0.4, inplace=False)
  (fc2): Linear(in_features=512, out_features=256, bias=True)
  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (drop2): Dropout(p=0.4, inplace=False)
  (fc3): Linear(in_features=256, out_features=40, bias=True)
)
</code></pre><h5 id="4-handpointnet-代码阅读">4. HandPointNet 代码阅读</h5>
<ul>
<li><strong>数据处理部分</strong></li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-matlab" data-lang="matlab"><span style="display:flex;"><span><span style="color:#75715e">% create point cloud from depth image</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">% author: Liuhao Ge</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>clc;clear;close all;
</span></span><span style="display:flex;"><span><span style="color:#75715e">%使用 fread，文件标识符无效。使用 fopen 生成有效的文件标识符。 这个错误是文件路径不对。</span>
</span></span><span style="display:flex;"><span>dataset_dir=<span style="color:#e6db74">&#39;C:\Users\liudongdong\OneDrive - tju.edu.cn\桌面\HandPointNet\data\cvpr15_MSRAHandGestureDB\&#39;</span>;<span style="color:#75715e">%&#39;../data/cvpr15_MSRAHandGestureDB/&#39;</span>
</span></span><span style="display:flex;"><span>save_dir=<span style="color:#e6db74">&#39;./&#39;</span>;
</span></span><span style="display:flex;"><span>subject_names={<span style="color:#e6db74">&#39;P0&#39;</span>,<span style="color:#e6db74">&#39;P1&#39;</span>,<span style="color:#e6db74">&#39;P2&#39;</span>,<span style="color:#e6db74">&#39;P3&#39;</span>,<span style="color:#e6db74">&#39;P4&#39;</span>,<span style="color:#e6db74">&#39;P5&#39;</span>,<span style="color:#e6db74">&#39;P6&#39;</span>,<span style="color:#e6db74">&#39;P7&#39;</span>,<span style="color:#e6db74">&#39;P8&#39;</span>};
</span></span><span style="display:flex;"><span><span style="color:#75715e">%subject_names={&#39;P0&#39;};</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">%gesture_names={&#39;1&#39;};</span>
</span></span><span style="display:flex;"><span>gesture_names={<span style="color:#e6db74">&#39;1&#39;</span>,<span style="color:#e6db74">&#39;2&#39;</span>,<span style="color:#e6db74">&#39;3&#39;</span>,<span style="color:#e6db74">&#39;4&#39;</span>,<span style="color:#e6db74">&#39;5&#39;</span>,<span style="color:#e6db74">&#39;6&#39;</span>,<span style="color:#e6db74">&#39;7&#39;</span>,<span style="color:#e6db74">&#39;8&#39;</span>,<span style="color:#e6db74">&#39;9&#39;</span>,<span style="color:#e6db74">&#39;I&#39;</span>,<span style="color:#e6db74">&#39;IP&#39;</span>,<span style="color:#e6db74">&#39;L&#39;</span>,<span style="color:#e6db74">&#39;MP&#39;</span>,<span style="color:#e6db74">&#39;RP&#39;</span>,<span style="color:#e6db74">&#39;T&#39;</span>,<span style="color:#e6db74">&#39;TIP&#39;</span>,<span style="color:#e6db74">&#39;Y&#39;</span>};
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>JOINT_NUM = <span style="color:#ae81ff">21</span>;
</span></span><span style="display:flex;"><span>SAMPLE_NUM = <span style="color:#ae81ff">1024</span>;
</span></span><span style="display:flex;"><span>sample_num_level1 = <span style="color:#ae81ff">512</span>;
</span></span><span style="display:flex;"><span>sample_num_level2 = <span style="color:#ae81ff">128</span>;
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>load(<span style="color:#e6db74">&#39;msra_valid.mat&#39;</span>);
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> sub_idx = <span style="color:#ae81ff">1</span>:length(subject_names)
</span></span><span style="display:flex;"><span>    mkdir([save_dir subject_names{sub_idx}]);
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> ges_idx = <span style="color:#ae81ff">1</span>:length(gesture_names)
</span></span><span style="display:flex;"><span>        gesture_dir = [dataset_dir subject_names{sub_idx} <span style="color:#e6db74">&#39;/&#39;</span> gesture_names{ges_idx}];
</span></span><span style="display:flex;"><span>        depth_files = dir([gesture_dir, <span style="color:#e6db74">&#39;/*.bin&#39;</span>]);
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#75715e">% 1. read ground truth</span>
</span></span><span style="display:flex;"><span>        fileID = fopen([gesture_dir <span style="color:#e6db74">&#39;/joint.txt&#39;</span>]);
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        frame_num = fscanf(fileID,<span style="color:#e6db74">&#39;%d&#39;</span>,<span style="color:#ae81ff">1</span>);    <span style="color:#75715e">% 读取帧的个数</span>
</span></span><span style="display:flex;"><span>        A = fscanf(fileID,<span style="color:#e6db74">&#39;%f&#39;</span>, frame_num<span style="color:#f92672">*</span><span style="color:#ae81ff">21</span><span style="color:#f92672">*</span><span style="color:#ae81ff">3</span>);   <span style="color:#75715e">% 读取所有帧的关键点数据</span>
</span></span><span style="display:flex;"><span>        gt_wld=reshape(A,[<span style="color:#ae81ff">3</span>,<span style="color:#ae81ff">21</span>,frame_num]);     <span style="color:#75715e">% 数据reshape操作</span>
</span></span><span style="display:flex;"><span>        gt_wld(<span style="color:#ae81ff">3</span>,:,:) = <span style="color:#f92672">-</span>gt_wld(<span style="color:#ae81ff">3</span>,:,:);
</span></span><span style="display:flex;"><span>        gt_wld=permute(gt_wld, [<span style="color:#ae81ff">3</span> <span style="color:#ae81ff">2</span> <span style="color:#ae81ff">1</span>]);
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        fclose(fileID);
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#75715e">% 2. get point cloud and surface normal</span>
</span></span><span style="display:flex;"><span>        save_gesture_dir = [save_dir subject_names{sub_idx} <span style="color:#e6db74">&#39;/&#39;</span> gesture_names{ges_idx}];  <span style="color:#75715e">%matlab 文件拼接</span>
</span></span><span style="display:flex;"><span>        mkdir(save_gesture_dir);    <span style="color:#75715e">%创建存储的路径文件</span>
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        display(save_gesture_dir);    <span style="color:#75715e">%显示变量的信息</span>
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        Point_Cloud_FPS = zeros(frame_num,SAMPLE_NUM,<span style="color:#ae81ff">6</span>);
</span></span><span style="display:flex;"><span>        Volume_rotate = zeros(frame_num,<span style="color:#ae81ff">3</span>,<span style="color:#ae81ff">3</span>);
</span></span><span style="display:flex;"><span>        Volume_length = zeros(frame_num,<span style="color:#ae81ff">1</span>);
</span></span><span style="display:flex;"><span>        Volume_offset = zeros(frame_num,<span style="color:#ae81ff">3</span>);
</span></span><span style="display:flex;"><span>        Volume_GT_XYZ = zeros(frame_num,JOINT_NUM,<span style="color:#ae81ff">3</span>);
</span></span><span style="display:flex;"><span>        valid = msra_valid{sub_idx, ges_idx};
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> frm_idx = <span style="color:#ae81ff">1</span>:length(depth_files)
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">if</span> <span style="color:#f92672">~</span>valid(frm_idx)                 <span style="color:#75715e">%valid 数组主要用于判断这个数据帧是不是有效的</span>
</span></span><span style="display:flex;"><span>                <span style="color:#66d9ef">continue</span>;
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">end</span>
</span></span><span style="display:flex;"><span>            <span style="color:#75715e">%% 2.1 read binary file</span>
</span></span><span style="display:flex;"><span>            fileID = fopen([gesture_dir <span style="color:#e6db74">&#39;/&#39;</span> num2str(frm_idx<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,<span style="color:#e6db74">&#39;%06d&#39;</span>), <span style="color:#e6db74">&#39;_depth.bin&#39;</span>]);   <span style="color:#75715e">%num2str(id,&#39;%06d&#39;)  文件数据格式</span>
</span></span><span style="display:flex;"><span>            img_width = fread(fileID,<span style="color:#ae81ff">1</span>,<span style="color:#e6db74">&#39;int32&#39;</span>);
</span></span><span style="display:flex;"><span>            img_height = fread(fileID,<span style="color:#ae81ff">1</span>,<span style="color:#e6db74">&#39;int32&#39;</span>);
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            bb_left = fread(fileID,<span style="color:#ae81ff">1</span>,<span style="color:#e6db74">&#39;int32&#39;</span>);
</span></span><span style="display:flex;"><span>            bb_top = fread(fileID,<span style="color:#ae81ff">1</span>,<span style="color:#e6db74">&#39;int32&#39;</span>);
</span></span><span style="display:flex;"><span>            bb_right = fread(fileID,<span style="color:#ae81ff">1</span>,<span style="color:#e6db74">&#39;int32&#39;</span>);
</span></span><span style="display:flex;"><span>            bb_bottom = fread(fileID,<span style="color:#ae81ff">1</span>,<span style="color:#e6db74">&#39;int32&#39;</span>);
</span></span><span style="display:flex;"><span>            bb_width = bb_right <span style="color:#f92672">-</span> bb_left;
</span></span><span style="display:flex;"><span>            bb_height = bb_bottom <span style="color:#f92672">-</span> bb_top;
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            valid_pixel_num = bb_width<span style="color:#f92672">*</span>bb_height;
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            hand_depth = fread(fileID,[bb_width, bb_height],<span style="color:#e6db74">&#39;float32&#39;</span>);     <span style="color:#75715e">%读取手部区域有效的深度信息</span>
</span></span><span style="display:flex;"><span>            hand_depth = hand_depth<span style="color:#f92672">&#39;</span>;
</span></span><span style="display:flex;"><span>            
</span></span><span style="display:flex;"><span>            fclose(fileID);
</span></span><span style="display:flex;"><span>            
</span></span><span style="display:flex;"><span>            <span style="color:#75715e">%% 2.2 convert depth to xyz</span>
</span></span><span style="display:flex;"><span>            fFocal_MSRA_ = <span style="color:#ae81ff">241.42</span>;	<span style="color:#75715e">% mm</span>
</span></span><span style="display:flex;"><span>            hand_3d = zeros(valid_pixel_num,<span style="color:#ae81ff">3</span>);
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">for</span> ii=<span style="color:#ae81ff">1</span>:bb_height
</span></span><span style="display:flex;"><span>                <span style="color:#66d9ef">for</span> jj=<span style="color:#ae81ff">1</span>:bb_width
</span></span><span style="display:flex;"><span>                    idx = (jj<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>)<span style="color:#f92672">*</span>bb_height<span style="color:#f92672">+</span>ii;      <span style="color:#75715e">% 手部区域深度图中每一个像素索引，按列优先</span>
</span></span><span style="display:flex;"><span>                    hand_3d(idx, <span style="color:#ae81ff">1</span>) = <span style="color:#f92672">-</span>(img_width<span style="color:#f92672">/</span><span style="color:#ae81ff">2</span> <span style="color:#f92672">-</span> (jj<span style="color:#f92672">+</span>bb_left<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>))<span style="color:#f92672">*</span>hand_depth(ii,jj)<span style="color:#f92672">/</span>fFocal_MSRA_;
</span></span><span style="display:flex;"><span>                    hand_3d(idx, <span style="color:#ae81ff">2</span>) = (img_height<span style="color:#f92672">/</span><span style="color:#ae81ff">2</span> <span style="color:#f92672">-</span> (ii<span style="color:#f92672">+</span>bb_top<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>))<span style="color:#f92672">*</span>hand_depth(ii,jj)<span style="color:#f92672">/</span>fFocal_MSRA_;
</span></span><span style="display:flex;"><span>                    hand_3d(idx, <span style="color:#ae81ff">3</span>) = hand_depth(ii,jj);     <span style="color:#75715e">% 深度距离值，   这个真实的z应该  是x*x+y*y+z*z=d*d  ??</span>
</span></span><span style="display:flex;"><span>                <span style="color:#66d9ef">end</span>
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">end</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            valid_idx = <span style="color:#ae81ff">1</span>:valid_pixel_num;
</span></span><span style="display:flex;"><span>            valid_idx = valid_idx(hand_3d(:,<span style="color:#ae81ff">1</span>)<span style="color:#f92672">~=</span><span style="color:#ae81ff">0</span> <span style="color:#f92672">|</span> hand_3d(:,<span style="color:#ae81ff">2</span>)<span style="color:#f92672">~=</span><span style="color:#ae81ff">0</span> <span style="color:#f92672">|</span> hand_3d(:,<span style="color:#ae81ff">3</span>)<span style="color:#f92672">~=</span><span style="color:#ae81ff">0</span>);
</span></span><span style="display:flex;"><span>            hand_points = hand_3d(valid_idx,:);             <span style="color:#75715e">%过滤无效的数据</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            jnt_xyz = squeeze(gt_wld(frm_idx,:,:));
</span></span><span style="display:flex;"><span>            
</span></span><span style="display:flex;"><span>            <span style="color:#75715e">%% 2.3 create OBB</span>
</span></span><span style="display:flex;"><span>            [coeff,score,latent] = pca(hand_points);   <span style="color:#75715e">%coeff = pca(X) 返回 n×p 数据矩阵 X 的主成分系数，也称为载荷。X 的行对应于观测值，列对应于变量。</span>
</span></span><span style="display:flex;"><span>                                              <span style="color:#75715e">%系数矩阵是 p×p 矩阵。coeff 的每列包含一个主成分的系数，并且这些列按成分方差的降序排列。默认情况下，pca 将数据中心化，并使用奇异值分解 (SVD) 算法。</span>
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">if</span> coeff(<span style="color:#ae81ff">2</span>,<span style="color:#ae81ff">1</span>)<span style="color:#f92672">&lt;</span><span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>                coeff(:,<span style="color:#ae81ff">1</span>) = <span style="color:#f92672">-</span>coeff(:,<span style="color:#ae81ff">1</span>);
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">end</span>
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">if</span> coeff(<span style="color:#ae81ff">3</span>,<span style="color:#ae81ff">3</span>)<span style="color:#f92672">&lt;</span><span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>                coeff(:,<span style="color:#ae81ff">3</span>) = <span style="color:#f92672">-</span>coeff(:,<span style="color:#ae81ff">3</span>);
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">end</span>
</span></span><span style="display:flex;"><span>            coeff(:,<span style="color:#ae81ff">2</span>)=cross(coeff(:,<span style="color:#ae81ff">3</span>),coeff(:,<span style="color:#ae81ff">1</span>));   <span style="color:#75715e">% 这里几步不太明白作用？</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            ptCloud = pointCloud(hand_points);
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            hand_points_rotate = hand_points<span style="color:#f92672">*</span>coeff;    <span style="color:#75715e">%类似归一化处理，是的bounding box 的朝向基本一致</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            <span style="color:#75715e">%% 2.4 sampling                        %数据少的时候只是在原有的点基础上重复使用了一些点，  这里不知道可不可以直接使用</span>
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">if</span> size(hand_points,<span style="color:#ae81ff">1</span>)<span style="color:#f92672">&lt;</span>SAMPLE_NUM
</span></span><span style="display:flex;"><span>                tmp = floor(SAMPLE_NUM<span style="color:#f92672">/</span>size(hand_points,<span style="color:#ae81ff">1</span>));
</span></span><span style="display:flex;"><span>                rand_ind = [];
</span></span><span style="display:flex;"><span>                <span style="color:#66d9ef">for</span> tmp_i = <span style="color:#ae81ff">1</span>:tmp
</span></span><span style="display:flex;"><span>                    rand_ind = [rand_ind <span style="color:#ae81ff">1</span>:size(hand_points,<span style="color:#ae81ff">1</span>)];
</span></span><span style="display:flex;"><span>                <span style="color:#66d9ef">end</span>
</span></span><span style="display:flex;"><span>                rand_ind = [rand_ind randperm(size(hand_points,<span style="color:#ae81ff">1</span>), mod(SAMPLE_NUM, size(hand_points,<span style="color:#ae81ff">1</span>)))];  <span style="color:#75715e">%返回行向量，其中包含在 1 到 size(hand_points,1) 之间随机选择的 k 个唯一整数。  </span>
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">else</span>
</span></span><span style="display:flex;"><span>                rand_ind = randperm(size(hand_points,<span style="color:#ae81ff">1</span>),SAMPLE_NUM);
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">end</span>
</span></span><span style="display:flex;"><span>            hand_points_sampled = hand_points(rand_ind,:);
</span></span><span style="display:flex;"><span>            hand_points_rotate_sampled = hand_points_rotate(rand_ind,:);
</span></span><span style="display:flex;"><span>            
</span></span><span style="display:flex;"><span>            <span style="color:#75715e">%% 2.5 compute surface normal</span>
</span></span><span style="display:flex;"><span>            normal_k = <span style="color:#ae81ff">30</span>;
</span></span><span style="display:flex;"><span>            normals = pcnormals(ptCloud, normal_k);
</span></span><span style="display:flex;"><span>            normals_sampled = normals(rand_ind,:);
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            sensorCenter = [<span style="color:#ae81ff">0</span> <span style="color:#ae81ff">0</span> <span style="color:#ae81ff">0</span>];
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">for</span> k = <span style="color:#ae81ff">1</span> : SAMPLE_NUM
</span></span><span style="display:flex;"><span>               p1 = sensorCenter <span style="color:#f92672">-</span> hand_points_sampled(k,:);
</span></span><span style="display:flex;"><span>               <span style="color:#75715e">% Flip the normal vector if it is not pointing towards the sensor.</span>
</span></span><span style="display:flex;"><span>               angle = atan2(norm(cross(p1,normals_sampled(k,:))),p1<span style="color:#f92672">*</span>normals_sampled(k,:)<span style="color:#f92672">&#39;</span>);
</span></span><span style="display:flex;"><span>               <span style="color:#66d9ef">if</span> angle <span style="color:#f92672">&gt;</span> pi<span style="color:#f92672">/</span><span style="color:#ae81ff">2</span> <span style="color:#f92672">||</span> angle <span style="color:#f92672">&lt;</span> <span style="color:#f92672">-</span>pi<span style="color:#f92672">/</span><span style="color:#ae81ff">2</span>
</span></span><span style="display:flex;"><span>                   normals_sampled(k,:) = <span style="color:#f92672">-</span>normals_sampled(k,:);
</span></span><span style="display:flex;"><span>               <span style="color:#66d9ef">end</span>
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">end</span>
</span></span><span style="display:flex;"><span>            normals_sampled_rotate = normals_sampled<span style="color:#f92672">*</span>coeff;
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            <span style="color:#75715e">%% 2.6 Normalize Point Cloud    %通过每一轴的最值*scale进行 缩放处理</span>
</span></span><span style="display:flex;"><span>            x_min_max = [min(hand_points_rotate(:,<span style="color:#ae81ff">1</span>)), max(hand_points_rotate(:,<span style="color:#ae81ff">1</span>))];
</span></span><span style="display:flex;"><span>            y_min_max = [min(hand_points_rotate(:,<span style="color:#ae81ff">2</span>)), max(hand_points_rotate(:,<span style="color:#ae81ff">2</span>))];
</span></span><span style="display:flex;"><span>            z_min_max = [min(hand_points_rotate(:,<span style="color:#ae81ff">3</span>)), max(hand_points_rotate(:,<span style="color:#ae81ff">3</span>))];
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            scale = <span style="color:#ae81ff">1.2</span>;
</span></span><span style="display:flex;"><span>            bb3d_x_len = scale<span style="color:#f92672">*</span>(x_min_max(<span style="color:#ae81ff">2</span>)<span style="color:#f92672">-</span>x_min_max(<span style="color:#ae81ff">1</span>));
</span></span><span style="display:flex;"><span>            bb3d_y_len = scale<span style="color:#f92672">*</span>(y_min_max(<span style="color:#ae81ff">2</span>)<span style="color:#f92672">-</span>y_min_max(<span style="color:#ae81ff">1</span>));
</span></span><span style="display:flex;"><span>            bb3d_z_len = scale<span style="color:#f92672">*</span>(z_min_max(<span style="color:#ae81ff">2</span>)<span style="color:#f92672">-</span>z_min_max(<span style="color:#ae81ff">1</span>));
</span></span><span style="display:flex;"><span>            max_bb3d_len = bb3d_x_len;
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            hand_points_normalized_sampled = hand_points_rotate_sampled<span style="color:#f92672">/</span>max_bb3d_len;
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">if</span> size(hand_points,<span style="color:#ae81ff">1</span>)<span style="color:#f92672">&lt;</span>SAMPLE_NUM
</span></span><span style="display:flex;"><span>                offset = mean(hand_points_rotate)<span style="color:#f92672">/</span>max_bb3d_len;
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">else</span>
</span></span><span style="display:flex;"><span>                offset = mean(hand_points_normalized_sampled);
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">end</span>
</span></span><span style="display:flex;"><span>            hand_points_normalized_sampled = hand_points_normalized_sampled <span style="color:#f92672">-</span> repmat(offset,SAMPLE_NUM,<span style="color:#ae81ff">1</span>);
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            <span style="color:#75715e">%% 2.7 FPS Sampling</span>
</span></span><span style="display:flex;"><span>            pc = [hand_points_normalized_sampled normals_sampled_rotate];
</span></span><span style="display:flex;"><span>            <span style="color:#75715e">% 1st level</span>
</span></span><span style="display:flex;"><span>            sampled_idx_l1 = farthest_point_sampling_fast(hand_points_normalized_sampled, sample_num_level1)<span style="color:#f92672">&#39;</span>;
</span></span><span style="display:flex;"><span>            other_idx = setdiff(<span style="color:#ae81ff">1</span>:SAMPLE_NUM, sampled_idx_l1);
</span></span><span style="display:flex;"><span>            new_idx = [sampled_idx_l1 other_idx];
</span></span><span style="display:flex;"><span>            pc = pc(new_idx,:);
</span></span><span style="display:flex;"><span>            <span style="color:#75715e">% 2nd level</span>
</span></span><span style="display:flex;"><span>            sampled_idx_l2 = farthest_point_sampling_fast(pc(<span style="color:#ae81ff">1</span>:sample_num_level1,<span style="color:#ae81ff">1</span>:<span style="color:#ae81ff">3</span>), sample_num_level2)<span style="color:#f92672">&#39;</span>;
</span></span><span style="display:flex;"><span>            other_idx = setdiff(<span style="color:#ae81ff">1</span>:sample_num_level1, sampled_idx_l2);
</span></span><span style="display:flex;"><span>            new_idx = [sampled_idx_l2 other_idx];
</span></span><span style="display:flex;"><span>            pc(<span style="color:#ae81ff">1</span>:sample_num_level1,:) = pc(new_idx,:);
</span></span><span style="display:flex;"><span>            
</span></span><span style="display:flex;"><span>            <span style="color:#75715e">%% 2.8 ground truth</span>
</span></span><span style="display:flex;"><span>            jnt_xyz_normalized = (jnt_xyz<span style="color:#f92672">*</span>coeff)<span style="color:#f92672">/</span>max_bb3d_len;
</span></span><span style="display:flex;"><span>            jnt_xyz_normalized = jnt_xyz_normalized <span style="color:#f92672">-</span> repmat(offset,JOINT_NUM,<span style="color:#ae81ff">1</span>);
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            Point_Cloud_FPS(frm_idx,:,:) = pc;
</span></span><span style="display:flex;"><span>            Volume_rotate(frm_idx,:,:) = coeff;
</span></span><span style="display:flex;"><span>            Volume_length(frm_idx) = max_bb3d_len;
</span></span><span style="display:flex;"><span>            Volume_offset(frm_idx,:) = offset;
</span></span><span style="display:flex;"><span>            Volume_GT_XYZ(frm_idx,:,:) = jnt_xyz_normalized;
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">end</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e">% 3. save files</span>
</span></span><span style="display:flex;"><span>        save([save_gesture_dir <span style="color:#e6db74">&#39;/Point_Cloud_FPS.mat&#39;</span>],<span style="color:#e6db74">&#39;Point_Cloud_FPS&#39;</span>);
</span></span><span style="display:flex;"><span>        save([save_gesture_dir <span style="color:#e6db74">&#39;/Volume_rotate.mat&#39;</span>],<span style="color:#e6db74">&#39;Volume_rotate&#39;</span>);
</span></span><span style="display:flex;"><span>        save([save_gesture_dir <span style="color:#e6db74">&#39;/Volume_length.mat&#39;</span>],<span style="color:#e6db74">&#39;Volume_length&#39;</span>);
</span></span><span style="display:flex;"><span>        save([save_gesture_dir <span style="color:#e6db74">&#39;/Volume_offset.mat&#39;</span>],<span style="color:#e6db74">&#39;Volume_offset&#39;</span>);
</span></span><span style="display:flex;"><span>        save([save_gesture_dir <span style="color:#e6db74">&#39;/Volume_GT_XYZ.mat&#39;</span>],<span style="color:#e6db74">&#39;Volume_GT_XYZ&#39;</span>);
</span></span><span style="display:flex;"><span>        save([save_gesture_dir <span style="color:#e6db74">&#39;/valid.mat&#39;</span>],<span style="color:#e6db74">&#39;valid&#39;</span>);
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">end</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">end</span>
</span></span></code></pre></div><ul>
<li><strong>网络代码部分</strong></li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>nstates_plus_1 <span style="color:#f92672">=</span> [<span style="color:#ae81ff">64</span>,<span style="color:#ae81ff">64</span>,<span style="color:#ae81ff">128</span>]
</span></span><span style="display:flex;"><span>nstates_plus_2 <span style="color:#f92672">=</span> [<span style="color:#ae81ff">128</span>,<span style="color:#ae81ff">128</span>,<span style="color:#ae81ff">256</span>]
</span></span><span style="display:flex;"><span>nstates_plus_3 <span style="color:#f92672">=</span> [<span style="color:#ae81ff">256</span>,<span style="color:#ae81ff">512</span>,<span style="color:#ae81ff">1024</span>,<span style="color:#ae81ff">1024</span>,<span style="color:#ae81ff">512</span>]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">PointNet_Plus</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, opt):
</span></span><span style="display:flex;"><span>        super(PointNet_Plus, self)<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>num_outputs <span style="color:#f92672">=</span> opt<span style="color:#f92672">.</span>PCA_SZ
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>knn_K <span style="color:#f92672">=</span> opt<span style="color:#f92672">.</span>knn_K
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>ball_radius2 <span style="color:#f92672">=</span> opt<span style="color:#f92672">.</span>ball_radius2
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>sample_num_level1 <span style="color:#f92672">=</span> opt<span style="color:#f92672">.</span>sample_num_level1
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>sample_num_level2 <span style="color:#f92672">=</span> opt<span style="color:#f92672">.</span>sample_num_level2
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>INPUT_FEATURE_NUM <span style="color:#f92672">=</span> opt<span style="color:#f92672">.</span>INPUT_FEATURE_NUM
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>netR_1 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Sequential(
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># B*INPUT_FEATURE_NUM*sample_num_level1*knn_K</span>
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Conv2d(self<span style="color:#f92672">.</span>INPUT_FEATURE_NUM, nstates_plus_1[<span style="color:#ae81ff">0</span>], kernel_size<span style="color:#f92672">=</span>(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>)),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>BatchNorm2d(nstates_plus_1[<span style="color:#ae81ff">0</span>]),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>ReLU(inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>),
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># B*64*sample_num_level1*knn_K</span>
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Conv2d(nstates_plus_1[<span style="color:#ae81ff">0</span>], nstates_plus_1[<span style="color:#ae81ff">1</span>], kernel_size<span style="color:#f92672">=</span>(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>)),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>BatchNorm2d(nstates_plus_1[<span style="color:#ae81ff">1</span>]),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>ReLU(inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>),
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># B*64*sample_num_level1*knn_K</span>
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Conv2d(nstates_plus_1[<span style="color:#ae81ff">1</span>], nstates_plus_1[<span style="color:#ae81ff">2</span>], kernel_size<span style="color:#f92672">=</span>(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>)),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>BatchNorm2d(nstates_plus_1[<span style="color:#ae81ff">2</span>]),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>ReLU(inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>),
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># B*128*sample_num_level1*knn_K</span>
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>MaxPool2d((<span style="color:#ae81ff">1</span>,self<span style="color:#f92672">.</span>knn_K),stride<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># B*128*sample_num_level1*1</span>
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>netR_2 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Sequential(
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># B*131*sample_num_level2*knn_K</span>
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Conv2d(<span style="color:#ae81ff">3</span><span style="color:#f92672">+</span>nstates_plus_1[<span style="color:#ae81ff">2</span>], nstates_plus_2[<span style="color:#ae81ff">0</span>], kernel_size<span style="color:#f92672">=</span>(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>)),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>BatchNorm2d(nstates_plus_2[<span style="color:#ae81ff">0</span>]),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>ReLU(inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>),
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># B*128*sample_num_level2*knn_K</span>
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Conv2d(nstates_plus_2[<span style="color:#ae81ff">0</span>], nstates_plus_2[<span style="color:#ae81ff">1</span>], kernel_size<span style="color:#f92672">=</span>(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>)),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>BatchNorm2d(nstates_plus_2[<span style="color:#ae81ff">1</span>]),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>ReLU(inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>),
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># B*128*sample_num_level2*knn_K</span>
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Conv2d(nstates_plus_2[<span style="color:#ae81ff">1</span>], nstates_plus_2[<span style="color:#ae81ff">2</span>], kernel_size<span style="color:#f92672">=</span>(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>)),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>BatchNorm2d(nstates_plus_2[<span style="color:#ae81ff">2</span>]),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>ReLU(inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>),
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># B*256*sample_num_level2*knn_K</span>
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>MaxPool2d((<span style="color:#ae81ff">1</span>,self<span style="color:#f92672">.</span>knn_K),stride<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># B*256*sample_num_level2*1</span>
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>netR_3 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Sequential(
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># B*259*sample_num_level2*1</span>
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Conv2d(<span style="color:#ae81ff">3</span><span style="color:#f92672">+</span>nstates_plus_2[<span style="color:#ae81ff">2</span>], nstates_plus_3[<span style="color:#ae81ff">0</span>], kernel_size<span style="color:#f92672">=</span>(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>)),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>BatchNorm2d(nstates_plus_3[<span style="color:#ae81ff">0</span>]),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>ReLU(inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>),
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># B*256*sample_num_level2*1</span>
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Conv2d(nstates_plus_3[<span style="color:#ae81ff">0</span>], nstates_plus_3[<span style="color:#ae81ff">1</span>], kernel_size<span style="color:#f92672">=</span>(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>)),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>BatchNorm2d(nstates_plus_3[<span style="color:#ae81ff">1</span>]),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>ReLU(inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>),
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># B*512*sample_num_level2*1</span>
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Conv2d(nstates_plus_3[<span style="color:#ae81ff">1</span>], nstates_plus_3[<span style="color:#ae81ff">2</span>], kernel_size<span style="color:#f92672">=</span>(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>)),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>BatchNorm2d(nstates_plus_3[<span style="color:#ae81ff">2</span>]),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>ReLU(inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>),
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># B*1024*sample_num_level2*1</span>
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>MaxPool2d((self<span style="color:#f92672">.</span>sample_num_level2,<span style="color:#ae81ff">1</span>),stride<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>),
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># B*1024*1*1</span>
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>netR_FC <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Sequential(
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># B*1024</span>
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Linear(nstates_plus_3[<span style="color:#ae81ff">2</span>], nstates_plus_3[<span style="color:#ae81ff">3</span>]),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>BatchNorm1d(nstates_plus_3[<span style="color:#ae81ff">3</span>]),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>ReLU(inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>),
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># B*1024</span>
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Linear(nstates_plus_3[<span style="color:#ae81ff">3</span>], nstates_plus_3[<span style="color:#ae81ff">4</span>]),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>BatchNorm1d(nstates_plus_3[<span style="color:#ae81ff">4</span>]),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>ReLU(inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>),
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># B*512</span>
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Linear(nstates_plus_3[<span style="color:#ae81ff">4</span>], self<span style="color:#f92672">.</span>num_outputs),
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># B*num_outputs</span>
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x, y):
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># x: B*INPUT_FEATURE_NUM*sample_num_level1*knn_K, y: B*3*sample_num_level1*1</span>
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>netR_1(x)
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># B*128*sample_num_level1*1</span>
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>cat((y, x),<span style="color:#ae81ff">1</span>)<span style="color:#f92672">.</span>squeeze(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># B*(3+128)*sample_num_level1</span>
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        inputs_level2, inputs_level2_center <span style="color:#f92672">=</span> group_points_2(x, self<span style="color:#f92672">.</span>sample_num_level1, self<span style="color:#f92672">.</span>sample_num_level2, self<span style="color:#f92672">.</span>knn_K, self<span style="color:#f92672">.</span>ball_radius2)
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># B*131*sample_num_level2*knn_K, B*3*sample_num_level2*1</span>
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># B*131*sample_num_level2*knn_K</span>
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>netR_2(inputs_level2)
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># B*256*sample_num_level2*1</span>
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>cat((inputs_level2_center, x),<span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># B*259*sample_num_level2*1</span>
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>netR_3(x)
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># B*1024*1*1</span>
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>view(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,nstates_plus_3[<span style="color:#ae81ff">2</span>])
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># B*1024</span>
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>netR_FC(x)
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># B*num_outputs</span>
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> x
</span></span></code></pre></div><ul>
<li>学习代码：  <a href="https://github.com/erikwijmans/Pointnet2_PyTorch.git"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/erikwijmans/Pointnet2_PyTorch.git<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></li>
</ul>
<p><strong>level</strong>: CVPR, CCF_A
<strong>author</strong>:Pavlo Molchanov, Xiaodong Yang (NVIDIA)
<strong>date</strong>: 2016
<strong>keyword</strong>:</p>
<ul>
<li>Hand Gesture,</li>
</ul>
<hr>
<h2 id="paper-r3dcnn-dynamic-hand">Paper: R3DCNN Dynamic Hand</h2>
<!-- raw HTML omitted -->
<h4 id="proble-statement-6">Proble Statement</h4>
<ul>
<li>Large diversity in how people perform gestures.</li>
<li>Work online to classify before competing a gesture.</li>
<li>Three overlapping phases: preparation, nucleus, and retraction.</li>
</ul>
<p>previous work:</p>
<ul>
<li>Hand-crafted spatio-temporal features.
<ul>
<li>Shape, appearance, motion cues( image gradients, optical flow).</li>
</ul>
</li>
<li>Feature representations by DNN.
<ul>
<li>uNeverova et al. combine color and depth data from hand regions and upper-body skeletons to recognize SL.</li>
</ul>
</li>
<li>Employ pre-segmented video sequences.</li>
<li>Treate detect and classify separately</li>
</ul>
<h4 id="methods-5">Methods</h4>
<ul>
<li><strong>Problem Formulation</strong>:</li>
</ul>
<p>Input: a video clip as volume $C_t$: $C_t\epsilon R^{k<em>l</em>c<em>m}$;  $m$: sequential frames; $C$: channels of size $k</em>l$ pixels.</p>
<p>$h_t\epsilon R_d$:  a hidden state vector;</p>
<p>$W_{in}\epsilon R^{d<em>q}$, $W_h\epsilon R^{d</em>d}$,$W_s\epsilon R^{w*d}$: weight matrices;</p>
<p>$b\epsilon R^w$: bias;</p>
<p>$S$: softmax functions, $R^w-&gt;R^w_{[0,1]},where [S(x)]<em>i=e^{x_i}/ \sum_ke^{xk}$
$$
F: R^{k<em>l</em>c*m}-&gt;R_q,where f_t=F(C_t)\
h_t=R(W</em>{in}f_t+W_hh_{t-1});\
s_t=S(W_sh_t+b);\
$$
For a video $V$ of $T$ clips, get the probabilities set $S$:
$$
S={s_0,s_1,&hellip;,s_{T-1}}\
S^{avg}=1/T\sum_{s\epsilon S}s\
predicted_label:y=argmax_i([s^{avg}]_i)
$$</p>
<ul>
<li><strong>system overview</strong>:</li>
</ul>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200711170646863.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200711170646863.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200711170646863.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200711170646863.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200711170646863.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200711170646863.png"/></p>
<p>【Pre-training the 3D-CNN】</p>
<ul>
<li>initialize the 3D-CNN with the C3D network [37] trained on the large-scale Sport1M [13] human action recognition dataset.</li>
<li>append a softmax prediction layer to the last fully-connected layer and ﬁne-tune by back-propagation with negative log-likelihood to predict gestures classes from individual clips $C_i$.</li>
</ul>
<p>【Cost Function】</p>
<ul>
<li>For Log-likelihood cost function:</li>
</ul>
<p>$$
L_v=-1/P \sum_{i=0}^{P-1}log(p(y_i|V_i))\
p(y_i|V_i)=[s^{avg}]_{y_i}
$$</p>
<p>【Learning Rule】</p>
<ul>
<li>To optimize the network parameters $W$ with respect to either of the loss functions we use stochastic gradient descent (SGD) with a momentum term$µ = 0.9$. We update each parameter of the network θ ∈ W at every back-propagation step i by:</li>
</ul>
<p>$$
\theta_i=\theta_{i-1}+v_i-yj\theta_{i-1}\
v_i=uv_{i-1}-jJ(&lt;\sigma E/\sigma \theta&gt;_{batch})
$$</p>
<h4 id="evaluation-4">Evaluation</h4>
<ul>
<li><strong>Environment</strong>:
<ul>
<li>Dataset: used the SoftKinetic DS325 sensor to acquire frontview color and depth videos and a top-mounted DUO 3D sensor to record a pair of stereo-IR streams.</li>
<li>randomly split the data by subject into training (70%) and test (30%) sets, resulting in 1050 training and 482 test videos.</li>
<li><strong>SKIG</strong> contains 1080 RGBD hand gesture sequences by 6 subjects collected with a Kinect sensor</li>
<li><strong>ChaLearn 2014 dataset</strong> contains more than 13K RGBD videos of 20 upper-body Italian sign language gestures performed by 20 subjects</li>
</ul>
</li>
<li><strong>Results</strong>:</li>
</ul>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200711173946207.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200711173946207.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200711173946207.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200711173946207.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200711173946207.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200711173946207.png"/></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200711174007019.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200711174007019.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200711174007019.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200711174007019.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200711174007019.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200711174007019.png"/></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200711174023573.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200711174023573.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200711174023573.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200711174023573.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200711174023573.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200711174023573.png"/></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200711174040803.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200711174040803.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200711174040803.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200711174040803.png 2x"
    data-sizes="auto"
    alt="predictions with various modalities"
    title="predictions with various modalities"/></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200711174056380.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200711174056380.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200711174056380.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200711174056380.png 2x"
    data-sizes="auto"
    alt="Comparison of 2D-CNN and 3D-CNN trained with different architectures"
    title="Comparison of 2D-CNN and 3D-CNN trained with different architectures"/></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200711174220754.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200711174220754.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200711174220754.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200711174220754.png 2x"
    data-sizes="auto"
    alt="Gesture Detection"
    title="Gesture Detection"/></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200711174344337.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200711174344337.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200711174344337.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200711174344337.png 2x"
    data-sizes="auto"
    alt="SKIG RGBD gesture dataset"
    title="SKIG RGBD gesture dataset"/></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200711174326620.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200711174326620.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200711174326620.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200711174326620.png 2x"
    data-sizes="auto"
    alt="Chalearn 2014 dataset"
    title="Chalearn 2014 dataset"/></p>
<h4 id="conclusion-5">Conclusion</h4>
<ul>
<li>Design R3DCNN to performs simultaneous detection and classification.</li>
<li>Using CTC model to predict label from in-progress gesture in unsegmented input streams.</li>
<li>Achieves high accuracy of 88.4%.</li>
</ul>
</div>
<div class="post-footer" id="post-footer">
  <div class="post-info">
    <div class="post-info-line">
      <div class="post-info-mod">
        <span title=2023-09-28&#32;23:21:36>更新于 2023-09-28&nbsp;</span>
      </div><div class="post-info-license">
          <span><a rel="license external nofollow noopener noreferrer" href="https://creativecommons.org/licenses/by-nc/4.0/" target="_blank">CC BY-NC 4.0</a></span>
        </div></div>
    <div class="post-info-line">
      <div class="post-info-md"><span><a href="/hand-analyse-record/index.md" title="阅读原始文档" class="link-to-markdown">阅读原始文档</a></span><span><a href="https://liudongdong1.github.io/edit/master/content/posts%5c%e8%a7%86%e8%a7%89%e8%bf%90%e5%8a%a8%5cDataGlove%5cHand-Analyse-Record.md" title="编辑此页"target="_blank" rel="external nofollow noopener noreferrer" class="link-to-edit">编辑此页</a></span></div>
      <div class="post-info-share">
        <span><a href="javascript:void(0);" title="分享到 Twitter" data-sharer="twitter" data-url="https://liudongdong1.github.io/hand-analyse-record/" data-title="Hand Analyse Record" data-hashtags="CV"><i class="fa-brands fa-twitter fa-fw" aria-hidden="true"></i></a>
  <a href="javascript:void(0);" title="分享到 Facebook" data-sharer="facebook" data-url="https://liudongdong1.github.io/hand-analyse-record/" data-hashtag="CV"><i class="fa-brands fa-facebook-square fa-fw" aria-hidden="true"></i></a>
  <a href="javascript:void(0);" title="分享到 微博" data-sharer="weibo" data-url="https://liudongdong1.github.io/hand-analyse-record/" data-title="Hand Analyse Record" data-image="https://gitee.com/github-25970295/blogImage/raw/master/img/dataglove.jpg"><i class="fa-brands fa-weibo fa-fw" aria-hidden="true"></i></a>
  </span>
      </div>
    </div>
  </div>

  <div class="post-info-more">
    <section class="post-tags"><i class="fa-solid fa-tags fa-fw" aria-hidden="true"></i>&nbsp;<a href="/tags/cv/">CV</a></section>
    <section>
      <span><a href="javascript:void(0);" onclick="window.history.back();">返回</a></span>&nbsp;|&nbsp;<span><a href="/">主页</a></span>
    </section>
  </div>

  <div class="post-nav"><a href="/rfid-actionrecognition/" class="prev" rel="prev" title="RFID ActionRecognition"><i class="fa-solid fa-angle-left fa-fw" aria-hidden="true"></i>RFID ActionRecognition</a>
      <a href="/bluepaperrecord/" class="next" rel="next" title="BlueTooth Paper">BlueTooth Paper<i class="fa-solid fa-angle-right fa-fw" aria-hidden="true"></i></a></div>
</div>
</article></main><footer class="footer">
    <div class="footer-container"><div class="footer-line powered">由 <a href="https://gohugo.io/" target="_blank" rel="external nofollow noopener noreferrer" title="Hugo 0.118.2">Hugo</a> 强力驱动 | 主题 - <a href="https://github.com/hugo-fixit/FixIt" target="_blank" rel="external" title="FixIt v0.2.17-RC"><img class="fixit-icon" src="/fixit.min.svg" alt="FixIt logo" />&nbsp;FixIt</a>
        </div><div class="footer-line copyright" itemscope itemtype="http://schema.org/CreativeWork"><i class="fa-regular fa-copyright fa-fw" aria-hidden="true"></i>
            <span itemprop="copyrightYear">2020 - 2023</span><span class="author" itemprop="copyrightHolder">
              <a href="https://liudongdong1.github.io/"target="_blank" rel="external nofollow noopener noreferrer">LiuDongdong</a></span><span class="license footer-divider"><a rel="license external nofollow noopener noreferrer" href="https://creativecommons.org/licenses/by-nc/4.0/" target="_blank">CC BY-NC 4.0</a></span></div><div class="footer-line statistics"><span class="site-time" title='网站运行中 ...'><i class="fa-solid fa-heartbeat fa-fw animate-icon" aria-hidden="true"></i>&nbsp;<span class="run-times">网站运行中 ...</span></span></div><div class="footer-line ibruce">
          <span id="busuanzi_container_site_uv" title='总访客数'><i class="fa-regular fa-user fa-fw" aria-hidden="true"></i>&nbsp;<span id="busuanzi_value_site_uv"><i class="fa-solid fa-spinner fa-spin fa-fw" aria-hidden="true"></i></span></span><span id="busuanzi_container_site_pv" class="footer-divider" title='总访问量'><i class="fa-regular fa-eye fa-fw" aria-hidden="true"></i>&nbsp;<span id="busuanzi_value_site_pv"><i class="fa-solid fa-spinner fa-spin fa-fw" aria-hidden="true"></i></span></span>
        </div></div>
  </footer></div><div class="widgets"><div class="fixed-buttons animate__faster d-none"><div class="fixed-button back-to-top" role="button" aria-label="回到顶部"><i class="fa-solid fa-arrow-up fa-fw" aria-hidden="true"></i><span class="variant-numeric">0%</span>
        </div></div><a href="https://liudongdong1.github.io/" title="在 GitHub 上查看源代码"target="_blank" rel="external nofollow" class="github-corner right d-none-mobile"><svg viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><div id="mask"></div><div class="reading-progress-bar" style="left: 0;top: 0;--bg-progress: #0076ff;--bg-progress-dark: #fff;"></div><noscript>
    <div class="noscript-warning">FixIt 主题在启用 JavaScript 的情况下效果最佳。</div>
  </noscript>
</div><link rel="stylesheet" href="/lib/katex/katex.min.css"><link rel="stylesheet" href="/lib/cookieconsent/cookieconsent.min.css"><script src="/lib/autocomplete/autocomplete.min.js" defer></script><script src="/lib/algoliasearch/algoliasearch-lite.umd.min.js" defer></script><script src="/lib/lazysizes/lazysizes.min.js" async defer></script><script src="/lib/sharer/sharer.min.js" async defer></script><script src="/lib/typeit/index.umd.js" defer></script><script src="/lib/katex/katex.min.js" defer></script><script src="/lib/katex/auto-render.min.js" defer></script><script src="/lib/katex/copy-tex.min.js" defer></script><script src="/lib/katex/mhchem.min.js" defer></script><script src="/lib/cookieconsent/cookieconsent.min.js" defer></script><script src="/lib/pangu/pangu.min.js" defer></script><script src="/lib/cell-watermark/watermark.min.js" defer></script><script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async defer></script><script>window.config={"code":{"copyTitle":"复制到剪贴板","editLockTitle":"锁定可编辑代码块","editUnLockTitle":"解锁可编辑代码块","editable":true,"maxShownLines":10},"comment":{"enable":false},"cookieconsent":{"content":{"dismiss":"同意","link":"了解更多","message":"本网站使用 Cookies 来改善您的浏览体验。"},"enable":true,"palette":{"button":{"background":"#f0f0f0"},"popup":{"background":"#1aa3ff"}},"theme":"edgeless"},"data":{"typeit-header-subtitle-desktop":"\u003cspan style='font-family: MMT,\"沐目体\";'\u003e吾日三省吾身\u003c/span\u003e","typeit-header-subtitle-mobile":"\u003cspan style='font-family: MMT,\"沐目体\";'\u003e吾日三省吾身\u003c/span\u003e"},"enablePWA":true,"enablePangu":true,"math":{"delimiters":[{"display":true,"left":"$$","right":"$$"},{"display":true,"left":"\\[","right":"\\]"},{"display":true,"left":"\\begin{equation}","right":"\\end{equation}"},{"display":true,"left":"\\begin{equation*}","right":"\\end{equation*}"},{"display":true,"left":"\\begin{align}","right":"\\end{align}"},{"display":true,"left":"\\begin{align*}","right":"\\end{align*}"},{"display":true,"left":"\\begin{alignat}","right":"\\end{alignat}"},{"display":true,"left":"\\begin{alignat*}","right":"\\end{alignat*}"},{"display":true,"left":"\\begin{gather}","right":"\\end{gather}"},{"display":true,"left":"\\begin{CD}","right":"\\end{CD}"},{"display":false,"left":"$","right":"$"},{"display":false,"left":"\\(","right":"\\)"}],"strict":false},"search":{"algoliaAppID":"2R1K9SKLQZ","algoliaIndex":"index.zh-cn","algoliaSearchKey":"4a226aa1c5c98d6859e4d1386adb2bc7","highlightTag":"em","maxResultLength":10,"noResultsFound":"没有找到结果","snippetLength":50,"type":"algolia"},"siteTime":"2020-12-18T16:15:22+08:00","typeit":{"cursorChar":"|","cursorSpeed":1000,"data":{"typeit-header-subtitle-desktop":["typeit-header-subtitle-desktop"],"typeit-header-subtitle-mobile":["typeit-header-subtitle-mobile"]},"duration":-1,"speed":100},"watermark":{"appendto":".wrapper\u003emain","colspacing":30,"content":"\u003cimg class=\"fixit-icon\" src=\"/fixit.min.svg\" alt=\"FixIt logo\" /\u003e FixIt 主题","enable":true,"fontfamily":"inherit","fontsize":0.85,"height":21,"opacity":0.0125,"rotate":15,"rowspacing":60,"width":150}};</script><script src="/js/theme.min.js" defer></script><script src="/js/custom.min.js" defer></script></body>
</html>
