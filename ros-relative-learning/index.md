# ROS Relative Learning


> Common functional pieces of autonomous vehicles often fall into sensing, computing, and actuation.The sensing devices or sensor include cameras, laser scanners (LiDAR), milliwave radars, and GNSS/IMU.  Using sensor data , autonomous vehicles perform localization, detection, prediction, planning, and control.

# 1. ROS OpenSource Relative


**keyword**:

- ROS, System Design, cyber-physical systems

------

# Paper: Autoware on Board

<div align=center>
<br/>
<b>Autoware on Board: Enabling Autonomous Vehicles with Embedded Systems</b>
</div>

#### Summary

1. **Autoware:** a popular open-source software project that provides a complete set of self-driving modules, including localization, detection, prediction, planning, and control.
2. this paper customize and extend the software stack of autoware to accommodate embedded computing capabilities.

#### Research Objective

  - **Application Area**:  auto
- **Purpose**:  building autoware with embedded systems while clarifying its performance in  real environments.

#### Proble Statement

**Background:** 

- open-source contribution:  Apollo [40] originated from the Baidu's alliance depends on high-precision GNSS location and 3D perception.
-  PolySync[41] provides an integrated development environment for the backend framework of self-driving technology.
- ROS

![](https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200614073656730.png)

#### Methods

- **Problem Formulation**:

- **system overview**:

![Basic Control&Data Flow](https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200614072728804.png)

![Software stack](https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200614072905488.png)

【**Function Module**】**Sensing**

> **Sensing**:  LiDAR scanners measure the distance to objects by illuminating a target with pulsed lasers and measuring the time of the reflected pulses. Point Cloud data from LiDAR scanners is used to generate digital 3D representations of the scanned objects.  Cameras are used to recognize traffic lights and extract additional features of the scanned objects.
>
> Voxed grid filtering is required to localize, map, and detect objects in the point cloud, by downsampling the point cloud, and then use NDT to obtain the relative position between two point clouds[12].

**【Function Module】Computing**

> **Computing:** compute the final trajectory and communicate with the actuation modules, and core modules including perception, decision-making, and planning.

> **Perception:** to calculate the accurate position of the ego-vehicle inside a 3D map, and recognition objects in the surrounding scene as well as status of the traffic.
>
> **As for Localization:** exploit scan matching between 3D maps and LiDAR scanners. using Slam, NDT algorithm, Iterative Closet Point etc. <font color=red>the LiDAR 3D point cloud can be projected to 2D image to get the depth information</font>
>
> **As for Detection:**  detect the surrounding objects, like vehicles, pedestrians, traffic signal etc. Pre-processed and segmented using the NN algorithm, to get the clustered point cloud, and using the distance of each cluster in conjunction with the classified  2D images processing algorithm,objects can be tracked on a time to improve the perception information.
>
> **As for Prediction:** object deteciton algorithms are computed on each synchronized camera frame and point cloud scan along with time, so the trajectory of moving objects can be estimated for the mission and motion planning modules.
>
> ​		**Kalman filters** are used under the assumption the the ego-vehicle is driving at a constant velocity while the tracking moving objects.
>
> ​    	**Particle Filters:** applied to non-linear tracking scenarios, which are appropriate for realistic driving.

**【Sub Function Module】Decision**

> Once understand the surrounding environment, estimating other moving objects trajectories, and make decision.

**【Sub Function Module】Planning**

> Path planning is classified into mission and motion planning. Autoware makes a plan for global trajectories based on the current location and the given destination. Motion planning module generate local trajectories along with global trajectories. <font color=red>the graph-search algorithms include hybrid-state A*, trajectory generation algorithm( lattice-based methods).
>
> **Mission Planning:** basic policy is to follow the center lines of the lanes over the route generated by the map navigation system, involving the high-definition 3D map information.
>
> **Motion Planning:** in charge of generating local feasible trajectories along with the given global trajectories, taking into account the vehicle states, drivable areas, the surrounding objects, the traffic rules,and the desire goal.

**【Function Module】Actuation**

> **Path Following:** the puresuit algorithm[28] generate the actuation commands for the ego-vehicle, and breaks down the path into multiple waypoints to represent path.
>
> **Vehicle Control:** to deal with phenomenon that the steering and the throttle of the ego-vehicle are successfully controlled in accordance with the velocity and angle output causing the trajectory deviate, using PID control, and updating the waypoints in consideration of obstacles in the heading lane area.

![](https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200614091914605.png)

![](https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200614082848509.png)



![](https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200625164643570.png)

https://mp.weixin.qq.com/s/XxEakqch2lVBP_rOd7HBmw

---

> 作者: liudongdong1  
> URL: liudongdong1.github.io/ros-relative-learning/  

