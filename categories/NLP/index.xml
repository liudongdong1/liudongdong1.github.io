<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title>NLP - 分类 - DAY By DAY</title><link>liudongdong1.github.io/categories/nlp/</link><description>NLP - 分类 - DAY By DAY</description><generator>Hugo -- gohugo.io</generator><language>zh-CN</language><managingEditor>3463264078@qq.cn (LiuDongdong)</managingEditor><webMaster>3463264078@qq.cn (LiuDongdong)</webMaster><copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright><lastBuildDate>Thu, 13 May 2021 21:31:56 +0000</lastBuildDate><atom:link href="liudongdong1.github.io/categories/nlp/" rel="self" type="application/rss+xml"/><item><title>Framework_Transformers</title><link>liudongdong1.github.io/framework_transformers/</link><pubDate>Thu, 13 May 2021 21:31:56 +0000</pubDate><author>liudongdong1</author><guid>liudongdong1.github.io/framework_transformers/</guid><description><![CDATA[<div class="featured-image">
        <img src="https://cdn.stocksnap.io/img-thumbs/280h/DYRNHWZGAN.jpg" referrerpolicy="no-referrer">
      </div>provides thousands of pretrained models to perform tasks on texts such as classification, information extraction, question answering, summarization, translation, text generation and more. [code] tutorial Transformers is backed by the three most popular deep learning libraries&ndash;Jax, PyTorch, and TensorFlow. Masked word completion with BERT Name Entity Recognition with Electra Text generation with GPT-2 Natural Language Inference with RoBERTa Summarization with BART Question answering with DistilBERT Translation with T5 1. Example]]></description></item><item><title>QA_relative</title><link>liudongdong1.github.io/qa_relative/</link><pubDate>Sun, 13 Dec 2020 09:14:57 +0000</pubDate><author>liudongdong1</author><guid>liudongdong1.github.io/qa_relative/</guid><description>&lt;div class="featured-image">
&lt;img src="https://gitee.com/github-25970295/blogImage/raw/master/img/20210501113233.png" referrerpolicy="no-referrer">
&lt;/div></description></item><item><title>Rec_paper</title><link>liudongdong1.github.io/rec_paper/</link><pubDate>Sat, 12 Dec 2020 18:14:57 +0000</pubDate><author>liudongdong1</author><guid>liudongdong1.github.io/rec_paper/</guid><description><![CDATA[<div class="featured-image">
        <img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201212194845004.png" referrerpolicy="no-referrer">
      </div>Liu, Hongtao, et al. &ldquo;NRPA: Neural Recommendation with Personalized Attention.&rdquo; Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval. 2019. Paper: NRPA Summary propose a neural recommendation approach with personalized attention to learn personalized representations of users and items from reviews, to select different important words and reviews for different users/items. review encoder to learn representations of reviews from words and user/item encoder to]]></description></item><item><title>Recommandation</title><link>liudongdong1.github.io/recommandation/</link><pubDate>Sun, 15 Nov 2020 07:56:09 +0000</pubDate><author>liudongdong1</author><guid>liudongdong1.github.io/recommandation/</guid><description>&lt;div class="featured-image">
&lt;img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201129142519025.png" referrerpolicy="no-referrer">
&lt;/div>Recommendation Summary User 数据（用户的基础属性数据，如性别、年龄、关系链、兴趣偏好等） 对于用户兴趣偏好，一般简单地采用文本 embedding 方法来得到各标签的 embedding 向量，然后根据用</description></item><item><title>Torchtext</title><link>liudongdong1.github.io/torchtext_vocab/</link><pubDate>Tue, 27 Oct 2020 08:56:09 +0000</pubDate><author>liudongdong1</author><guid>liudongdong1.github.io/torchtext_vocab/</guid><description>&lt;div class="featured-image">
&lt;img src="https://gitee.com/github-25970295/blogImage/raw/master/img/32.jpeg" referrerpolicy="no-referrer">
&lt;/div>torchtext.data.Example : 用来表示一个样本，数据+标签 torchtext.vocab.Vocab: 词汇表相关 torchtext.data.Datasets: 数据集类，getitem 返回 Example实例 torchtext.data.Field : 用来定义字段的处理方法（文本字段，标签字段）</description></item><item><title>AllenNLPIntroduce</title><link>liudongdong1.github.io/allennlpintroduce/</link><pubDate>Tue, 20 Oct 2020 07:56:09 +0000</pubDate><author>liudongdong1</author><guid>liudongdong1.github.io/allennlpintroduce/</guid><description>&lt;div class="featured-image">
&lt;img src="https://gitee.com/github-25970295/blogImage/raw/master/img/20210501113113.png" referrerpolicy="no-referrer">
&lt;/div>you can write your own script to construct the dataset reader and model and run the training loop, or you can write a configuration file and use the allennlp train command 1. Text Classification Spam filtering Detect and filter spam emails Email Spam / Not spam Sentiment analysis Detect the polarity of text Tweet, review Positive / Negative Topic detection Detect the topic of text News article, blog post Business</description></item><item><title>WordEmbedding</title><link>liudongdong1.github.io/wordembedding/</link><pubDate>Tue, 20 Oct 2020 07:56:09 +0000</pubDate><author>liudongdong1</author><guid>liudongdong1.github.io/wordembedding/</guid><description>&lt;div class="featured-image">
&lt;img src="https://gitee.com/github-25970295/blogImage/raw/master/img/view-of-coffee-beans.jpg" referrerpolicy="no-referrer">
&lt;/div>TEXT processing deals with humongous amount of text to perform different range of tasks like clustering in the g oogle search example, classification in the second and Machine Translation. How to create a representation for words that capture their meanings, semantic relationships and the different types of contexts they are used in. 作为 Embedding 层嵌入到深度模型中，实现将高维</description></item><item><title>RelationExtraction</title><link>liudongdong1.github.io/relationextraction/</link><pubDate>Mon, 19 Oct 2020 07:56:09 +0000</pubDate><author>liudongdong1</author><guid>liudongdong1.github.io/relationextraction/</guid><description>&lt;div class="featured-image">
&lt;img src="https://gitee.com/github-25970295/blogImage/raw/master/img/79.jpeg" referrerpolicy="no-referrer">
&lt;/div>1. Challenges **数据规模问题：**人工精准地标注句子级别的数据代价十分高昂，需要耗费大量的时间和人力。在实际场景中，面向数以千计的关系、数以千万计的</description></item><item><title>NLPHotTopic</title><link>liudongdong1.github.io/nlphottopic/</link><pubDate>Sat, 15 Aug 2020 07:56:09 +0000</pubDate><author>liudongdong1</author><guid>liudongdong1.github.io/nlphottopic/</guid><description>&lt;div class="featured-image">
&lt;img src="https://gitee.com/github-25970295/blogImage/raw/master/img/20210501113138.png" referrerpolicy="no-referrer">
&lt;/div>This week i get a summary knowledge of NLP, and learn some direction for further learning. And in this blog, i will record what i learned this weak by searching some information on Internet, the content is organized as follows: the Preparatory knowledge which need to be master in the following years, and some direction in NLP areas from model sides, application sides and the scene task, and some paper</description></item><item><title>NLPRelative</title><link>liudongdong1.github.io/nlprelative/</link><pubDate>Sat, 15 Aug 2020 07:56:09 +0000</pubDate><author>liudongdong1</author><guid>liudongdong1.github.io/nlprelative/</guid><description><![CDATA[<div class="featured-image">
        <img src="https://gitee.com/github-25970295/blogImage/raw/master/img/79.jpeg" referrerpolicy="no-referrer">
      </div>Semantic Parsing: aims to translate a natural languages sentence into its corresponding executable programming language, which relieves users from the burden of learning techniques behind the programming language. 1. Grammar-based Recent Question as context Precedent SQL as context level: author: Kelvin Guu date: 2020 keyword: NLP Guu, K., Lee, K., Tung, Z., Pasupat, P., &amp; Chang, M. W. (2020). Realm: Retrieval-augmented language model pre-training. arXiv preprint arXiv:2002.08909. Paper: REALM Proble]]></description></item></channel></rss>