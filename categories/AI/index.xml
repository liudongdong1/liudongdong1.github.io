<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  <channel>
    <title>AI - 分类 - DAY By DAY</title>
    <link>liudongdong1.github.io/categories/ai/</link>
    <description>AI - 分类 - DAY By DAY</description>
    <generator>Hugo -- gohugo.io</generator><language>zh-CN</language><managingEditor>3463264078@qq.cn (LiuDongdong)</managingEditor>
      <webMaster>3463264078@qq.cn (LiuDongdong)</webMaster><copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright><lastBuildDate>Wed, 19 Jan 2022 07:56:09 &#43;0000</lastBuildDate><atom:link href="liudongdong1.github.io/categories/ai/" rel="self" type="application/rss+xml" /><item>
  <title>VoiceResearch</title>
  <link>liudongdong1.github.io/voiceresearch/</link>
  <pubDate>Wed, 19 Jan 2022 07:56:09 &#43;0000</pubDate>
  <author>liudongdong1</author>
  <guid>liudongdong1.github.io/voiceresearch/</guid>
  <description><![CDATA[<div class="featured-image">
        <img src="https://cdn.pixabay.com/photo/2019/03/27/15/24/animal-4085255__340.jpg" referrerpolicy="no-referrer">
      </div>1. 语音激活检测 （VAD) 近场识别场景: 比如使用语音输入法时，用户可以用手按着语音按键说话，结束之后松开，由于近场情况下信噪比（signal to]]></description>
</item>
<item>
  <title>CircleGan</title>
  <link>liudongdong1.github.io/circlegan/</link>
  <pubDate>Sat, 20 Nov 2021 22:45:45 &#43;0000</pubDate>
  <author>liudongdong1</author>
  <guid>liudongdong1.github.io/circlegan/</guid>
  <description><![CDATA[<div class="featured-image">
        <img src="https://cdn.stocksnap.io/img-thumbs/280h/TDAU1ERCD4.jpg" referrerpolicy="no-referrer">
      </div>Zhu, Jun-Yan, et al. &ldquo;Unpaired image-to-image translation using cycle-consistent adversarial networks.&rdquo; Proceedings of the IEEE international conference on computer vision. 2017. cite 10600 [pdf] [code] Paper: CircleGAN Summary present a method that can learn to do the same, capturing special characteristics of one image collection and figureing out how these characteristics could be translated into the other image collection. Research Objective Application Area: Collection style transfer: learns to mimic the]]></description>
</item>
<item>
  <title>Frame_BasicSR</title>
  <link>liudongdong1.github.io/frame_basicsr/</link>
  <pubDate>Sat, 23 Oct 2021 21:45:45 &#43;0000</pubDate>
  <author>liudongdong1</author>
  <guid>liudongdong1.github.io/frame_basicsr/</guid>
  <description><![CDATA[<div class="featured-image">
        <img src="https://cdn.stocksnap.io/img-thumbs/280h/5U2FV0UNXF.jpg" referrerpolicy="no-referrer">
      </div>BasicSR (Basic Super Restoration) 是一个基于 PyTorch 的开源图像视频复原工具箱, 比如 超分辨率, 去噪, 去模糊, 去 JPEG 压缩噪声等. Real-ESRGAN: 通用图像复原的实用算法 GFPGAN: 真实场景人脸复原的实用算]]></description>
</item>
<item>
  <title>Rendering</title>
  <link>liudongdong1.github.io/rendering/</link>
  <pubDate>Mon, 11 Oct 2021 21:45:45 &#43;0000</pubDate>
  <author>liudongdong1</author>
  <guid>liudongdong1.github.io/rendering/</guid>
  <description><![CDATA[<div class="featured-image">
        <img src="https://cdn.stocksnap.io/img-thumbs/280h/IQ2LORUXQB.jpg" referrerpolicy="no-referrer">
      </div>Rückert D, Franke L, Stamminger M. ADOP: Approximate Differentiable One-Pixel Point Rendering[J]. arXiv preprint arXiv:2110.06635, 2021. pdf code star 538
Paper: ADOP Summary present a novel point-based, differentiable neural rendering pipeline for scene refinement and novel view synthesis. the point cloud rendering is performed by a differentiable renderer using multi-resolution one-pixel point rasterization. after rendering , the neural image pyramid is passed through a deep neural network for shading calculations and hole-filling.]]></description>
</item>
<item>
  <title>SuperResolution</title>
  <link>liudongdong1.github.io/superresolution/</link>
  <pubDate>Thu, 23 Sep 2021 21:45:45 &#43;0000</pubDate>
  <author>liudongdong1</author>
  <guid>liudongdong1.github.io/superresolution/</guid>
  <description><![CDATA[<div class="featured-image">
        <img src="https://cdn.stocksnap.io/img-thumbs/280h/QSCRVBUU2G.jpg" referrerpolicy="no-referrer">
      </div>Tian Y, Zhang Y, Fu Y, et al. Tdan: Temporally-deformable alignment network for video super-resolution[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020: 3360-3369. Paper: Tdan Summary propose a temporally-deformable alignment network(TDAN) to adaptively align the reference frame and each supporting frame a the feature level without computing optical flow. use features from both the reference frame and each supporting frame to dynamically predict offsets of sampling]]></description>
</item>
<item>
  <title>Vicon动捕</title>
  <link>liudongdong1.github.io/vicon%E5%8A%A8%E6%8D%95/</link>
  <pubDate>Mon, 13 Sep 2021 10:30:29 &#43;0000</pubDate>
  <author>liudongdong1</author>
  <guid>liudongdong1.github.io/vicon%E5%8A%A8%E6%8D%95/</guid>
  <description><![CDATA[<div class="featured-image">
        <img src="https://cdn.stocksnap.io/img-thumbs/280h/8THWMOJNHY.jpg" referrerpolicy="no-referrer">
      </div>Vicon 光学动作捕捉系统系统是一组网络连接的 Vicon 运动捕捉摄像机和其它设备以提 供实时光学数据，这些数据可以被应用于实时在线或者离线的运动捕捉、分析，应]]></description>
</item>
<item>
  <title>Ai拟声</title>
  <link>liudongdong1.github.io/ai%E6%8B%9F%E5%A3%B0/</link>
  <pubDate>Sat, 28 Aug 2021 21:59:57 &#43;0000</pubDate>
  <author>liudongdong1</author>
  <guid>liudongdong1.github.io/ai%E6%8B%9F%E5%A3%B0/</guid>
  <description><![CDATA[<div class="featured-image">
        <img src="https://cdn.stocksnap.io/img-thumbs/280h/8LT22UWAUZ.jpg" referrerpolicy="no-referrer">
      </div>1. MockingBird AI拟声: 5秒内克隆您的声音并生成任意语音内容 Clone a voice in 5 seconds to generate arbitrary speech in real-time]]></description>
</item>
<item>
  <title>HumanPoseProject</title>
  <link>liudongdong1.github.io/humanposeproject/</link>
  <pubDate>Fri, 13 Aug 2021 10:30:29 &#43;0000</pubDate>
  <author>liudongdong1</author>
  <guid>liudongdong1.github.io/humanposeproject/</guid>
  <description><![CDATA[<div class="featured-image">
        <img src="https://images.unsplash.com/photo-1624788998865-126ccbb55e40?ixid=MnwxMjA3fDB8MHx0b3BpYy1mZWVkfDJ8NnNNVmpUTFNrZVF8fGVufDB8fHx8&amp;ixlib=rb-1.2.1&amp;auto=format&amp;fit=crop&amp;w=500&amp;q=60" referrerpolicy="no-referrer">
      </div>1. residual_pose Hourglass model for multi-person 2D pose estimation from depth images. Our regressor NN architecture for 3D human pose estimation. 3D pose prior for recovering from 2D missed detections. Tranined models for 2D and 3D pose estimation. Code for obtaining 2D and 3D pose from a depth image. 11 months ago. star12 2. depth_human_synthesis We have created a collection of 24 human characters, 12 men and 12 women, with]]></description>
</item>
<item>
  <title>YoloRelative</title>
  <link>liudongdong1.github.io/yolorelative/</link>
  <pubDate>Mon, 26 Jul 2021 18:04:14 &#43;0000</pubDate>
  <author>liudongdong1</author>
  <guid>liudongdong1.github.io/yolorelative/</guid>
  <description><![CDATA[<div class="featured-image">
        <img src="https://cdn.stocksnap.io/img-thumbs/280h/bird-egret_54R1KQELFW.jpg" referrerpolicy="no-referrer">
      </div>1 Yolox相关基础知识点 1.1 Yolox的论文及代码 Yolox论文名：《YOLOX: Exceeding YOLO Series in 2021》 Yolox论文地址：https://ar]]></description>
</item>
<item>
  <title>LightestDetection</title>
  <link>liudongdong1.github.io/lightestdetection/</link>
  <pubDate>Sun, 13 Jun 2021 16:00:04 &#43;0000</pubDate>
  <author>liudongdong1</author>
  <guid>liudongdong1.github.io/lightestdetection/</guid>
  <description><![CDATA[<div class="featured-image">
        <img src="https://cdn.pixabay.com/photo/2016/05/16/15/40/texture-1395982__340.jpg" referrerpolicy="no-referrer">
      </div>minMaxLoc寻找矩阵(一维数组当作向量,用Mat定义) 中最小值和最大值的位置. 1. BrightArea import numpy as np import argparse import cv2 # construct the argument parse and parse the arguments ap = argparse.ArgumentParser() ap.add_argument(&#34;-i&#34;, &#34;--image&#34;, help = &#34;path to]]></description>
</item>
</channel>
</rss>
