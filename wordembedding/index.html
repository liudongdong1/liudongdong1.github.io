<!DOCTYPE html>
<html itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">
  <head>
    
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
    <meta name="robots" content="noodp" />
    <title>WordEmbedding - DAY By DAY</title><meta name="author" content="LiuDongdong">
<meta name="author-link" content="https://liudongdong1.github.io/">
<meta name="description" content="TEXT processing deals with humongous amount of text to perform different range of tasks like clustering in the g oogle search example, classification in the second and Machine Translation. How to create a representation for words that capture their meanings, semantic relationships and the different types of contexts they are used in. 作为 Embedding 层嵌入到深度模型中，实现将高维" /><meta name="keywords" content='embedding' /><meta itemprop="name" content="WordEmbedding">
<meta itemprop="description" content="TEXT processing deals with humongous amount of text to perform different range of tasks like clustering in the g oogle search example, classification in the second and Machine Translation. How to create a representation for words that capture their meanings, semantic relationships and the different types of contexts they are used in. 作为 Embedding 层嵌入到深度模型中，实现将高维"><meta itemprop="datePublished" content="2020-10-20T07:56:09+00:00" />
<meta itemprop="dateModified" content="2023-12-31T13:47:03+08:00" />
<meta itemprop="wordCount" content="6958"><meta itemprop="image" content="https://liudongdong1.github.io/logo.png"/>
<meta itemprop="keywords" content="embedding," /><meta property="og:title" content="WordEmbedding" />
<meta property="og:description" content="TEXT processing deals with humongous amount of text to perform different range of tasks like clustering in the g oogle search example, classification in the second and Machine Translation. How to create a representation for words that capture their meanings, semantic relationships and the different types of contexts they are used in. 作为 Embedding 层嵌入到深度模型中，实现将高维" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://liudongdong1.github.io/wordembedding/" /><meta property="og:image" content="https://liudongdong1.github.io/logo.png"/><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2020-10-20T07:56:09+00:00" />
<meta property="article:modified_time" content="2023-12-31T13:47:03+08:00" />
<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://liudongdong1.github.io/logo.png"/>

<meta name="twitter:title" content="WordEmbedding"/>
<meta name="twitter:description" content="TEXT processing deals with humongous amount of text to perform different range of tasks like clustering in the g oogle search example, classification in the second and Machine Translation. How to create a representation for words that capture their meanings, semantic relationships and the different types of contexts they are used in. 作为 Embedding 层嵌入到深度模型中，实现将高维"/>
<meta name="application-name" content="DAY By DAY">
<meta name="apple-mobile-web-app-title" content="DAY By DAY"><meta name="theme-color" data-light="#f8f8f8" data-dark="#252627" content="#f8f8f8"><meta name="msapplication-TileColor" content="#da532c"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" />
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"><link rel="manifest" href="/site.webmanifest"><link rel="canonical" href="https://liudongdong1.github.io/wordembedding/" /><link rel="prev" href="https://liudongdong1.github.io/relationextraction/" /><link rel="next" href="https://liudongdong1.github.io/allennlpintroduce/" /><link rel="stylesheet" href="/css/style.min.css"><link rel="stylesheet" href="/lib/fontawesome-free/all.min.css"><link rel="stylesheet" href="/lib/animate/animate.min.css"><script type="application/ld+json">
  {
    "@context": "http://schema.org",
    "@type": "BlogPosting",
    "headline": "WordEmbedding",
    "inLanguage": "zh-CN",
    "mainEntityOfPage": {
      "@type": "WebPage",
      "@id": "https:\/\/liudongdong1.github.io\/wordembedding\/"
    },"genre": "posts","keywords": "embedding","wordcount":  6958 ,
    "url": "https:\/\/liudongdong1.github.io\/wordembedding\/","datePublished": "2020-10-20T07:56:09+00:00","dateModified": "2023-12-31T13:47:03+08:00","license": "This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.","publisher": {
      "@type": "Organization",
      "name": "LiuDongdong","logo": "https:\/\/liudongdong1.github.io\/images\/person.png"},"author": {
        "@type": "Person",
        "name": "liudongdong1"
      },"description": ""
  }
  </script></head>
  <body data-header-desktop="auto" data-header-mobile="auto"><script>(window.localStorage?.getItem('theme') ? localStorage.getItem('theme') === 'dark' : ('auto' === 'auto' ? window.matchMedia('(prefers-color-scheme: dark)').matches : 'auto' === 'dark')) && document.body.setAttribute('data-theme', 'dark');</script><div class="wrapper"><script type="text/javascript"
        async
        src="https://cdnjs.cloudflare.com/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$']],
    displayMath: [['$$','$$']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
});

MathJax.Hub.Queue(function() {
    
    
    
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>

<style>
code.has-jax {
    font: inherit;
    font-size: 100%;
    background: inherit;
    border: inherit;
    color: #515151;
}
</style>
<header class="desktop animate__faster" id="header-desktop">
  <div class="header-wrapper" data-github-corner="right">
    <div class="header-title">
      <a href="/" title="DAY By DAY"><img
    class="lazyload logo"
    src="/svg/loading.min.svg"
    data-src="/fixit.min.svg"
    data-srcset="/fixit.min.svg, /fixit.min.svg 1.5x, /fixit.min.svg 2x"
    data-sizes="auto"
    alt="DAY By DAY"
    title="DAY By DAY"/><span class="header-title-text"></span></a><span id="typeit-header-subtitle-desktop" class="typeit header-subtitle"></span></div>
    <nav>
      <ul class="menu"><li class="menu-item">
              <a
                class="menu-link"
                href="/posts/"
                
                
              ><i class="fa-solid fa-archive fa-fw fa-sm" aria-hidden="true"></i> 所有文章</a></li><li class="menu-item">
              <a
                class="menu-link"
                href="/categories/"
                
                
              ><i class="fa-solid fa-th fa-fw fa-sm" aria-hidden="true"></i> 分类</a></li><li class="menu-item">
              <a
                class="menu-link"
                href="/tags/"
                
                
              ><i class="fa-solid fa-tags fa-fw fa-sm" aria-hidden="true"></i> 标签</a></li><li class="menu-item">
              <a
                class="menu-link"
                href="/friends/"
                title="友情链接"
                
              ><i class="fa-solid fa-users fa-fw fa-sm" aria-hidden="true"></i> 友链</a></li><li class="menu-item">
              <a
                class="menu-link"
                href="/about/"
                
                
              ><i class="fa-solid fa-info-circle fa-fw fa-sm" aria-hidden="true"></i> 关于</a></li><li class="menu-item delimiter"></li><li class="menu-item language">
            <span role="button" aria-label="选择语言" title="选择语言">简体中文<i class="dropdown-icon fa-solid fa-chevron-down" aria-hidden="true"></i>
            </span>
            <ul class="sub-menu"><li class="menu-item">没有更多翻译</li></ul>
          </li><li class="menu-item search" id="search-desktop">
            <input type="text" placeholder="搜索文章标题或内容 ..." id="search-input-desktop">
            <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-desktop" title="搜索">
              <i class="fa-solid fa-search fa-fw" aria-hidden="true"></i>
            </a>
            <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-desktop" title="清空">
              <i class="fa-solid fa-times-circle fa-fw" aria-hidden="true"></i>
            </a>
            <span class="search-button search-loading" id="search-loading-desktop">
              <i class="fa-solid fa-spinner fa-fw fa-spin" aria-hidden="true"></i>
            </span>
          </li><li class="menu-item theme-switch" title="切换主题">
          <i class="fa-solid fa-adjust fa-fw" aria-hidden="true"></i>
        </li>
      </ul>
    </nav>
  </div>
</header><header class="mobile animate__faster" id="header-mobile">
  <div class="header-container">
    <div class="header-wrapper">
      <div class="header-title">
        <a href="/" title="DAY By DAY"><img
    class="lazyload logo"
    src="/svg/loading.min.svg"
    data-src="/fixit.min.svg"
    data-srcset="/fixit.min.svg, /fixit.min.svg 1.5x, /fixit.min.svg 2x"
    data-sizes="auto"
    alt="/fixit.min.svg"
    title="/fixit.min.svg"/><span class="header-title-text"></span></a><span id="typeit-header-subtitle-mobile" class="typeit header-subtitle"></span></div>
      <div class="menu-toggle" id="menu-toggle-mobile">
        <span></span><span></span><span></span>
      </div>
    </div>
    <nav>
      <ul class="menu" id="menu-mobile"><li class="search-wrapper">
            <div class="search mobile" id="search-mobile">
              <input type="text" placeholder="搜索文章标题或内容 ..." id="search-input-mobile">
              <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-mobile" title="搜索">
                <i class="fa-solid fa-search fa-fw" aria-hidden="true"></i>
              </a>
              <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-mobile" title="清空">
                <i class="fa-solid fa-times-circle fa-fw" aria-hidden="true"></i>
              </a>
              <span class="search-button search-loading" id="search-loading-mobile">
                <i class="fa-solid fa-spinner fa-fw fa-spin" aria-hidden="true"></i>
              </span>
            </div>
            <a href="javascript:void(0);" class="search-cancel" id="search-cancel-mobile">
              取消
            </a>
          </li><li
              class="menu-item"
            ><a
                  class="menu-link"
                  href="/posts/"
                  
                  
                ><i class="fa-solid fa-archive fa-fw fa-sm" aria-hidden="true"></i> 所有文章</a></li><li
              class="menu-item"
            ><a
                  class="menu-link"
                  href="/categories/"
                  
                  
                ><i class="fa-solid fa-th fa-fw fa-sm" aria-hidden="true"></i> 分类</a></li><li
              class="menu-item"
            ><a
                  class="menu-link"
                  href="/tags/"
                  
                  
                ><i class="fa-solid fa-tags fa-fw fa-sm" aria-hidden="true"></i> 标签</a></li><li
              class="menu-item"
            ><a
                  class="menu-link"
                  href="/friends/"
                  title="友情链接"
                  
                ><i class="fa-solid fa-users fa-fw fa-sm" aria-hidden="true"></i> 友链</a></li><li
              class="menu-item"
            ><a
                  class="menu-link"
                  href="/about/"
                  
                  
                ><i class="fa-solid fa-info-circle fa-fw fa-sm" aria-hidden="true"></i> 关于</a></li><li
              class="menu-item text-center"
            ><a
                  class="menu-link"
                  href="/"
                  title="GitHub"
                  
                ><i class='fa-brands fa-github fa-fw' aria-hidden='true'></i> </a></li><li class="menu-item theme-switch" title="切换主题">
          <i class="fa-solid fa-adjust fa-fw" aria-hidden="true"></i>
        </li><li class="menu-item language">
            <span role="button" aria-label="选择语言" title="选择语言">简体中文<i class="dropdown-icon fa-solid fa-chevron-down" aria-hidden="true"></i>
            </span>
            <select class="language-select" onchange="location = this.value;"><option disabled>没有更多翻译</option></select>
          </li></ul>
    </nav>
  </div>
</header><div class="search-dropdown desktop">
    <div id="search-dropdown-desktop"></div>
  </div>
  <div class="search-dropdown mobile">
    <div id="search-dropdown-mobile"></div>
  </div><main class="container" data-page-style="normal"><aside class="toc" id="toc-auto"><h2 class="toc-title">目录 <i class="toc-icon fa-solid fa-angle-down fa-fw"></i></h2>
      <div class="toc-content" id="toc-content-auto"></div></aside>

  <aside class="aside-custom" id="aside-sakana">
    

<div class="sakana-widget">
  <div class="sakana-item" id="takina-widget"></div>
  <div class="sakana-item" id="chisato-widget"></div>
</div>
<script>
  function initSakanaWidget() {
    const takina = SakanaWidget.getCharacter('takina')
    SakanaWidget.registerCharacter('takina-slow', takina);
    new SakanaWidget({
      character: 'takina-slow',
      controls: false,
      autoFit: true,
      stroke: {
        color: "#b4b4b4",
        width: 2
      }
    }).mount('#takina-widget');

    const chisato = SakanaWidget.getCharacter('chisato')
    SakanaWidget.registerCharacter('chisato-slow', chisato);
    new SakanaWidget({
      character: 'chisato-slow',
      controls: false,
      autoFit: true,
      stroke: {
        color: "#b4b4b4",
        width: 2
      }
    }).mount('#chisato-widget');
  }
</script>
<script async onload="initSakanaWidget()" src="https://cdn.jsdelivr.net/npm/sakana-widget@2.3.0/lib/sakana.min.js">
</script></aside>

  <article class="page single">
    <div class="header"><h1 class="single-title animate__animated animate__flipInX">
        <span>WordEmbedding</span>
      </h1></div><div class="post-meta">
      <div class="post-meta-line"><span class="post-author"><span class="author"><i class="fa-solid fa-user-circle" aria-hidden="true"></i>
      liudongdong1</span></span>
          <span class="post-category">收录于 <a href="/categories/"><i class="fa-regular fa-folder fa-fw"></i>&nbsp;Categories</a>&ensp;<a href="/categories/nlp/"><i class="fa-regular fa-folder fa-fw"></i>&nbsp;NLP</a></span></div>
      <div class="post-meta-line"><span title=2020-10-20&#32;07:56:09>
            <i class="fa-regular fa-calendar-alt fa-fw"></i>&nbsp;<time datetime="2020-10-20" >2020-10-20</time>
          </span>&nbsp;<i class="fa-solid fa-pencil-alt fa-fw"></i>&nbsp;约 6958 字&nbsp;
        <i class="fa-regular fa-clock fa-fw"></i>&nbsp;预计阅读 14 分钟&nbsp;<span id="busuanzi_container_page_pv" class="busuanzi_visitors comment-visitors" data-flag-title="WordEmbedding">
            <i class="fa-regular fa-eye fa-fw"></i>&nbsp;<span id="busuanzi_value_page_pv">-</span>&nbsp;次阅读
          </span>&nbsp;</div>
    </div><div class="featured-image"><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/view-of-coffee-beans.jpg"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/view-of-coffee-beans.jpg, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/view-of-coffee-beans.jpg 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/view-of-coffee-beans.jpg 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/view-of-coffee-beans.jpg"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/view-of-coffee-beans.jpg"/></div><div class="details toc" id="toc-static" kept="true">
        <div class="details-summary toc-title">
          <span>目录</span>
          <span><i class="details-icon fa-solid fa-angle-right"></i></span>
        </div>
        <div class="details-content toc-content" id="toc-content-static"><nav id="TableOfContents">
  <ul>
    <li><a href="#1-item-embedding">1. Item Embedding</a></li>
    <li><a href="#2-img-embedding">2. Img Embedding</a></li>
    <li><a href="#3-methods">3. Methods</a>
      <ul>
        <li><a href="#31-frequency-based-embedding">3.1. Frequency based Embedding</a></li>
        <li><a href="#32-bag-of-wordsbow">3.2. Bag of Words(BOW)</a></li>
        <li><a href="#33-静态向量">3.3. 静态向量</a></li>
        <li><a href="#34--动态向量">3.4.  动态向量</a></li>
        <li><a href="#35-graph-embedding">3.5. Graph Embedding</a></li>
      </ul>
    </li>
    <li><a href="#4-视文分析">4. 视文分析</a>
      <ul>
        <li><a href="#41-预训练模型">4.1. 预训练模型</a></li>
      </ul>
    </li>
    <li><a href="#5-学习链接">5. 学习链接</a></li>
  </ul>
</nav></div>
      </div><div
      class="content"
      id="content"
      
      
    ><blockquote>
<p><strong>TEXT processing</strong> deals with humongous amount of text to perform different range of tasks like clustering in the g    oogle search example, classification in the second and Machine Translation. How to create a representation for words that capture their <em>meanings</em>, <em>semantic relationships</em> and the different types of contexts they are used in.</p>
</blockquote>
<blockquote>
<ul>
<li>作为 Embedding 层嵌入到深度模型中，实现将高维稀疏特征到低维稠密特征的转换（如 Wide&amp;Deep、DeepFM 等模型）；</li>
<li>作为预训练的 Embedding 特征向量，与其他特征向量拼接后，一同作为深度学习模型输入进行训练（如 FNN）；</li>
<li>在召回层中，通过计算用户和物品的 Embedding 向量相似度，作为召回策略（比 Youtube 推荐模型等）；</li>
<li>实时计算用户和物品的 Embedding 向量，并将其作为实时特征输入到深度学习模型中（比 Airbnb 的 embedding 应用）。</li>
</ul>
</blockquote>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20200721081514640.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20200721081514640.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20200721081514640.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20200721081514640.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20200721081514640.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20200721081514640.png"/></p>
<h2 id="1-item-embedding">1. Item Embedding</h2>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20200721081648881.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20200721081648881.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20200721081648881.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20200721081648881.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20200721081648881.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20200721081648881.png"/></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20200721083502813.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20200721083502813.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20200721083502813.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20200721083502813.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20200721083502813.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20200721083502813.png"/></p>
<h2 id="2-img-embedding">2. Img Embedding</h2>
<blockquote>
<p>图片作为文章的门面特征，对推荐也很重要，可以通过 resnet 得到图片的向量，还可以通过 image caption  得到对一张图片的中文描述，对于娱乐类的新闻，还可以利用 facenet 识别出组图中，哪一张包含明星，对于动漫类类的新闻可以利用 OCR 识别出漫画里的文字，对于年龄，性别有明显倾向的场景还可以利用 resnet 改变图片的风格。</p>
</blockquote>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20200721084331320.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20200721084331320.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20200721084331320.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20200721084331320.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20200721084331320.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20200721084331320.png"/></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20200721084622194.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20200721084622194.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20200721084622194.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20200721084622194.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20200721084622194.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20200721084622194.png"/></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20200721084850357.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20200721084850357.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20200721084850357.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20200721084850357.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20200721084850357.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20200721084850357.png"/></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20200721090031252.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20200721090031252.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20200721090031252.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20200721090031252.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20200721090031252.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20200721090031252.png"/></p>
<h2 id="3-methods">3. Methods</h2>
<h3 id="31-frequency-based-embedding">3.1. Frequency based Embedding</h3>
<ul>
<li><strong>Count Vector:</strong> using top numbers words based on frequency and then prepare a dictionary.</li>
</ul>
<blockquote>
<p>Consider a Corpus C of D documents {d1,d2…..dD} and N unique tokens extracted out of the corpus C. The N tokens will form our dictionary and the size of the Count Vector matrix M will be given by D X N. Each row in the matrix M contains the frequency of tokens in document D(i).</p>
</blockquote>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20200716234329746.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20200716234329746.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20200716234329746.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20200716234329746.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20200716234329746.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20200716234329746.png"/></p>
<p>Prediction based Embedding</p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20200716234619560.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20200716234619560.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20200716234619560.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20200716234619560.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20200716234619560.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20200716234619560.png"/></p>
<ul>
<li>
<p>**TF-IDF vectorization:**takes into account not just the occurrence of a word in a single document but in the entire corpus. (1）搜索引擎；（2）关键词提取；（3）文本相似性；（4）文本摘要</p>
<ul>
<li>TF词频</li>
</ul>
<p>$$
tf_{ij}=n_{i,j}/\sum_kn_{k,j}\
$$</p>
<p>$n_{i,j}$ 是该词再文件$d_j$ 中出现的次数，分母是文件$d_j$中所有词汇出现的次数和。</p>
<ul>
<li>IDF逆向文件频率：某一特定词语的IDF，可以由<strong>总文件数目除以包含该词语的文件的数目</strong>，<strong>再将得到的商取对数得到</strong>。</li>
</ul>
<p>$$
idf_i=log|D|/|{j:t_i\epsilon d_j}|
$$</p>
<p>|D|是预料库中文件总数，$|{j:t_i\epsilon d_j}|$ 表示包含词语$t_i$ 的文件数目。
$$
TF-IDF=TF*IDF
$$</p>
</li>
<li>
<h4 id="co-occurrence-matrix-with-a-fixed-context-window">Co-Occurrence Matrix with a fixed context window</h4>
<blockquote>
<p>Similar words tend to occur together and will have similar context for example .Co-occurrence matrix is decomposed using techniques like PCA, SVD etc. into factors and combination of these factors forms the word vector representation.</p>
</blockquote>
<ul>
<li>Co-occurrence – For a given corpus, the co-occurrence of a pair of words say w1 and w2 is the number of times they have appeared together in a Context Window.</li>
<li>Context Window – Context window is specified by a number and the direction.</li>
</ul>
</li>
</ul>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20200717101404690.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20200717101404690.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20200717101404690.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20200717101404690.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20200717101404690.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20200717101404690.png"/></p>
<ul>
<li>Advantage:
<ul>
<li><code>preserves the semantic relationship</code> between words,</li>
<li>uses SVD at its core, producing more accurate word vector representations.</li>
<li>uses factorization which is a well-defined problem and can be efficiently solved.</li>
<li><code>computed once and can be use anytime once computed</code>.</li>
</ul>
</li>
<li><strong>Disadvantages:</strong> requires huge memory to store the co-occurrence matrix.</li>
</ul>
<h3 id="32-bag-of-wordsbow">3.2. Bag of Words(BOW)</h3>
<blockquote>
<ol>
<li>Tokenize the text into sentences</li>
<li>Tokenize sentences into words</li>
<li>Remove punctuation or stop words</li>
<li>Convert the words to lower text</li>
<li><code>Create the frequency distribution of words</code></li>
</ol>
</blockquote>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e">#Creating frequency distribution of words using nltk</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> nltk.tokenize <span style="color:#f92672">import</span> sent_tokenize
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> nltk.tokenize <span style="color:#f92672">import</span> word_tokenize
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.feature_extraction.text <span style="color:#f92672">import</span> CountVectorizer
</span></span><span style="display:flex;"><span>text<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;&#34;&#34;Achievers are not afraid of Challenges, rather they relish them, thrive in them, use them. Challenges makes is stronger.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        Challenges makes us uncomfortable. If you get comfortable with uncomfort then you will grow. Challenge the challenge &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#Tokenize the sentences from the text corpus</span>
</span></span><span style="display:flex;"><span>tokenized_text<span style="color:#f92672">=</span>sent_tokenize(text)
</span></span><span style="display:flex;"><span><span style="color:#75715e">#using CountVectorizer and removing stopwords in english language</span>
</span></span><span style="display:flex;"><span>cv1<span style="color:#f92672">=</span> CountVectorizer(lowercase<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>,stop_words<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;english&#39;</span>)
</span></span><span style="display:flex;"><span><span style="color:#75715e">#fitting the tonized senetnecs to the countvectorizer</span>
</span></span><span style="display:flex;"><span>text_counts<span style="color:#f92672">=</span>cv1<span style="color:#f92672">.</span>fit_transform(tokenized_text)
</span></span><span style="display:flex;"><span><span style="color:#75715e"># printing the vocabulary and the frequency distribution pf vocabulary in tokinzed sentences</span>
</span></span><span style="display:flex;"><span>print(cv1<span style="color:#f92672">.</span>vocabulary_)
</span></span><span style="display:flex;"><span>print(text_counts<span style="color:#f92672">.</span>toarray())
</span></span></code></pre></div><p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201024214303559.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201024214303559.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201024214303559.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201024214303559.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201024214303559.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201024214303559.png"/></p>
<blockquote>
<p>each document is represented as a word-count vector, these counts can be <code>binary counts or absolute counts</code>, but the size equal to the size(Voc);</p>
<ul>
<li>huge amount of weights;</li>
<li>computationally intensive;</li>
<li><code>lack of meaningful relations and no consideration for order of words</code>;</li>
</ul>
</blockquote>
<h3 id="33-静态向量">3.3. 静态向量</h3>
<blockquote>
<p><code>translate large sparse vectors into a lower-dimensional space that preserves semantic relationships</code>.</p>
</blockquote>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201024215025229.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201024215025229.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201024215025229.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201024215025229.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201024215025229.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201024215025229.png"/></p>
<h4 id="331--latent-semantic-analysis-lsa">3.3.1.  Latent Semantic Analysis (LSA)</h4>
<h4 id="332--latent-dirichlet-allocation-lda">3.3.2.  Latent Dirichlet Allocation (LDA)</h4>
<h4 id="333-pca">3.3.3. PCA</h4>
<h4 id="334-word2vectorcbow-skip-ngram">3.3.4. Word2Vector(CBOW, Skip-ngram)</h4>
<p>In paper:&quot;<strong>Efficient Estimation of Word Representations in Vector Space</strong>&quot;:  the training complexity is proportional to $O=E<em>T</em>Q$; E is the epoch; T is the number of words in the training set, and Q is the defined further for each model architecture;</p>
<ul>
<li>
<p><strong>NNLM(Feedforward Neural Net Language Model):</strong> consists of input, projection, hidden and output layers; N previous words are encoded using 1-of-V coding, V: the vocabulary size; $N<em>D:$ the projection layer dimensionality; H: the hidden  size; the computational complexity per each example: $Q=N</em>D+N* D* H+H* V$</p>
</li>
<li>
<p><strong>Recurrent Neural Net Language Model(RNNLM):</strong> the word representations D have the same dimensionality as the hidden layer H, $Q=H<em>H+H</em>V$;</p>
</li>
<li>
<p><strong>New Log-linear Models:</strong>  neural network language model can be successfully trained in two steps: first <code>continuous word vectors are learned using simple model</code>, and then <code>N-gram NNLM is trained on the top of these distributed representations of words</code>;</p>
<ul>
<li><strong><code>Continuous Bag-of-Words Models</code>:</strong> similar to the feed forward NNLM, where the non-linear hidden layer is removed and the projection layer is shared for all words, all words projected into the same position(their vectors are averaged), the order of words in the history does not influence the projection. $Q=N<em>D+D</em>log_2(V)$;</li>
</ul>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201025125100209.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201025125100209.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201025125100209.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201025125100209.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201025125100209.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201025125100209.png"/></p>
<ul>
<li><strong><code>Continuous Skip-gram Model</code>:</strong> instead of predicting the current word based on the context, it tries to maximize classification of a word based on another word in the same sentence. In other words: use each current word as an input to a log-linear classifier with continuous projection layer and predict words within a certain range before and after the current word; $Q=C*(D+D*log_2(V))$; C: the maximum distance of the words;</li>
</ul>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201025123844292.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201025123844292.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201025123844292.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201025123844292.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201025123844292.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201025123844292.png"/></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201025123923520.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201025123923520.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201025123923520.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201025123923520.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201025123923520.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201025123923520.png"/></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201025124909812.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201025124909812.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201025124909812.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201025124909812.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201025124909812.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201025124909812.png"/></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201025122012098.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201025122012098.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201025122012098.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201025122012098.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201025122012098.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201025122012098.png"/></p>
<blockquote>
<p>New model architectures. The CBOW architecture predicts the current word based on the context, and the Skip-gram predicts surrounding words given the current word.</p>
<ul>
<li>support algebraic operations with the vector representation of words, Vector=vector(&ldquo;biggest&rdquo;)-vector(&ldquo;big&rdquo;)+vector(&ldquo;small&rdquo;)=vector(&ldquo;smallest&rdquo;)</li>
<li>when train high dimensional word vectors on large amount data, the resulting vectors can be used to answer very subtle semantic relationships between words, which is good for machine translation, information retrieval and question answering systems;</li>
</ul>
</blockquote>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201025122757936.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201025122757936.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201025122757936.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201025122757936.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201025122757936.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201025122757936.png"/></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201025122930252.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201025122930252.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201025122930252.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201025122930252.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201025122930252.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201025122930252.png"/></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201025123038373.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201025123038373.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201025123038373.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201025123038373.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201025123038373.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201025123038373.png"/></p>
</li>
</ul>
<h4 id="335-glove-embedding">3.3.5. GloVe Embedding</h4>
<blockquote>
<p>In paper: &ldquo;GloVe: Global Vectors for Word Representation&rdquo;:</p>
<ul>
<li>analyze and make explicit the model properties needed for such regularities to emerge in word vectors, and propose a new global log-bilinear regression model that combines the advantages of the two major model families in the literature: <code>global matrix factorization and local context window methods</code>;</li>
<li>efficiently leverages statistical information by training only on the nonzero elements in a <code>word-word co-occurence matrix</code>;</li>
<li>open source: <a href="http://nlp.stanford.edu/projects/glove/"target="_blank" rel="external nofollow noopener noreferrer">http://nlp.stanford.edu/projects/glove/<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>.</li>
</ul>
</blockquote>
<ul>
<li>$X$:  the matrix of word-word co-occurrence counts, $X_{ij}$: the number of times word j occurs in the context of word i;</li>
<li>$X_i=\sum_k X_{ik}$ : the number of times any word appears in the context of word i;</li>
<li>$P_{ij}=P(j|i)=x_{ij}/X_i$: the probability that word j appear in the context of word i;</li>
</ul>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201025131020291.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201025131020291.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201025131020291.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201025131020291.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201025131020291.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201025131020291.png"/></p>
<blockquote>
<p>The ratio is better able to distinguish relevant words (solid and gas) from irrelevant words (water and fashion) and it is also better able to discriminate between the two relevant words.</p>
</blockquote>
<ul>
<li>$F(w_i, w_j, w_k)=P_{ik}/P_{jk}$:   w: the word vector; $w_k$: the separate context word vectors;</li>
<li>loss function:  <img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201025134201555.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201025134201555.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201025134201555.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201025134201555.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201025134201555.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201025134201555.png"/></li>
</ul>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201025131931244.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201025131931244.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201025131931244.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201025131931244.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201025131931244.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201025131931244.png"/></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201025132137995.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201025132137995.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201025132137995.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201025132137995.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201025132137995.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201025132137995.png"/></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201025132259638.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201025132259638.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201025132259638.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201025132259638.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201025132259638.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201025132259638.png"/></p>
<p><strong>改良 SkipGram</strong></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201025132437364.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201025132437364.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201025132437364.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201025132437364.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201025132437364.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201025132437364.png"/></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201025132513525.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201025132513525.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201025132513525.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201025132513525.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201025132513525.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201025132513525.png"/></p>
<blockquote>
<ul>
<li>word2vec是局部语料库训练的，其特征提取是基于滑窗的；而glove的滑窗是为了构建co-occurance matrix，是基于全局语料的，可见glove需要事先统计共现概率；因此，word2vec可以进行在线学习，glove则需要统计固定语料信息。</li>
<li>word2vec是无监督学习，同样由于不需要人工标注；glove通常被认为是无监督学习，但实际上glove还是有label的，即共现次数<img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://www.zhihu.com/equation?tex=log%28X_%7Bij%7D%29"
    data-srcset="https://www.zhihu.com/equation?tex=log%28X_%7Bij%7D%29, https://www.zhihu.com/equation?tex=log%28X_%7Bij%7D%29 1.5x, https://www.zhihu.com/equation?tex=log%28X_%7Bij%7D%29 2x"
    data-sizes="auto"
    alt="[公式]"
    title="[公式]"/>。</li>
<li>word2vec损失函数实质上是带权重的交叉熵，权重固定；glove的损失函数是最小平方损失函数，权重可以做映射变换。</li>
<li>总体来看，<strong>glove可以被看作是更换了目标函数和权重函数的全局word2vec</strong>。</li>
</ul>
</blockquote>
<h4 id="336-fasttext">3.3.6. FastText</h4>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201126120608705.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201126120608705.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201126120608705.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201126120608705.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201126120608705.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201126120608705.png"/></p>
<h3 id="34--动态向量">3.4.  动态向量</h3>
<blockquote>
<p>由于静态向量表示中每个词被表示成一个固定的向量，无法有效解决一词多义的问题。在动态向量表示中，模型不再是向量对应关系，而是一个训练好的模型。</p>
</blockquote>
<h4 id="341-elmo">3.4.1. ELMo</h4>
<ul>
<li>(1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy);</li>
<li>Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pretrained on a large text corpus.</li>
<li>ELMo（Embeddings from Language Models）是2018年3月发表，获得了NAACL18的Best Paper</li>
</ul>
<blockquote>
<ol>
<li>预训练biLM模型，通常由两层bi-LSTM组成，之间用residual connection连接起来。</li>
<li>在任务语料上fine tuning上一步得到的biLM模型，这里可以看做是biLM的domain transfer。</li>
<li><code>利用ELMo提取word embedding</code>，将word embedding<code>作为输入</code>来对任务进行训练。</li>
</ol>
</blockquote>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201126120802633.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201126120802633.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201126120802633.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201126120802633.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201126120802633.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201126120802633.png"/></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201126121055516.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201126121055516.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201126121055516.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201126121055516.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201126121055516.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201126121055516.png"/></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201126121352272.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201126121352272.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201126121352272.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201126121352272.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201126121352272.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201126121352272.png"/></p>
<h4 id="342-gpt">3.4.2. GPT</h4>
<blockquote>
<p>GPT-1（Generative Pre-Training）是OpenAI在2018年提出的，采用pre-training和fine-tuning的下游统一框架，将预训练和finetune的结构进行了统一，解决了之前两者分离的使用的不确定性（例如ELMo）。此外，GPT使用了Transformer结构克服了LSTM不能捕获远距离信息的缺点。GPT主要分为两个阶段：pre-training和fine-tuning.</p>
</blockquote>
<ul>
<li><strong>Pre-training</strong></li>
</ul>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201126132635179.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201126132635179.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201126132635179.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201126132635179.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201126132635179.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201126132635179.png"/></p>
<ul>
<li><strong>Fine-tuning</strong> :采用无监督学习预训练好模型后后，可以把模型模型迁移到新的任务中，并根据新任务来调整模型的参数。</li>
</ul>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201126132849556.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201126132849556.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201126132849556.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201126132849556.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201126132849556.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201126132849556.png"/></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201126132916870.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201126132916870.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201126132916870.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201126132916870.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201126132916870.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201126132916870.png"/></p>
<h4 id="343-bertbidirectional-encoder-representations-from-transformers">3.4.3. BERT(Bidirectional Encoder Representations from Transformers)</h4>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201126133039218.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201126133039218.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201126133039218.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201126133039218.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201126133039218.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201126133039218.png"/></p>
<blockquote>
<p>BERT进一步增强了词向的型泛化能力，充分描述字符级、词级、句子级甚至句间的关系特征。BERT的输入的编码向量（长度为512）是3种Embedding特征element-wise和.</p>
</blockquote>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201126133152652.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201126133152652.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201126133152652.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201126133152652.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201126133152652.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201126133152652.png"/></p>
<ul>
<li>
<p><strong>Input Features:</strong></p>
<ul>
<li>Token Embedding (WordPiece)：将<code>单词划分成一组有限的公共词单元</code>，能在单词的有效性和字符的灵活性之间取得一个折中的平衡。如图中的“playing”被拆分成了“play”和“ing”；</li>
<li>Segment Embedding：用于<code>区分两个句子</code>，如B是否是A的下文（对话场景，问答场景等）。对于<code>句子对，第一个句子的特征值是0，第二个句子的特征值是1</code>；</li>
<li>Position Embedding：<code>将单词的位置信息编码成特征向量</code>，Position embedding能有效将单词的位置关系引入到模型中，提升模型对句子理解能力；</li>
<li><code>[CLS]</code>表示该特征用于分类模型，对非分类模型，该符合可以省去。<code>[SEP]</code>表示分句符号，用于断开输入语料中的两个句子。</li>
</ul>
</li>
<li>
<p><strong>Masked Language Model(MLM)</strong></p>
</li>
</ul>
<blockquote>
<p>是指在<code>训练时随机从输入语料中mask掉一些单词</code>，然后通过该词上下文来预测它（非常像让模型来做完形填空）。80%<code>概率直接替换为</code>[MASK]； <code>10%</code>概率替换为其他任意Token； <code>10%</code>概率保留为原始Token 。</p>
</blockquote>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201126133616522.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201126133616522.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201126133616522.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201126133616522.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201126133616522.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201126133616522.png"/></p>
<ul>
<li><strong>Next Sentence Predictions（NSP）</strong></li>
</ul>
<blockquote>
<p>BERT采用NSP任务来增强模型对句子关系的理解，即给出两个句子A、B，模型预测B是否是A的下一句。</p>
</blockquote>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201126133832576.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201126133832576.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201126133832576.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201126133832576.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201126133832576.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201126133832576.png"/></p>
<ul>
<li><strong>Fine-tuning</strong>
<ul>
<li>句对关系判断：第一个起始符号[CLS]经过编码后，增加Softmax层，即可用于分类；</li>
<li>单句分类任务：实现同“句对关系判断”；</li>
<li><code>问答类任务</code>：问答系统输入文本序列的question和包含answer的段落，并在序列中标记answer，让BERT模型学习标记answer开始和结束的向量来训练模型；</li>
<li>序列标准任务：识别系统输入标记好实体类别（人、组织、位置、其他无名实体）文本序列进行微调训练，识别实体类别时，将序列的每个Token向量送到预测NER标签的分类层进行识别。</li>
</ul>
</li>
</ul>
<h4 id="345-unilm">3.4.5. UniLM</h4>
<p>给定一个输入序列$x=x_1&hellip;x_n$，UniLM 通过下图的方式获取每个词条的基于上下文的向量表示。整个预训练过程利用单向的语言建模（unidirectional LM），双向的语言建模（bidirectional LM）和 Seq2Seq 语言建模（sequence-to-sequence LM）优化共享的 Transformer 网络。</p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201128145004244.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201128145004244.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201128145004244.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201128145004244.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201128145004244.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201128145004244.png"/></p>
<h3 id="35-graph-embedding">3.5. Graph Embedding</h3>
<blockquote>
<p>Graph Embedding是一种将图结构数据映射为低微稠密向量的过程，从而捕捉到图的拓扑结构、顶点与顶点的关系、以及其他的信息。目前，Graph Embedding方法大致可以分为两大类：1）浅层图模型；2）深度图模型。**图嵌入（Graph / Network Embedding）<strong>和</strong>图神经网络（Graph Neural Networks, GNN）**是两个类似的研究领域。<code>图嵌入旨在将图的节点表示成一个低维向量空间</code>，同时<code>保留网络的拓扑结构和节点信息</code>，以便在后续的图分析任务中可以直接使用现有的机器学习算法。</p>
</blockquote>
<h4 id="351-浅层图">3.5.1. 浅层图</h4>
<blockquote>
<p>浅层图模型主要是采用<code>random-walk + skip-gram</code>模式的embedding方法。主要是通过在图中采用<code>随机游走策略来生成多条节点列表</code>，然后将<code>每个列表相当于含有多个单词（图中的节点）的句子</code>，再用<code>skip-gram模型</code>来训练每个<code>节点的向量</code>。这些方法主要包括<code>DeepWalk、Node2vec、Metapath2vec</code>等。</p>
</blockquote>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201128145849897.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201128145849897.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201128145849897.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201128145849897.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201128145849897.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201128145849897.png"/></p>
<h5 id="1-deepwalk">1. DeepWalk</h5>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201126134723219.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201126134723219.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201126134723219.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201126134723219.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201126134723219.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201126134723219.png"/></p>
<p>到达节点后，下一步遍历其邻居节点的概率：</p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201126134840114.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201126134840114.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201126134840114.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201126134840114.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201126134840114.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201126134840114.png"/></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201126134933080.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201126134933080.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201126134933080.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201126134933080.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201126134933080.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201126134933080.png"/></p>
<h5 id="2-node2vec">2. Node2vec</h5>
<blockquote>
<p>该模型通过调整random walk权重的方法使得节点的embedding向量更倾向于体现网络的同质性或结构性。Node2vec无法指定游走路径，且仅适用于解决只包含一种类型节点的同构网络，无法有效表示包含多种类型节点和边类型的复杂网络。</p>
</blockquote>
<ul>
<li><code>同质性</code>：指得是<code>距离相近的节点的embedding向量应近似</code>，如下图中，与节点相连的节点s1、s2、s3和s4的embedding向量应相似。为了使embedding向量能够表达网络的同质性，需要让随机游走更倾向于<code>DFS</code>，因为DFS更有可能通过多次跳转，到达远方的节点上，使游走序列集中在一个较大的集合内部，使得在一个集合内部的节点具有更高的相似性，从而表达图的同质性。</li>
<li><code>结构性</code>：<code>结构相似的节点的embedding向量应近似</code>，如下图中，与节点结构相似的节点的embedding向量应相似。为了表达结构性，需要随机游走更倾向于<code>BFS</code>，因为BFS会更多的在当前节点的邻域中游走，相当于对当前节点的网络结构进行扫描，从而使得embedding向量能刻画节点邻域的结构信息。</li>
</ul>
<h5 id="httpslddpictureoss-cn-beijingaliyuncscompictureimage-20201126135506214png3-metapath2vec"><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201126135506214.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201126135506214.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201126135506214.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201126135506214.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201126135506214.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201126135506214.png"/>3. Metapath2vec</h5>
<blockquote>
<p>主要是在随机游走上使用基于meta-path的random walk来构建节点序列，然后用Skip-gram模型来完成顶点的Embedding。</p>
</blockquote>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201126140116288.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201126140116288.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201126140116288.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201126140116288.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201126140116288.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201126140116288.png"/></p>
<h5 id="4-app">4. APP</h5>
<blockquote>
<p>DeepWalk，node2vec 等，都无法保留图中的非对称信息。然而非对称性在很多问题，例如：社交网络中的链路预测、电商中的推荐等，中至关重要。</p>
</blockquote>
<h4 id="352-深度图">3.5.2. 深度图</h4>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210608161356940.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210608161356940.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210608161356940.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210608161356940.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210608161356940.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210608161356940.png"/></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/propa_step.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/propa_step.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/propa_step.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/propa_step.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/propa_step.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/propa_step.png"/></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/table-1623202159387.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/table-1623202159387.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/table-1623202159387.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/table-1623202159387.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/table-1623202159387.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/table-1623202159387.png"/></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/gnn_table.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/gnn_table.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/gnn_table.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/gnn_table.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/gnn_table.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/gnn_table.png"/></p>
<blockquote>
<p>将图与深度模型结合，实现end-to-end训练模型，从而在图中提取拓扑图的空间特征。主要分为四大类：<code>Graph Convolution Networks (GCN)</code>，<code>Graph Attention Networks (GAT)</code>，<code>Graph AutoEncoder (GAE)</code>和<code>Graph Generative Networks (GGN)</code>。</p>
</blockquote>
<ul>
<li>基于<strong>spatial domain</strong>：基于<code>空域卷积</code>的方法直接<code>将卷积操作定义在每个结点的连接关系上</code>，跟传统的卷积神经网络中的卷积更相似一些。主要有两个问题：1）按照什么条件去<code>找</code>中心节点的邻居，也就是如何确定receptive field；2）按照什么方式<code>处理</code>包含不同数目邻居的特征。</li>
<li>基于<strong>spectral domain</strong>：借助卷积定理可以通过定义<code>频谱域上</code>的<code>内积</code>操作来得到<code>空间域图</code>上的卷积操作。</li>
</ul>
<h5 id="0-gnn">0. GNN</h5>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201128151414174.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201128151414174.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201128151414174.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201128151414174.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201128151414174.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201128151414174.png"/></p>
<p>令 $H,O,X,X_n$  分别表示为 状态， 输出，特征，所有节点特征的向量:
$$
\begin{align}
H&amp;7=F(H,X)\
O&amp;=G(H,X_n)\
H^{t+1}&amp;=F(H^t,X) \
loss&amp;=\sum{i=1}^p(t_i-o_i)
\end{align}
$$</p>
<h5 id="1-gcn">1. GCN</h5>
<blockquote>
<p>核心思想在于学习一个函数 $f$，通过聚合节点 $v_i$ <code>自身的特征</code> $X_i$ 和<code>邻居的特征</code> $X_j $获得节点的表示，其中 $j∈N(v_i) $为节点的邻居。</p>
</blockquote>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201128151958945.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201128151958945.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201128151958945.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201128151958945.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201128151958945.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201128151958945.png"/></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201128152016958.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201128152016958.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201128152016958.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201128152016958.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201128152016958.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201128152016958.png"/></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201128152040805.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201128152040805.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201128152040805.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201128152040805.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201128152040805.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201128152040805.png"/></p>
<h6 id="11-基于频谱spectral-methods">1.1. 基于频谱（Spectral Methods）</h6>
<blockquote>
<p>(1)<code>对图结构的小小扰动将会导致不同的特征基</code>;(2)特征分解需要<code>较为庞大的计算代价</code>;(3)学习到<code>的滤波器是针对特定问题的,不能够将其进行推广到更丰富的图结构上</code>.ChebNet及其一阶近似是局部卷积操作,从而可以在图的不同位置共享相同的滤波器参数.  基于频谱方法的一个关键缺陷是其需要将整个图的信息载入内存中,这使得其在大规模的图结构(如大规模的社交网络分析)上不能有效的进行应用.</p>
</blockquote>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201128152717815.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201128152717815.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201128152717815.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201128152717815.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201128152717815.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201128152717815.png"/></p>
<h6 id="1-频谱卷积神经网络">.1. 频谱卷积神经网络</h6>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210608160028439.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210608160028439.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210608160028439.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210608160028439.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210608160028439.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210608160028439.png"/></p>
<h6 id="2-契比雪夫频谱卷积网络chebnet">.2. 契比雪夫频谱卷积网络(ChebNet)</h6>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210608160042565.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210608160042565.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210608160042565.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210608160042565.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210608160042565.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210608160042565.png"/></p>
<h6 id="3-chebnet的一阶近似">.3. ChebNet的一阶近似</h6>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210608160110144.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210608160110144.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210608160110144.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210608160110144.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210608160110144.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210608160110144.png"/></p>
<h6 id="12-基于空间的方法spatial-methods">1.2. 基于空间的方法（Spatial Methods）</h6>
<blockquote>
<p>基于空间的方法通过节点的空间关系来定义图卷积操作。为了将图像和图关联起来，可以将图像视为一个特殊形式的图，每个像素点表示一个节点.</p>
</blockquote>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201128152918580.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201128152918580.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201128152918580.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201128152918580.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201128152918580.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201128152918580.png"/></p>
<h5 id="2-grn">2. GRN</h5>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201128153113989.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201128153113989.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201128153113989.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201128153113989.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201128153113989.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201128153113989.png"/></p>
<p>节点 $v$ 首先从邻居汇总信息，其中 $A_v $为图邻接矩阵$ A$ 的子矩阵表示节点$ v$ 及其邻居的连接。类似 GRU 的更新函数，通过结合其他节点和上一时间的信息更新节点的隐状态。$a $用于获取节点 $v $邻居的信息，$z $和 $r$ 分别为更新和重置门。</p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201128153323524.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201128153323524.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201128153323524.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201128153323524.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201128153323524.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201128153323524.png"/></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201128153337702.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201128153337702.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201128153337702.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201128153337702.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201128153337702.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201128153337702.png"/></p>
<h5 id="3-graph-attention">3. Graph Attention</h5>
<blockquote>
<p>与 GCN 对于节点所有的邻居平等对待相比，注意力机制可以<code>为每个邻居分配不同的注意力评分</code>，从而识别更重要的邻居。</p>
</blockquote>
<h5 id="4-graphsage">4. <strong>GraphSAGE</strong></h5>
<blockquote>
<p>GraphSAGE（Graph SAmple and aggreGatE）是基于空间域方法，其思想与基于频谱域方法相反，是直接在图上定义卷积操作，对<code>空间上相邻的节点</code>上进行运算。其计算流程主要分为三部：</p>
<ul>
<li><code>对图中每个节点领节点进行采样</code></li>
<li>根据<code>聚合函数聚合邻居节点信息（特征）</code></li>
<li>得到图中<code>各节点的embedding向量</code>，供下游任务使用</li>
</ul>
</blockquote>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201126145200343.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201126145200343.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201126145200343.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201126145200343.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201126145200343.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201126145200343.png"/></p>
<h5 id="5-dngr">5. DNGR</h5>
<blockquote>
<p>一种利用基于 Stacked Denoising Autoencoder（SDAE）提取特征的网络表示学习算法。</p>
</blockquote>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201128150733829.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201128150733829.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201128150733829.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201128150733829.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201128150733829.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201128150733829.png"/></p>
<h4 id="3-research-area">3. Research Area</h4>
<blockquote>
<p>图分析任务可以大致抽象为以下四类: ( a )节点分类，( b )链接预测，( c )聚类，以及( d )可视化。</p>
</blockquote>
<h4 id="1-姿态识别预测">.1. 姿态识别&amp;预测</h4>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201128150910711.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201128150910711.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201128150910711.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201128150910711.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201128150910711.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201128150910711.png"/></p>
<h4 id="2-超图">.2. 超图</h4>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210608161529742.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210608161529742.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210608161529742.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210608161529742.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210608161529742.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210608161529742.png"/></p>
<h4 id="3-图构建">.3. 图构建</h4>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210608161558819.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210608161558819.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210608161558819.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210608161558819.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210608161558819.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210608161558819.png"/></p>
<h4 id="4-子图嵌入">.4. 子图嵌入</h4>
<blockquote>
<p>图嵌入（Graph Embedding，也叫Network Embedding）是一种将图数据（通常为高维稠密的矩阵）映射为低微稠密向量的过程，能够很好地解决图数据难以高效输入机器学习算法的问题。图嵌入是将属性图转换为向量或向量集。嵌入应该捕获图的<strong>拓扑结构、顶点到顶点的关系以及关于图、子图和顶点的其他相关信息</strong>。</p>
<ul>
<li>
<p>节点的分布式表示；节点之间的相似性表示链接强度； 编码网络信息并生成节点表示</p>
</li>
<li>
<p>顶点嵌入:每个顶点(节点)用其自身的向量表示进行编码。这种嵌入一般用于在顶点层次上执行可视化或预测。比如，在2D平面上显示顶点，或者基于顶点相似性预测新的连接。</p>
</li>
<li>
<p>图嵌入:用单个向量表示整个图。这种嵌入用于在图的层次上做出预测，可者想要比较或可视化整个图。例如，比较化学结构。</p>
</li>
</ul>
</blockquote>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201128150931910.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201128150931910.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201128150931910.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201128150931910.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201128150931910.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201128150931910.png"/></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201128150955897.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201128150955897.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201128150955897.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201128150955897.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201128150955897.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201128150955897.png"/></p>
<h2 id="4-视文分析">4. 视文分析</h2>
<h3 id="41-预训练模型">4.1. 预训练模型</h3>
<ul>
<li>**模型越来越大。**比如 Transformer 的层数变化，从12层的 Base 模型到24层的 Large 模型。导致模型的参数越来越大，比如 GPT 110 M，到 GPT-2 是1.5 Billion，图灵是 17 Billion，而 GPT-3 达到了惊人的 175 Billion。一般而言模型大了，其能力也会越来越强，但是训练代价确实非常大。</li>
<li><strong>预训练方法也在不断增加</strong>，从自回归 LM，到自动编码的各种方法，以及各种多任务训练等。</li>
<li><strong>，还有从语言、多语言到多模态不断演进</strong>。最后就是模型压缩，使之能在实际应用中经济的使用，比如在手机端。这就涉及到知识蒸馏和 teacher-student models，把大模型作为 teacher，让一个小模型作为 student 来学习，接近大模型的能力，但是模型的参数减少很多。</li>
</ul>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201126150533100.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201126150533100.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201126150533100.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201126150533100.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201126150533100.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201126150533100.png"/></p>
<h4 id="411-layoutlm">4.1.1. LayoutLM</h4>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201126150648546.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201126150648546.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201126150648546.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201126150648546.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201126150648546.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201126150648546.png"/></p>
<ul>
<li>
<p><code>二维位置嵌入 2-D Position Embedding</code>：根据 <code>OCR</code> 获得的文本边界框 (Bounding Box)，能获取文本在文档中的具体位置。在将对应坐标转化为虚拟坐标之后，则可以计算该坐标对应在 <code>x、y、w、h</code> 四个 Embedding 子层的表示，最终的<code> 2-D Position Embedding 为四个子层的 Embedding 之和</code>。</p>
</li>
<li>
<p><code>图嵌入 Image Embedding</code>：将每个文本相应的边界框 (Bounding Box) 当作 Faster R-CNN 中的候选框（Proposal），从而提取对应的局部特征。其特别之处在于，由于 <code>[CLS] 符号用于表示整个输入文本的语义</code>，所以同样<code>使用整张文档图像作为该位置的 Image Embedding</code>，从而保持模态对齐。</p>
</li>
<li>
<p>预训练：</p>
<ul>
<li><code>掩码视觉语言模型</code>（Masked Visual-Language Model，MVLM）：大量实验已经证明 MLM 能够在预训练阶段有效地进行自监督学习。研究员们在此基础上进行了修改：<code>在遮盖当前词之后，保留对应的 2-D Position Embedding 暗示，让模型预测对应的词</code>。在这种方法下，模型根据已有的上下文和对应的视觉暗示预测被掩码的词，从而让模型更好地学习文本位置和文本语义的模态对齐关系。</li>
<li><code>多标签文档分类</code>（Multi-label Document Classification，MDC）：MLM 能够有效的表示词级别的信息，但是对于文档级的表示，还需要将文档级的预训练任务引入更高层的语义信息。在预训练阶段研究员们使用的 <code>IIT-CDIP 数据集为每个文档提供了多标签的文档类型标注</code>，并引入 <code>MDC 多标签文档分类任务</code>。该任务使得模型可以利用这些监督信号，聚合相应的文档类别并捕捉文档类型信息，从而获得更有效的高层语义表示。</li>
</ul>
</li>
</ul>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201126151137411.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201126151137411.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201126151137411.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201126151137411.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201126151137411.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201126151137411.png"/></p>
<h2 id="5-学习链接">5. 学习链接</h2>
<ul>
<li>
<p><a href="https://zhuanlan.zhihu.com/p/39562499"target="_blank" rel="external nofollow noopener noreferrer">https://zhuanlan.zhihu.com/p/39562499<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
</li>
<li>
<p><a href="https://zhuanlan.zhihu.com/p/101179171"target="_blank" rel="external nofollow noopener noreferrer">https://zhuanlan.zhihu.com/p/101179171<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
</li>
<li>
<p><a href="https://mp.weixin.qq.com/s?__biz=MzIyNDY5NjEzNQ==&amp;mid=2247486604&amp;idx=2&amp;sn=4805abb34e3a94243bb182ec74fff550&amp;chksm=e80a4ea4df7dc7b244e39535ae6adf34eece9d10e428e07d52c9675d2629fe111c58e158127c&amp;scene=126&amp;sessionid=1606356123&amp;key=d29f68c0cc2770c7a295460b63af1f2b438bd8d34e43fd923ffab1503f5419358cb801a0ec1cb521815173b06edbb8b4391e82bac9ca99bd324a470299662d0074b19f1c710ebb605f21ad6653f596049aeffc3ecf67018c3a6e1f06e4d967a80aa1117ee0badb2637c6f98ab2b9a0158f340990857e17d4ffc094708a5f04b9&amp;ascene=1&amp;uin=MzE0ODMxOTQzMQ%3D%3D&amp;devicetype=Windows&#43;10&#43;x64&amp;version=6300002f&amp;lang=zh_CN&amp;exportkey=A6O1Vab3FB0dKfbgjPLgUOI%3D&amp;pass_ticket=dWuSAMKgl2YK7zg1wPn7XPBZPohIpbR0IPLY%2Fi1CvZ%2B0Hp9NIxue%2FHPzD4K1r4vD&amp;wx_header=0"target="_blank" rel="external nofollow noopener noreferrer">用万字长文聊一聊 Embedding 技术 (qq.com)<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
</li>
<li>
<p>“Document Visual Question Answering”：https://medium.com/@anishagunjal7/document-visual-question-answering-e6090f3bddee</p>
</li>
<li>
<p>LayoutLM 论文：https://arxiv.org/abs/1912.13318</p>
</li>
<li>
<p>LayoutLM 代码&amp;模型：https://aka.ms/layoutlm</p>
</li>
<li>
<p><a href="https://leovan.me/cn/2020/04/graph-embedding-and-gnn/"target="_blank" rel="external nofollow noopener noreferrer">https://leovan.me/cn/2020/04/graph-embedding-and-gnn/<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
</li>
</ul>
</div>
<div class="post-footer" id="post-footer">
  <div class="post-info">
    <div class="post-info-line">
      <div class="post-info-mod">
        <span title=2023-12-31&#32;13:47:03>更新于 2023-12-31&nbsp;</span>
      </div><div class="post-info-license">
          <span><a rel="license external nofollow noopener noreferrer" href="https://creativecommons.org/licenses/by-nc/4.0/" target="_blank">CC BY-NC 4.0</a></span>
        </div></div>
    <div class="post-info-line">
      <div class="post-info-md"><span><a href="/wordembedding/index.md" title="阅读原始文档" class="link-to-markdown">阅读原始文档</a></span><span><a href="https://liudongdong1.github.io/edit/master/content/posts%5c%e8%87%aa%e7%84%b6%e8%af%ad%e8%a8%80%5cWordEmbedding.md" title="编辑此页"target="_blank" rel="external nofollow noopener noreferrer" class="link-to-edit">编辑此页</a></span></div>
      <div class="post-info-share">
        <span><a href="javascript:void(0);" title="分享到 Twitter" data-sharer="twitter" data-url="https://liudongdong1.github.io/wordembedding/" data-title="WordEmbedding" data-hashtags="embedding"><i class="fa-brands fa-twitter fa-fw" aria-hidden="true"></i></a>
  <a href="javascript:void(0);" title="分享到 Facebook" data-sharer="facebook" data-url="https://liudongdong1.github.io/wordembedding/" data-hashtag="embedding"><i class="fa-brands fa-facebook-square fa-fw" aria-hidden="true"></i></a>
  <a href="javascript:void(0);" title="分享到 微博" data-sharer="weibo" data-url="https://liudongdong1.github.io/wordembedding/" data-title="WordEmbedding" data-image="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/view-of-coffee-beans.jpg"><i class="fa-brands fa-weibo fa-fw" aria-hidden="true"></i></a>
  </span>
      </div>
    </div>
  </div>

  <div class="post-info-more">
    <section class="post-tags"><i class="fa-solid fa-tags fa-fw" aria-hidden="true"></i>&nbsp;<a href="/tags/embedding/">embedding</a></section>
    <section>
      <span><a href="javascript:void(0);" onclick="window.history.back();">返回</a></span>&nbsp;|&nbsp;<span><a href="/">主页</a></span>
    </section>
  </div>

  <div class="post-nav"><a href="/relationextraction/" class="prev" rel="prev" title="RelationExtraction"><i class="fa-solid fa-angle-left fa-fw" aria-hidden="true"></i>RelationExtraction</a>
      <a href="/allennlpintroduce/" class="next" rel="next" title="AllenNLPIntroduce">AllenNLPIntroduce<i class="fa-solid fa-angle-right fa-fw" aria-hidden="true"></i></a></div>
</div>
</article></main><footer class="footer">
    <div class="footer-container"><div class="footer-line powered">由 <a href="https://gohugo.io/" target="_blank" rel="external nofollow noopener noreferrer" title="Hugo 0.118.2">Hugo</a> 强力驱动 | 主题 - <a href="https://github.com/hugo-fixit/FixIt" target="_blank" rel="external" title="FixIt v0.2.17-RC"><img class="fixit-icon" src="/fixit.min.svg" alt="FixIt logo" />&nbsp;FixIt</a>
        </div><div class="footer-line copyright" itemscope itemtype="http://schema.org/CreativeWork"><i class="fa-regular fa-copyright fa-fw" aria-hidden="true"></i>
            <span itemprop="copyrightYear">2020 - 2023</span><span class="author" itemprop="copyrightHolder">
              <a href="https://liudongdong1.github.io/"target="_blank" rel="external nofollow noopener noreferrer">LiuDongdong</a></span><span class="license footer-divider"><a rel="license external nofollow noopener noreferrer" href="https://creativecommons.org/licenses/by-nc/4.0/" target="_blank">CC BY-NC 4.0</a></span></div><div class="footer-line statistics"><span class="site-time" title='网站运行中 ...'><i class="fa-solid fa-heartbeat fa-fw animate-icon" aria-hidden="true"></i>&nbsp;<span class="run-times">网站运行中 ...</span></span></div><div class="footer-line ibruce">
          <span id="busuanzi_container_site_uv" title='总访客数'><i class="fa-regular fa-user fa-fw" aria-hidden="true"></i>&nbsp;<span id="busuanzi_value_site_uv"><i class="fa-solid fa-spinner fa-spin fa-fw" aria-hidden="true"></i></span></span><span id="busuanzi_container_site_pv" class="footer-divider" title='总访问量'><i class="fa-regular fa-eye fa-fw" aria-hidden="true"></i>&nbsp;<span id="busuanzi_value_site_pv"><i class="fa-solid fa-spinner fa-spin fa-fw" aria-hidden="true"></i></span></span>
        </div></div>
  </footer></div><div class="widgets"><div class="fixed-buttons animate__faster d-none"><div class="fixed-button back-to-top" role="button" aria-label="回到顶部"><i class="fa-solid fa-arrow-up fa-fw" aria-hidden="true"></i><span class="variant-numeric">0%</span>
        </div></div><a href="https://liudongdong1.github.io/" title="在 GitHub 上查看源代码"target="_blank" rel="external nofollow" class="github-corner right d-none-mobile"><svg viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><div id="mask"></div><div class="reading-progress-bar" style="left: 0;top: 0;--bg-progress: #0076ff;--bg-progress-dark: #fff;"></div><noscript>
    <div class="noscript-warning">FixIt 主题在启用 JavaScript 的情况下效果最佳。</div>
  </noscript>
</div><link rel="stylesheet" href="/lib/katex/katex.min.css"><link rel="stylesheet" href="/lib/cookieconsent/cookieconsent.min.css"><script src="/lib/autocomplete/autocomplete.min.js" defer></script><script src="/lib/algoliasearch/algoliasearch-lite.umd.min.js" defer></script><script src="/lib/lazysizes/lazysizes.min.js" async defer></script><script src="/lib/sharer/sharer.min.js" async defer></script><script src="/lib/typeit/index.umd.js" defer></script><script src="/lib/katex/katex.min.js" defer></script><script src="/lib/katex/auto-render.min.js" defer></script><script src="/lib/katex/copy-tex.min.js" defer></script><script src="/lib/katex/mhchem.min.js" defer></script><script src="/lib/cookieconsent/cookieconsent.min.js" defer></script><script src="/lib/pangu/pangu.min.js" defer></script><script src="/lib/cell-watermark/watermark.min.js" defer></script><script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async defer></script><script>window.config={"code":{"copyTitle":"复制到剪贴板","editLockTitle":"锁定可编辑代码块","editUnLockTitle":"解锁可编辑代码块","editable":true,"maxShownLines":10},"comment":{"enable":false},"cookieconsent":{"content":{"dismiss":"同意","link":"了解更多","message":"本网站使用 Cookies 来改善您的浏览体验。"},"enable":true,"palette":{"button":{"background":"#f0f0f0"},"popup":{"background":"#1aa3ff"}},"theme":"edgeless"},"data":{"typeit-header-subtitle-desktop":"\u003cspan style='font-family: MMT,\"沐目体\";'\u003e吾日三省吾身\u003c/span\u003e","typeit-header-subtitle-mobile":"\u003cspan style='font-family: MMT,\"沐目体\";'\u003e吾日三省吾身\u003c/span\u003e"},"enablePWA":true,"enablePangu":true,"math":{"delimiters":[{"display":true,"left":"$$","right":"$$"},{"display":true,"left":"\\[","right":"\\]"},{"display":true,"left":"\\begin{equation}","right":"\\end{equation}"},{"display":true,"left":"\\begin{equation*}","right":"\\end{equation*}"},{"display":true,"left":"\\begin{align}","right":"\\end{align}"},{"display":true,"left":"\\begin{align*}","right":"\\end{align*}"},{"display":true,"left":"\\begin{alignat}","right":"\\end{alignat}"},{"display":true,"left":"\\begin{alignat*}","right":"\\end{alignat*}"},{"display":true,"left":"\\begin{gather}","right":"\\end{gather}"},{"display":true,"left":"\\begin{CD}","right":"\\end{CD}"},{"display":false,"left":"$","right":"$"},{"display":false,"left":"\\(","right":"\\)"}],"strict":false},"search":{"algoliaAppID":"2R1K9SKLQZ","algoliaIndex":"index.zh-cn","algoliaSearchKey":"4a226aa1c5c98d6859e4d1386adb2bc7","highlightTag":"em","maxResultLength":10,"noResultsFound":"没有找到结果","snippetLength":50,"type":"algolia"},"siteTime":"2020-12-18T16:15:22+08:00","typeit":{"cursorChar":"|","cursorSpeed":1000,"data":{"typeit-header-subtitle-desktop":["typeit-header-subtitle-desktop"],"typeit-header-subtitle-mobile":["typeit-header-subtitle-mobile"]},"duration":-1,"speed":100},"watermark":{"appendto":".wrapper\u003emain","colspacing":30,"content":"\u003cimg class=\"fixit-icon\" src=\"/fixit.min.svg\" alt=\"FixIt logo\" /\u003e FixIt 主题","enable":true,"fontfamily":"inherit","fontsize":0.85,"height":21,"opacity":0.0125,"rotate":15,"rowspacing":60,"width":150}};</script><script src="/js/theme.min.js" defer></script><script src="/js/custom.min.js" defer></script></body>
</html>
