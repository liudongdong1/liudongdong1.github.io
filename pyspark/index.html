<!DOCTYPE html>
<html itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">
  <head>
    
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
    <meta name="robots" content="noodp" />
    <title>PySpark - DAY By DAY</title><meta name="author" content="LiuDongdong">
<meta name="author-link" content="https://liudongdong1.github.io/">
<meta name="description" content="Apart from real-time and batch processing, Apache Spark supports interactive queries and iterative algorithms. Using PySpark, you can work with RDDs in Python programming language also. It is because of a library called Py4j that they are able to achieve this. Resilient Distributed Dataset (RDD): RDD is an immutable (read-only), fundamental collection of elements or items that can be operated on many devices at the same time (parallel processing). Each" /><meta name="keywords" content='stream, spark' /><meta itemprop="name" content="PySpark">
<meta itemprop="description" content="Apart from real-time and batch processing, Apache Spark supports interactive queries and iterative algorithms. Using PySpark, you can work with RDDs in Python programming language also. It is because of a library called Py4j that they are able to achieve this. Resilient Distributed Dataset (RDD): RDD is an immutable (read-only), fundamental collection of elements or items that can be operated on many devices at the same time (parallel processing). Each"><meta itemprop="datePublished" content="2020-07-13T21:59:57+00:00" />
<meta itemprop="dateModified" content="2023-12-31T16:55:50+08:00" />
<meta itemprop="wordCount" content="5161"><meta itemprop="image" content="https://liudongdong1.github.io/logo.png"/>
<meta itemprop="keywords" content="stream,spark," /><meta property="og:title" content="PySpark" />
<meta property="og:description" content="Apart from real-time and batch processing, Apache Spark supports interactive queries and iterative algorithms. Using PySpark, you can work with RDDs in Python programming language also. It is because of a library called Py4j that they are able to achieve this. Resilient Distributed Dataset (RDD): RDD is an immutable (read-only), fundamental collection of elements or items that can be operated on many devices at the same time (parallel processing). Each" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://liudongdong1.github.io/pyspark/" /><meta property="og:image" content="https://liudongdong1.github.io/logo.png"/><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2020-07-13T21:59:57+00:00" />
<meta property="article:modified_time" content="2023-12-31T16:55:50+08:00" />
<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://liudongdong1.github.io/logo.png"/>

<meta name="twitter:title" content="PySpark"/>
<meta name="twitter:description" content="Apart from real-time and batch processing, Apache Spark supports interactive queries and iterative algorithms. Using PySpark, you can work with RDDs in Python programming language also. It is because of a library called Py4j that they are able to achieve this. Resilient Distributed Dataset (RDD): RDD is an immutable (read-only), fundamental collection of elements or items that can be operated on many devices at the same time (parallel processing). Each"/>
<meta name="application-name" content="DAY By DAY">
<meta name="apple-mobile-web-app-title" content="DAY By DAY"><meta name="theme-color" data-light="#f8f8f8" data-dark="#252627" content="#f8f8f8"><meta name="msapplication-TileColor" content="#da532c"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" />
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"><link rel="manifest" href="/site.webmanifest"><link rel="canonical" href="https://liudongdong1.github.io/pyspark/" /><link rel="prev" href="https://liudongdong1.github.io/pytorchpoint/" /><link rel="next" href="https://liudongdong1.github.io/openmmlab/" /><link rel="stylesheet" href="/css/style.min.css"><link rel="stylesheet" href="/lib/fontawesome-free/all.min.css"><link rel="stylesheet" href="/lib/animate/animate.min.css"><script type="application/ld+json">
  {
    "@context": "http://schema.org",
    "@type": "BlogPosting",
    "headline": "PySpark",
    "inLanguage": "zh-CN",
    "mainEntityOfPage": {
      "@type": "WebPage",
      "@id": "https:\/\/liudongdong1.github.io\/pyspark\/"
    },"genre": "posts","keywords": "stream, spark","wordcount":  5161 ,
    "url": "https:\/\/liudongdong1.github.io\/pyspark\/","datePublished": "2020-07-13T21:59:57+00:00","dateModified": "2023-12-31T16:55:50+08:00","license": "This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.","publisher": {
      "@type": "Organization",
      "name": "LiuDongdong","logo": "https:\/\/liudongdong1.github.io\/images\/person.png"},"author": {
        "@type": "Person",
        "name": "liudongdong1"
      },"description": ""
  }
  </script></head>
  <body data-header-desktop="auto" data-header-mobile="auto"><script>(window.localStorage?.getItem('theme') ? localStorage.getItem('theme') === 'dark' : ('auto' === 'auto' ? window.matchMedia('(prefers-color-scheme: dark)').matches : 'auto' === 'dark')) && document.body.setAttribute('data-theme', 'dark');</script><div class="wrapper"><script type="text/javascript"
        async
        src="https://cdnjs.cloudflare.com/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$']],
    displayMath: [['$$','$$']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
});

MathJax.Hub.Queue(function() {
    
    
    
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>

<style>
code.has-jax {
    font: inherit;
    font-size: 100%;
    background: inherit;
    border: inherit;
    color: #515151;
}
</style>
<header class="desktop animate__faster" id="header-desktop">
  <div class="header-wrapper" data-github-corner="right">
    <div class="header-title">
      <a href="/" title="DAY By DAY"><img
    class="lazyload logo"
    src="/svg/loading.min.svg"
    data-src="/fixit.min.svg"
    data-srcset="/fixit.min.svg, /fixit.min.svg 1.5x, /fixit.min.svg 2x"
    data-sizes="auto"
    alt="DAY By DAY"
    title="DAY By DAY"/><span class="header-title-text"></span></a><span id="typeit-header-subtitle-desktop" class="typeit header-subtitle"></span></div>
    <nav>
      <ul class="menu"><li class="menu-item">
              <a
                class="menu-link"
                href="/posts/"
                
                
              ><i class="fa-solid fa-archive fa-fw fa-sm" aria-hidden="true"></i> 所有文章</a></li><li class="menu-item">
              <a
                class="menu-link"
                href="/categories/"
                
                
              ><i class="fa-solid fa-th fa-fw fa-sm" aria-hidden="true"></i> 分类</a></li><li class="menu-item">
              <a
                class="menu-link"
                href="/tags/"
                
                
              ><i class="fa-solid fa-tags fa-fw fa-sm" aria-hidden="true"></i> 标签</a></li><li class="menu-item">
              <a
                class="menu-link"
                href="/friends/"
                title="友情链接"
                
              ><i class="fa-solid fa-users fa-fw fa-sm" aria-hidden="true"></i> 友链</a></li><li class="menu-item">
              <a
                class="menu-link"
                href="/about/"
                
                
              ><i class="fa-solid fa-info-circle fa-fw fa-sm" aria-hidden="true"></i> 关于</a></li><li class="menu-item delimiter"></li><li class="menu-item language">
            <span role="button" aria-label="选择语言" title="选择语言">简体中文<i class="dropdown-icon fa-solid fa-chevron-down" aria-hidden="true"></i>
            </span>
            <ul class="sub-menu"><li class="menu-item">没有更多翻译</li></ul>
          </li><li class="menu-item search" id="search-desktop">
            <input type="text" placeholder="搜索文章标题或内容 ..." id="search-input-desktop">
            <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-desktop" title="搜索">
              <i class="fa-solid fa-search fa-fw" aria-hidden="true"></i>
            </a>
            <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-desktop" title="清空">
              <i class="fa-solid fa-times-circle fa-fw" aria-hidden="true"></i>
            </a>
            <span class="search-button search-loading" id="search-loading-desktop">
              <i class="fa-solid fa-spinner fa-fw fa-spin" aria-hidden="true"></i>
            </span>
          </li><li class="menu-item theme-switch" title="切换主题">
          <i class="fa-solid fa-adjust fa-fw" aria-hidden="true"></i>
        </li>
      </ul>
    </nav>
  </div>
</header><header class="mobile animate__faster" id="header-mobile">
  <div class="header-container">
    <div class="header-wrapper">
      <div class="header-title">
        <a href="/" title="DAY By DAY"><img
    class="lazyload logo"
    src="/svg/loading.min.svg"
    data-src="/fixit.min.svg"
    data-srcset="/fixit.min.svg, /fixit.min.svg 1.5x, /fixit.min.svg 2x"
    data-sizes="auto"
    alt="/fixit.min.svg"
    title="/fixit.min.svg"/><span class="header-title-text"></span></a><span id="typeit-header-subtitle-mobile" class="typeit header-subtitle"></span></div>
      <div class="menu-toggle" id="menu-toggle-mobile">
        <span></span><span></span><span></span>
      </div>
    </div>
    <nav>
      <ul class="menu" id="menu-mobile"><li class="search-wrapper">
            <div class="search mobile" id="search-mobile">
              <input type="text" placeholder="搜索文章标题或内容 ..." id="search-input-mobile">
              <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-mobile" title="搜索">
                <i class="fa-solid fa-search fa-fw" aria-hidden="true"></i>
              </a>
              <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-mobile" title="清空">
                <i class="fa-solid fa-times-circle fa-fw" aria-hidden="true"></i>
              </a>
              <span class="search-button search-loading" id="search-loading-mobile">
                <i class="fa-solid fa-spinner fa-fw fa-spin" aria-hidden="true"></i>
              </span>
            </div>
            <a href="javascript:void(0);" class="search-cancel" id="search-cancel-mobile">
              取消
            </a>
          </li><li
              class="menu-item"
            ><a
                  class="menu-link"
                  href="/posts/"
                  
                  
                ><i class="fa-solid fa-archive fa-fw fa-sm" aria-hidden="true"></i> 所有文章</a></li><li
              class="menu-item"
            ><a
                  class="menu-link"
                  href="/categories/"
                  
                  
                ><i class="fa-solid fa-th fa-fw fa-sm" aria-hidden="true"></i> 分类</a></li><li
              class="menu-item"
            ><a
                  class="menu-link"
                  href="/tags/"
                  
                  
                ><i class="fa-solid fa-tags fa-fw fa-sm" aria-hidden="true"></i> 标签</a></li><li
              class="menu-item"
            ><a
                  class="menu-link"
                  href="/friends/"
                  title="友情链接"
                  
                ><i class="fa-solid fa-users fa-fw fa-sm" aria-hidden="true"></i> 友链</a></li><li
              class="menu-item"
            ><a
                  class="menu-link"
                  href="/about/"
                  
                  
                ><i class="fa-solid fa-info-circle fa-fw fa-sm" aria-hidden="true"></i> 关于</a></li><li
              class="menu-item text-center"
            ><a
                  class="menu-link"
                  href="/"
                  title="GitHub"
                  
                ><i class='fa-brands fa-github fa-fw' aria-hidden='true'></i> </a></li><li class="menu-item theme-switch" title="切换主题">
          <i class="fa-solid fa-adjust fa-fw" aria-hidden="true"></i>
        </li><li class="menu-item language">
            <span role="button" aria-label="选择语言" title="选择语言">简体中文<i class="dropdown-icon fa-solid fa-chevron-down" aria-hidden="true"></i>
            </span>
            <select class="language-select" onchange="location = this.value;"><option disabled>没有更多翻译</option></select>
          </li></ul>
    </nav>
  </div>
</header><div class="search-dropdown desktop">
    <div id="search-dropdown-desktop"></div>
  </div>
  <div class="search-dropdown mobile">
    <div id="search-dropdown-mobile"></div>
  </div><main class="container" data-page-style="normal"><aside class="toc" id="toc-auto"><h2 class="toc-title">目录 <i class="toc-icon fa-solid fa-angle-down fa-fw"></i></h2>
      <div class="toc-content" id="toc-content-auto"></div></aside>

  <aside class="aside-custom" id="aside-sakana">
    

<div class="sakana-widget">
  <div class="sakana-item" id="takina-widget"></div>
  <div class="sakana-item" id="chisato-widget"></div>
</div>
<script>
  function initSakanaWidget() {
    const takina = SakanaWidget.getCharacter('takina')
    SakanaWidget.registerCharacter('takina-slow', takina);
    new SakanaWidget({
      character: 'takina-slow',
      controls: false,
      autoFit: true,
      stroke: {
        color: "#b4b4b4",
        width: 2
      }
    }).mount('#takina-widget');

    const chisato = SakanaWidget.getCharacter('chisato')
    SakanaWidget.registerCharacter('chisato-slow', chisato);
    new SakanaWidget({
      character: 'chisato-slow',
      controls: false,
      autoFit: true,
      stroke: {
        color: "#b4b4b4",
        width: 2
      }
    }).mount('#chisato-widget');
  }
</script>
<script async onload="initSakanaWidget()" src="https://cdn.jsdelivr.net/npm/sakana-widget@2.3.0/lib/sakana.min.js">
</script></aside>

  <article class="page single">
    <div class="header"><h1 class="single-title animate__animated animate__flipInX">
        <span>PySpark</span>
      </h1></div><div class="post-meta">
      <div class="post-meta-line"><span class="post-author"><span class="author"><i class="fa-solid fa-user-circle" aria-hidden="true"></i>
      liudongdong1</span></span>
          <span class="post-category">收录于 <a href="/categories/"><i class="fa-regular fa-folder fa-fw"></i>&nbsp;Categories</a>&ensp;<a href="/categories/framework/"><i class="fa-regular fa-folder fa-fw"></i>&nbsp;Framework</a></span></div>
      <div class="post-meta-line"><span title=2020-07-13&#32;21:59:57>
            <i class="fa-regular fa-calendar-alt fa-fw"></i>&nbsp;<time datetime="2020-07-13" >2020-07-13</time>
          </span>&nbsp;<i class="fa-solid fa-pencil-alt fa-fw"></i>&nbsp;约 5161 字&nbsp;
        <i class="fa-regular fa-clock fa-fw"></i>&nbsp;预计阅读 11 分钟&nbsp;<span id="busuanzi_container_page_pv" class="busuanzi_visitors comment-visitors" data-flag-title="PySpark">
            <i class="fa-regular fa-eye fa-fw"></i>&nbsp;<span id="busuanzi_value_page_pv">-</span>&nbsp;次阅读
          </span>&nbsp;</div>
    </div><div class="featured-image"><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://cdn.stocksnap.io/img-thumbs/280h/LUYHIZXNWX.jpg"
    data-srcset="https://cdn.stocksnap.io/img-thumbs/280h/LUYHIZXNWX.jpg, https://cdn.stocksnap.io/img-thumbs/280h/LUYHIZXNWX.jpg 1.5x, https://cdn.stocksnap.io/img-thumbs/280h/LUYHIZXNWX.jpg 2x"
    data-sizes="auto"
    alt="https://cdn.stocksnap.io/img-thumbs/280h/LUYHIZXNWX.jpg"
    title="https://cdn.stocksnap.io/img-thumbs/280h/LUYHIZXNWX.jpg"/></div><div class="details toc" id="toc-static" kept="true">
        <div class="details-summary toc-title">
          <span>目录</span>
          <span><i class="details-icon fa-solid fa-angle-right"></i></span>
        </div>
        <div class="details-content toc-content" id="toc-content-static"><nav id="TableOfContents">
  <ul>
    <li><a href="#0-architecture">0. Architecture</a></li>
    <li><a href="#1-core-class">1. core class</a>
      <ul>
        <li><a href="#10-submit-model">1.0. Submit Model</a></li>
        <li><a href="#11-rdd">1.1. RDD</a></li>
        <li><a href="#12-sqldataframes">1.2. SQL&amp;DataFrames</a></li>
        <li><a href="#13--rdddataframedataset">1.3.  RDD&amp;DataFrame&amp;Dataset</a></li>
        <li><a href="#14-summary-statistics">1.4. Summary statistics</a></li>
        <li><a href="#1-4-ml">1. 4. ML</a></li>
      </ul>
    </li>
  </ul>
</nav></div>
      </div><div
      class="content"
      id="content"
      
      
    ><blockquote>
<p>Apart from real-time and batch processing, Apache Spark supports interactive queries and iterative algorithms. Using PySpark, you can work with <strong>RDDs</strong> in Python programming language also. It is because of a library called <strong>Py4j</strong> that they are able to achieve this.</p>
</blockquote>
<blockquote>
<ul>
<li><strong>Resilient Distributed Dataset (RDD)</strong>: RDD is an immutable (read-only), fundamental <code>collection of elements or items</code> that can be operated on many devices at the same time (parallel processing). Each dataset in an RDD can be divided into logical portions, which are then executed on different nodes of a cluster.</li>
<li><strong>Directed Acyclic Graph (DAG)</strong>: DAG is the <code>scheduling layer of the Apache Spark architecture</code> that implements <strong>stage-oriented scheduling</strong>. Compared to MapReduce that creates a graph in two stages, Map and Reduce, Apache Spark can create DAGs that contain many stages.</li>
</ul>
</blockquote>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201119211239377.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201119211239377.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201119211239377.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201119211239377.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201119211239377.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201119211239377.png"/></p>
<h2 id="0-architecture">0. Architecture</h2>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201119211608027.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201119211608027.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201119211608027.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201119211608027.png 2x"
    data-sizes="auto"
    alt="Basic"
    title="Basic"/></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201119211639658.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201119211639658.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201119211639658.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201119211639658.png 2x"
    data-sizes="auto"
    alt="standalone"
    title="standalone"/></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201119211701746.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201119211701746.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201119211701746.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201119211701746.png 2x"
    data-sizes="auto"
    alt="Yarn"
    title="Yarn"/></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201119211800146.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201119211800146.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201119211800146.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201119211800146.png 2x"
    data-sizes="auto"
    alt="component"
    title="component"/></p>
<h2 id="1-core-class">1. core class</h2>
<p><a href="https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.SparkContext"target="_blank" rel="external nofollow noopener noreferrer"><code>pyspark.SparkContext</code><i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>Main entry point for Spark functionality.</p>
<p><a href="https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD"target="_blank" rel="external nofollow noopener noreferrer"><code>pyspark.RDD</code><i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>A Resilient Distributed Dataset (RDD), the basic abstraction in Spark.</p>
<p><a href="https://spark.apache.org/docs/latest/api/python/pyspark.streaming.html#pyspark.streaming.StreamingContext"target="_blank" rel="external nofollow noopener noreferrer"><code>pyspark.streaming.StreamingContext</code><i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>Main entry point for Spark Streaming functionality.</p>
<p><a href="https://spark.apache.org/docs/latest/api/python/pyspark.streaming.html#pyspark.streaming.DStream"target="_blank" rel="external nofollow noopener noreferrer"><code>pyspark.streaming.DStream</code><i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>A Discretized Stream (DStream), the basic abstraction in Spark Streaming.</p>
<p><a href="https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.SparkSession"target="_blank" rel="external nofollow noopener noreferrer"><code>pyspark.sql.SparkSession</code><i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>Main entry point for DataFrame and SQL functionality.</p>
<p><a href="https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame"target="_blank" rel="external nofollow noopener noreferrer"><code>pyspark.sql.DataFrame</code><i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>A distributed collection of data grouped into named columns.</p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210225232741569.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210225232741569.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210225232741569.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210225232741569.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210225232741569.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210225232741569.png"/></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span><span style="color:#75715e">#连接spark cluster</span>
</span></span><span style="display:flex;"><span>from pyspark import SparkContext, SparkConf
</span></span><span style="display:flex;"><span>conf <span style="color:#f92672">=</span> SparkConf<span style="color:#f92672">()</span>.setAppName<span style="color:#f92672">(</span><span style="color:#e6db74">&#34;sparkAppExample&#34;</span><span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>sc <span style="color:#f92672">=</span> SparkContext<span style="color:#f92672">(</span>conf<span style="color:#f92672">=</span>conf<span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#使用session</span>
</span></span><span style="display:flex;"><span>from pyspark.sql import SparkSession
</span></span><span style="display:flex;"><span>spark <span style="color:#f92672">=</span> SparkSession.builder <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>          .master<span style="color:#f92672">(</span><span style="color:#e6db74">&#34;local&#34;</span><span style="color:#f92672">)</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>          .appName<span style="color:#f92672">(</span><span style="color:#e6db74">&#34;Word Count&#34;</span><span style="color:#f92672">)</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>          .config<span style="color:#f92672">(</span><span style="color:#e6db74">&#34;spark.some.config.option&#34;</span>, <span style="color:#e6db74">&#34;some-value&#34;</span><span style="color:#f92672">)</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>          .getOrCreate<span style="color:#f92672">()</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 如果使用 hive table 则加上 .enableHiveSupport()</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#spark.sparkContext._conf.getAll()  # check the config</span>
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> pyspark <span style="color:#f92672">import</span> SparkContext, SparkConf
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> pyspark.sql <span style="color:#f92672">import</span> SparkSession
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">create_sc</span>():
</span></span><span style="display:flex;"><span>    sc_conf <span style="color:#f92672">=</span> SparkConf()
</span></span><span style="display:flex;"><span>    sc_conf<span style="color:#f92672">.</span>setMaster(<span style="color:#e6db74">&#39;spark://master:7077&#39;</span>)
</span></span><span style="display:flex;"><span>    sc_conf<span style="color:#f92672">.</span>setAppName(<span style="color:#e6db74">&#39;my-app&#39;</span>)
</span></span><span style="display:flex;"><span>    sc_conf<span style="color:#f92672">.</span>set(<span style="color:#e6db74">&#39;spark.executor.memory&#39;</span>, <span style="color:#e6db74">&#39;2g&#39;</span>)  <span style="color:#75715e">#executor memory是每个节点上占用的内存。每一个节点可使用内存</span>
</span></span><span style="display:flex;"><span>    sc_conf<span style="color:#f92672">.</span>set(<span style="color:#e6db74">&#34;spark.executor.cores&#34;</span>, <span style="color:#e6db74">&#39;4&#39;</span>) <span style="color:#75715e">#spark.executor.cores：顾名思义这个参数是用来指定executor的cpu内核个数，分配更多的内核意味着executor并发能力越强，能够同时执行更多的task</span>
</span></span><span style="display:flex;"><span>    sc_conf<span style="color:#f92672">.</span>set(<span style="color:#e6db74">&#39;spark.cores.max&#39;</span>, <span style="color:#ae81ff">40</span>)    <span style="color:#75715e">#spark.cores.max：为一个application分配的最大cpu核心数，如果没有设置这个值默认为spark.deploy.defaultCores</span>
</span></span><span style="display:flex;"><span>    sc_conf<span style="color:#f92672">.</span>set(<span style="color:#e6db74">&#39;spark.logConf&#39;</span>, <span style="color:#66d9ef">True</span>)    <span style="color:#75715e">#当SparkContext启动时，将有效的SparkConf记录为INFO。</span>
</span></span><span style="display:flex;"><span>    print(sc_conf<span style="color:#f92672">.</span>getAll())
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    sc <span style="color:#f92672">=</span> SparkContext(conf<span style="color:#f92672">=</span>sc_conf)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> sc
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> pyspark.conf <span style="color:#f92672">import</span> SparkConf
</span></span><span style="display:flex;"><span>conf<span style="color:#f92672">=</span>SparkConf()
</span></span><span style="display:flex;"><span>        conf<span style="color:#f92672">.</span>set(<span style="color:#e6db74">&#39;spark.sql.execute.arrow.enabled&#39;</span>,<span style="color:#e6db74">&#39;true&#39;</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> os<span style="color:#f92672">.</span>getenv(<span style="color:#e6db74">&#34;APP_MODE&#34;</span>) <span style="color:#f92672">==</span> <span style="color:#e6db74">&#39;prod&#39;</span>:
</span></span><span style="display:flex;"><span>            <span style="color:#e6db74">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            集群环境
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>            url <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;spark://master:7077&#39;</span>
</span></span><span style="display:flex;"><span>            conf<span style="color:#f92672">.</span>setAppName(<span style="color:#e6db74">&#39;prod-practice-info&#39;</span>)<span style="color:#f92672">.</span>setMaster(url)<span style="color:#f92672">.</span>set(<span style="color:#e6db74">&#34;spark.driver.maxResultSize&#34;</span>, <span style="color:#e6db74">&#34;12g&#34;</span>)<span style="color:#f92672">.</span>set(<span style="color:#e6db74">&#34;spark.executor.memory&#34;</span>, <span style="color:#e6db74">&#39;4g&#39;</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>            <span style="color:#e6db74">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            本地环境
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>            print(<span style="color:#e6db74">&#34;本地环境&#34;</span>)
</span></span><span style="display:flex;"><span>            url <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;local[*]&#39;</span>
</span></span><span style="display:flex;"><span>            conf<span style="color:#f92672">.</span>setAppName(<span style="color:#e6db74">&#39;prod-practice-info&#39;</span>)<span style="color:#f92672">.</span>setMaster(url)
</span></span><span style="display:flex;"><span>        spark <span style="color:#f92672">=</span> SparkSession<span style="color:#f92672">.</span>builder<span style="color:#f92672">.</span> \
</span></span><span style="display:flex;"><span>            config(conf<span style="color:#f92672">=</span>conf)<span style="color:#f92672">.</span>\
</span></span><span style="display:flex;"><span>            getOrCreate()
</span></span></code></pre></div><h3 id="10-submit-model">1.0. Submit Model</h3>
<ul>
<li><code>--class</code>: The entry point for your application (e.g. <code>org.apache.spark.examples.SparkPi</code>)</li>
<li><code>--master</code>: The <a href="https://spark.apache.org/docs/latest/submitting-applications.html#master-urls"target="_blank" rel="external nofollow noopener noreferrer">master URL<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a> for the cluster (e.g. <code>spark://23.195.26.187:7077</code>)</li>
<li><code>--deploy-mode</code>: Whether to deploy your driver on the worker nodes (<code>cluster</code>) or locally as an external client (<code>client</code>) (default: <code>client</code>) <strong>†</strong></li>
<li><code>--conf</code>: Arbitrary Spark configuration property in key=value format. For values that contain spaces wrap “key=value” in quotes (as shown). Multiple configurations should be passed as separate arguments. (e.g. <code>--conf &lt;key&gt;=&lt;value&gt; --conf &lt;key2&gt;=&lt;value2&gt;</code>)</li>
<li><code>application-jar</code>: Path to a bundled jar including your application and all dependencies. The URL must be globally visible inside of your cluster, for instance, an <code>hdfs://</code> path or a <code>file://</code> path that is present on all nodes.</li>
<li><code>application-arguments</code>: Arguments passed to the main method of your main class.</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span><span style="color:#75715e"># Run application locally on 8 cores</span>
</span></span><span style="display:flex;"><span>./bin/spark-submit <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --class org.apache.spark.examples.SparkPi <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --master local<span style="color:#f92672">[</span>8<span style="color:#f92672">]</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  /path/to/examples.jar <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  <span style="color:#ae81ff">100</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Run on a Spark standalone cluster in client deploy mode</span>
</span></span><span style="display:flex;"><span>./bin/spark-submit <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --class org.apache.spark.examples.SparkPi <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --master spark://207.184.161.138:7077 <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --executor-memory 20G <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --total-executor-cores <span style="color:#ae81ff">100</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  /path/to/examples.jar <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  <span style="color:#ae81ff">1000</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Run on a Spark standalone cluster in cluster deploy mode with supervise</span>
</span></span><span style="display:flex;"><span>./bin/spark-submit <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --class org.apache.spark.examples.SparkPi <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --master spark://207.184.161.138:7077 <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --deploy-mode cluster <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --supervise <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --executor-memory 20G <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --total-executor-cores <span style="color:#ae81ff">100</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  /path/to/examples.jar <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  <span style="color:#ae81ff">1000</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Run on a YARN cluster</span>
</span></span><span style="display:flex;"><span>export HADOOP_CONF_DIR<span style="color:#f92672">=</span>XXX
</span></span><span style="display:flex;"><span>./bin/spark-submit <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --class org.apache.spark.examples.SparkPi <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --master yarn <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --deploy-mode cluster <span style="color:#ae81ff">\ </span> <span style="color:#75715e"># can be client for client mode</span>
</span></span><span style="display:flex;"><span>  --executor-memory 20G <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --num-executors <span style="color:#ae81ff">50</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  /path/to/examples.jar <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  <span style="color:#ae81ff">1000</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Run a Python application on a Spark standalone cluster</span>
</span></span><span style="display:flex;"><span>./bin/spark-submit <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --master spark://207.184.161.138:7077 <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  examples/src/main/python/pi.py <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  <span style="color:#ae81ff">1000</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Run on a Mesos cluster in cluster deploy mode with supervise</span>
</span></span><span style="display:flex;"><span>./bin/spark-submit <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --class org.apache.spark.examples.SparkPi <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --master mesos://207.184.161.138:7077 <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --deploy-mode cluster <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --supervise <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --executor-memory 20G <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --total-executor-cores <span style="color:#ae81ff">100</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  http://path/to/examples.jar <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  <span style="color:#ae81ff">1000</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Run on a Kubernetes cluster in cluster deploy mode</span>
</span></span><span style="display:flex;"><span>./bin/spark-submit <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --class org.apache.spark.examples.SparkPi <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --master k8s://xx.yy.zz.ww:443 <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --deploy-mode cluster <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --executor-memory 20G <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  --num-executors <span style="color:#ae81ff">50</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  http://path/to/examples.jar <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  <span style="color:#ae81ff">1000</span>
</span></span></code></pre></div><h3 id="11-rdd">1.1. RDD</h3>
<blockquote>
<p>支持两种类型的操作： 转化操作（transformation） 和行动操作（action）。转化操作会由一个RDD 生成一个新的RDD。行动操作是对的RDD 内容进行操作，它们会把最终求得的结果返回到驱动器程序，或者写入外部存储系统中。由于行动操作需要生成实际的输出，它们会强制执行那些求值必须用到的RDD 的转化操作。RDD的转化操作与行动操作不同，是惰性求值的，也就是在被调用行动操作之前Spark 不会开始计算。同样创建操作也是一样，数据并没有被立刻读取到内存中，只是记录了读取操作需要的相关信息。我理解为这与tensorflow的网络构建类似，我们之前编写的代码只是记录了整个操作过程的计算流程图，只有当计算操作被激活时，数据才会沿着之前定义的计算图进行计算.</p>
</blockquote>
<h4 id="111-rdd-creation">1.1.1. RDD creation</h4>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e">#---  Creating a RDD from a file</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> urllib
</span></span><span style="display:flex;"><span>f <span style="color:#f92672">=</span> urllib<span style="color:#f92672">.</span>urlretrieve (<span style="color:#e6db74">&#34;http://kdd.ics.uci.edu/databases/kddcup99/kddcup.data_10_percent.gz&#34;</span>, <span style="color:#e6db74">&#34;kddcup.data_10_percent.gz&#34;</span>)
</span></span><span style="display:flex;"><span>data_file <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;./kddcup.data_10_percent.gz&#34;</span>
</span></span><span style="display:flex;"><span>raw_data <span style="color:#f92672">=</span> sc<span style="color:#f92672">.</span>textFile(data_file)<span style="color:#f92672">.</span>cache()
</span></span><span style="display:flex;"><span><span style="color:#75715e">#---  Creating and RDD using parallelize</span>
</span></span><span style="display:flex;"><span>a <span style="color:#f92672">=</span> range(<span style="color:#ae81ff">100</span>)
</span></span><span style="display:flex;"><span>data <span style="color:#f92672">=</span> sc<span style="color:#f92672">.</span>parallelize(a)
</span></span><span style="display:flex;"><span><span style="color:#75715e">#---  makeRDD 操作</span>
</span></span><span style="display:flex;"><span>val rdd02 <span style="color:#f92672">=</span> sc<span style="color:#f92672">.</span>makeRDD(Array(<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">2</span>,<span style="color:#ae81ff">3</span>,<span style="color:#ae81ff">4</span>,<span style="color:#ae81ff">5</span>,<span style="color:#ae81ff">6</span>))
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e">#----  create key-value data</span>
</span></span><span style="display:flex;"><span>key_value_data <span style="color:#f92672">=</span> csv_data<span style="color:#f92672">.</span>map(<span style="color:#66d9ef">lambda</span> x: (x[<span style="color:#ae81ff">41</span>], x)) <span style="color:#75715e"># x[41] contains the network interaction tag</span>
</span></span><span style="display:flex;"><span>durations_by_key <span style="color:#f92672">=</span> key_value_duration<span style="color:#f92672">.</span>reduceByKey(<span style="color:#66d9ef">lambda</span> x, y: x <span style="color:#f92672">+</span> y)
</span></span><span style="display:flex;"><span>counts_by_key <span style="color:#f92672">=</span> key_value_data<span style="color:#f92672">.</span>countByKey()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>head_rows <span style="color:#f92672">=</span> raw_data<span style="color:#f92672">.</span>take(<span style="color:#ae81ff">5</span>)<span style="color:#75715e"># 查看前面5个</span>
</span></span><span style="display:flex;"><span>count<span style="color:#f92672">=</span>raw_data<span style="color:#f92672">.</span>count() <span style="color:#75715e"># 查看个数</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#-------   sample</span>
</span></span><span style="display:flex;"><span>raw_data_sample <span style="color:#f92672">=</span> raw_data<span style="color:#f92672">.</span>sample(<span style="color:#66d9ef">False</span>, <span style="color:#ae81ff">0.1</span>, <span style="color:#ae81ff">1234</span>)  <span style="color:#75715e"># # whether the sampling is done with replacement, sample size as a fraction, random seed;</span>
</span></span><span style="display:flex;"><span>raw_data_sample <span style="color:#f92672">=</span> raw_data<span style="color:#f92672">.</span>takeSample(<span style="color:#66d9ef">False</span>, <span style="color:#ae81ff">400000</span>, <span style="color:#ae81ff">1234</span>)<span style="color:#75715e">#grab a sample of raw data from our RDD into local memory, number samples</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#----     set operation</span>
</span></span><span style="display:flex;"><span>attack_raw_data <span style="color:#f92672">=</span> raw_data<span style="color:#f92672">.</span>subtract(normal_raw_data)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>csv_data <span style="color:#f92672">=</span> raw_data<span style="color:#f92672">.</span>map(<span style="color:#66d9ef">lambda</span> x: x<span style="color:#f92672">.</span>split(<span style="color:#e6db74">&#34;,&#34;</span>))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>normal_raw_data <span style="color:#f92672">=</span> raw_data<span style="color:#f92672">.</span>filter(<span style="color:#66d9ef">lambda</span> x: <span style="color:#e6db74">&#39;normal.&#39;</span> <span style="color:#f92672">in</span> x)  <span style="color:#75715e">#count how many normal</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">parse_interaction</span>(line):
</span></span><span style="display:flex;"><span>    elems <span style="color:#f92672">=</span> line<span style="color:#f92672">.</span>split(<span style="color:#e6db74">&#34;,&#34;</span>)
</span></span><span style="display:flex;"><span>    tag <span style="color:#f92672">=</span> elems[<span style="color:#ae81ff">41</span>]
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> (tag, elems)
</span></span><span style="display:flex;"><span>key_csv_data <span style="color:#f92672">=</span> raw_data<span style="color:#f92672">.</span>map(parse_interaction)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>all_raw_data <span style="color:#f92672">=</span> raw_data<span style="color:#f92672">.</span>collect()<span style="color:#75715e">#get all the elements in the RDD into memory for us to work with them.</span>
</span></span></code></pre></div><h4 id="112--transformop">1.1.2.  TransformOp</h4>
<blockquote>
<p>皆产生新的 RDD,且直保存运算逻辑，依赖原始 rdd</p>
</blockquote>
<h5 id="1-map--mapvalues">1. map &amp; mapValues</h5>
<p><code>对于每个元素</code>都应用这个func</p>
<ul>
<li>入参：
<ul>
<li>func表示需要应用到每个元素的方法</li>
<li>preservesPartitioning是否保持当前分区方式，默认重新分区</li>
</ul>
</li>
<li>返回：
<ul>
<li><code>返回的结果是一个RDD</code></li>
</ul>
</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>rdd <span style="color:#f92672">=</span> sc<span style="color:#f92672">.</span>parallelize([<span style="color:#e6db74">&#34;b&#34;</span>, <span style="color:#e6db74">&#34;a&#34;</span>, <span style="color:#e6db74">&#34;c&#34;</span>])
</span></span><span style="display:flex;"><span>sorted(rdd<span style="color:#f92672">.</span>map(<span style="color:#66d9ef">lambda</span> x: (x, <span style="color:#ae81ff">1</span>))<span style="color:#f92672">.</span>collect()) <span style="color:#75715e">#[(&#39;a&#39;, 1), (&#39;b&#39;, 1), (&#39;c&#39;, 1)]</span>
</span></span></code></pre></div><p><code>对键值对中每个value都应用这个func</code>，并保持key不变</p>
<ul>
<li>入参：
<ul>
<li>func表示需要应用到每个元素值上的方法</li>
</ul>
</li>
<li>返回：
<ul>
<li>返回的结果是一个RDD</li>
</ul>
</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>x <span style="color:#f92672">=</span> sc<span style="color:#f92672">.</span>parallelize([(<span style="color:#e6db74">&#34;a&#34;</span>, [<span style="color:#e6db74">&#34;apple&#34;</span>, <span style="color:#e6db74">&#34;banana&#34;</span>, <span style="color:#e6db74">&#34;lemon&#34;</span>]), (<span style="color:#e6db74">&#34;b&#34;</span>, [<span style="color:#e6db74">&#34;grapes&#34;</span>])])
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">f</span>(x): <span style="color:#66d9ef">return</span> len(x)
</span></span><span style="display:flex;"><span>x<span style="color:#f92672">.</span>mapValues(f)<span style="color:#f92672">.</span>collect() <span style="color:#75715e"># [(&#39;a&#39;, 3), (&#39;b&#39;, 1)]</span>
</span></span></code></pre></div><h5 id="2-flatmap">2. flatmap</h5>
<p>遍历全部元素，将传入方法应用到每个元素上，并将<code>最后结果展平（压成一个List）</code></p>
<ul>
<li>入参：
<ul>
<li>func表示需要应用到每个元素的方法</li>
<li>preservesPartitioning是否保持当前分区方式，默认重新分区</li>
</ul>
</li>
<li>返回：
<ul>
<li>返回的结果是一个RDD</li>
</ul>
</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>rdd <span style="color:#f92672">=</span> sc<span style="color:#f92672">.</span>parallelize([<span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">4</span>])
</span></span><span style="display:flex;"><span>sorted(rdd<span style="color:#f92672">.</span>flatMap(<span style="color:#66d9ef">lambda</span> x: range(<span style="color:#ae81ff">1</span>, x))<span style="color:#f92672">.</span>collect()) <span style="color:#75715e">#[1, 1, 1, 2, 2, 3]</span>
</span></span><span style="display:flex;"><span>sorted(rdd<span style="color:#f92672">.</span>flatMap(<span style="color:#66d9ef">lambda</span> x: [(x, x), (x, x)])<span style="color:#f92672">.</span>collect()) <span style="color:#75715e">#[(2, 2), (2, 2), (3, 3), (3, 3), (4, 4), (4, 4)]</span>
</span></span></code></pre></div><p>遍历某个元素的元素值，将传入方法应用到每个元素值上，并将最后结果展平（压成一个List）</p>
<ul>
<li>入参：
<ul>
<li>func表示需要应用到每个元素值的方法</li>
</ul>
</li>
<li>返回：
<ul>
<li>返回的结果是一个RDD</li>
</ul>
</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>x <span style="color:#f92672">=</span> sc<span style="color:#f92672">.</span>parallelize([(<span style="color:#e6db74">&#34;a&#34;</span>, [<span style="color:#e6db74">&#34;x&#34;</span>, <span style="color:#e6db74">&#34;y&#34;</span>, <span style="color:#e6db74">&#34;z&#34;</span>]), (<span style="color:#e6db74">&#34;b&#34;</span>, [<span style="color:#e6db74">&#34;p&#34;</span>, <span style="color:#e6db74">&#34;r&#34;</span>])])
</span></span><span style="display:flex;"><span>x<span style="color:#f92672">.</span>flatMapValues(<span style="color:#66d9ef">lambda</span> val: val)<span style="color:#f92672">.</span>collect() <span style="color:#75715e"># [(&#39;a&#39;, &#39;x&#39;), (&#39;a&#39;, &#39;y&#39;), (&#39;a&#39;, &#39;z&#39;), (&#39;b&#39;, &#39;p&#39;), (&#39;b&#39;, &#39;r&#39;)]</span>
</span></span></code></pre></div><h5 id="3-filter">3. filter</h5>
<p>遍历全部元素，筛选符合传入方法的元素</p>
<ul>
<li>入参：
<ul>
<li>func表示需要应用到每个元素的筛选方法</li>
</ul>
</li>
<li>返回：
<ul>
<li>返回的结果是一个RDD</li>
</ul>
</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>rdd <span style="color:#f92672">=</span> sc<span style="color:#f92672">.</span>parallelize([<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">5</span>])
</span></span><span style="display:flex;"><span>rdd<span style="color:#f92672">.</span>filter(<span style="color:#66d9ef">lambda</span> x: x <span style="color:#f92672">%</span> <span style="color:#ae81ff">2</span> <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>print(rdd<span style="color:#f92672">.</span>collect()) <span style="color:#75715e"># [2, 4]</span>
</span></span><span style="display:flex;"><span>kvRDD1<span style="color:#f92672">.</span>fiter(<span style="color:#66d9ef">lambda</span> x:x[<span style="color:#ae81ff">0</span>]<span style="color:#f92672">&lt;</span><span style="color:#ae81ff">5</span>) 根据 key过滤
</span></span><span style="display:flex;"><span>kvRDD1<span style="color:#f92672">.</span>filter(<span style="color:#66d9ef">lambda</span> x:x[<span style="color:#ae81ff">1</span>]<span style="color:#f92672">&lt;</span><span style="color:#ae81ff">5</span>) 根据值过滤
</span></span></code></pre></div><h5 id="4-distinct">4. distinct</h5>
<p>遍历全部元素，<code>并返回包含的不同元素的总数</code></p>
<ul>
<li>入参：
<ul>
<li>numPartitions表示需要将此操作分割成多少个分区</li>
</ul>
</li>
<li>返回：
<ul>
<li>返回的结果是一个Int</li>
</ul>
</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>print(rddInt<span style="color:#f92672">.</span>distinct()<span style="color:#f92672">.</span>collect())
</span></span></code></pre></div><h5 id="5-cartesian">5. cartesian</h5>
<p>返回自己与传入rdd的笛卡尔积</p>
<ul>
<li>入参：
<ul>
<li>rdd表示一个rdd对象，可以存储不同数据类型 RDD</li>
</ul>
</li>
<li>返回：
<ul>
<li>返回的结果是一个RDD</li>
</ul>
</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>num_rdd <span style="color:#f92672">=</span> sc<span style="color:#f92672">.</span>parallelize([<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>])
</span></span><span style="display:flex;"><span>str_rdd <span style="color:#f92672">=</span> sc<span style="color:#f92672">.</span>parallelize([<span style="color:#e6db74">&#39;a&#39;</span>, <span style="color:#e6db74">&#39;y&#39;</span>])
</span></span><span style="display:flex;"><span>result <span style="color:#f92672">=</span> num_rdd<span style="color:#f92672">.</span>cartesian(str_rdd)
</span></span><span style="display:flex;"><span>print(result<span style="color:#f92672">.</span>collect()) <span style="color:#75715e"># [(1, &#39;a&#39;), (1, &#39;y&#39;), (2, &#39;a&#39;), (2, &#39;y&#39;)]</span>
</span></span></code></pre></div><h4 id="113-actionop">1.1.3. ActionOp</h4>
<h5 id="1-collect">1. collect</h5>
<p>将数据以List取回本地
<a href="https://spark.apache.org/docs/latest/api/python/pyspark.html"target="_blank" rel="external nofollow noopener noreferrer">官网<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>提示，建议只在任务结束时在调用collect方法.</p>
<ul>
<li>返回：
<ul>
<li>返回的结果是一个List</li>
</ul>
</li>
</ul>
<h5 id="2-counttakenum">2. count&amp;&amp;take(num)</h5>
<h5 id="3--countbyvalue-countbykey">3.  countByValue&amp; countByKey</h5>
<p>返回<code>每个key对应的元素数量</code></p>
<ul>
<li>返回：
<ul>
<li>返回的结果是一个Dict</li>
</ul>
</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>rdd <span style="color:#f92672">=</span> sc<span style="color:#f92672">.</span>parallelize([(<span style="color:#e6db74">&#34;a&#34;</span>, <span style="color:#ae81ff">1</span>), (<span style="color:#e6db74">&#34;b&#34;</span>, <span style="color:#ae81ff">1</span>), (<span style="color:#e6db74">&#34;a&#34;</span>, <span style="color:#ae81ff">1</span>)])
</span></span><span style="display:flex;"><span>print(rdd<span style="color:#f92672">.</span>countByKey()) <span style="color:#75715e"># defaultdict(&lt;class &#39;int&#39;&gt;, {&#39;a&#39;: 2, &#39;b&#39;: 1})</span>
</span></span></code></pre></div><p>返回<code>每个value出现的次数</code></p>
<ul>
<li>返回：
<ul>
<li>返回的结果是一个Dict</li>
</ul>
</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>rdd2 <span style="color:#f92672">=</span> sc<span style="color:#f92672">.</span>parallelize([<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">2</span>], <span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>print(rdd2<span style="color:#f92672">.</span>countByValue())  <span style="color:#75715e"># defaultdict(&lt;class &#39;int&#39;&gt;, {1: 2, 2: 3})</span>
</span></span></code></pre></div><h5 id="4-reduce">4. reduce</h5>
<p>对于每个元素值都应用这个func</p>
<ul>
<li>入参：
<ul>
<li>func表示需要应用到每个元素的方法</li>
</ul>
</li>
<li>返回：
<ul>
<li>返回的结果是一个Python obj, 与元素值得数据类型一致</li>
</ul>
</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>x <span style="color:#f92672">=</span> sc<span style="color:#f92672">.</span>parallelize([<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">3</span>])
</span></span><span style="display:flex;"><span>y <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>reduce(<span style="color:#66d9ef">lambda</span> a, b : a <span style="color:#f92672">+</span> b )
</span></span><span style="display:flex;"><span>print(x<span style="color:#f92672">.</span>collect()) <span style="color:#75715e"># [1, 2, 3]</span>
</span></span><span style="display:flex;"><span>print(y) <span style="color:#75715e"># 6</span>
</span></span></code></pre></div><h5 id="5-fold">5. fold</h5>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># 和reduce() 一样， 但是需要提供初始值</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">&gt;&gt;&gt;</span> rdd <span style="color:#f92672">=</span> sc<span style="color:#f92672">.</span>parallelize([<span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">5</span>, <span style="color:#ae81ff">5</span>])
</span></span><span style="display:flex;"><span><span style="color:#f92672">&gt;&gt;&gt;</span> rdd<span style="color:#f92672">.</span>fold(<span style="color:#ae81ff">0</span>, <span style="color:#66d9ef">lambda</span> x, y: x <span style="color:#f92672">+</span> y)
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">21</span>
</span></span></code></pre></div><h5 id="6-aggregate">6. aggregate</h5>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># 和reduce() 相似， 但是通常返回不同类型的函数</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">&gt;&gt;&gt;</span> seqOp <span style="color:#f92672">=</span> (<span style="color:#66d9ef">lambda</span> x, y: (x[<span style="color:#ae81ff">0</span>] <span style="color:#f92672">+</span> y, x[<span style="color:#ae81ff">1</span>] <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>))
</span></span><span style="display:flex;"><span><span style="color:#f92672">&gt;&gt;&gt;</span> combOp <span style="color:#f92672">=</span> (<span style="color:#66d9ef">lambda</span> x, y: (x[<span style="color:#ae81ff">0</span>] <span style="color:#f92672">+</span> y[<span style="color:#ae81ff">0</span>], x[<span style="color:#ae81ff">1</span>] <span style="color:#f92672">+</span> y[<span style="color:#ae81ff">1</span>]))
</span></span><span style="display:flex;"><span><span style="color:#f92672">&gt;&gt;&gt;</span> sc<span style="color:#f92672">.</span>parallelize([<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">4</span>])<span style="color:#f92672">.</span>aggregate((<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>), seqOp, combOp)
</span></span><span style="display:flex;"><span>(<span style="color:#ae81ff">10</span>, <span style="color:#ae81ff">4</span>)
</span></span></code></pre></div><h5 id="7-foreach">7. foreach</h5>
<p>用于遍历RDD中的元素,将函数func应用于每一个元素。</p>
<ul>
<li>入参：
<ul>
<li>func表示需要应用到每个元素的方法, 但这个方法不会在客户端执行</li>
</ul>
</li>
<li>返回：
<ul>
<li>返回的结果是一个RDD</li>
</ul>
</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">f</span>(x): print(x)
</span></span><span style="display:flex;"><span>sc<span style="color:#f92672">.</span>parallelize([<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">5</span>])<span style="color:#f92672">.</span>foreach(f)
</span></span></code></pre></div><h3 id="12-sqldataframes">1.2. SQL&amp;DataFrames</h3>
<blockquote>
<ul>
<li><strong>Use of Input Optimization Engine</strong>: DataFrames <code>make use of the input optimization engines</code>, e.g., <strong>Catalyst Optimizer</strong>, to process data efficiently. We can use the same engine for all Python, Java, Scala, and R DataFrame APIs.</li>
<li><strong>Handling of Structured Data</strong>: DataFrames provide a<code> schematic view of data</code>. Here, the data has some meaning to it when it is being stored.</li>
<li><strong>Custom Memory Management</strong>: In <code>RDDs, the data is stored in memory</code>, whereas <code>DataFrames store data off-heap</code> (outside the main Java Heap space, but still inside RAM), which in turn reduces the garbage collection overload.</li>
<li><strong>Flexibility</strong>: <code>DataFrames, like RDDs</code>, can support various formats of data, such as CSV, <a href="https://intellipaat.com/blog/apache-cassandra-a-brief-intro/"target="_blank" rel="external nofollow noopener noreferrer">Cassandra<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>, etc.</li>
<li><strong>Scalability</strong>: <code>DataFrames can be integrated with various other [Big Data tools](https://intellipaat.com/blog/big-data-analytics-tools-performance-testing/),</code> and they allow processing megabytes to petabytes of data at once.</li>
</ul>
</blockquote>
<ul>
<li>pyspark.sql.SQLContext： DataFrame和SQL方法的主入口</li>
<li>pyspark.sql.DataFrame： 将分布式数据集分组到指定列名的数据框中</li>
<li>pyspark.sql.Column ：DataFrame中的列<code>Row(name=&quot;Alice&quot;, age=11).asDict() == {'name': 'Alice', 'age': 11}</code></li>
<li>pyspark.sql.Row： DataFrame数据的行</li>
<li>pyspark.sql.HiveContext： 访问Hive数据的主入口</li>
<li>pyspark.sql.GroupedData： 由DataFrame.groupBy()创建的聚合方法集</li>
<li>pyspark.sql.DataFrameNaFunctions： 处理丢失数据(空数据)的方法</li>
<li>pyspark.sql.DataFrameStatFunctions： 统计功能的方法
-pyspark.sql.functions DataFrame：可用的内置函数</li>
<li>pyspark.sql.types： 可用的数据类型列表</li>
<li>pyspark.sql.Window： 用于处理窗口函数</li>
</ul>
<h5 id="1-creation">1. creation</h5>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e">#--- create from csv</span>
</span></span><span style="display:flex;"><span>fifa_df <span style="color:#f92672">=</span> spark<span style="color:#f92672">.</span>read<span style="color:#f92672">.</span>csv(<span style="color:#e6db74">&#34;path-of-file/fifa_players.csv&#34;</span>, inferSchema <span style="color:#f92672">=</span> <span style="color:#66d9ef">True</span>, header <span style="color:#f92672">=</span> <span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span><span style="color:#75715e">#--- create from json</span>
</span></span><span style="display:flex;"><span>val jsondata<span style="color:#f92672">=</span>spark<span style="color:#f92672">.</span>read<span style="color:#f92672">.</span>json(<span style="color:#e6db74">&#34;file.json&#34;</span>)
</span></span><span style="display:flex;"><span><span style="color:#75715e">#--- create from exitRDD</span>
</span></span><span style="display:flex;"><span>df<span style="color:#f92672">=</span>spark<span style="color:#f92672">.</span>createDataFrame(rdd)<span style="color:#f92672">.</span>toDF(<span style="color:#e6db74">&#34;key&#34;</span>,<span style="color:#e6db74">&#34;cube&#34;</span>)
</span></span><span style="display:flex;"><span><span style="color:#75715e">#--- create from dict</span>
</span></span><span style="display:flex;"><span>df <span style="color:#f92672">=</span> spark<span style="color:#f92672">.</span>createDataFrame([{<span style="color:#e6db74">&#39;name&#39;</span>:<span style="color:#e6db74">&#39;Alice&#39;</span>,<span style="color:#e6db74">&#39;age&#39;</span>:<span style="color:#ae81ff">1</span>},
</span></span><span style="display:flex;"><span>    {<span style="color:#e6db74">&#39;name&#39;</span>:<span style="color:#e6db74">&#39;Polo&#39;</span>,<span style="color:#e6db74">&#39;age&#39;</span>:<span style="color:#ae81ff">1</span>}]) 
</span></span><span style="display:flex;"><span><span style="color:#75715e">#--- create from schema</span>
</span></span><span style="display:flex;"><span>schema <span style="color:#f92672">=</span> StructType([
</span></span><span style="display:flex;"><span>    StructField(<span style="color:#e6db74">&#34;id&#34;</span>, LongType(), <span style="color:#66d9ef">True</span>),   
</span></span><span style="display:flex;"><span>    StructField(<span style="color:#e6db74">&#34;name&#34;</span>, StringType(), <span style="color:#66d9ef">True</span>),
</span></span><span style="display:flex;"><span>    StructField(<span style="color:#e6db74">&#34;age&#34;</span>, LongType(), <span style="color:#66d9ef">True</span>),
</span></span><span style="display:flex;"><span>    StructField(<span style="color:#e6db74">&#34;eyeColor&#34;</span>, StringType(), <span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>])
</span></span><span style="display:flex;"><span>df <span style="color:#f92672">=</span> spark<span style="color:#f92672">.</span>createDataFrame(csvRDD, schema)
</span></span><span style="display:flex;"><span><span style="color:#75715e">#--- create from pandas</span>
</span></span><span style="display:flex;"><span>colors <span style="color:#f92672">=</span> [<span style="color:#e6db74">&#39;white&#39;</span>,<span style="color:#e6db74">&#39;green&#39;</span>,<span style="color:#e6db74">&#39;yellow&#39;</span>,<span style="color:#e6db74">&#39;red&#39;</span>,<span style="color:#e6db74">&#39;brown&#39;</span>,<span style="color:#e6db74">&#39;pink&#39;</span>]
</span></span><span style="display:flex;"><span>color_df<span style="color:#f92672">=</span>pd<span style="color:#f92672">.</span>DataFrame(colors,columns<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#39;color&#39;</span>])
</span></span><span style="display:flex;"><span>color_df[<span style="color:#e6db74">&#39;length&#39;</span>]<span style="color:#f92672">=</span>color_df[<span style="color:#e6db74">&#39;color&#39;</span>]<span style="color:#f92672">.</span>apply(len)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>color_df<span style="color:#f92672">=</span>spark<span style="color:#f92672">.</span>createDataFrame(color_df)
</span></span><span style="display:flex;"><span>color_df<span style="color:#f92672">.</span>show()
</span></span></code></pre></div><h5 id="2-show">2. show</h5>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>df<span style="color:#f92672">.</span>printSchema()
</span></span><span style="display:flex;"><span>df<span style="color:#f92672">.</span>columns()
</span></span><span style="display:flex;"><span>df<span style="color:#f92672">.</span>count()
</span></span><span style="display:flex;"><span>df<span style="color:#f92672">.</span>describe(<span style="color:#e6db74">&#39;column name&#39;</span>)<span style="color:#f92672">.</span>show() <span style="color:#75715e"># look at the summary of particular column</span>
</span></span><span style="display:flex;"><span>df<span style="color:#f92672">.</span>dtypes <span style="color:#75715e">#将所有列名称及其数据类型作为列表返回。</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 查找每列出现次数占总的30%以上频繁项目</span>
</span></span><span style="display:flex;"><span>df<span style="color:#f92672">.</span>stat<span style="color:#f92672">.</span>freqItems([<span style="color:#e6db74">&#34;id&#34;</span>, <span style="color:#e6db74">&#34;gender&#34;</span>], <span style="color:#ae81ff">0.3</span>)<span style="color:#f92672">.</span>show()
</span></span><span style="display:flex;"><span><span style="color:#75715e"># ----  缺失值</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 计算每列空值数目</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> col <span style="color:#f92672">in</span> df<span style="color:#f92672">.</span>columns:
</span></span><span style="display:flex;"><span>    print(col, <span style="color:#e6db74">&#34;</span><span style="color:#ae81ff">\t</span><span style="color:#e6db74">&#34;</span>, <span style="color:#e6db74">&#34;with null values: &#34;</span>, 
</span></span><span style="display:flex;"><span>          df<span style="color:#f92672">.</span>filter(df[col]<span style="color:#f92672">.</span>isNull())<span style="color:#f92672">.</span>count())
</span></span></code></pre></div><h5 id="3-column-select">3. column select</h5>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>df<span style="color:#f92672">.</span>select(<span style="color:#e6db74">&#39;column name&#39;</span>,<span style="color:#e6db74">&#39;name2&#39;</span>)<span style="color:#f92672">.</span>show()
</span></span><span style="display:flex;"><span>color_df<span style="color:#f92672">.</span>filter(color_df[<span style="color:#e6db74">&#39;length&#39;</span>]<span style="color:#f92672">&gt;=</span><span style="color:#ae81ff">4</span>)<span style="color:#f92672">.</span>show()   <span style="color:#75715e"># filter方法</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 返回具有新指定列名的DataFrame</span>
</span></span><span style="display:flex;"><span>df<span style="color:#f92672">.</span>toDF(<span style="color:#e6db74">&#39;f1&#39;</span>, <span style="color:#e6db74">&#39;f2&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>first_row <span style="color:#f92672">=</span> df<span style="color:#f92672">.</span>head()
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Row(address=Row(city=&#39;Nanjing&#39;, country=&#39;China&#39;), age=12, name=&#39;Li&#39;)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 读取行内某一列的属性值</span>
</span></span><span style="display:flex;"><span>first_row[<span style="color:#e6db74">&#39;age&#39;</span>]           <span style="color:#75715e"># 12</span>
</span></span><span style="display:flex;"><span>first_row<span style="color:#f92672">.</span>age              <span style="color:#75715e"># 12</span>
</span></span><span style="display:flex;"><span>getattr(first_row, <span style="color:#e6db74">&#39;age&#39;</span>)  <span style="color:#75715e"># 12</span>
</span></span><span style="display:flex;"><span>first_row<span style="color:#f92672">.</span>address
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Row(city=&#39;Nanjing&#39;, country=&#39;China&#39;)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># -------------- column -----------------------</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>first_col <span style="color:#f92672">=</span> df[<span style="color:#ae81ff">0</span>]
</span></span><span style="display:flex;"><span>first_col <span style="color:#f92672">=</span> df[<span style="color:#e6db74">&#39;adress&#39;</span>]
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Column&lt;b&#39;address&#39;&gt;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># copy column[s]</span>
</span></span><span style="display:flex;"><span>address_copy <span style="color:#f92672">=</span> first_col<span style="color:#f92672">.</span>alias(<span style="color:#e6db74">&#39;address_copy&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># rename column / create new column</span>
</span></span><span style="display:flex;"><span>df<span style="color:#f92672">.</span>withColumnRenamed(<span style="color:#e6db74">&#39;age&#39;</span>, <span style="color:#e6db74">&#39;birth_age&#39;</span>)
</span></span><span style="display:flex;"><span>df<span style="color:#f92672">.</span>withColumn(<span style="color:#e6db74">&#39;age_copy&#39;</span>, df[<span style="color:#e6db74">&#39;age&#39;</span>])<span style="color:#f92672">.</span>show(<span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span><span style="color:#e6db74">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">+----------------+---+----+--------+
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">|         address|age|name|age_copy|
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">+----------------+---+----+--------+
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">|[Nanjing, China]| 12|  Li|      12|
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">+----------------+---+----+--------+
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">only showing top 1 row
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>df<span style="color:#f92672">.</span>withColumn(<span style="color:#e6db74">&#39;age_over_18&#39;</span>,df[<span style="color:#e6db74">&#39;age&#39;</span>] <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">18</span>)<span style="color:#f92672">.</span>show(<span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span><span style="color:#e6db74">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">+----------------+---+----+-----------+
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">|         address|age|name|age_over_18|
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">+----------------+---+----+-----------+
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">|[Nanjing, China]| 12|  Li|      false|
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">+----------------+---+----+-----------+
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">only showing top 1 row
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">&#34;&#34;&#34;</span>
</span></span></code></pre></div><h5 id="4--filter">4.  filter</h5>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>df<span style="color:#f92672">.</span>filter(df<span style="color:#f92672">.</span>MathchID<span style="color:#f92672">==</span><span style="color:#e6db74">&#39;1111&#39;</span>)<span style="color:#f92672">.</span>show()
</span></span></code></pre></div><h5 id="5-sort">5. sort</h5>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>df<span style="color:#f92672">.</span>orderBy(df<span style="color:#f92672">.</span>column)  <span style="color:#75715e"># default ascending</span>
</span></span><span style="display:flex;"><span>color_df<span style="color:#f92672">.</span>sort(<span style="color:#e6db74">&#39;column&#39;</span>,ascending<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)<span style="color:#f92672">.</span>show()
</span></span></code></pre></div><h5 id="6-group">6. group</h5>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>df<span style="color:#f92672">.</span>goupby(<span style="color:#e6db74">&#39;columnname&#39;</span>)<span style="color:#f92672">.</span>count()<span style="color:#f92672">.</span>show()
</span></span></code></pre></div><p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201119164459904.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201119164459904.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201119164459904.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201119164459904.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201119164459904.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201119164459904.png"/></p>
<h5 id="7-sql">7. SQL</h5>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>df<span style="color:#f92672">.</span>registerTempTable(<span style="color:#e6db74">&#39;tablename&#39;</span>) <span style="color:#75715e"># 注册表</span>
</span></span><span style="display:flex;"><span>sqlContext<span style="color:#f92672">.</span>sql(<span style="color:#e6db74">&#39;select * from tablename&#39;</span>)
</span></span><span style="display:flex;"><span><span style="color:#75715e">#  方式二：</span>
</span></span><span style="display:flex;"><span>df<span style="color:#f92672">.</span>createOrReplaceTempView(tablename)
</span></span><span style="display:flex;"><span>result<span style="color:#f92672">=</span>spark<span style="color:#f92672">.</span>sql(<span style="color:#e6db74">&#34;sql sentences&#34;</span>)
</span></span><span style="display:flex;"><span><span style="color:#75715e">#方式三 sql function</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> pyspark.sql <span style="color:#f92672">import</span> functions <span style="color:#66d9ef">as</span> F
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> datetime <span style="color:#66d9ef">as</span> dt
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 装饰器使用</span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">@F.udf</span>()
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">calculate_birth_year</span>(age):
</span></span><span style="display:flex;"><span>    this_year <span style="color:#f92672">=</span> dt<span style="color:#f92672">.</span>datetime<span style="color:#f92672">.</span>today()<span style="color:#f92672">.</span>year
</span></span><span style="display:flex;"><span>    birth_year <span style="color:#f92672">=</span> this_year <span style="color:#f92672">-</span> age
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> birth_year 
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>calculated_df <span style="color:#f92672">=</span> df<span style="color:#f92672">.</span>select(<span style="color:#e6db74">&#34;*&#34;</span>, calculate_birth_year(<span style="color:#e6db74">&#39;age&#39;</span>)<span style="color:#f92672">.</span>alias(<span style="color:#e6db74">&#39;birth_year&#39;</span>))
</span></span><span style="display:flex;"><span>calculated_df <span style="color:#f92672">.</span>show(<span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span><span style="color:#e6db74">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">+------------------+---+-------+----------+
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">|           address|age|   name|birth_year|
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">+------------------+---+-------+----------+
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">|  [Nanjing, China]| 12|     Li|      2008|
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">|[Los Angeles, USA]| 14|Richard|      2006|
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">+------------------+---+-------+----------+
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">only showing top 2 rows
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">&#34;&#34;&#34;</span>
</span></span></code></pre></div><h5 id="8-drop">8. drop</h5>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>df<span style="color:#f92672">.</span>drop(<span style="color:#e6db74">&#39;column name&#39;</span>)<span style="color:#75715e">#：cols - 要删除的列的字符串名称，要删除的列或要删除的列的字符串名称的列表。</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#新增一列</span>
</span></span><span style="display:flex;"><span>df<span style="color:#f92672">.</span>withColumn(colName, col)
</span></span><span style="display:flex;"><span><span style="color:#75715e">#通过为原数据框添加一个新列或替换已存在的同名列而返回一个新数据框。colName —— 是一个字符串, 为新列的名字。必须是已存在的列的名字</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#col —— 为这个新列的 Column 表达式。必须是含有列的表达式。如果不是它会报错 AssertionError: col should be Column</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 重新命名聚合后结果的列名(需要修改多个列名就跟多个：withColumnRenamed)</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 聚合之后不修改列名则会显示：count(member_name)</span>
</span></span><span style="display:flex;"><span>df_res<span style="color:#f92672">.</span>agg({<span style="color:#e6db74">&#39;member_name&#39;</span>: <span style="color:#e6db74">&#39;count&#39;</span>, <span style="color:#e6db74">&#39;income&#39;</span>: <span style="color:#e6db74">&#39;sum&#39;</span>, <span style="color:#e6db74">&#39;num&#39;</span>: <span style="color:#e6db74">&#39;sum&#39;</span>})
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">.</span>withColumnRenamed(<span style="color:#e6db74">&#34;count(member_name)&#34;</span>, <span style="color:#e6db74">&#34;member_num&#34;</span>)<span style="color:#f92672">.</span>show()
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span><span style="color:#75715e">#修改数据类型</span>
</span></span><span style="display:flex;"><span>df <span style="color:#f92672">=</span> df<span style="color:#f92672">.</span>withColumn(<span style="color:#e6db74">&#34;height&#34;</span>, df[<span style="color:#e6db74">&#34;height&#34;</span>]<span style="color:#f92672">.</span>cast(IntegerType()))
</span></span></code></pre></div><h5 id="9-collect">9. collect</h5>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201119220742676.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201119220742676.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201119220742676.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201119220742676.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201119220742676.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201119220742676.png"/></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>print(data1[<span style="color:#ae81ff">0</span>][<span style="color:#e6db74">&#34;words&#34;</span>])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>df<span style="color:#f92672">.</span>collect() <span style="color:#75715e">#Row列表形式返回所有记录。</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 在只有一列的情况下可以用 [0] 来获取值</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 获取一列的所有值，或者多列的所有值</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># collect()函数将分布式的dataframe转成local类型的 list-row格式</span>
</span></span><span style="display:flex;"><span>rows<span style="color:#f92672">=</span> df<span style="color:#f92672">.</span>select(<span style="color:#e6db74">&#39;col_1&#39;</span>, <span style="color:#e6db74">&#39;col_2&#39;</span>)<span style="color:#f92672">.</span>collect()
</span></span><span style="display:flex;"><span>value <span style="color:#f92672">=</span> [[ row<span style="color:#f92672">.</span>col_1, row<span style="color:#f92672">.</span>col_2 ] <span style="color:#66d9ef">for</span> row <span style="color:#f92672">in</span> rows ]
</span></span><span style="display:flex;"><span><span style="color:#75715e">#获取第一行的多个值，返回普通python变量</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># first() 返回的是 Row 类型，可以看做是dict类型，用 row.col_name 来获取值</span>
</span></span><span style="display:flex;"><span>row <span style="color:#f92672">=</span> df<span style="color:#f92672">.</span>select(<span style="color:#e6db74">&#39;col_1&#39;</span>, <span style="color:#e6db74">&#39;col_2&#39;</span>)<span style="color:#f92672">.</span>first()
</span></span><span style="display:flex;"><span>col_1_value <span style="color:#f92672">=</span> row<span style="color:#f92672">.</span>col_1
</span></span><span style="display:flex;"><span>col_2_value <span style="color:#f92672">=</span> row<span style="color:#f92672">.</span>col_2
</span></span></code></pre></div><h5 id="10-rdd-dfpandas">10. RDD &amp;DF&amp;Pandas</h5>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>rdd_df <span style="color:#f92672">=</span> df<span style="color:#f92672">.</span>rdd	  <span style="color:#75715e"># DF转RDD</span>
</span></span><span style="display:flex;"><span>df <span style="color:#f92672">=</span> rdd_df<span style="color:#f92672">.</span>toDF()  <span style="color:#75715e"># RDD转DF</span>
</span></span><span style="display:flex;"><span>pandas_df <span style="color:#f92672">=</span> spark_df<span style="color:#f92672">.</span>toPandas()	
</span></span><span style="display:flex;"><span>spark_df <span style="color:#f92672">=</span> sqlContext<span style="color:#f92672">.</span>createDataFrame(pandas_df)
</span></span></code></pre></div><h5 id="11-udf">11. UDF</h5>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e">#udf(f=None, returnType=StringType)</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#Parameters：</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#  f – python函数（如果用作独立函数）</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#  returnType – 用户定义函数的返回类型。</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> pyspark.sql.functions <span style="color:#f92672">import</span> udf
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> pyspark.sql.types <span style="color:#f92672">import</span> StructType, StructField, IntegerType, FloatType, StringType, ArrayType
</span></span><span style="display:flex;"><span>stopwords <span style="color:#f92672">=</span> [k<span style="color:#f92672">.</span>strip() <span style="color:#66d9ef">for</span> k <span style="color:#f92672">in</span> open(<span style="color:#e6db74">&#39;./data/stopwords.txt&#39;</span>, encoding<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;utf-8&#39;</span>) <span style="color:#66d9ef">if</span> k<span style="color:#f92672">.</span>strip() <span style="color:#f92672">!=</span> <span style="color:#e6db74">&#39;&#39;</span>]
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">clearTxt</span>(line):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> line <span style="color:#f92672">!=</span> <span style="color:#e6db74">&#39;&#39;</span>:
</span></span><span style="display:flex;"><span>        line <span style="color:#f92672">=</span> line<span style="color:#f92672">.</span>strip()
</span></span><span style="display:flex;"><span>        <span style="color:#75715e">#去除文本中的英文和数字</span>
</span></span><span style="display:flex;"><span>        line <span style="color:#f92672">=</span> re<span style="color:#f92672">.</span>sub(<span style="color:#e6db74">&#34;[a-zA-Z0-9]&#34;</span>,<span style="color:#e6db74">&#34;&#34;</span>,line)
</span></span><span style="display:flex;"><span>         
</span></span><span style="display:flex;"><span>        <span style="color:#75715e">#去除文本中的中文符号和英文符号</span>
</span></span><span style="display:flex;"><span>        line <span style="color:#f92672">=</span> re<span style="color:#f92672">.</span>sub(<span style="color:#e6db74">&#34;[\s+\.\!\/_,$%^*(+</span><span style="color:#ae81ff">\&#34;\&#39;</span><span style="color:#e6db74">；：“”．]+|[+——！，。？?、~@#￥%……&amp;*（）]+&#34;</span>, <span style="color:#e6db74">&#34;&#34;</span>, line)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> line
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> <span style="color:#e6db74">&#39;Empyt Line&#39;</span>
</span></span><span style="display:flex;"><span>cutWords_list<span style="color:#f92672">=</span>[]
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">cutwordshandle</span>(jobinfo):
</span></span><span style="display:flex;"><span>    jobinfo<span style="color:#f92672">=</span>clearTxt(jobinfo)
</span></span><span style="display:flex;"><span>    cutWords <span style="color:#f92672">=</span> [k <span style="color:#66d9ef">for</span> k <span style="color:#f92672">in</span> jieba<span style="color:#f92672">.</span>cut(jobinfo,cut_all<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>) <span style="color:#66d9ef">if</span> k <span style="color:#f92672">not</span> <span style="color:#f92672">in</span> stopwords]
</span></span><span style="display:flex;"><span>    <span style="color:#75715e">#print(len(cutWords_list))</span>
</span></span><span style="display:flex;"><span>    cutWords_list<span style="color:#f92672">.</span>append(cutWords)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> cutWords
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>cutwords <span style="color:#f92672">=</span> udf(<span style="color:#66d9ef">lambda</span> z: cutwordshandle(z), ArrayType(StringType()))
</span></span><span style="display:flex;"><span>sqlcontext<span style="color:#f92672">.</span>udf<span style="color:#f92672">.</span>register(<span style="color:#e6db74">&#34;label&#34;</span>, cutwords)
</span></span><span style="display:flex;"><span>df3 <span style="color:#f92672">=</span> df2<span style="color:#f92672">.</span>withColumn( <span style="color:#e6db74">&#39;words&#39;</span>,cutwords(<span style="color:#e6db74">&#39;待遇&#39;</span>))
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> pyspark.sql <span style="color:#f92672">import</span> SQLContext
</span></span><span style="display:flex;"><span>sqlContext <span style="color:#f92672">=</span> SQLContext(sc)
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> pyspark.sql <span style="color:#f92672">import</span> Row
</span></span><span style="display:flex;"><span>csv_data <span style="color:#f92672">=</span> raw_data<span style="color:#f92672">.</span>map(<span style="color:#66d9ef">lambda</span> l: l<span style="color:#f92672">.</span>split(<span style="color:#e6db74">&#34;,&#34;</span>))
</span></span><span style="display:flex;"><span>row_data <span style="color:#f92672">=</span> csv_data<span style="color:#f92672">.</span>map(<span style="color:#66d9ef">lambda</span> p: Row(
</span></span><span style="display:flex;"><span>    duration<span style="color:#f92672">=</span>int(p[<span style="color:#ae81ff">0</span>]), 
</span></span><span style="display:flex;"><span>    protocol_type<span style="color:#f92672">=</span>p[<span style="color:#ae81ff">1</span>],
</span></span><span style="display:flex;"><span>    service<span style="color:#f92672">=</span>p[<span style="color:#ae81ff">2</span>],
</span></span><span style="display:flex;"><span>    flag<span style="color:#f92672">=</span>p[<span style="color:#ae81ff">3</span>],
</span></span><span style="display:flex;"><span>    src_bytes<span style="color:#f92672">=</span>int(p[<span style="color:#ae81ff">4</span>]),
</span></span><span style="display:flex;"><span>    dst_bytes<span style="color:#f92672">=</span>int(p[<span style="color:#ae81ff">5</span>])
</span></span><span style="display:flex;"><span>    )
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>interactions_df <span style="color:#f92672">=</span> sqlContext<span style="color:#f92672">.</span>createDataFrame(row_data)
</span></span><span style="display:flex;"><span>interactions_df<span style="color:#f92672">.</span>registerTempTable(<span style="color:#e6db74">&#34;interactions&#34;</span>)
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Select tcp network interactions with more than 1 second duration and no transfer from destination</span>
</span></span><span style="display:flex;"><span>tcp_interactions <span style="color:#f92672">=</span> sqlContext<span style="color:#f92672">.</span>sql(<span style="color:#e6db74">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    SELECT duration, dst_bytes FROM interactions WHERE protocol_type = &#39;tcp&#39; AND duration &gt; 1000 AND dst_bytes = 0
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">&#34;&#34;&#34;</span>)
</span></span><span style="display:flex;"><span>tcp_interactions<span style="color:#f92672">.</span>show()
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Output duration together with dst_bytes</span>
</span></span><span style="display:flex;"><span>tcp_interactions_out <span style="color:#f92672">=</span> tcp_interactions<span style="color:#f92672">.</span>map(<span style="color:#66d9ef">lambda</span> p: <span style="color:#e6db74">&#34;Duration: </span><span style="color:#e6db74">{}</span><span style="color:#e6db74">, Dest. bytes: </span><span style="color:#e6db74">{}</span><span style="color:#e6db74">&#34;</span><span style="color:#f92672">.</span>format(p<span style="color:#f92672">.</span>duration, p<span style="color:#f92672">.</span>dst_bytes))
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> ti_out <span style="color:#f92672">in</span> tcp_interactions_out<span style="color:#f92672">.</span>collect():
</span></span><span style="display:flex;"><span>  print ti_out
</span></span><span style="display:flex;"><span>interactions_df<span style="color:#f92672">.</span>printSchema()  <span style="color:#75715e"># printdata schema</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#---   dataframe query</span>
</span></span><span style="display:flex;"><span>interactions_df<span style="color:#f92672">.</span>select(<span style="color:#e6db74">&#34;protocol_type&#34;</span>, <span style="color:#e6db74">&#34;duration&#34;</span>, <span style="color:#e6db74">&#34;dst_bytes&#34;</span>)<span style="color:#f92672">.</span>filter(interactions_df<span style="color:#f92672">.</span>duration<span style="color:#f92672">&gt;</span><span style="color:#ae81ff">1000</span>)<span style="color:#f92672">.</span>filter(interactions_df<span style="color:#f92672">.</span>dst_bytes<span style="color:#f92672">==</span><span style="color:#ae81ff">0</span>)<span style="color:#f92672">.</span>groupBy(<span style="color:#e6db74">&#34;protocol_type&#34;</span>)<span style="color:#f92672">.</span>count()<span style="color:#f92672">.</span>show()
</span></span></code></pre></div><h5 id="111-pandas-udf">11.1. Pandas UDF</h5>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> pyspark.sql.functions <span style="color:#f92672">import</span> udf
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 使用 udf 定义一个 row-at-a-time 的 udf</span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">@udf</span>(<span style="color:#e6db74">&#39;double&#39;</span>)
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 输入/输出都是单个 double 类型的值</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">plus_one</span>(v):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> v <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>df<span style="color:#f92672">.</span>withColumn(<span style="color:#e6db74">&#39;v2&#39;</span>, plus_one(df<span style="color:#f92672">.</span>v))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> pyspark.sql.functions <span style="color:#f92672">import</span> pandas_udf, PandasUDFType
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 使用 pandas_udf 定义一个 Pandas UDF</span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">@pandas_udf</span>(<span style="color:#e6db74">&#39;double&#39;</span>, PandasUDFType<span style="color:#f92672">.</span>SCALAR)
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 输入/输出都是 double 类型的 pandas.Series</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">pandas_plus_one</span>(v):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> v <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>df<span style="color:#f92672">.</span>withColumn(<span style="color:#e6db74">&#39;v2&#39;</span>, pandas_plus_one(df<span style="color:#f92672">.</span>v))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#最小二乘法举例</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> statsmodels.api <span style="color:#66d9ef">as</span> sm
</span></span><span style="display:flex;"><span><span style="color:#75715e"># df has four columns: id, y, x1, x2</span>
</span></span><span style="display:flex;"><span>group_column <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;id&#39;</span>
</span></span><span style="display:flex;"><span>y_column <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;y&#39;</span>
</span></span><span style="display:flex;"><span>x_columns <span style="color:#f92672">=</span> [<span style="color:#e6db74">&#39;x1&#39;</span>, <span style="color:#e6db74">&#39;x2&#39;</span>]
</span></span><span style="display:flex;"><span>schema <span style="color:#f92672">=</span> df<span style="color:#f92672">.</span>select(group_column, <span style="color:#f92672">*</span>x_columns)<span style="color:#f92672">.</span>schema
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">@pandas_udf</span>(schema, PandasUDFType<span style="color:#f92672">.</span>GROUPED_MAP)
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Input/output are both a pandas.DataFrame</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">ols</span>(pdf):
</span></span><span style="display:flex;"><span>    group_key <span style="color:#f92672">=</span> pdf[group_column]<span style="color:#f92672">.</span>iloc[<span style="color:#ae81ff">0</span>]
</span></span><span style="display:flex;"><span>    y <span style="color:#f92672">=</span> pdf[y_column]
</span></span><span style="display:flex;"><span>    X <span style="color:#f92672">=</span> pdf[x_columns]
</span></span><span style="display:flex;"><span>      X <span style="color:#f92672">=</span> sm<span style="color:#f92672">.</span>add_constant(X)
</span></span><span style="display:flex;"><span>    model <span style="color:#f92672">=</span> sm<span style="color:#f92672">.</span>OLS(y, X)<span style="color:#f92672">.</span>fit()
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> pd<span style="color:#f92672">.</span>DataFrame([[group_key] <span style="color:#f92672">+</span> [model<span style="color:#f92672">.</span>params[i] <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span>   x_columns]], columns<span style="color:#f92672">=</span>[group_column] <span style="color:#f92672">+</span> x_columns)
</span></span><span style="display:flex;"><span>beta <span style="color:#f92672">=</span> df<span style="color:#f92672">.</span>groupby(group_column)<span style="color:#f92672">.</span>apply(ols)
</span></span></code></pre></div><h5 id="12-schema">12. schema</h5>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> pyspark.sql.types <span style="color:#f92672">import</span> StructField, MapType, StringType, IntegerType, StructType
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 常用的还包括 DateType 等</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>people_schema<span style="color:#f92672">=</span> StructType([
</span></span><span style="display:flex;"><span>    StructField(<span style="color:#e6db74">&#39;address&#39;</span>, MapType(StringType(), StringType()), <span style="color:#66d9ef">True</span>),
</span></span><span style="display:flex;"><span>    StructField(<span style="color:#e6db74">&#39;age&#39;</span>, LongType(), <span style="color:#66d9ef">True</span>),
</span></span><span style="display:flex;"><span>    StructField(<span style="color:#e6db74">&#39;name&#39;</span>, StringType(), <span style="color:#66d9ef">True</span>),
</span></span><span style="display:flex;"><span>])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>df <span style="color:#f92672">=</span> spark<span style="color:#f92672">.</span>read<span style="color:#f92672">.</span>json(<span style="color:#e6db74">&#39;people.json&#39;</span>, schema<span style="color:#f92672">=</span>people_schema)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>df<span style="color:#f92672">.</span>show(<span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span><span style="color:#e6db74">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">+--------------------+---+----+
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">|             address|age|name|
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">+--------------------+---+----+
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">|[country -&gt; China...| 12|  Li|
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">+--------------------+---+----+
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">only showing top 1 row
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>df<span style="color:#f92672">.</span>dtypes
</span></span><span style="display:flex;"><span><span style="color:#75715e"># [(&#39;address&#39;, &#39;map&lt;string,string&gt;&#39;), (&#39;age&#39;, &#39;bigint&#39;), (&#39;name&#39;, &#39;string&#39;)]</span>
</span></span></code></pre></div><h3 id="13--rdddataframedataset">1.3.  RDD&amp;DataFrame&amp;Dataset</h3>
<table>
<thead>
<tr>
<th><strong>Basis of Difference</strong></th>
<th><strong>Spark RDD</strong></th>
<th><strong>Spark DataFrame</strong></th>
<th><strong>Spark Dataset</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>What is it?</strong></td>
<td>A low-level API</td>
<td>A high-level abstraction</td>
<td>A <code>combination</code> of both RDDs and DataFrames</td>
</tr>
<tr>
<td><strong>Input Optimization Engine</strong></td>
<td><code>Cannot make use of input optimization engines</code></td>
<td>Uses input optimization engines to generate logical queries</td>
<td>Uses Catalyst Optimizer for input optimization, as DataFrames do</td>
</tr>
<tr>
<td><strong>Data Representation</strong></td>
<td><code>Distributed across multiple nodes of a cluster</code></td>
<td><code>A collection of rows and named columns</code></td>
<td>An extension of DataFrames, providing the functionalities of both RDDs and DataFrames</td>
</tr>
<tr>
<td><strong>Benefit</strong></td>
<td>A simple API</td>
<td>Gives a schema for the distributed data</td>
<td><code>Improves memory usage</code></td>
</tr>
<tr>
<td><strong>Immutability and Interoperability</strong></td>
<td>Tracks data lineage information to recover the lost data</td>
<td>Once transformed into a DataFrame, not possible to get the domain object</td>
<td>Can regenerate RDDs</td>
</tr>
<tr>
<td><strong>Performance Limitation</strong></td>
<td>Java Serialization and Garbage Collection overheads</td>
<td>Offers huge performance improvement over RDDs</td>
<td>Operations are performed on serialized data to improve performance</td>
</tr>
</tbody>
</table>
<h3 id="14-summary-statistics">1.4. Summary statistics</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> pyspark.mllib.stat <span style="color:#f92672">import</span> Statistics 
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> math <span style="color:#f92672">import</span> sqrt 
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Compute column summary statistics.</span>
</span></span><span style="display:flex;"><span>summary <span style="color:#f92672">=</span> Statistics<span style="color:#f92672">.</span>colStats(vector_data)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print <span style="color:#e6db74">&#34;Duration Statistics:&#34;</span>
</span></span><span style="display:flex;"><span>print <span style="color:#e6db74">&#34; Mean: </span><span style="color:#e6db74">{}</span><span style="color:#e6db74">&#34;</span><span style="color:#f92672">.</span>format(round(summary<span style="color:#f92672">.</span>mean()[<span style="color:#ae81ff">0</span>],<span style="color:#ae81ff">3</span>))
</span></span><span style="display:flex;"><span>print <span style="color:#e6db74">&#34; St. deviation: </span><span style="color:#e6db74">{}</span><span style="color:#e6db74">&#34;</span><span style="color:#f92672">.</span>format(round(sqrt(summary<span style="color:#f92672">.</span>variance()[<span style="color:#ae81ff">0</span>]),<span style="color:#ae81ff">3</span>))
</span></span><span style="display:flex;"><span>print <span style="color:#e6db74">&#34; Max value: </span><span style="color:#e6db74">{}</span><span style="color:#e6db74">&#34;</span><span style="color:#f92672">.</span>format(round(summary<span style="color:#f92672">.</span>max()[<span style="color:#ae81ff">0</span>],<span style="color:#ae81ff">3</span>))
</span></span><span style="display:flex;"><span>print <span style="color:#e6db74">&#34; Min value: </span><span style="color:#e6db74">{}</span><span style="color:#e6db74">&#34;</span><span style="color:#f92672">.</span>format(round(summary<span style="color:#f92672">.</span>min()[<span style="color:#ae81ff">0</span>],<span style="color:#ae81ff">3</span>))
</span></span><span style="display:flex;"><span>print <span style="color:#e6db74">&#34; Total value count: </span><span style="color:#e6db74">{}</span><span style="color:#e6db74">&#34;</span><span style="color:#f92672">.</span>format(summary<span style="color:#f92672">.</span>count())
</span></span><span style="display:flex;"><span>print <span style="color:#e6db74">&#34; Number of non-zero values: </span><span style="color:#e6db74">{}</span><span style="color:#e6db74">&#34;</span><span style="color:#f92672">.</span>format(summary<span style="color:#f92672">.</span>numNonzeros()[<span style="color:#ae81ff">0</span>])
</span></span></code></pre></div><h3 id="1-4-ml">1. 4. ML</h3>
<ul>
<li><strong>mllib.classification</strong>: The spark.mllib package offers support for various methods to perform binary classification, regression analysis, and multiclass classification. Some of the most used algorithms in classifications are Naive Bayes, decision trees, etc.</li>
<li><strong>mllib.clustering</strong>: In clustering, you can perform the grouping of subsets of entities on the basis of some similarities in the elements or entities.</li>
<li><strong>mllib.linalg</strong>: This algorithm offers MLlib utilities to support linear algebra.</li>
<li><strong>mllib.recommendation</strong>: This algorithm is used for recommender systems to fill in the missing entries in any dataset.</li>
<li><strong>spark.mllib</strong>: This supports collaborative filtering, where Spark uses ALS (Alternating Least Squares) to predict the missing entries in the sets of descriptions of users and products.</li>
</ul>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201119215234237.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201119215234237.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201119215234237.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201119215234237.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201119215234237.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201119215234237.png"/></p>
<ul>
<li><i class="fa-regular fa-square fa-fw" aria-hidden="true"></i> <a href="https://spark.apache.org/docs/latest/ml-features"target="_blank" rel="external nofollow noopener noreferrer">https://spark.apache.org/docs/latest/ml-features<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>  后期学习使用这里面的api，但是ML基本算法必须熟练掌握；</li>
<li><i class="fa-regular fa-square fa-fw" aria-hidden="true"></i> 后面遇到比较好的代码，案例可以多多积累；</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> urllib
</span></span><span style="display:flex;"><span>f <span style="color:#f92672">=</span> urllib<span style="color:#f92672">.</span>urlretrieve (<span style="color:#e6db74">&#34;http://kdd.ics.uci.edu/databases/kddcup99/kddcup.data.gz&#34;</span>, <span style="color:#e6db74">&#34;kddcup.data.gz&#34;</span>)
</span></span><span style="display:flex;"><span>data_file <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;./kddcup.data.gz&#34;</span>
</span></span><span style="display:flex;"><span>raw_data <span style="color:#f92672">=</span> sc<span style="color:#f92672">.</span>textFile(data_file)
</span></span><span style="display:flex;"><span>print <span style="color:#e6db74">&#34;Train data size is </span><span style="color:#e6db74">{}</span><span style="color:#e6db74">&#34;</span><span style="color:#f92672">.</span>format(raw_data<span style="color:#f92672">.</span>count())
</span></span><span style="display:flex;"><span>ft <span style="color:#f92672">=</span> urllib<span style="color:#f92672">.</span>urlretrieve(<span style="color:#e6db74">&#34;http://kdd.ics.uci.edu/databases/kddcup99/corrected.gz&#34;</span>, <span style="color:#e6db74">&#34;corrected.gz&#34;</span>)
</span></span><span style="display:flex;"><span>test_data_file <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;./corrected.gz&#34;</span>
</span></span><span style="display:flex;"><span>test_raw_data <span style="color:#f92672">=</span> sc<span style="color:#f92672">.</span>textFile(test_data_file)
</span></span><span style="display:flex;"><span>print <span style="color:#e6db74">&#34;Test data size is </span><span style="color:#e6db74">{}</span><span style="color:#e6db74">&#34;</span><span style="color:#f92672">.</span>format(test_raw_data<span style="color:#f92672">.</span>count())
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> pyspark.mllib.regression <span style="color:#f92672">import</span> LabeledPoint
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> numpy <span style="color:#f92672">import</span> array
</span></span><span style="display:flex;"><span>csv_data <span style="color:#f92672">=</span> raw_data<span style="color:#f92672">.</span>map(<span style="color:#66d9ef">lambda</span> x: x<span style="color:#f92672">.</span>split(<span style="color:#e6db74">&#34;,&#34;</span>))
</span></span><span style="display:flex;"><span>test_csv_data <span style="color:#f92672">=</span> test_raw_data<span style="color:#f92672">.</span>map(<span style="color:#66d9ef">lambda</span> x: x<span style="color:#f92672">.</span>split(<span style="color:#e6db74">&#34;,&#34;</span>))
</span></span><span style="display:flex;"><span>protocols <span style="color:#f92672">=</span> csv_data<span style="color:#f92672">.</span>map(<span style="color:#66d9ef">lambda</span> x: x[<span style="color:#ae81ff">1</span>])<span style="color:#f92672">.</span>distinct()<span style="color:#f92672">.</span>collect()
</span></span><span style="display:flex;"><span>services <span style="color:#f92672">=</span> csv_data<span style="color:#f92672">.</span>map(<span style="color:#66d9ef">lambda</span> x: x[<span style="color:#ae81ff">2</span>])<span style="color:#f92672">.</span>distinct()<span style="color:#f92672">.</span>collect()
</span></span><span style="display:flex;"><span>flags <span style="color:#f92672">=</span> csv_data<span style="color:#f92672">.</span>map(<span style="color:#66d9ef">lambda</span> x: x[<span style="color:#ae81ff">3</span>])<span style="color:#f92672">.</span>distinct()<span style="color:#f92672">.</span>collect()
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">create_labeled_point</span>(line_split):
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># leave_out = [41]</span>
</span></span><span style="display:flex;"><span>    clean_line_split <span style="color:#f92672">=</span> line_split[<span style="color:#ae81ff">0</span>:<span style="color:#ae81ff">41</span>]
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># convert protocol to numeric categorical variable</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">try</span>: 
</span></span><span style="display:flex;"><span>        clean_line_split[<span style="color:#ae81ff">1</span>] <span style="color:#f92672">=</span> protocols<span style="color:#f92672">.</span>index(clean_line_split[<span style="color:#ae81ff">1</span>])
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">except</span>:
</span></span><span style="display:flex;"><span>        clean_line_split[<span style="color:#ae81ff">1</span>] <span style="color:#f92672">=</span> len(protocols)  
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># convert service to numeric categorical variable</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">try</span>:
</span></span><span style="display:flex;"><span>        clean_line_split[<span style="color:#ae81ff">2</span>] <span style="color:#f92672">=</span> services<span style="color:#f92672">.</span>index(clean_line_split[<span style="color:#ae81ff">2</span>])
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">except</span>:
</span></span><span style="display:flex;"><span>        clean_line_split[<span style="color:#ae81ff">2</span>] <span style="color:#f92672">=</span> len(services)
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># convert flag to numeric categorical variable</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">try</span>:
</span></span><span style="display:flex;"><span>        clean_line_split[<span style="color:#ae81ff">3</span>] <span style="color:#f92672">=</span> flags<span style="color:#f92672">.</span>index(clean_line_split[<span style="color:#ae81ff">3</span>])
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">except</span>:
</span></span><span style="display:flex;"><span>        clean_line_split[<span style="color:#ae81ff">3</span>] <span style="color:#f92672">=</span> len(flags)
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># convert label to binary label</span>
</span></span><span style="display:flex;"><span>    attack <span style="color:#f92672">=</span> <span style="color:#ae81ff">1.0</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> line_split[<span style="color:#ae81ff">41</span>]<span style="color:#f92672">==</span><span style="color:#e6db74">&#39;normal.&#39;</span>:
</span></span><span style="display:flex;"><span>        attack <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.0</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> LabeledPoint(attack, array([float(x) <span style="color:#66d9ef">for</span> x <span style="color:#f92672">in</span> ]))
</span></span><span style="display:flex;"><span>training_data <span style="color:#f92672">=</span> csv_data<span style="color:#f92672">.</span>map(create_labeled_point)
</span></span><span style="display:flex;"><span>test_data <span style="color:#f92672">=</span> test_csv_data<span style="color:#f92672">.</span>map(create_labeled_point)
</span></span><span style="display:flex;"><span><span style="color:#75715e">#--- training the classifier</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> pyspark.mllib.tree <span style="color:#f92672">import</span> DecisionTree, DecisionTreeModel
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> time <span style="color:#f92672">import</span> time
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Build the model</span>
</span></span><span style="display:flex;"><span>t0 <span style="color:#f92672">=</span> time()
</span></span><span style="display:flex;"><span>tree_model <span style="color:#f92672">=</span> DecisionTree<span style="color:#f92672">.</span>trainClassifier(training_data, numClasses<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>, 
</span></span><span style="display:flex;"><span>                                          categoricalFeaturesInfo<span style="color:#f92672">=</span>{<span style="color:#ae81ff">1</span>: len(protocols), <span style="color:#ae81ff">2</span>: len(services), <span style="color:#ae81ff">3</span>: len(flags)}, impurity<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;gini&#39;</span>, maxDepth<span style="color:#f92672">=</span><span style="color:#ae81ff">4</span>, maxBins<span style="color:#f92672">=</span><span style="color:#ae81ff">100</span>)
</span></span><span style="display:flex;"><span>tt <span style="color:#f92672">=</span> time() <span style="color:#f92672">-</span> t0
</span></span><span style="display:flex;"><span>print <span style="color:#e6db74">&#34;Classifier trained in </span><span style="color:#e6db74">{}</span><span style="color:#e6db74"> seconds&#34;</span><span style="color:#f92672">.</span>format(round(tt,<span style="color:#ae81ff">3</span>))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#--- predict</span>
</span></span><span style="display:flex;"><span>predictions <span style="color:#f92672">=</span> tree_model<span style="color:#f92672">.</span>predict(test_data<span style="color:#f92672">.</span>map(<span style="color:#66d9ef">lambda</span> p: p<span style="color:#f92672">.</span>features))
</span></span><span style="display:flex;"><span>labels_and_preds <span style="color:#f92672">=</span> test_data<span style="color:#f92672">.</span>map(<span style="color:#66d9ef">lambda</span> p: p<span style="color:#f92672">.</span>label)<span style="color:#f92672">.</span>zip(predictions)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>t0 <span style="color:#f92672">=</span> time()
</span></span><span style="display:flex;"><span>test_accuracy <span style="color:#f92672">=</span> labels_and_preds<span style="color:#f92672">.</span>filter(<span style="color:#66d9ef">lambda</span> (v, p): v <span style="color:#f92672">==</span> p)<span style="color:#f92672">.</span>count() <span style="color:#f92672">/</span> float(test_data<span style="color:#f92672">.</span>count())
</span></span><span style="display:flex;"><span>tt <span style="color:#f92672">=</span> time() <span style="color:#f92672">-</span> t0
</span></span><span style="display:flex;"><span>print <span style="color:#e6db74">&#34;Prediction made in </span><span style="color:#e6db74">{}</span><span style="color:#e6db74"> seconds. Test accuracy is </span><span style="color:#e6db74">{}</span><span style="color:#e6db74">&#34;</span><span style="color:#f92672">.</span>format(round(tt,<span style="color:#ae81ff">3</span>), round(test_accuracy,<span style="color:#ae81ff">4</span>))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print <span style="color:#e6db74">&#34;Learned classification tree model:&#34;</span>
</span></span><span style="display:flex;"><span>print tree_model<span style="color:#f92672">.</span>toDebugString()
</span></span></code></pre></div><p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://intellipaat.com/mediaFiles/2019/03/spark-and-rdd-cheat-sheet-1.png"
    data-srcset="https://intellipaat.com/mediaFiles/2019/03/spark-and-rdd-cheat-sheet-1.png, https://intellipaat.com/mediaFiles/2019/03/spark-and-rdd-cheat-sheet-1.png 1.5x, https://intellipaat.com/mediaFiles/2019/03/spark-and-rdd-cheat-sheet-1.png 2x"
    data-sizes="auto"
    alt="https://intellipaat.com/mediaFiles/2019/03/spark-and-rdd-cheat-sheet-1.png"
    title="https://intellipaat.com/mediaFiles/2019/03/spark-and-rdd-cheat-sheet-1.png"/></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201119212743127.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201119212743127.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201119212743127.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201119212743127.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201119212743127.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201119212743127.png"/></p>
<ul>
<li><a href="https://github.com/jadianes/spark-py-notebooks"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/jadianes/spark-py-notebooks<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></li>
<li><a href="https://www.cnblogs.com/sight-tech/p/12990579.html"target="_blank" rel="external nofollow noopener noreferrer">https://www.cnblogs.com/sight-tech/p/12990579.html<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></li>
<li><a href="https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame"target="_blank" rel="external nofollow noopener noreferrer">https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></li>
<li><a href="https://intellipaat.com/mediaFiles/2019/03/PySpark-SQL-cheat-sheet.pdf"target="_blank" rel="external nofollow noopener noreferrer">https://intellipaat.com/mediaFiles/2019/03/PySpark-SQL-cheat-sheet.pdf<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></li>
<li><a href="https://www.cnblogs.com/liaowuhen1314/p/12792202.html"target="_blank" rel="external nofollow noopener noreferrer">https://www.cnblogs.com/liaowuhen1314/p/12792202.html<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></li>
<li><a href="https://spark.apache.org/docs/2.2.0/sql-programming-guide.html"target="_blank" rel="external nofollow noopener noreferrer">https://spark.apache.org/docs/2.2.0/sql-programming-guide.html<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></li>
</ul>
</div>
<div class="post-footer" id="post-footer">
  <div class="post-info">
    <div class="post-info-line">
      <div class="post-info-mod">
        <span title=2023-12-31&#32;16:55:50>更新于 2023-12-31&nbsp;</span>
      </div><div class="post-info-license">
          <span><a rel="license external nofollow noopener noreferrer" href="https://creativecommons.org/licenses/by-nc/4.0/" target="_blank">CC BY-NC 4.0</a></span>
        </div></div>
    <div class="post-info-line">
      <div class="post-info-md"><span><a href="/pyspark/index.md" title="阅读原始文档" class="link-to-markdown">阅读原始文档</a></span><span><a href="https://liudongdong1.github.io/edit/master/content/posts%5c%e6%a1%86%e6%9e%b6%e8%ae%be%e8%ae%a1%5csparkhadoop%5cPySpark.md" title="编辑此页"target="_blank" rel="external nofollow noopener noreferrer" class="link-to-edit">编辑此页</a></span></div>
      <div class="post-info-share">
        <span><a href="javascript:void(0);" title="分享到 Twitter" data-sharer="twitter" data-url="https://liudongdong1.github.io/pyspark/" data-title="PySpark" data-hashtags="stream,spark"><i class="fa-brands fa-twitter fa-fw" aria-hidden="true"></i></a>
  <a href="javascript:void(0);" title="分享到 Facebook" data-sharer="facebook" data-url="https://liudongdong1.github.io/pyspark/" data-hashtag="stream"><i class="fa-brands fa-facebook-square fa-fw" aria-hidden="true"></i></a>
  <a href="javascript:void(0);" title="分享到 微博" data-sharer="weibo" data-url="https://liudongdong1.github.io/pyspark/" data-title="PySpark" data-image="https://cdn.stocksnap.io/img-thumbs/280h/LUYHIZXNWX.jpg"><i class="fa-brands fa-weibo fa-fw" aria-hidden="true"></i></a>
  </span>
      </div>
    </div>
  </div>

  <div class="post-info-more">
    <section class="post-tags"><i class="fa-solid fa-tags fa-fw" aria-hidden="true"></i>&nbsp;<a href="/tags/stream/">stream</a>,&nbsp;<a href="/tags/spark/">spark</a></section>
    <section>
      <span><a href="javascript:void(0);" onclick="window.history.back();">返回</a></span>&nbsp;|&nbsp;<span><a href="/">主页</a></span>
    </section>
  </div>

  <div class="post-nav"><a href="/pytorchpoint/" class="prev" rel="prev" title="PytorchPoint"><i class="fa-solid fa-angle-left fa-fw" aria-hidden="true"></i>PytorchPoint</a>
      <a href="/openmmlab/" class="next" rel="next" title="OpenMMLab">OpenMMLab<i class="fa-solid fa-angle-right fa-fw" aria-hidden="true"></i></a></div>
</div>
</article></main><footer class="footer">
    <div class="footer-container"><div class="footer-line powered">由 <a href="https://gohugo.io/" target="_blank" rel="external nofollow noopener noreferrer" title="Hugo 0.118.2">Hugo</a> 强力驱动 | 主题 - <a href="https://github.com/hugo-fixit/FixIt" target="_blank" rel="external" title="FixIt v0.2.17-RC"><img class="fixit-icon" src="/fixit.min.svg" alt="FixIt logo" />&nbsp;FixIt</a>
        </div><div class="footer-line copyright" itemscope itemtype="http://schema.org/CreativeWork"><i class="fa-regular fa-copyright fa-fw" aria-hidden="true"></i>
            <span itemprop="copyrightYear">2020 - 2024</span><span class="author" itemprop="copyrightHolder">
              <a href="https://liudongdong1.github.io/"target="_blank" rel="external nofollow noopener noreferrer">LiuDongdong</a></span><span class="license footer-divider"><a rel="license external nofollow noopener noreferrer" href="https://creativecommons.org/licenses/by-nc/4.0/" target="_blank">CC BY-NC 4.0</a></span></div><div class="footer-line statistics"><span class="site-time" title='网站运行中 ...'><i class="fa-solid fa-heartbeat fa-fw animate-icon" aria-hidden="true"></i>&nbsp;<span class="run-times">网站运行中 ...</span></span></div><div class="footer-line ibruce">
          <span id="busuanzi_container_site_uv" title='总访客数'><i class="fa-regular fa-user fa-fw" aria-hidden="true"></i>&nbsp;<span id="busuanzi_value_site_uv"><i class="fa-solid fa-spinner fa-spin fa-fw" aria-hidden="true"></i></span></span><span id="busuanzi_container_site_pv" class="footer-divider" title='总访问量'><i class="fa-regular fa-eye fa-fw" aria-hidden="true"></i>&nbsp;<span id="busuanzi_value_site_pv"><i class="fa-solid fa-spinner fa-spin fa-fw" aria-hidden="true"></i></span></span>
        </div></div>
  </footer></div><div class="widgets"><div class="fixed-buttons animate__faster d-none"><div class="fixed-button back-to-top" role="button" aria-label="回到顶部"><i class="fa-solid fa-arrow-up fa-fw" aria-hidden="true"></i><span class="variant-numeric">0%</span>
        </div></div><a href="https://liudongdong1.github.io/" title="在 GitHub 上查看源代码"target="_blank" rel="external nofollow" class="github-corner right d-none-mobile"><svg viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><div id="mask"></div><div class="reading-progress-bar" style="left: 0;top: 0;--bg-progress: #0076ff;--bg-progress-dark: #fff;"></div><noscript>
    <div class="noscript-warning">FixIt 主题在启用 JavaScript 的情况下效果最佳。</div>
  </noscript>
</div><link rel="stylesheet" href="/lib/katex/katex.min.css"><link rel="stylesheet" href="/lib/cookieconsent/cookieconsent.min.css"><script src="/lib/autocomplete/autocomplete.min.js" defer></script><script src="/lib/algoliasearch/algoliasearch-lite.umd.min.js" defer></script><script src="/lib/lazysizes/lazysizes.min.js" async defer></script><script src="/lib/sharer/sharer.min.js" async defer></script><script src="/lib/typeit/index.umd.js" defer></script><script src="/lib/katex/katex.min.js" defer></script><script src="/lib/katex/auto-render.min.js" defer></script><script src="/lib/katex/copy-tex.min.js" defer></script><script src="/lib/katex/mhchem.min.js" defer></script><script src="/lib/cookieconsent/cookieconsent.min.js" defer></script><script src="/lib/pangu/pangu.min.js" defer></script><script src="/lib/cell-watermark/watermark.min.js" defer></script><script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async defer></script><script>window.config={"code":{"copyTitle":"复制到剪贴板","editLockTitle":"锁定可编辑代码块","editUnLockTitle":"解锁可编辑代码块","editable":true,"maxShownLines":10},"comment":{"enable":false},"cookieconsent":{"content":{"dismiss":"同意","link":"了解更多","message":"本网站使用 Cookies 来改善您的浏览体验。"},"enable":true,"palette":{"button":{"background":"#f0f0f0"},"popup":{"background":"#1aa3ff"}},"theme":"edgeless"},"data":{"typeit-header-subtitle-desktop":"\u003cspan style='font-family: MMT,\"沐目体\";'\u003e吾日三省吾身\u003c/span\u003e","typeit-header-subtitle-mobile":"\u003cspan style='font-family: MMT,\"沐目体\";'\u003e吾日三省吾身\u003c/span\u003e"},"enablePWA":true,"enablePangu":true,"math":{"delimiters":[{"display":true,"left":"$$","right":"$$"},{"display":true,"left":"\\[","right":"\\]"},{"display":true,"left":"\\begin{equation}","right":"\\end{equation}"},{"display":true,"left":"\\begin{equation*}","right":"\\end{equation*}"},{"display":true,"left":"\\begin{align}","right":"\\end{align}"},{"display":true,"left":"\\begin{align*}","right":"\\end{align*}"},{"display":true,"left":"\\begin{alignat}","right":"\\end{alignat}"},{"display":true,"left":"\\begin{alignat*}","right":"\\end{alignat*}"},{"display":true,"left":"\\begin{gather}","right":"\\end{gather}"},{"display":true,"left":"\\begin{CD}","right":"\\end{CD}"},{"display":false,"left":"$","right":"$"},{"display":false,"left":"\\(","right":"\\)"}],"strict":false},"search":{"algoliaAppID":"2R1K9SKLQZ","algoliaIndex":"index.zh-cn","algoliaSearchKey":"4a226aa1c5c98d6859e4d1386adb2bc7","highlightTag":"em","maxResultLength":10,"noResultsFound":"没有找到结果","snippetLength":50,"type":"algolia"},"siteTime":"2020-12-18T16:15:22+08:00","typeit":{"cursorChar":"|","cursorSpeed":1000,"data":{"typeit-header-subtitle-desktop":["typeit-header-subtitle-desktop"],"typeit-header-subtitle-mobile":["typeit-header-subtitle-mobile"]},"duration":-1,"speed":100},"watermark":{"appendto":".wrapper\u003emain","colspacing":30,"content":"\u003cimg class=\"fixit-icon\" src=\"/fixit.min.svg\" alt=\"FixIt logo\" /\u003e FixIt 主题","enable":true,"fontfamily":"inherit","fontsize":0.85,"height":21,"opacity":0.0125,"rotate":15,"rowspacing":60,"width":150}};</script><script src="/js/theme.min.js" defer></script><script src="/js/custom.min.js" defer></script></body>
</html>
