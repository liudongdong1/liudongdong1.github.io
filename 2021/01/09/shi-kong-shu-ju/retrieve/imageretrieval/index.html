<!DOCTYPE HTML>
<html lang="en">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="ImageRetrieval, AIOT,Space&amp;Temporal Sequence Analysis,SpringBoot,liudongdong1,cloud">
    <meta name="description" content="">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>ImageRetrieval | DaybyDay</title>
    <link rel="icon" type="image/png" href="/favicon.png">

    <link rel="stylesheet" type="text/css" href="/libs/awesome/css/all.css">
    <link rel="stylesheet" type="text/css" href="/libs/materialize/materialize.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/aos/aos.css">
    <link rel="stylesheet" type="text/css" href="/libs/animate/animate.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/lightGallery/css/lightgallery.min.css">
    <link rel="stylesheet" type="text/css" href="/css/matery.css">
    <link rel="stylesheet" type="text/css" href="/css/my.css">

    <script src="/libs/jquery/jquery.min.js"></script>

<meta name="generator" content="Hexo 4.2.0"><link rel="alternate" href="/atom.xml" title="DaybyDay" type="application/atom+xml">
<link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css"></head>


<body>
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/" class="waves-effect waves-light">
                    
                    <img src="/medias/logo.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">DaybyDay</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>Index</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>Tags</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>Categories</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>Archives</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/" class="waves-effect waves-light">

      
      <i class="fas fa-user-circle" style="zoom: 0.6;"></i>
      
      <span>About</span>
      <i class="fas fa-chevron-down" aria-hidden="true" style="zoom: 0.6;"></i>
    </a>
    <ul class="sub-nav menus_item_child ">
      
      <li>
        <a href="/about">
          
          <i class="fas fa-user-circle" style="margin-top: -20px; zoom: 0.6;"></i>
          
          <span>about</span>
        </a>
      </li>
      
      <li>
        <a href="/resume">
          
          <i class="fa fa-user-secret" style="margin-top: -20px; zoom: 0.6;"></i>
          
          <span>resume</span>
        </a>
      </li>
      
    </ul>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/friends" class="waves-effect waves-light">
      
      <i class="fas fa-address-book" style="zoom: 0.6;"></i>
      
      <span>Friends</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/gallery" class="waves-effect waves-light">
      
      <i class="fas fa-camera" style="zoom: 0.6;"></i>
      
      <span>Galleries</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="Search" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/medias/logo.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">DaybyDay</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			Index
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			Tags
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			Categories
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			Archives
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="javascript:;">
			
				<i class="fa-fw fas fa-user-circle"></i>
			
			About
			<span class="m-icon"><i class="fas fa-chevron-right"></i></span>
		</a>
            <ul  style="background:  ;" >
              
                <li>

                  <a href="/about " style="margin-left:75px">
				  
				   <i class="fa fas fa-user-circle" style="position: absolute;left:50px" ></i>
			      
		          <span>about</span>
                  </a>
                </li>
              
                <li>

                  <a href="/resume " style="margin-left:75px">
				  
				   <i class="fa fa fa-user-secret" style="position: absolute;left:50px" ></i>
			      
		          <span>resume</span>
                  </a>
                </li>
              
            </ul>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/friends" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-address-book"></i>
			
			Friends
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/gallery" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-camera"></i>
			
			Galleries
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/liudongdong1" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/liudongdong1" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://cdn.stocksnap.io/img-thumbs/280h/VTRBWVOXY6.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">ImageRetrieval</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <link rel="stylesheet" href="/libs/tocbot/tocbot.css">
<style>
    #articleContent h1::before,
    #articleContent h2::before,
    #articleContent h3::before,
    #articleContent h4::before,
    #articleContent h5::before,
    #articleContent h6::before {
        display: block;
        content: " ";
        height: 100px;
        margin-top: -100px;
        visibility: hidden;
    }

    #articleContent :focus {
        outline: none;
    }

    .toc-fixed {
        position: fixed;
        top: 64px;
    }

    .toc-widget {
        width: 345px;
        padding-left: 20px;
    }

    .toc-widget .toc-title {
        margin: 35px 0 15px 0;
        padding-left: 17px;
        font-size: 1.5rem;
        font-weight: bold;
        line-height: 1.5rem;
    }

    .toc-widget ol {
        padding: 0;
        list-style: none;
    }

    #toc-content {
        height: calc(100vh - 250px);
        overflow: auto;
    }

    #toc-content ol {
        padding-left: 10px;
    }

    #toc-content ol li {
        padding-left: 10px;
    }

    #toc-content .toc-link:hover {
        color: #42b983;
        font-weight: 700;
        text-decoration: underline;
    }

    #toc-content .toc-link::before {
        background-color: transparent;
        max-height: 25px;

        position: absolute;
        right: 23.5vw;
        display: block;
    }

    .toc-fixed .toc-link::before{
        position: fixed!important;/*当toc的位置改为fixed时，.toc-link::before也要改为fixed*/
    }

    #toc-content .is-active-link {
        color: #42b983;
    }

    #toc-content .is-active-link::before {
        background-color: #42b983;
    }

    #floating-toc-btn {
        position: fixed;
        right: 15px;
        bottom: 76px;
        padding-top: 15px;
        margin-bottom: 0;
        z-index: 998;
    }

    #floating-toc-btn .btn-floating {
        width: 48px;
        height: 48px;
    }

    #floating-toc-btn .btn-floating i {
        line-height: 48px;
        font-size: 1.4rem;
    }
</style>
<div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/tags/retrieval/">
                                <span class="chip bg-color">retrieval</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/categories/%E8%A7%86%E8%A7%89AI/" class="post-category">
                                视觉AI
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>Publish Date:&nbsp;&nbsp;
                    2021-01-09
                </div>
                

                <!-- 
                    <i class="fa fa-pencil"></i> Author: liudongdong1
                  -->

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>Update Date:&nbsp;&nbsp;
                    2021-12-13
                </div>
                

                <!-- 
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>Word Count:&nbsp;&nbsp;
                    3.5k
                </div>
                 -->

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>Read Times:&nbsp;&nbsp;
                    21 Min
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>Read Count:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">
        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>给定一个包含特定实例(例如特定目标、场景、建筑等)的查询图像，图像检索旨在从<code>数据库图像</code>中找到包含<code>相同实例的图像</code>。但由于<code>不同图像的拍摄视角、光照、或遮挡情况不同</code>，如何设计出能应对这些类内差异的有效且高效的图像检索算法仍是一项研究难题。</p>
</blockquote>
<h1 id="1-Survey"><a href="#1-Survey" class="headerlink" title="1. Survey"></a>1. Survey</h1><blockquote>
<p>Chen W, Liu Y, Wang W, et al. Deep image retrieval: A survey[J]. arXiv preprint arXiv:2101.11282, 2021. <a href="chrome-extension://ikhdkkncnoglghljlkmcimlnlhkeamad/pdf-viewer/web/viewer.html?file=https%3A%2F%2Farxiv.org%2Fpdf%2F2101.11282.pdf">url</a></p>
</blockquote>
<ul>
<li><strong>content based image retrieval(CBIR)</strong> is the problem of searching for semantically matched or similar image in a large image gallery by analyzing their visua, content.  <code>campact yet rich feature representations</code><ul>
<li><code>Application era:</code> Person re-identification, remote sensing, medical image search, shopping recommendation in onlene markets;</li>
<li><code>instance level</code>: a query image of a particular object or scene is given and the goal is to find images containing the same object or scene that ma be captured under different conditions;</li>
<li><code>cactegory level:</code> find images of the same class as the query;</li>
<li><code>feature engineering era:</code> hand-engineered feature descriptors SIFT;</li>
<li><code>feature learning era:</code>AlexNet, ImageNet;</li>
</ul>
</li>
<li><strong>Challenges &amp; Goal:</strong><ul>
<li>reducing the <code>semantic gap</code></li>
<li>improving retrival scalability: <code>domain shift</code></li>
<li>balancing<code>retrieval accuracy and efficiency</code>:  high dimensional anc contain more semantic-aware information</li>
</ul>
</li>
</ul>
<p><img src="https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211006104414562.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211006104630732.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211006105237367.png" alt=""></p>
<ul>
<li><strong>single feedforward pass methods:</strong> take the whole image and feed it into model to extract features.   lacks geometric invariance and spatial information.</li>
<li><strong>multiple feedforward pass methods:</strong> using sliding windows or spatial pyramid model to create multi-scale image patches, and each patch is fed into the model before being encodeded as a final global feature.  instead of generating multi-scale image patches randomly and densely, <code>region proposal methods</code> are introduced like RPNs, CKNs.</li>
</ul>
<blockquote>
<p>(a)-(b) <code>Non-parametric mechanisms:</code> The attention is based on convolutional feature maps x with size H ×W ×C. <code>Channel-wise attention in (a)</code> produces a C-dimensional importance vector α1 [10], [30]. <code>Spatial-wise attention in (b)</code>computes a 2-dimensional attention map α2 [10], [28], [59], [79]. (c)-(d) <code>Parametric mechanisms</code>: The attention weights β are provided by a sub-network with trainable parameters (e.g. θ in (c)) [97], [98]. Likewise, some off-the-shelf models [91], [99] can predict the attention maps from the input image directly.  </p>
</blockquote>
<p><img src="https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211006122356475.png" alt="Attention mechanisms"></p>
<blockquote>
<p>Representative methods in single feedforward frameworks, focusing on convolutional feature maps x with<br>size H ×W ×C: MAC [47], R-MAC [27], GeM pooling [41], SPoC with the Gaussian weighting scheme [7], CroW [10], and CAM+CroW [28]. Note that g1(·) and g2(·) represent spatialwise and channel-wise weighting functions, respectively.  </p>
</blockquote>
<p><img src="https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211006110245921.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211006111137946.png" alt="Image patch generation"></p>
<ul>
<li><strong>Deep feature selection:</strong><ul>
<li>a fully-connected layer has a global receptive field, but<code>lack of spatial information (using multiple feedforward passes)</code>; <code>lack of local geometric invariance (leverage intermediate convolutional layers)</code></li>
<li>a convolutional layer arranges the spatial information well and produces location-adaptive features.  sum-pooling convolutional features(SPoC) to compact descriptors pre-processed with Gaussian center prior. use BoW model to embed convolutional featuers separately, use VLAD to encode local features into VLAD features.</li>
</ul>
</li>
<li><strong>Feature fustion strategy</strong><ul>
<li><code>a layer-level fusion</code>: fusing feature from different layers with different balancing weights aims at combining different feature properties within a feature extractor.  <code>features from fully-connected layers(global features) and features from convolutional layers(local features) can complement each other when measuring semantic similarity and guarantee retrival performance</code>.</li>
<li><code>model-level fusion</code>: combine features on different models to achieve improved performance, categorized into intra-model and inter-model.<ul>
<li>intra-model: multiple deep models having similar or highly compatible structures.</li>
<li>inter-model: involves models with more differin structures.</li>
<li><code>early fusion</code>: straightforward to fuse all types of features from the candidate models and then learning a metric based on the concatenated feature; <code>late fusion:</code> learn optimal metrics separately for the features from each model, and then to uniformly combine these metrics for final retrieval ranking. <code>What features are the best to combined</code></li>
</ul>
</li>
</ul>
</li>
<li><strong>Deep feature Enhancement</strong><ul>
<li><code>feature aggregation:</code>   sum/average pooling is less disciminative, taking into all activated outputs from conv layer, which weakening the effect of highly activated features. <code>max pooling</code>: for sparse features that have a low probability of being active.   conv feature maps can be directly aggregated to produce global features by spatial pooling.</li>
<li><code>feature embedding:</code> embed the conv feature maps into a high dimensional space to obtain compact features, lik BoW, VLAD, FV. And using PCA to reduce embedding demision.</li>
<li><code>Attention Mechanisms</code>: to highlight th emost relevant features and to avoid the influence of irrelevant activations.</li>
<li><code>Deep hash Embedding</code>: to tranform deep features into more compact codes, hash functions can be plugged as a layer into deep networks, so that the hasn codes can be trained and optimized with model simultaneously. The hash codes of originally similar images are embedded as close as possible.<ul>
<li>preserving image similarity: to minimize the inconsistencies between the real-valued features and corresponding hash codes. <ul>
<li>class label available: loss function are designed to learn hash codes in a Hamming space. like optimize the difference between  matrices computed from the binary codes and their supervision lavels.  <code>Siamese loss, triplet loss, adversarial learning</code> is used to retain semantic similarity where only dissiilar pairs keep their distance within a margin.</li>
<li>unsupervised hash learning: using Bayes classifiers, KNN graphs, K-means algorithms, AutoEncoders, Generative adversarial networks</li>
</ul>
</li>
<li>improving hash function quality: aims at making the binary codes uniformly distributed.</li>
</ul>
</li>
</ul>
</li>
<li><strong>Supervised Fine-tuning</strong>: <ul>
<li>classification-based Fine-tuning: improves the model-lebel adaptability for new datasets, but may have some difficulties in learning discriminative intra-class variability to distinguish particular objects.</li>
<li>verification-based Fine-tuning: learn an optimal metric which minimizes or maximizes the distance of pairs to validate and maintain their similarity. Focus on both inter-class and intra-class samples.<ul>
<li>a pair-wise constraint, corresponding to a Siamese network, in which input images are paired with either a positive or negative sample.</li>
<li>a triplet constraint, associated with triplet netwroks, in which anchor images are paired with both similar and dissimilar samples.</li>
<li>glovally supervised approaces(c,d) learn a metric on gloval features by satisfying all constraints, locally supervised approaches focus on local areas by only satisfying the given local constriants.</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><img src="https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211006124924358.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211006125645651.png" alt="sample mining strategies"></p>
<ul>
<li><strong>Unsupervised Fine-tuning</strong>: <ul>
<li><code>mining samples with Manifold learning:</code> to capture intrinsic correlations on the manifold structure to mine or deduce revelance.</li>
<li><code>AutoEncoder-based Frameworks</code>: to reconstruct its output as closely as possible to its input.</li>
</ul>
</li>
</ul>
<blockquote>
<p>First stage: the affinity matrix is interpreted as a weighted kNN graph, where each vector is represented by a node, and edges are difined by the pairwise affinities of two connected nodes. the pairwise affinities are re-evaluated in the context of all other elements by diffusing the similarity values through the graph. the difference among random walk are lie primarily in three aspects</p>
<ul>
<li>similarity initialization: affects the subsequenct KNN graph construction in an affinity matrix;</li>
<li>transition matrix definition: a row-stochastic matrix, determines the probabilities of transiting from one node to another in the graph.</li>
<li>iteration scheme: to re-valuate and update the values in affinity matrix by the manifold similarity until some kind of convergence is achieved.</li>
</ul>
</blockquote>
<p><img src="https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211006130746382.png" alt=""></p>
<h1 id="2-Paper-Reading"><a href="#2-Paper-Reading" class="headerlink" title="2. Paper Reading"></a>2. Paper Reading</h1><blockquote>
<p>Yoon, Sangwoong, et al. “Image-to-Image Retrieval by Learning Similarity between Scene Graphs.” <em>arXiv preprint arXiv:2012.14700</em> (2020).</p>
</blockquote>
<hr>
<h2 id="Paper-Scene-Graphs"><a href="#Paper-Scene-Graphs" class="headerlink" title="Paper: Scene Graphs"></a>Paper: Scene Graphs</h2><div align="center">
<br>
<b>Image-to-Image Retrieval
by Learning Similarity between Scene Graphs
</b>
</div>

<h4 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h4><ol>
<li>performing image retrieval with complex images that have multiple objects and various relationships between them remains challenging:<ul>
<li>overly sensitive to low-level and local visual features;</li>
<li>no publicly available labeled data to train and evaluate the image retrieval system for complex image.</li>
</ul>
</li>
<li>propose IRSGS, a novel image retrieval framework that utilizes the  similarty between scene graphs computed from a graph neural network to retrieve semantically similar images;</li>
<li>propose to train the proposed retrieval framework with the surrogate relevance measure obtained from image captions and a pre-trained language model;</li>
</ol>
<ul>
<li>the scene graph  S={objects, attributes, relations};  the surrogate relevance measure between two images as the similarity between their captions;</li>
</ul>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210109154029944.png" alt=""></p>
<h4 id="Relative-work"><a href="#Relative-work" class="headerlink" title="Relative work:"></a>Relative work:</h4><ul>
<li><strong>Image Retrieval:</strong> using visual feature representations; object categories, text descriptions;</li>
<li><strong>Scene Graphs</strong>: image captioning; visual question answering; image-ground dialog;</li>
<li><strong>Graph Similarity learning:</strong> use the learned graph representations of two graph to calculate similarity;</li>
</ul>
<p><img src="https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20210109153411093.png" alt="image-20210109153411093"></p>
<blockquote>
<p>Teichmann, Marvin, et al. “Detect-to-retrieve: Efficient regional aggregation for image search.” <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em>. 2019.</p>
</blockquote>
<hr>
<h2 id="Paper-Detect-to-retrieve"><a href="#Paper-Detect-to-retrieve" class="headerlink" title="Paper: Detect-to-retrieve"></a>Paper: Detect-to-retrieve</h2><div align="center">
<br>
<b>Detect-to-retrieve: Efficient regional aggregation for image search
</b>
</div>

<h4 id="Summary-1"><a href="#Summary-1" class="headerlink" title="Summary"></a>Summary</h4><ol>
<li>improving region selection: introduce a dataset of manually boxed landmark images, with 86k images from 15k unique classes;</li>
<li>leverage the trained detector and produce more efficient regional search systems, which improves accuracy for small objects with only a modest increase to the databases size;</li>
<li>propose regional aggregated match kernels to leverage selected image regions and produce a discriminative image representation.</li>
</ol>
<p><img src="https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20210110083533162.png" alt=""></p>
<blockquote>
<p>Deep local features and object regions are extracted from an image.   Regional aggregation proceeds in two steps, using a large codebook of visual words: first, per-region <code>VLAD description</code>; second, sum pooling and per-visual word normalization.</p>
</blockquote>
<h4 id="RelatedWork"><a href="#RelatedWork" class="headerlink" title="RelatedWork:"></a>RelatedWork:</h4><ul>
<li>Region search and aggregation: <ul>
<li>regional search: selected regions are encoded independently in the database; using <code>VLAD</code> or <code>Fisher Vectors</code>;</li>
<li>regional aggregation: selected regions are used to improve image representations. like leverage the grid structure form to pool pretained CNN features;</li>
</ul>
</li>
</ul>
<h4 id="Features"><a href="#Features" class="headerlink" title="Features:"></a>Features:</h4><ul>
<li><p>*<em>Regional Search&amp;&amp;Aggregation: *</em> <code>build on top of deep local features(DELF) and aggregated selective match kernels(ASMK)</code>;</p>
<ul>
<li>match kernel framework: <ul>
<li>Image X with M local descriptors: <img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210110084612921.png" alt=""></li>
<li>code book C comprising C visual words, learned using k-means, is used to quantize the descriptors;</li>
<li><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210110084743327.png" alt=""></li>
<li>encompasses popular local feature aggregation techniques such as Bag-of-Words, VLAD, and ASMK. Similarity: <img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210110084818574.png" alt=""></li>
<li>an aggregated vector representation: <img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210110085056545.png" alt=""></li>
<li>a scalar selectivity function: $\sigma(.)$;<img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210110085307986.png" alt=""></li>
<li>normalization factor: <img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210110085732839.png" alt=""></li>
</ul>
</li>
</ul>
</li>
<li><p><strong>Regional Search:</strong> query image X &amp;&amp; database image $Y^{(n)}$;</p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210110090030382.png" alt=""></p>
</li>
<li><p><strong>Regional Aggregated Match Kernels:</strong></p>
<ul>
<li><p>storing descriptors of each region independently in the database incurs additional cost for both memory and search computation.</p>
</li>
<li><p>utilizing the detected bounding boxes to instead improve the aggregated representations of database images–producing discriminative descriptors at no additional cost.</p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210110090415987.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210110090445547.png" alt="For VLAD"></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210110090613560.png" alt="R-ASMK"></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210110090643427.png" alt="R-AMK"></p>
</li>
</ul>
</li>
</ul>
<blockquote>
<p>Weyand, Tobias, et al. “Google Landmarks Dataset v2-A Large-Scale Benchmark for Instance-Level Recognition and Retrieval.” <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>. 2020.</p>
</blockquote>
<hr>
<h2 id="Paper-Google-Landmarks-Dataset-v2"><a href="#Paper-Google-Landmarks-Dataset-v2" class="headerlink" title="Paper: Google Landmarks Dataset v2"></a>Paper: Google Landmarks Dataset v2</h2><div align="center">
<br>
<b>A Large-Scale Benchmark for Instance-Level Recognition and Retrieval
</b>
</div>

<h4 id="Summary-2"><a href="#Summary-2" class="headerlink" title="Summary"></a>Summary</h4><ul>
<li>the new dataset has several long-tailed class distribution, a large fraction of out-of-domain test photos and large intro-class variability.<ul>
<li>class distribution;  intra-class variation;  out-of-domain query images;</li>
</ul>
</li>
<li>introduce the Google Landmarks Dataset v2, a new large-scale dataset for instance-level recognition and retrieval, includes over 5M images of over 200k human-made and natural landmarks.</li>
</ul>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210110093703863.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210110094638916.png" alt=""></p>
<blockquote>
<p>Caron M, Touvron H, Misra I, et al. Emerging properties in self-supervised vision transformers[J]. arXiv preprint arXiv:2104.14294, 2021.</p>
</blockquote>
<hr>
<h1 id="Paper-ViT"><a href="#Paper-ViT" class="headerlink" title="Paper: ViT"></a>Paper: ViT</h1><div align="center">
<br>
<b>Emerging properties in self-supervised vision transformersTitle</b>
</div>


<h4 id="Summary-3"><a href="#Summary-3" class="headerlink" title="Summary"></a>Summary</h4><ol>
<li>self-supervised ViT features contain explicit information about the <code>semantic segmentation of an image</code>, which does not emerge as clearly with supervised ViTs, nor with convnets.  These features are also excellent k-NN classifiers.</li>
</ol>
<h4 id="Application-Area"><a href="#Application-Area" class="headerlink" title="Application Area:"></a><strong>Application Area</strong>:</h4><ul>
<li>Image Retrieval<ul>
<li>Copy detection</li>
<li>discovering the semantic layout of scenes</li>
<li>video instance segmentation</li>
</ul>
</li>
</ul>
<h4 id="Methods"><a href="#Methods" class="headerlink" title="Methods"></a>Methods</h4><ul>
<li><p><strong>Problem Formulation</strong>:</p>
</li>
<li><p><strong>system overview</strong>:</p>
</li>
</ul>
<p><img src="https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211007135531081.png" alt=""></p>
<h1 id="3-project-Code"><a href="#3-project-Code" class="headerlink" title="3. project Code"></a>3. project Code</h1><h3 id="1-image-match"><a href="#1-image-match" class="headerlink" title=".1. image-match"></a>.1. <a href="https://github.com/ProvenanceLabs/image-match" target="_blank" rel="noopener">image-match</a></h3><blockquote>
<p>Wong H C, Bern M, Goldberg D. An image signature for any kind of image[C]//Proceedings. International Conference on Image Processing. IEEE, 2002, 1: I-I.  <a href="https://github.com/dsys/match" target="_blank" rel="noopener">code</a></p>
<ul>
<li>star 2.6k, 1.6k, tutorial: <a href="https://image-match.readthedocs.io/en/latest/start.html" target="_blank" rel="noopener">link</a>, traditional methods, including <code>python, elesticserach</code></li>
<li>sensitive enough to allow efficient nearest-neighbor search,  to effectively filter a database for possible duplicates, and yet robutst enough to find duplicates that hve been resized, rescanned, or lossily compressed.</li>
</ul>
</blockquote>
<ol>
<li>if the image is color, first convert it to 8-bit gray scale using the standard color-conversion algorithms, include djpeg and ppmtopgm. 255: pure white; 0: pure black;</li>
<li>impose a 9*9 grid of points on the image. fro each column of the image, compute teh sum of absolute values of differences between adjacent pixels, compute the total of all columns, and crop the image at 5% and 95% columns. and crop the rows of the image the same way. Then divede the croped image into 10*10 grid of blocks, and setting a 9*9 grid of points on the image.</li>
<li>at each grid point, compute the average gray level of the P*P square centered at the grid point.</li>
<li>for each grid point, compute an 8-element array whose elements give a comparision of the average gray level of the grid point square with those of its eight neighbors.</li>
<li>the signature of an image is simply the concatenation of the 8-element arrays corresponding to the grid points, ordered left-right, top-bottom. 9*9*8=648;</li>
</ol>
<p><img src="https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211006135153568.png" alt=""></p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> image_match.goldberg <span class="keyword">import</span> ImageSignature</span><br><span class="line">gis = ImageSignature()</span><br><span class="line">a = gis.generate_signature(<span class="string">'https://upload.wikimedia.org/wikipedia/commons/thumb/e/ec/Mona_Lisa,_by_Leonardo_da_Vinci,_from_C2RMF_retouched.jpg/687px-Mona_Lisa,_by_Leonardo_da_Vinci,_from_C2RMF_retouched.jpg'</span>)</span><br><span class="line">b = gis.generate_signature(<span class="string">'https://pixabay.com/static/uploads/photo/2012/11/28/08/56/mona-lisa-67506_960_720.jpg'</span>)</span><br><span class="line">gis.normalized_distance(a, b)  <span class="comment">#计算俩个图像之间距离</span></span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> elasticsearch <span class="keyword">import</span> Elasticsearch</span><br><span class="line"><span class="keyword">from</span> image_match.elasticsearch_driver <span class="keyword">import</span> SignatureES</span><br><span class="line"></span><br><span class="line">es = Elasticsearch()</span><br><span class="line">ses = SignatureES(es)</span><br><span class="line"></span><br><span class="line">ses.add_image(<span class="string">'https://upload.wikimedia.org/wikipedia/commons/thumb/e/ec/Mona_Lisa,_by_Leonardo_da_Vinci,_from_C2RMF_retouched.jpg/687px-Mona_Lisa,_by_Leonardo_da_Vinci,_from_C2RMF_retouched.jpg'</span>)</span><br><span class="line">ses.add_image(<span class="string">'https://pixabay.com/static/uploads/photo/2012/11/28/08/56/mona-lisa-67506_960_720.jpg'</span>)</span><br><span class="line">ses.add_image(<span class="string">'https://upload.wikimedia.org/wikipedia/commons/e/e0/Caravaggio_-_Cena_in_Emmaus.jpg'</span>)</span><br><span class="line">ses.add_image(<span class="string">'https://c2.staticflickr.com/8/7158/6814444991_08d82de57e_z.jpg'</span>)</span><br><span class="line"></span><br><span class="line">ses.search_image(<span class="string">'https://pixabay.com/static/uploads/photo/2012/11/28/08/56/mona-lisa-67506_960_720.jpg'</span>)</span><br></pre></td></tr></tbody></table></figure>

<h3 id="2-EagleEye"><a href="#2-EagleEye" class="headerlink" title=".2. EagleEye"></a>.2. <strong><a href="https://github.com/ThoughtfulDev/EagleEye" target="_blank" rel="noopener">EagleEye</a></strong></h3><blockquote>
<ul>
<li>You enter this data into EagleEye and it tries to find Instagram, Youtube, Facebook, and Twitter Profiles of this person.</li>
<li>using python, dlib, face_reognition, Selenum, find person, star 2.8k</li>
<li>including facebook, google, imageraider, instagram 代码。</li>
</ul>
</blockquote>
<h3 id="3-PyRetri"><a href="#3-PyRetri" class="headerlink" title=".3. PyRetri"></a>.3. <strong><a href="https://github.com/PyRetri/PyRetri" target="_blank" rel="noopener"><code>PyRetri</code></a></strong></h3><blockquote>
<p>Hu B, Song R J, Wei X S, et al. PyRetri: A PyTorch-based library for unsupervised image retrieval by Deep Convolutional Neural Networks[C]//Proceedings of the 28th ACM International Conference on Multimedia. 2020: 4461-4464.   <a href="chrome-extension://ikhdkkncnoglghljlkmcimlnlhkeamad/pdf-viewer/web/viewer.html?file=https%3A%2F%2Farxiv.org%2Fpdf%2F2005.02154.pdf">pdf</a>  star: 946</p>
</blockquote>
<ul>
<li>an open source library for deep learning based unsupervised image retrival, encapsulating the retrieval process in several stages and provides functionality tha tcovers various prominent methods for each stage.</li>
<li>propose the first open sourve framework to unify the pipeline of deep learning based unsupervised image retrieval.</li>
<li>provide high quality implementations of CBIR algorithms to solve retrieval tasks with emphasis on usaility</li>
</ul>
<p><img src="https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211006140532252.png" alt=""></p>
<h4 id="1-Pre-processing-methods"><a href="#1-Pre-processing-methods" class="headerlink" title=".1. Pre-processing methods"></a>.1. Pre-processing methods</h4><ul>
<li>DirectResize (DR): Scaling the height and width of the image to the target size directly.</li>
<li>PadResize (PR): Scaling the longer side of the image to the target size and filling the remaining pixels with the meanvalues of ImageNet.</li>
<li>ShorterResize (SR): Scaling the shorter side of the image to the target size.</li>
<li>TwoFlip (TF): Returning the original image and the corresponding horizontally flipped image.</li>
<li>CenterCrop (CC): Cropping the image from its center region according to the given size.</li>
<li>TenCrop (TC): Cropping the original image and the flipping image from up down left right and center, respectively.</li>
</ul>
<h4 id="2-Feature-Represention-Methods"><a href="#2-Feature-Represention-Methods" class="headerlink" title=".2. Feature Represention Methods"></a>.2. Feature Represention Methods</h4><ul>
<li>GAP: Global average pooling.</li>
<li>GMP: Global max pooling.</li>
<li>R-MAC [14]: Calculating feature vectors based on the regionalmaximum activation of convolutions.</li>
<li>SPoC [2]: Assigning larger weights to the central descriptorsduring aggregation.</li>
<li>CroW [7]: A weighted pooling method for both spatial- andchannel-wise.</li>
<li>SCDA [17]: Keeping useful deep descriptors based on the sum-mation of feature map activations.</li>
<li>GeM [11]: Exploiting the generalized mean to reserve theinformation of each channel.</li>
<li>PWA [19]: Aggregating the regional representations weightedby the selected part detectors’ output.</li>
<li>PCB [13]: Outputting a convolutional descriptor consisting ofseveral part-level features</li>
</ul>
<h4 id="3-Post-precessing-Methods"><a href="#3-Post-precessing-Methods" class="headerlink" title=".3. Post-precessing Methods"></a>.3. Post-precessing Methods</h4><ul>
<li>SVD [6]: Reducing feature dimension through singular valuedecomposition of matrix.</li>
<li>PCA [18]: Projecting high-dimensional features into fewerinformative dimensions.</li>
<li>DBA [1]: Every feature in the database is replaced with aweighted sum of the point’s own value and those of its topknearest neighbors (k-NN).</li>
<li>QE [3]: Combining the retrieved top-knearest neighbors withthe original query and doing another retrieval.</li>
<li>k-reciprocal [22]: Encodingk-reciprocal nearest neighbors toenhance the accuracy of retrieval</li>
</ul>
<h4 id="4-Database"><a href="#4-Database" class="headerlink" title=".4. Database"></a>.4. Database</h4><ul>
<li>Oxford5k [9] collects crawling images fromFlickrusing thenames of 11 different landmarks in Oxford, which is a repre-sentative landmark retrieval task.</li>
<li>CUB-200-2011 [15] contains photos of 200 bird species, whichrepresents fine-grained image retrieval.</li>
<li>Indoor [10] contains indoor scene images with 67 categories,representing for the scene retrieval/recognition task.</li>
<li>Caltech101 [4] consists pictures of objects belonging to 101categories, standing for the generic image retrieval task.</li>
<li>•Market-1501 [20] contains images taken on the Tsinghua cam-pus under six camera viewpoints, which is the benchmarkdataset for person re-identification.</li>
<li>DukeMTMC-reID [12] contains images captured by eight cam-eras, which is a more challenging person Re-ID dataset.</li>
</ul>
<h3 id="4-natural-language-image-search"><a href="#4-natural-language-image-search" class="headerlink" title="4. natural-language-image-search"></a>4. <a href="https://github.com/haltakov/natural-language-image-search" target="_blank" rel="noopener">natural-language-image-search</a></h3><blockquote>
<p>Radford A, Kim J W, Hallacy C, et al. Learning transferable visual models from natural language supervision[J]. arXiv preprint arXiv:2103.00020, 2021. <code>star 4.9k</code>  <a href="https://github.com/openai/CLIP" target="_blank" rel="noopener">url</a>   <a href="chrome-extension://ikhdkkncnoglghljlkmcimlnlhkeamad/pdf-viewer/web/viewer.html?file=https%3A%2F%2Farxiv.org%2Fpdf%2F2103.00020.pdf">pdf</a></p>
</blockquote>
<blockquote>
<p>OpenAI’s <a href="https://openai.com/blog/clip/" target="_blank" rel="noopener">CLIP</a> neural networs is able to transform both images and text into the same latent space, where they can be compared using a similarity measure. all photos from the full <a href="https://unsplash.com/data" target="_blank" rel="noopener">Unsplash Dataset</a> (almost 2M photos) were downloaded and processed with CLIP. The precomputed feature vectors for all images can then be used to find the best match to a natural language search query.</p>
</blockquote>
<blockquote>
<p>introducing a neural network called <code>CLIP</code> which efficiently learns <code>visual concepts from natural language supervision</code>. CLIP can be applied to any visual classification benchmark by simply providing the names of the visual categories to be recognized, similar to the “zero-shot” capabilities of GPT-2 and GPT-3.</p>
</blockquote>
<blockquote>
<p>CLIP pre-trains an image encoder and a text encoder to predict which images were paired with which texts in our dataset. We then use this behavior to turn CLIP into a zero-shot classifier. We convert all of a dataset’s classes into captions such as “a photo of a <em>dog</em>” and predict the class of the caption CLIP estimates best pairs with a given image.</p>
</blockquote>
<p><img src="https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211006141807218.png" alt=""></p>
<h3 id="5-sis"><a href="#5-sis" class="headerlink" title="5. sis"></a>5. <a href="https://github.com/matsui528/sis" target="_blank" rel="noopener">sis</a></h3><p><img src="https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211006142200903.png" alt=""></p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras.preprocessing <span class="keyword">import</span> image</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.applications.vgg16 <span class="keyword">import</span> VGG16, preprocess_input</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.models <span class="keyword">import</span> Model</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># See https://keras.io/api/applications/ for details</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FeatureExtractor</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        base_model = VGG16(weights=<span class="string">'imagenet'</span>)</span><br><span class="line">        self.model = Model(inputs=base_model.input, outputs=base_model.get_layer(<span class="string">'fc1'</span>).output)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">extract</span><span class="params">(self, img)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Extract a deep feature from an input image</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            img: from PIL.Image.open(path) or tensorflow.keras.preprocessing.image.load_img(path)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            feature (np.ndarray): deep feature with the shape=(4096, )</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        img = img.resize((<span class="number">224</span>, <span class="number">224</span>))  <span class="comment"># VGG must take a 224x224 img as an input</span></span><br><span class="line">        img = img.convert(<span class="string">'RGB'</span>)  <span class="comment"># Make sure img is color</span></span><br><span class="line">        x = image.img_to_array(img)  <span class="comment"># To np.array. Height x Width x Channel. dtype=float32</span></span><br><span class="line">        x = np.expand_dims(x, axis=<span class="number">0</span>)  <span class="comment"># (H, W, C)-&gt;(1, H, W, C), where the first elem is the number of img</span></span><br><span class="line">        x = preprocess_input(x)  <span class="comment"># Subtracting avg values for each pixel</span></span><br><span class="line">        feature = self.model.predict(x)[<span class="number">0</span>]  <span class="comment"># (1, 4096) -&gt; (4096, )</span></span><br><span class="line">        <span class="keyword">return</span> feature / np.linalg.norm(feature)  <span class="comment"># Normalize</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    fe = FeatureExtractor()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> img_path <span class="keyword">in</span> sorted(Path(<span class="string">"./static/img"</span>).glob(<span class="string">"*.jpg"</span>)):</span><br><span class="line">        print(img_path)  <span class="comment"># e.g., ./static/img/xxx.jpg</span></span><br><span class="line">        feature = fe.extract(img=Image.open(img_path))</span><br><span class="line">        feature_path = Path(<span class="string">"./static/feature"</span>) / (img_path.stem + <span class="string">".npy"</span>)  <span class="comment"># e.g., ./static/feature/xxx.npy</span></span><br><span class="line">        np.save(feature_path, feature)</span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> feature_extractor <span class="keyword">import</span> FeatureExtractor</span><br><span class="line"><span class="keyword">from</span> datetime <span class="keyword">import</span> datetime</span><br><span class="line"><span class="keyword">from</span> flask <span class="keyword">import</span> Flask, request, render_template</span><br><span class="line"><span class="keyword">from</span> pathlib <span class="keyword">import</span> Path</span><br><span class="line"></span><br><span class="line">app = Flask(__name__)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Read image features</span></span><br><span class="line">fe = FeatureExtractor()</span><br><span class="line">features = []</span><br><span class="line">img_paths = []</span><br><span class="line"><span class="keyword">for</span> feature_path <span class="keyword">in</span> Path(<span class="string">"./static/feature"</span>).glob(<span class="string">"*.npy"</span>):</span><br><span class="line">    features.append(np.load(feature_path))</span><br><span class="line">    img_paths.append(Path(<span class="string">"./static/img"</span>) / (feature_path.stem + <span class="string">".jpg"</span>))</span><br><span class="line">features = np.array(features)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@app.route('/', methods=['GET', 'POST'])</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">index</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">if</span> request.method == <span class="string">'POST'</span>:</span><br><span class="line">        file = request.files[<span class="string">'query_img'</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Save query image</span></span><br><span class="line">        img = Image.open(file.stream)  <span class="comment"># PIL image</span></span><br><span class="line">        uploaded_img_path = <span class="string">"static/uploaded/"</span> + datetime.now().isoformat().replace(<span class="string">":"</span>, <span class="string">"."</span>) + <span class="string">"_"</span> + file.filename</span><br><span class="line">        img.save(uploaded_img_path)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Run search</span></span><br><span class="line">        query = fe.extract(img)</span><br><span class="line">        dists = np.linalg.norm(features-query, axis=<span class="number">1</span>)  <span class="comment"># L2 distances to features</span></span><br><span class="line">        ids = np.argsort(dists)[:<span class="number">30</span>]  <span class="comment"># Top 30 results</span></span><br><span class="line">        scores = [(dists[id], img_paths[id]) <span class="keyword">for</span> id <span class="keyword">in</span> ids]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> render_template(<span class="string">'index.html'</span>,</span><br><span class="line">                               query_path=uploaded_img_path,</span><br><span class="line">                               scores=scores)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> render_template(<span class="string">'index.html'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">"__main__"</span>:</span><br><span class="line">    app.run(<span class="string">"0.0.0.0"</span>)</span><br></pre></td></tr></tbody></table></figure>





<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script>
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        Author:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://liudongdong1.github.io" rel="external nofollow noreferrer">liudongdong1</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        Link:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://liudongdong1.github.io/2021/01/09/shi-kong-shu-ju/retrieve/imageretrieval/">https://liudongdong1.github.io/2021/01/09/shi-kong-shu-ju/retrieve/imageretrieval/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        Reprint policy:
                    </i>
                </span>
                <span class="reprint-info">
                    All articles in this blog are used except for special statements
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    reprint polocy. If reproduced, please indicate source
                    <a href="https://liudongdong1.github.io" target="_blank">liudongdong1</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>Copied successfully, please follow the reprint policy of this article</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">more</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/tags/retrieval/">
                                    <span class="chip bg-color">retrieval</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="qq,qzone,wechat,weibo,douban" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
                <style>
    #reward {
        margin: 40px 0;
        text-align: center;
    }

    #reward .reward-link {
        font-size: 1.4rem;
        line-height: 38px;
    }

    #reward .btn-floating:hover {
        box-shadow: 0 6px 12px rgba(0, 0, 0, 0.2), 0 5px 15px rgba(0, 0, 0, 0.2);
    }

    #rewardModal {
        width: 320px;
        height: 350px;
    }

    #rewardModal .reward-title {
        margin: 15px auto;
        padding-bottom: 5px;
    }

    #rewardModal .modal-content {
        padding: 10px;
    }

    #rewardModal .close {
        position: absolute;
        right: 15px;
        top: 15px;
        color: rgba(0, 0, 0, 0.5);
        font-size: 1.3rem;
        line-height: 20px;
        cursor: pointer;
    }

    #rewardModal .close:hover {
        color: #ef5350;
        transform: scale(1.3);
        -moz-transform:scale(1.3);
        -webkit-transform:scale(1.3);
        -o-transform:scale(1.3);
    }

    #rewardModal .reward-tabs {
        margin: 0 auto;
        width: 210px;
    }

    .reward-tabs .tabs {
        height: 38px;
        margin: 10px auto;
        padding-left: 0;
    }

    .reward-content ul {
        padding-left: 0 !important;
    }

    .reward-tabs .tabs .tab {
        height: 38px;
        line-height: 38px;
    }

    .reward-tabs .tab a {
        color: #fff;
        background-color: #ccc;
    }

    .reward-tabs .tab a:hover {
        background-color: #ccc;
        color: #fff;
    }

    .reward-tabs .wechat-tab .active {
        color: #fff !important;
        background-color: #22AB38 !important;
    }

    .reward-tabs .alipay-tab .active {
        color: #fff !important;
        background-color: #019FE8 !important;
    }

    .reward-tabs .reward-img {
        width: 210px;
        height: 210px;
    }
</style>

<div id="reward">
    <a href="#rewardModal" class="reward-link modal-trigger btn-floating btn-medium waves-effect waves-light red">赏</a>

    <!-- Modal Structure -->
    <div id="rewardModal" class="modal">
        <div class="modal-content">
            <a class="close modal-close"><i class="fas fa-times"></i></a>
            <h4 class="reward-title">你的赏识是我前进的动力</h4>
            <div class="reward-content">
                <div class="reward-tabs">
                    <ul class="tabs row">
                        <li class="tab col s6 alipay-tab waves-effect waves-light"><a href="#alipay">支付宝</a></li>
                        <li class="tab col s6 wechat-tab waves-effect waves-light"><a href="#wechat">微 信</a></li>
                    </ul>
                    <div id="alipay">
                        <img src="/medias/reward/alipay.jpg" class="reward-img" alt="支付宝打赏二维码">
                    </div>
                    <div id="wechat">
                        <img src="/medias/reward/wechat.png" class="reward-img" alt="微信打赏二维码">
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>

<script>
    $(function () {
        $('.tabs').tabs();
    });
</script>

            
        </div>
    </div>

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;Previous</div>
            <div class="card">
                <a href="/2021/01/09/shi-jue-ai/dataglove/leapmotionapp/">
                    <div class="card-image">
                        
                        <img src="https://gitee.com/github-25970295/blogImage/raw/master/img/20210501133311.png" class="responsive-img" alt="LeapmotionApp">
                        
                        <span class="card-title">LeapmotionApp</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            1. air mouse

https://github.com/search?q=leap+motion+air+mouse&amp;type=Repositories
2. Slides presentation

https://gi
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2021-01-09
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/%E8%A7%86%E8%A7%89AI/" class="post-category">
                                    视觉AI
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/leap-motion/">
                        <span class="chip bg-color">leap motion</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                Next&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/2021/01/06/aiot/rfid/rfidsetting/">
                    <div class="card-image">
                        
                        <img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210121160903612.png" class="responsive-img" alt="RFIDSetting">
                        
                        <span class="card-title">RFIDSetting</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            
The Octane SDK includes the core library by acting as a wrapper for extraction, modifying, and the application of a Rea
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2021-01-06
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/AIOT/" class="post-category">
                                    AIOT
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/demo/">
                        <span class="chip bg-color">demo</span>
                    </a>
                    
                    <a href="/tags/RFID/">
                        <span class="chip bg-color">RFID</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/libs/codeBlock/codeBlockFuction.js"></script>

<!-- 代码语言 -->

<script type="text/javascript" src="/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/libs/codeBlock/codeShrink.js"></script>


<!-- 代码块折行 -->

<style type="text/css">
code[class*="language-"], pre[class*="language-"] { white-space: pre !important; }
</style>


    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;TOC</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // modify the toc link href to support Chinese.
        let i = 0;
        let tocHeading = 'toc-heading-';
        $('#toc-content a').each(function () {
            $(this).attr('href', '#' + tocHeading + (++i));
        });

        // modify the heading title id to support Chinese.
        i = 0;
        $('#articleContent').children('h2, h3, h4').each(function () {
            $(this).attr('id', tocHeading + (++i));
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>

    

</main>




    <!-- <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="463294659"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/libs/aplayer/APlayer.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/meting@2/dist/Meting.min.js"></script>

    
    <div class="container row center-align" style="margin-bottom: 0px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            <span id="year">2019</span>
            <a href="https://liudongdong1.github.io" target="_blank">liudongdong</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            <br>
            
            &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                class="white-color">1206.4k</span>&nbsp;字
            
            
            
            
            
            
            <span id="busuanzi_container_site_pv">
                |&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;<span id="busuanzi_value_site_pv"
                    class="white-color"></span>&nbsp;次
            </span>
            
            
            <span id="busuanzi_container_site_uv">
                |&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;<span id="busuanzi_value_site_uv"
                    class="white-color"></span>&nbsp;人
            </span>
            
            <br>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/liudongdong1/" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:3463264078@qq.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>














    <a href="https://blog.csdn.net/liudongdong19/" class="tooltipped" target="_blank" data-tooltip="关注我的CSDN: https://blog.csdn.net/liudongdong19/" data-position="top" data-delay="50">
        <i class="fab fa-csdn">C</i>
    </a>





</div>
    </div>
</footer>

<div class="progress-bar"></div>
 -->

    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;Search</span>
            <input type="search" id="searchInput" name="s" placeholder="Please enter a search keyword"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script src="/js/search.js"></script>
<script type="text/javascript">
$(function () {
    searchFunc("/search.xml", 'searchInput', 'searchResult');
});
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script type="text/javascript" src="/js/CFS.Snow.min.js"></script>
    <!-- 点击爆灯效果 -->
    <canvas class="fireworks" style="position: fixed;left: 0;top: 0;z-index: 1; pointer-events: none;" ></canvas> 
    <script type="text/javascript" src="//cdn.bootcss.com/animejs/2.2.0/anime.min.js"></script> 
    <script type="text/javascript" src="/js/fireworks.js"></script>
    <!--动态线条背景-->
    <script type="text/javascript"
        color="122 103 238" opacity='0.7' zIndex="-2" count="200" src="//cdn.bootcss.com/canvas-nest.js/1.0.0/canvas-nest.min.js">
    </script>
    <!-- 天气 -->
    <!-- weather -->
    <!-- weather -->
    <script type="text/javascript">
         WIDGET = {FID: 'knAMQaFanP'}
    </script>
    <script type="text/javascript" src="https://apip.weatherdt.com/float/static/js/r.js?v=1111"></script>
    <script type="text/javascript" src="https://apip.weatherdt.com/float/static/js/r.js?v=1111"></script>
    <script src="/libs/materialize/materialize.min.js"></script>
    <script src="/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/libs/aos/aos.js"></script>
    <script src="/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/js/matery.js"></script>

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    
    <script async src="/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    

    
    
    <script type="text/javascript" size="150" alpha='0.6'
        zIndex="-1" src="/libs/background/ribbon-refresh.min.js" async="async"></script>
    

    

    
    <script src="/libs/instantpage/instantpage.js" type="module"></script>
    
    <!-- {% include '_custom/custom.swig' %} -->

</body>

</html>
