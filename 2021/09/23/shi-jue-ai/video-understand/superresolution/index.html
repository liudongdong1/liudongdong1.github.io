<!DOCTYPE HTML>
<html lang="en">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="SuperResolution, AIOT,Space&amp;Temporal Sequence Analysis,SpringBoot,liudongdong1,cloud">
    <meta name="description" content="">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>SuperResolution | DaybyDay</title>
    <link rel="icon" type="image/png" href="/favicon.png">

    <link rel="stylesheet" type="text/css" href="/libs/awesome/css/all.css">
    <link rel="stylesheet" type="text/css" href="/libs/materialize/materialize.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/aos/aos.css">
    <link rel="stylesheet" type="text/css" href="/libs/animate/animate.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/lightGallery/css/lightgallery.min.css">
    <link rel="stylesheet" type="text/css" href="/css/matery.css">
    <link rel="stylesheet" type="text/css" href="/css/my.css">

    <script src="/libs/jquery/jquery.min.js"></script>

<meta name="generator" content="Hexo 4.2.0"><link rel="alternate" href="/atom.xml" title="DaybyDay" type="application/atom+xml">
<link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css"></head>


<body>
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/" class="waves-effect waves-light">
                    
                    <img src="/medias/logo.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">DaybyDay</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>Index</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>Tags</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>Categories</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>Archives</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/" class="waves-effect waves-light">

      
      <i class="fas fa-user-circle" style="zoom: 0.6;"></i>
      
      <span>About</span>
      <i class="fas fa-chevron-down" aria-hidden="true" style="zoom: 0.6;"></i>
    </a>
    <ul class="sub-nav menus_item_child ">
      
      <li>
        <a href="/about">
          
          <i class="fas fa-user-circle" style="margin-top: -20px; zoom: 0.6;"></i>
          
          <span>about</span>
        </a>
      </li>
      
      <li>
        <a href="/resume">
          
          <i class="fa fa-user-secret" style="margin-top: -20px; zoom: 0.6;"></i>
          
          <span>resume</span>
        </a>
      </li>
      
    </ul>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/friends" class="waves-effect waves-light">
      
      <i class="fas fa-address-book" style="zoom: 0.6;"></i>
      
      <span>Friends</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/gallery" class="waves-effect waves-light">
      
      <i class="fas fa-camera" style="zoom: 0.6;"></i>
      
      <span>Galleries</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="Search" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/medias/logo.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">DaybyDay</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			Index
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			Tags
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			Categories
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			Archives
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="javascript:;">
			
				<i class="fa-fw fas fa-user-circle"></i>
			
			About
			<span class="m-icon"><i class="fas fa-chevron-right"></i></span>
		</a>
            <ul  style="background:  ;" >
              
                <li>

                  <a href="/about " style="margin-left:75px">
				  
				   <i class="fa fas fa-user-circle" style="position: absolute;left:50px" ></i>
			      
		          <span>about</span>
                  </a>
                </li>
              
                <li>

                  <a href="/resume " style="margin-left:75px">
				  
				   <i class="fa fa fa-user-secret" style="position: absolute;left:50px" ></i>
			      
		          <span>resume</span>
                  </a>
                </li>
              
            </ul>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/friends" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-address-book"></i>
			
			Friends
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/gallery" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-camera"></i>
			
			Galleries
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/liudongdong1" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/liudongdong1" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://cdn.stocksnap.io/img-thumbs/280h/QSCRVBUU2G.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">SuperResolution</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <link rel="stylesheet" href="/libs/tocbot/tocbot.css">
<style>
    #articleContent h1::before,
    #articleContent h2::before,
    #articleContent h3::before,
    #articleContent h4::before,
    #articleContent h5::before,
    #articleContent h6::before {
        display: block;
        content: " ";
        height: 100px;
        margin-top: -100px;
        visibility: hidden;
    }

    #articleContent :focus {
        outline: none;
    }

    .toc-fixed {
        position: fixed;
        top: 64px;
    }

    .toc-widget {
        width: 345px;
        padding-left: 20px;
    }

    .toc-widget .toc-title {
        margin: 35px 0 15px 0;
        padding-left: 17px;
        font-size: 1.5rem;
        font-weight: bold;
        line-height: 1.5rem;
    }

    .toc-widget ol {
        padding: 0;
        list-style: none;
    }

    #toc-content {
        height: calc(100vh - 250px);
        overflow: auto;
    }

    #toc-content ol {
        padding-left: 10px;
    }

    #toc-content ol li {
        padding-left: 10px;
    }

    #toc-content .toc-link:hover {
        color: #42b983;
        font-weight: 700;
        text-decoration: underline;
    }

    #toc-content .toc-link::before {
        background-color: transparent;
        max-height: 25px;

        position: absolute;
        right: 23.5vw;
        display: block;
    }

    .toc-fixed .toc-link::before{
        position: fixed!important;/*当toc的位置改为fixed时，.toc-link::before也要改为fixed*/
    }

    #toc-content .is-active-link {
        color: #42b983;
    }

    #toc-content .is-active-link::before {
        background-color: #42b983;
    }

    #floating-toc-btn {
        position: fixed;
        right: 15px;
        bottom: 76px;
        padding-top: 15px;
        margin-bottom: 0;
        z-index: 998;
    }

    #floating-toc-btn .btn-floating {
        width: 48px;
        height: 48px;
    }

    #floating-toc-btn .btn-floating i {
        line-height: 48px;
        font-size: 1.4rem;
    }
</style>
<div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/tags/VideoAnalyse/">
                                <span class="chip bg-color">VideoAnalyse</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/categories/%E8%A7%86%E8%A7%89AI/" class="post-category">
                                视觉AI
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>Publish Date:&nbsp;&nbsp;
                    2021-09-23
                </div>
                

                <!-- 
                    <i class="fa fa-pencil"></i> Author: liudongdong1
                  -->

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>Update Date:&nbsp;&nbsp;
                    2021-11-06
                </div>
                

                <!-- 
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>Word Count:&nbsp;&nbsp;
                    3.9k
                </div>
                 -->

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>Read Times:&nbsp;&nbsp;
                    24 Min
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>Read Count:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">
        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>Tian Y, Zhang Y, Fu Y, et al. Tdan: Temporally-deformable alignment network for video super-resolution[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020: 3360-3369.</p>
</blockquote>
<h3 id="Paper-Tdan"><a href="#Paper-Tdan" class="headerlink" title="Paper: Tdan"></a>Paper: Tdan</h3><div align="center">
<br>
<b> Tdan: Temporally-deformable alignment network for video super-resolution</b>
</div>


<h4 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h4><ol>
<li>propose a temporally-deformable alignment network(TDAN) to <code>adaptively align the reference frame and each supporting frame a the feature level without computing optical flow.</code></li>
<li>use features from both the reference frame and each supporting frame to <code>dynamically predict offsets of sampling convolution kernels</code>, to transforms <code>supporting frames to align with the reference frame</code>.</li>
<li>taking aligned frames and the reference frame to predict the HR video frame.</li>
</ol>
<h4 id="Research-Objective"><a href="#Research-Objective" class="headerlink" title="Research Objective"></a>Research Objective</h4><ul>
<li><strong>Application Area</strong>: <code>Video super-resolution</code> aims to restore a photo-realistic high-resolution video frame from both its<code>corresponding low-resolution frame (reference frame)</code> and <code>multiple neighboring frames (supporting frames).</code><ul>
<li>varying motion of cameras, or objects, the reference frame and each support frame are not alighned.<ul>
<li><strong>Relative work</strong>:</li>
</ul>
</li>
</ul>
</li>
<li>optical flow to <code>predict motion fields</code> between the reference frame and supporting frames, then warp the supporting frames using their corresponding motion fields.  </li>
</ul>
<p><img src="https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211016224337688.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211016223010811.png" alt=""></p>
<ul>
<li><p>TDAN to <code>align each supporting frame with the reference frame</code>, a LR supproting frame, and referencfe frame–&gt;feeding 2N support frames to get  2N corresponding aligned LR frames.</p>
<ul>
<li>feature extraction: use one convolutional layer amd k1 residual blocksto extracts visual features.</li>
<li>deformable alignment: takes the features mentioned above to predict sampling parameters. (refers to the offsdets o fthe convolution kernels.)  the feature of the reference frame is only used for computing the offset, its information will not propagated into the aligned feature of the supporting frame.  The adaptively-learned offset will implicitly capture motion cues and explore neighboring features within the sma eimage structures for alignment.</li>
<li>aligned frame reconstruction: restore an aligned LR frame and utilize an alignment loss to enforce the deformable alignment module to sample useful features for accurate temporal alignment.</li>
</ul>
</li>
<li><p>supre resolution reconstruction network to predict the HR frame:  2N corresponding aligned LR frames+ reference frame –&gt; reconstruct the HR video frame.</p>
<ul>
<li>Temporal Fusion: concatenate the 2N+1 frames and then feed them into a 3*3 convolutional layer to output fused feature map;</li>
<li>Nonlinear Mapping: take th eshadow fused features as input to predict deep features.</li>
<li>utilize an upscaling layer to increase the resolution of th efeature map with a sub-pixel convolution.</li>
</ul>
</li>
<li><p>Loss Function:</p>
<ul>
<li>utilize the reference frame as the label and make the aligned LR frames close to the reference frame.</li>
<li>utilize the final HR video estimated frame with HR video frame.</li>
</ul>
<p><img src="https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211017123513481.png" alt="Lalign"></p>
<p><img src="https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211017123619469.png" alt="Lsr"></p>
</li>
</ul>
<p><img src="https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211022212648591.png" alt=""></p>
<blockquote>
<p>the aligned frame is reconstructed from features from the reference and supporting frames. Green points in the supporting frame indicate sampling positions for predicting corresponding pixels labeled withred color in the aligned frame.</p>
</blockquote>
<p><img src="https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211016223232945.png" alt=""></p>
<blockquote>
<p>the TDAN can expoit rich image contexts containing similar content (green regions) as target pixels (red points) from the supporting frame to employ accurately temporal alignment.</p>
</blockquote>
<p><img src="https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211016223752368.png" alt="Temporal alignment"></p>
<h4 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TDAN_VSR</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(TDAN_VSR, self).__init__()</span><br><span class="line">        self.name = <span class="string">'TDAN'</span></span><br><span class="line">        self.conv_first = nn.Conv2d(<span class="number">3</span>, <span class="number">64</span>, <span class="number">3</span>, padding=<span class="number">1</span>, bias=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        self.residual_layer = self.make_layer(Res_Block, <span class="number">5</span>)</span><br><span class="line">        self.relu = nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">        <span class="comment"># deformable</span></span><br><span class="line">        self.cr = nn.Conv2d(<span class="number">128</span>, <span class="number">64</span>, <span class="number">3</span>, padding=<span class="number">1</span>, bias=<span class="literal">True</span>)</span><br><span class="line">        self.off2d_1 = nn.Conv2d(<span class="number">64</span>, <span class="number">18</span> * <span class="number">8</span>, <span class="number">3</span>, padding=<span class="number">1</span>, bias=<span class="literal">True</span>)</span><br><span class="line">        self.dconv_1 = ConvOffset2d(<span class="number">64</span>, <span class="number">64</span>, <span class="number">3</span>, padding=<span class="number">1</span>, num_deformable_groups=<span class="number">8</span>)</span><br><span class="line">        self.off2d_2 = nn.Conv2d(<span class="number">64</span>, <span class="number">18</span> * <span class="number">8</span>, <span class="number">3</span>, padding=<span class="number">1</span>, bias=<span class="literal">True</span>)</span><br><span class="line">        self.deconv_2 = ConvOffset2d(<span class="number">64</span>, <span class="number">64</span>, <span class="number">3</span>, padding=<span class="number">1</span>, num_deformable_groups=<span class="number">8</span>)</span><br><span class="line">        self.off2d_3 = nn.Conv2d(<span class="number">64</span>, <span class="number">18</span> * <span class="number">8</span>, <span class="number">3</span>, padding=<span class="number">1</span>, bias=<span class="literal">True</span>)</span><br><span class="line">        self.deconv_3 = ConvOffset2d(<span class="number">64</span>, <span class="number">64</span>, <span class="number">3</span>, padding=<span class="number">1</span>, num_deformable_groups=<span class="number">8</span>)</span><br><span class="line">        self.off2d = nn.Conv2d(<span class="number">64</span>, <span class="number">18</span> * <span class="number">8</span>, <span class="number">3</span>, padding=<span class="number">1</span>, bias=<span class="literal">True</span>)</span><br><span class="line">        self.dconv = ConvOffset2d(<span class="number">64</span>, <span class="number">64</span>, (<span class="number">3</span>, <span class="number">3</span>), padding=(<span class="number">1</span>, <span class="number">1</span>), num_deformable_groups=<span class="number">8</span>)</span><br><span class="line">        self.recon_lr = nn.Conv2d(<span class="number">64</span>, <span class="number">3</span>, <span class="number">3</span>, padding=<span class="number">1</span>, bias=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        fea_ex = [nn.Conv2d(<span class="number">5</span> * <span class="number">3</span>, <span class="number">64</span>, <span class="number">3</span>, padding= <span class="number">1</span>, bias=<span class="literal">True</span>),</span><br><span class="line">                       nn.ReLU()]</span><br><span class="line"></span><br><span class="line">        self.fea_ex = nn.Sequential(*fea_ex)</span><br><span class="line">        self.recon_layer = self.make_layer(Res_Block, <span class="number">10</span>)     </span><br><span class="line">        upscaling = [</span><br><span class="line">            Upsampler(default_conv, <span class="number">4</span>, <span class="number">64</span>, act=<span class="literal">False</span>),      <span class="comment">#？？</span></span><br><span class="line">            nn.Conv2d(<span class="number">64</span>, <span class="number">3</span>, <span class="number">3</span>, padding=<span class="number">1</span>, bias=<span class="literal">False</span>)]</span><br><span class="line"></span><br><span class="line">        self.up = nn.Sequential(*upscaling)  </span><br><span class="line"></span><br><span class="line">        <span class="comment"># xavier initialization</span></span><br><span class="line">        <span class="keyword">for</span> m <span class="keyword">in</span> self.modules():</span><br><span class="line">            <span class="keyword">if</span> isinstance(m, nn.Conv2d):</span><br><span class="line">                n = m.kernel_size[<span class="number">0</span>] * m.kernel_size[<span class="number">1</span>] * m.out_channels</span><br><span class="line">                m.weight.data.normal_(<span class="number">0</span>, math.sqrt(<span class="number">2.</span> / n))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">align</span><span class="params">(self, x, x_center)</span>:</span></span><br><span class="line">        y = []</span><br><span class="line">        batch_size, num, ch, w, h = x.size()</span><br><span class="line">        center = num // <span class="number">2</span></span><br><span class="line">        ref = x[:, center, :, :, :].clone()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(num):</span><br><span class="line">            <span class="keyword">if</span> i == center:</span><br><span class="line">                y.append(x_center.unsqueeze(<span class="number">1</span>))</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            supp = x[:, i, :, :, :]</span><br><span class="line">            fea = torch.cat([ref, supp], dim=<span class="number">1</span>)  <span class="comment"># 按dim 维度进行拼接，</span></span><br><span class="line">            fea = self.cr(fea)</span><br><span class="line">            <span class="comment"># feature trans</span></span><br><span class="line">            offset1 = self.off2d_1(fea)</span><br><span class="line">            fea = (self.dconv_1(fea, offset1))</span><br><span class="line">            offset2 = self.off2d_2(fea)</span><br><span class="line">            fea = (self.deconv_2(fea, offset2))</span><br><span class="line">            offset3 = self.off2d_3(fea)</span><br><span class="line">            fea = (self.deconv_3(supp, offset3))</span><br><span class="line">            offset4 = self.off2d(fea)</span><br><span class="line">            aligned_fea = (self.dconv(fea, offset4))</span><br><span class="line">            im = self.recon_lr(aligned_fea).unsqueeze(<span class="number">1</span>)  <span class="comment">#去掉维数为1的的维度，比如是一行或者一列这种</span></span><br><span class="line">            y.append(im)</span><br><span class="line">        y = torch.cat(y, dim=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">make_layer</span><span class="params">(self, block, num_of_layer)</span>:</span></span><br><span class="line">        layers = []</span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> range(num_of_layer):</span><br><span class="line">            layers.append(block())</span><br><span class="line">        <span class="keyword">return</span> nn.Sequential(*layers)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line"></span><br><span class="line">        batch_size, num, ch, w, h = x.size()  <span class="comment"># 5 video frames</span></span><br><span class="line">        <span class="comment"># center frame interpolation</span></span><br><span class="line">        center = num // <span class="number">2</span></span><br><span class="line">        <span class="comment"># extract features</span></span><br><span class="line">        y = x.view(<span class="number">-1</span>, ch, w, h)     <span class="comment">#这个y作用是什么？原始图像，和recon_lr 特征提取后的数据融合在一起           # batch_size*num, ch, w, h</span></span><br><span class="line">        <span class="comment"># y = y.unsqueeze(1)</span></span><br><span class="line">        out = self.relu(self.conv_first(y))</span><br><span class="line">        x_center = x[:, center, :, :, :]</span><br><span class="line">        out = self.residual_layer(out)</span><br><span class="line">        out = out.view(batch_size, num, <span class="number">-1</span>, w, h)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># align supporting frames</span></span><br><span class="line">        lrs = self.align(out, x_center) <span class="comment"># motion alignments</span></span><br><span class="line">        y = lrs.view(batch_size, <span class="number">-1</span>, w, h)</span><br><span class="line">        <span class="comment"># reconstruction</span></span><br><span class="line">        fea = self.fea_ex(y)</span><br><span class="line"></span><br><span class="line">        out = self.recon_layer(fea)</span><br><span class="line">        out = self.up(out)</span><br><span class="line">        <span class="keyword">return</span> out, lrs</span><br></pre></td></tr></tbody></table></figure>

<blockquote>
<p>Lim B, Son S, Kim H, et al. Enhanced deep residual networks for single image super-resolution[C]//Proceedings of the IEEE conference on computer vision and pattern recognition workshops. 2017: 136-144.  cite  <a href="chrome-extension://ikhdkkncnoglghljlkmcimlnlhkeamad/pdf-viewer/web/viewer.html?file=https%3A%2F%2Farxiv.org%2Fpdf%2F1707.02921.pdf">pdf</a></p>
</blockquote>
<hr>
<h3 id="Paper-EDSR"><a href="#Paper-EDSR" class="headerlink" title="Paper: EDSR"></a>Paper: EDSR</h3><div align="center">
<br>
<b>Enhanced deep residual networks for single image super-resolution</b>
</div>
#### Summary

<ol>
<li>optimize the SRResNet architecture by analyzing and removing unnecessary modules to simplify the network architecture. Train the network with appropricate loss function and careful model modification upon training.</li>
<li>propose a new multi-scale architecture that shares most of the parameters across different scales.</li>
</ol>
<h4 id="previous-work"><a href="#previous-work" class="headerlink" title="previous work:"></a>previous work:</h4><ul>
<li><h4 id="interpolation-techniques-based-on-sampling-theory-limites-in-predicting-detailed-realistic-textures"><a href="#interpolation-techniques-based-on-sampling-theory-limites-in-predicting-detailed-realistic-textures" class="headerlink" title="interpolation techniques based on sampling theory limites in predicting detailed, realistic textures."></a><code>interpolation techniques</code> based on sampling theory limites in predicting detailed, realistic textures.</h4></li>
<li>learn the mapping functions between $I^{LR}$ to $I^{HR}$​, including neighbor embedding, to sparse coding.</li>
</ul>
<h4 id="Methods"><a href="#Methods" class="headerlink" title="Methods"></a>Methods</h4><ul>
<li><strong>system overview</strong>:</li>
</ul>
<p><img src="https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211023172415860.png" alt="Single-scale model"></p>
<blockquote>
<p>building a multi-scale architecture that takes the advantage of inter-scale correlation as VDSR, and introduce scale specific processing modules to handle the super-resolution at multiple scales.</p>
<ul>
<li><code>pre-processing modules</code> are located at the head of networks to reduce the variance from input images of different scales. each consists of two residual blocks with 5*5 kernels to keep the scale-specific part shallow while the larger receptive field is covered in early stages of networks.</li>
<li>at the end of the multi-scale model, <code>scale-specific upsampling modules</code> are located in parallel to handle multi-scale reconstruction.</li>
</ul>
</blockquote>
<p><img src="https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211024085357018.png" alt="MDSR"></p>
<blockquote>
<p>batch normalization layers normalize the features, <code>they get rid of range flexibility</code> from networks by normalizing the features. GPU memory usage sufficiently reduced.</p>
</blockquote>
<p><img src="https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211023172200022.png" alt=""></p>
<h4 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a>Evaluation</h4><ul>
<li>Dataset:  <ul>
<li>DIV2K dataset is a newly proposed high-quality(2K resolution) image dataset for image restoration tasks, consisting 800 training images, 100 valication images, and 100 test images.</li>
</ul>
</li>
<li>use the RGB input patches of size 48*48 from LR image with the corresponding HR patches.</li>
</ul>
<p><img src="https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211024090426859.png" alt=""></p>
<blockquote>
<p>public benchmark test results and DIV2K validation results( PSNR(db)/SSIM), red indicates the best performance and the blue indicates the second best.</p>
</blockquote>
<p><img src="https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211024090558584.png" alt=""></p>
<h4 id="Code-1"><a href="#Code-1" class="headerlink" title="Code"></a>Code</h4><ul>
<li>single-scale EDSR network</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@ARCH_REGISTRY.register()</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EDSR</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""EDSR network structure.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Paper: Enhanced Deep Residual Networks for Single Image Super-Resolution.</span></span><br><span class="line"><span class="string">    Ref git repo: https://github.com/thstkdgus35/EDSR-PyTorch</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        num_in_ch (int): Channel number of inputs.</span></span><br><span class="line"><span class="string">        num_out_ch (int): Channel number of outputs.</span></span><br><span class="line"><span class="string">        num_feat (int): Channel number of intermediate features.</span></span><br><span class="line"><span class="string">            Default: 64.</span></span><br><span class="line"><span class="string">        num_block (int): Block number in the trunk network. Default: 16.</span></span><br><span class="line"><span class="string">        upscale (int): Upsampling factor. Support 2^n and 3.</span></span><br><span class="line"><span class="string">            Default: 4.</span></span><br><span class="line"><span class="string">        res_scale (float): Used to scale the residual in residual block.</span></span><br><span class="line"><span class="string">            Default: 1.</span></span><br><span class="line"><span class="string">        img_range (float): Image range. Default: 255.</span></span><br><span class="line"><span class="string">        rgb_mean (tuple[float]): Image mean in RGB orders.</span></span><br><span class="line"><span class="string">            Default: (0.4488, 0.4371, 0.4040), calculated from DIV2K dataset.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,</span></span></span><br><span class="line"><span class="function"><span class="params">                 num_in_ch,</span></span></span><br><span class="line"><span class="function"><span class="params">                 num_out_ch,</span></span></span><br><span class="line"><span class="function"><span class="params">                 num_feat=<span class="number">64</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 num_block=<span class="number">16</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 upscale=<span class="number">4</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 res_scale=<span class="number">1</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 img_range=<span class="number">255.</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 rgb_mean=<span class="params">(<span class="number">0.4488</span>, <span class="number">0.4371</span>, <span class="number">0.4040</span>)</span>)</span>:</span></span><br><span class="line">        super(EDSR, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.img_range = img_range</span><br><span class="line">        self.mean = torch.Tensor(rgb_mean).view(<span class="number">1</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        self.conv_first = nn.Conv2d(num_in_ch, num_feat, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        self.body = make_layer(ResidualBlockNoBN, num_block, num_feat=num_feat, res_scale=res_scale, pytorch_init=<span class="literal">True</span>)</span><br><span class="line">        self.conv_after_body = nn.Conv2d(num_feat, num_feat, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        self.upsample = Upsample(upscale, num_feat)</span><br><span class="line">        self.conv_last = nn.Conv2d(num_feat, num_out_ch, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        self.mean = self.mean.type_as(x)</span><br><span class="line"></span><br><span class="line">        x = (x - self.mean) * self.img_range</span><br><span class="line">        x = self.conv_first(x)</span><br><span class="line">        res = self.conv_after_body(self.body(x))</span><br><span class="line">        res += x</span><br><span class="line"></span><br><span class="line">        x = self.conv_last(self.upsample(res))</span><br><span class="line">        x = x / self.img_range + self.mean</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></tbody></table></figure>

<blockquote>
<p>Wang X, Chan K C K, Yu K, et al. Edvr: Video restoration with enhanced deformable convolutional networks[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops. 2019: 0-0.  <a href="chrome-extension://ikhdkkncnoglghljlkmcimlnlhkeamad/pdf-viewer/web/viewer.html?file=https%3A%2F%2Farxiv.org%2Fpdf%2F1905.02716.pdf">pdf</a> </p>
</blockquote>
<hr>
<h3 id="Paper-Edvr"><a href="#Paper-Edvr" class="headerlink" title="Paper: Edvr"></a>Paper: Edvr</h3><div align="center">
<br>
<b> Edvr: Video restoration with enhanced deformable convolutional networks</b>
</div>


<h4 id="Summary-1"><a href="#Summary-1" class="headerlink" title="Summary"></a>Summary</h4><ol>
<li>devise a Pyramid, Cascading and Deformable alignment module, in which frame alignment is done at the feature level using deformable convolutions in a coarse-to-fine manner, to handle large motions.</li>
<li>propose a Temporal and Spatial Attention fusion module, in which attention is applied both temporally and spatially, so as to emphasize important features for subsequenct restoration.</li>
</ol>
<h4 id="Proble-Statement"><a href="#Proble-Statement" class="headerlink" title="Proble Statement"></a>Proble Statement</h4><ul>
<li>how to align multiple frames given large motions?</li>
<li>how to effectively fuse different frames with diverse motion and blur?</li>
</ul>
<h4 id="Relative-Work"><a href="#Relative-Work" class="headerlink" title="Relative Work"></a>Relative Work</h4><ul>
<li><strong>Video Super-Resolution</strong>: RCAN, DeepSR, BayesSR, VESPCN, SPMC, TOFlow, FRVSR, DUF, RBPN on three testing datasets, Vid4, Vimeo-90K-T, REDS4.</li>
<li><strong>Video Deblurring:</strong> DeepDeblur, DeblurGAN, SRNDEblur, DBN on the REDS4 dataset.</li>
</ul>
<h4 id="Methods-1"><a href="#Methods-1" class="headerlink" title="Methods"></a>Methods</h4><ul>
<li><strong>system overview</strong>:</li>
</ul>
<blockquote>
<p>Given 2N+1 consecutive low-quality frames $I_{t-N:t+N}$ , denote the middle frame $I_t$ as the reference frame and the other frames as neighboring frames, to estimate a high-quality reference frame $Q_t$​. </p>
<ul>
<li>each neighboring frame is aligned to the reference one by the PCD alignment module at the feature level.</li>
<li>TSA fusion module fuses image information of different frames.</li>
</ul>
</blockquote>
<p><img src="https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211025201730909.png" alt=""></p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@ARCH_REGISTRY.register()</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EDVR</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""EDVR network structure for video super-resolution.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Now only support X4 upsampling factor.</span></span><br><span class="line"><span class="string">    Paper:</span></span><br><span class="line"><span class="string">        EDVR: Video Restoration with Enhanced Deformable Convolutional Networks</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        num_in_ch (int): Channel number of input image. Default: 3.</span></span><br><span class="line"><span class="string">        num_out_ch (int): Channel number of output image. Default: 3.</span></span><br><span class="line"><span class="string">        num_feat (int): Channel number of intermediate features. Default: 64.</span></span><br><span class="line"><span class="string">        num_frame (int): Number of input frames. Default: 5.</span></span><br><span class="line"><span class="string">        deformable_groups (int): Deformable groups. Defaults: 8.</span></span><br><span class="line"><span class="string">        num_extract_block (int): Number of blocks for feature extraction.</span></span><br><span class="line"><span class="string">            Default: 5.</span></span><br><span class="line"><span class="string">        num_reconstruct_block (int): Number of blocks for reconstruction.</span></span><br><span class="line"><span class="string">            Default: 10.</span></span><br><span class="line"><span class="string">        center_frame_idx (int): The index of center frame. Frame counting from</span></span><br><span class="line"><span class="string">            0. Default: Middle of input frames.</span></span><br><span class="line"><span class="string">        hr_in (bool): Whether the input has high resolution. Default: False.</span></span><br><span class="line"><span class="string">        with_predeblur (bool): Whether has predeblur module.</span></span><br><span class="line"><span class="string">            Default: False.</span></span><br><span class="line"><span class="string">        with_tsa (bool): Whether has TSA module. Default: True.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,</span></span></span><br><span class="line"><span class="function"><span class="params">                 num_in_ch=<span class="number">3</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 num_out_ch=<span class="number">3</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 num_feat=<span class="number">64</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 num_frame=<span class="number">5</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 deformable_groups=<span class="number">8</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 num_extract_block=<span class="number">5</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 num_reconstruct_block=<span class="number">10</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 center_frame_idx=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                 hr_in=False,</span></span></span><br><span class="line"><span class="function"><span class="params">                 with_predeblur=False,</span></span></span><br><span class="line"><span class="function"><span class="params">                 with_tsa=True)</span>:</span></span><br><span class="line">        super(EDVR, self).__init__()</span><br><span class="line">        <span class="keyword">if</span> center_frame_idx <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            self.center_frame_idx = num_frame // <span class="number">2</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.center_frame_idx = center_frame_idx</span><br><span class="line">        self.hr_in = hr_in</span><br><span class="line">        self.with_predeblur = with_predeblur</span><br><span class="line">        self.with_tsa = with_tsa</span><br><span class="line"></span><br><span class="line">        <span class="comment"># extract features for each frame</span></span><br><span class="line">        <span class="keyword">if</span> self.with_predeblur:</span><br><span class="line">            self.predeblur = PredeblurModule(num_feat=num_feat, hr_in=self.hr_in)</span><br><span class="line">            self.conv_1x1 = nn.Conv2d(num_feat, num_feat, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.conv_first = nn.Conv2d(num_in_ch, num_feat, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># extract pyramid features</span></span><br><span class="line">        self.feature_extraction = make_layer(ResidualBlockNoBN, num_extract_block, num_feat=num_feat)</span><br><span class="line">        self.conv_l2_1 = nn.Conv2d(num_feat, num_feat, <span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">        self.conv_l2_2 = nn.Conv2d(num_feat, num_feat, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        self.conv_l3_1 = nn.Conv2d(num_feat, num_feat, <span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">        self.conv_l3_2 = nn.Conv2d(num_feat, num_feat, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># pcd and tsa module</span></span><br><span class="line">        self.pcd_align = PCDAlignment(num_feat=num_feat, deformable_groups=deformable_groups)</span><br><span class="line">        <span class="keyword">if</span> self.with_tsa:</span><br><span class="line">            self.fusion = TSAFusion(num_feat=num_feat, num_frame=num_frame, center_frame_idx=self.center_frame_idx)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.fusion = nn.Conv2d(num_frame * num_feat, num_feat, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># reconstruction</span></span><br><span class="line">        self.reconstruction = make_layer(ResidualBlockNoBN, num_reconstruct_block, num_feat=num_feat)</span><br><span class="line">        <span class="comment"># upsample</span></span><br><span class="line">        self.upconv1 = nn.Conv2d(num_feat, num_feat * <span class="number">4</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        self.upconv2 = nn.Conv2d(num_feat, <span class="number">64</span> * <span class="number">4</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        self.pixel_shuffle = nn.PixelShuffle(<span class="number">2</span>)</span><br><span class="line">        self.conv_hr = nn.Conv2d(<span class="number">64</span>, <span class="number">64</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        self.conv_last = nn.Conv2d(<span class="number">64</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># activation function</span></span><br><span class="line">        self.lrelu = nn.LeakyReLU(negative_slope=<span class="number">0.1</span>, inplace=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        b, t, c, h, w = x.size()</span><br><span class="line">        <span class="keyword">if</span> self.hr_in:</span><br><span class="line">            <span class="keyword">assert</span> h % <span class="number">16</span> == <span class="number">0</span> <span class="keyword">and</span> w % <span class="number">16</span> == <span class="number">0</span>, (<span class="string">'The height and width must be multiple of 16.'</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">assert</span> h % <span class="number">4</span> == <span class="number">0</span> <span class="keyword">and</span> w % <span class="number">4</span> == <span class="number">0</span>, (<span class="string">'The height and width must be multiple of 4.'</span>)</span><br><span class="line"></span><br><span class="line">        x_center = x[:, self.center_frame_idx, :, :, :].contiguous()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># extract features for each frame</span></span><br><span class="line">        <span class="comment"># L1</span></span><br><span class="line">        <span class="keyword">if</span> self.with_predeblur:</span><br><span class="line">            feat_l1 = self.conv_1x1(self.predeblur(x.view(<span class="number">-1</span>, c, h, w)))</span><br><span class="line">            <span class="keyword">if</span> self.hr_in:</span><br><span class="line">                h, w = h // <span class="number">4</span>, w // <span class="number">4</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            feat_l1 = self.lrelu(self.conv_first(x.view(<span class="number">-1</span>, c, h, w)))</span><br><span class="line"></span><br><span class="line">        feat_l1 = self.feature_extraction(feat_l1)</span><br><span class="line">        <span class="comment"># L2</span></span><br><span class="line">        feat_l2 = self.lrelu(self.conv_l2_1(feat_l1))</span><br><span class="line">        feat_l2 = self.lrelu(self.conv_l2_2(feat_l2))</span><br><span class="line">        <span class="comment"># L3</span></span><br><span class="line">        feat_l3 = self.lrelu(self.conv_l3_1(feat_l2))</span><br><span class="line">        feat_l3 = self.lrelu(self.conv_l3_2(feat_l3))</span><br><span class="line"></span><br><span class="line">        feat_l1 = feat_l1.view(b, t, <span class="number">-1</span>, h, w)</span><br><span class="line">        feat_l2 = feat_l2.view(b, t, <span class="number">-1</span>, h // <span class="number">2</span>, w // <span class="number">2</span>)</span><br><span class="line">        feat_l3 = feat_l3.view(b, t, <span class="number">-1</span>, h // <span class="number">4</span>, w // <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># PCD alignment</span></span><br><span class="line">        ref_feat_l = [  <span class="comment"># reference feature list</span></span><br><span class="line">            feat_l1[:, self.center_frame_idx, :, :, :].clone(), feat_l2[:, self.center_frame_idx, :, :, :].clone(),</span><br><span class="line">            feat_l3[:, self.center_frame_idx, :, :, :].clone()</span><br><span class="line">        ]</span><br><span class="line">        aligned_feat = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(t):</span><br><span class="line">            nbr_feat_l = [  <span class="comment"># neighboring feature list</span></span><br><span class="line">                feat_l1[:, i, :, :, :].clone(), feat_l2[:, i, :, :, :].clone(), feat_l3[:, i, :, :, :].clone()</span><br><span class="line">            ]</span><br><span class="line">            aligned_feat.append(self.pcd_align(nbr_feat_l, ref_feat_l))</span><br><span class="line">        aligned_feat = torch.stack(aligned_feat, dim=<span class="number">1</span>)  <span class="comment"># (b, t, c, h, w)  # 到这里每一张图片和reference都有一个对应关系</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.with_tsa:</span><br><span class="line">            aligned_feat = aligned_feat.view(b, <span class="number">-1</span>, h, w)</span><br><span class="line">        feat = self.fusion(aligned_feat)</span><br><span class="line"></span><br><span class="line">        out = self.reconstruction(feat)</span><br><span class="line">        out = self.lrelu(self.pixel_shuffle(self.upconv1(out)))</span><br><span class="line">        out = self.lrelu(self.pixel_shuffle(self.upconv2(out)))</span><br><span class="line">        out = self.lrelu(self.conv_hr(out))</span><br><span class="line">        out = self.conv_last(out)</span><br><span class="line">        <span class="keyword">if</span> self.hr_in:</span><br><span class="line">            base = x_center</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            base = F.interpolate(x_center, scale_factor=<span class="number">4</span>, mode=<span class="string">'bilinear'</span>, align_corners=<span class="literal">False</span>)</span><br><span class="line">        out += base</span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></tbody></table></figure>

<ul>
<li><strong>Alignment with Pyramid, Cascading and Deformable convolution:</strong> </li>
</ul>
<blockquote>
<p>To generate feature $F^l_{t+i}$​​ at the l-th level, use strided convolution filters to downsample the features at the (l-1)-th pyramid level by a factor of 2, obtaining L-level pyramids of feature representation. At the l-th level, offsets and aligned features are predicted also with the *2 upsampled offsets and aligned features from the upper (l+1)-th level.  (<code>下面这个公式没有看懂</code>)</p>
</blockquote>
<p><img src="https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211025210506574.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211025221603992.png" alt=""></p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PCDAlignment</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""Alignment module using Pyramid, Cascading and Deformable convolution</span></span><br><span class="line"><span class="string">    (PCD). It is used in EDVR.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Ref:</span></span><br><span class="line"><span class="string">        EDVR: Video Restoration with Enhanced Deformable Convolutional Networks</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        num_feat (int): Channel number of middle features. Default: 64.</span></span><br><span class="line"><span class="string">        deformable_groups (int): Deformable groups. Defaults: 8.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, num_feat=<span class="number">64</span>, deformable_groups=<span class="number">8</span>)</span>:</span></span><br><span class="line">        super(PCDAlignment, self).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Pyramid has three levels:</span></span><br><span class="line">        <span class="comment"># L3: level 3, 1/4 spatial size</span></span><br><span class="line">        <span class="comment"># L2: level 2, 1/2 spatial size</span></span><br><span class="line">        <span class="comment"># L1: level 1, original spatial size</span></span><br><span class="line">        self.offset_conv1 = nn.ModuleDict()</span><br><span class="line">        self.offset_conv2 = nn.ModuleDict()</span><br><span class="line">        self.offset_conv3 = nn.ModuleDict()</span><br><span class="line">        self.dcn_pack = nn.ModuleDict()</span><br><span class="line">        self.feat_conv = nn.ModuleDict()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Pyramids</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">3</span>, <span class="number">0</span>, <span class="number">-1</span>):</span><br><span class="line">            level = <span class="string">f'l<span class="subst">{i}</span>'</span></span><br><span class="line">            self.offset_conv1[level] = nn.Conv2d(num_feat * <span class="number">2</span>, num_feat, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">            <span class="keyword">if</span> i == <span class="number">3</span>:</span><br><span class="line">                self.offset_conv2[level] = nn.Conv2d(num_feat, num_feat, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                self.offset_conv2[level] = nn.Conv2d(num_feat * <span class="number">2</span>, num_feat, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">                self.offset_conv3[level] = nn.Conv2d(num_feat, num_feat, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">            self.dcn_pack[level] = DCNv2Pack(num_feat, num_feat, <span class="number">3</span>, padding=<span class="number">1</span>, deformable_groups=deformable_groups)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> i &lt; <span class="number">3</span>:</span><br><span class="line">                self.feat_conv[level] = nn.Conv2d(num_feat * <span class="number">2</span>, num_feat, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Cascading dcn</span></span><br><span class="line">        self.cas_offset_conv1 = nn.Conv2d(num_feat * <span class="number">2</span>, num_feat, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        self.cas_offset_conv2 = nn.Conv2d(num_feat, num_feat, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        self.cas_dcnpack = DCNv2Pack(num_feat, num_feat, <span class="number">3</span>, padding=<span class="number">1</span>, deformable_groups=deformable_groups)</span><br><span class="line"></span><br><span class="line">        self.upsample = nn.Upsample(scale_factor=<span class="number">2</span>, mode=<span class="string">'bilinear'</span>, align_corners=<span class="literal">False</span>)</span><br><span class="line">        self.lrelu = nn.LeakyReLU(negative_slope=<span class="number">0.1</span>, inplace=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, nbr_feat_l, ref_feat_l)</span>:</span></span><br><span class="line">        <span class="string">"""Align neighboring frame features to the reference frame features.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            nbr_feat_l (list[Tensor]): Neighboring feature list. It</span></span><br><span class="line"><span class="string">                contains three pyramid levels (L1, L2, L3),</span></span><br><span class="line"><span class="string">                each with shape (b, c, h, w).</span></span><br><span class="line"><span class="string">            ref_feat_l (list[Tensor]): Reference feature list. It</span></span><br><span class="line"><span class="string">                contains three pyramid levels (L1, L2, L3),</span></span><br><span class="line"><span class="string">                each with shape (b, c, h, w).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            Tensor: Aligned features.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment"># Pyramids</span></span><br><span class="line">        upsampled_offset, upsampled_feat = <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">3</span>, <span class="number">0</span>, <span class="number">-1</span>):</span><br><span class="line">            level = <span class="string">f'l<span class="subst">{i}</span>'</span></span><br><span class="line">            offset = torch.cat([nbr_feat_l[i - <span class="number">1</span>], ref_feat_l[i - <span class="number">1</span>]], dim=<span class="number">1</span>)</span><br><span class="line">            offset = self.lrelu(self.offset_conv1[level](offset))</span><br><span class="line">            <span class="keyword">if</span> i == <span class="number">3</span>:</span><br><span class="line">                offset = self.lrelu(self.offset_conv2[level](offset))</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                offset = self.lrelu(self.offset_conv2[level](torch.cat([offset, upsampled_offset], dim=<span class="number">1</span>)))</span><br><span class="line">                offset = self.lrelu(self.offset_conv3[level](offset))</span><br><span class="line"></span><br><span class="line">            feat = self.dcn_pack[level](nbr_feat_l[i - <span class="number">1</span>], offset)</span><br><span class="line">            <span class="keyword">if</span> i &lt; <span class="number">3</span>:</span><br><span class="line">                feat = self.feat_conv[level](torch.cat([feat, upsampled_feat], dim=<span class="number">1</span>))</span><br><span class="line">            <span class="keyword">if</span> i &gt; <span class="number">1</span>:</span><br><span class="line">                feat = self.lrelu(feat)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> i &gt; <span class="number">1</span>:  <span class="comment"># upsample offset and features</span></span><br><span class="line">                <span class="comment"># x2: when we upsample the offset, we should also enlarge</span></span><br><span class="line">                <span class="comment"># the magnitude.</span></span><br><span class="line">                upsampled_offset = self.upsample(offset) * <span class="number">2</span></span><br><span class="line">                upsampled_feat = self.upsample(feat)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Cascading</span></span><br><span class="line">        offset = torch.cat([feat, ref_feat_l[<span class="number">0</span>]], dim=<span class="number">1</span>)</span><br><span class="line">        offset = self.lrelu(self.cas_offset_conv2(self.lrelu(self.cas_offset_conv1(offset))))</span><br><span class="line">        feat = self.lrelu(self.cas_dcnpack(feat, offset))</span><br><span class="line">        <span class="keyword">return</span> feat</span><br></pre></td></tr></tbody></table></figure>

<ul>
<li><strong>Fusion with Temporal and Spatial Attention</strong></li>
</ul>
<blockquote>
<p>Inter-frame temporal relation and intra-frame spatial relation are critical in fusion:</p>
<ul>
<li><code>different neighboring frames are not equally informative</code> due to occlusion, blurry regions and parallax problems.</li>
<li><code>misalignment and unalignment</code> arising from the preceding alignment stage adversely affect the subsequent reconstruction performance.</li>
</ul>
<p>propose TSA fusion module to assign pixel-level aggregation weights on each frame, adopt temporal and spatial attentions during the fusion process.</p>
<ul>
<li>temporal attention is to compute frame similarity in an embedding space. In an embedding space, a neighboring frame that is more similar to the reference one, should be paid more attention.</li>
</ul>
</blockquote>
<p><img src="https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211025212147206.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211025211618576.png" alt=""></p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TSAFusion</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""Temporal Spatial Attention (TSA) fusion module.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Temporal: Calculate the correlation between center frame and</span></span><br><span class="line"><span class="string">        neighboring frames;</span></span><br><span class="line"><span class="string">    Spatial: It has 3 pyramid levels, the attention is similar to SFT.</span></span><br><span class="line"><span class="string">        (SFT: Recovering realistic texture in image super-resolution by deep</span></span><br><span class="line"><span class="string">            spatial feature transform.)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        num_feat (int): Channel number of middle features. Default: 64.</span></span><br><span class="line"><span class="string">        num_frame (int): Number of frames. Default: 5.</span></span><br><span class="line"><span class="string">        center_frame_idx (int): The index of center frame. Default: 2.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, num_feat=<span class="number">64</span>, num_frame=<span class="number">5</span>, center_frame_idx=<span class="number">2</span>)</span>:</span></span><br><span class="line">        super(TSAFusion, self).__init__()</span><br><span class="line">        self.center_frame_idx = center_frame_idx</span><br><span class="line">        <span class="comment"># temporal attention (before fusion conv)</span></span><br><span class="line">        self.temporal_attn1 = nn.Conv2d(num_feat, num_feat, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        self.temporal_attn2 = nn.Conv2d(num_feat, num_feat, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        self.feat_fusion = nn.Conv2d(num_frame * num_feat, num_feat, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># spatial attention (after fusion conv)</span></span><br><span class="line">        self.max_pool = nn.MaxPool2d(<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.avg_pool = nn.AvgPool2d(<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.spatial_attn1 = nn.Conv2d(num_frame * num_feat, num_feat, <span class="number">1</span>)</span><br><span class="line">        self.spatial_attn2 = nn.Conv2d(num_feat * <span class="number">2</span>, num_feat, <span class="number">1</span>)</span><br><span class="line">        self.spatial_attn3 = nn.Conv2d(num_feat, num_feat, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        self.spatial_attn4 = nn.Conv2d(num_feat, num_feat, <span class="number">1</span>)</span><br><span class="line">        self.spatial_attn5 = nn.Conv2d(num_feat, num_feat, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        self.spatial_attn_l1 = nn.Conv2d(num_feat, num_feat, <span class="number">1</span>)</span><br><span class="line">        self.spatial_attn_l2 = nn.Conv2d(num_feat * <span class="number">2</span>, num_feat, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        self.spatial_attn_l3 = nn.Conv2d(num_feat, num_feat, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        self.spatial_attn_add1 = nn.Conv2d(num_feat, num_feat, <span class="number">1</span>)</span><br><span class="line">        self.spatial_attn_add2 = nn.Conv2d(num_feat, num_feat, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        self.lrelu = nn.LeakyReLU(negative_slope=<span class="number">0.1</span>, inplace=<span class="literal">True</span>)</span><br><span class="line">        self.upsample = nn.Upsample(scale_factor=<span class="number">2</span>, mode=<span class="string">'bilinear'</span>, align_corners=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, aligned_feat)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            aligned_feat (Tensor): Aligned features with shape (b, t, c, h, w).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            Tensor: Features after TSA with the shape (b, c, h, w).</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        b, t, c, h, w = aligned_feat.size()</span><br><span class="line">        <span class="comment"># temporal attention</span></span><br><span class="line">        embedding_ref = self.temporal_attn1(aligned_feat[:, self.center_frame_idx, :, :, :].clone())</span><br><span class="line">        embedding = self.temporal_attn2(aligned_feat.view(<span class="number">-1</span>, c, h, w))</span><br><span class="line">        embedding = embedding.view(b, t, <span class="number">-1</span>, h, w)  <span class="comment"># (b, t, c, h, w)</span></span><br><span class="line"></span><br><span class="line">        corr_l = []  <span class="comment"># correlation list</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(t):</span><br><span class="line">            emb_neighbor = embedding[:, i, :, :, :]</span><br><span class="line">            corr = torch.sum(emb_neighbor * embedding_ref, <span class="number">1</span>)  <span class="comment"># (b, h, w)</span></span><br><span class="line">            corr_l.append(corr.unsqueeze(<span class="number">1</span>))  <span class="comment"># (b, 1, h, w)</span></span><br><span class="line">        corr_prob = torch.sigmoid(torch.cat(corr_l, dim=<span class="number">1</span>))  <span class="comment"># (b, t, h, w)</span></span><br><span class="line">        corr_prob = corr_prob.unsqueeze(<span class="number">2</span>).expand(b, t, c, h, w)</span><br><span class="line">        corr_prob = corr_prob.contiguous().view(b, <span class="number">-1</span>, h, w)  <span class="comment"># (b, t*c, h, w)</span></span><br><span class="line">        aligned_feat = aligned_feat.view(b, <span class="number">-1</span>, h, w) * corr_prob</span><br><span class="line"></span><br><span class="line">        <span class="comment"># fusion</span></span><br><span class="line">        feat = self.lrelu(self.feat_fusion(aligned_feat))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># spatial attention</span></span><br><span class="line">        attn = self.lrelu(self.spatial_attn1(aligned_feat))</span><br><span class="line">        attn_max = self.max_pool(attn)</span><br><span class="line">        attn_avg = self.avg_pool(attn)</span><br><span class="line">        attn = self.lrelu(self.spatial_attn2(torch.cat([attn_max, attn_avg], dim=<span class="number">1</span>)))</span><br><span class="line">        <span class="comment"># pyramid levels</span></span><br><span class="line">        attn_level = self.lrelu(self.spatial_attn_l1(attn))</span><br><span class="line">        attn_max = self.max_pool(attn_level)</span><br><span class="line">        attn_avg = self.avg_pool(attn_level)</span><br><span class="line">        attn_level = self.lrelu(self.spatial_attn_l2(torch.cat([attn_max, attn_avg], dim=<span class="number">1</span>)))</span><br><span class="line">        attn_level = self.lrelu(self.spatial_attn_l3(attn_level))</span><br><span class="line">        attn_level = self.upsample(attn_level)</span><br><span class="line"></span><br><span class="line">        attn = self.lrelu(self.spatial_attn3(attn)) + attn_level</span><br><span class="line">        attn = self.lrelu(self.spatial_attn4(attn))</span><br><span class="line">        attn = self.upsample(attn)</span><br><span class="line">        attn = self.spatial_attn5(attn)</span><br><span class="line">        attn_add = self.spatial_attn_add2(self.lrelu(self.spatial_attn_add1(attn)))</span><br><span class="line">        attn = torch.sigmoid(attn)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># after initialization, * 2 makes (attn * 2) to be close to 1.</span></span><br><span class="line">        feat = feat * attn * <span class="number">2</span> + attn_add</span><br><span class="line">        <span class="keyword">return</span> feat</span><br></pre></td></tr></tbody></table></figure>

<h4 id="Evaluation-1"><a href="#Evaluation-1" class="headerlink" title="Evaluation"></a>Evaluation</h4><p><img src="https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211025213641309.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211025213721167.png" alt=""></p>
<blockquote>
<p>Timofte R, Rothe R, Van Gool L. Seven ways to improve example-based single image super resolution[C] //Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2016: 1865-1873.  <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=7780575&amp;tag=1" target="_blank" rel="noopener">pdf</a></p>
</blockquote>
<hr>
<h3 id="Paper-Seven-ways"><a href="#Paper-Seven-ways" class="headerlink" title="Paper: Seven ways"></a>Paper: Seven ways</h3><div align="center">
<br>
<b>Seven ways to improve example-based single image super resolution</b>
</div>


<h4 id="Summary-2"><a href="#Summary-2" class="headerlink" title="Summary"></a>Summary</h4><ol>
<li>present seven techniques that everybody should know to improve example-based single image supre resolution. </li>
</ol>
<h4 id="Ways"><a href="#Ways" class="headerlink" title="Ways"></a>Ways</h4><ul>
<li>Augmentation of training data</li>
</ul>
<blockquote>
<p>If we rotate the original images by 90,180,270, and flip them upside-down, we get images without altered content. Using an interpolation for other rotation angles can corrupt edges and impact the performance.</p>
</blockquote>
<ul>
<li>large dictionary and hierarchical search</li>
</ul>
<blockquote>
<p>if the dictionary size(basis of samples/anchoring points) is increased, the performance for sparse coding or anchoed methods improves, as the learned model generalizes better.</p>
</blockquote>
<h3 id="Project"><a href="#Project" class="headerlink" title="Project"></a>Project</h3><h4 id="1-image-super-resolution-3k"><a href="#1-image-super-resolution-3k" class="headerlink" title="1. image-super-resolution 3k"></a>1. <strong><a href="https://github.com/idealo/image-super-resolution" target="_blank" rel="noopener">image-super-resolution 3k</a></strong></h4><p>This project contains <code>Keras implementations of different Residual Dense Networks for Single Image Super-Resolution (ISR)</code> as well as scripts to train these networks using content and adversarial loss components.</p>
<p>The implemented networks include:</p>
<ul>
<li>The super-scaling Residual Dense Network described in <a href="https://arxiv.org/abs/1802.08797" target="_blank" rel="noopener">Residual Dense Network for Image Super-Resolution</a> (Zhang et al. 2018)</li>
<li>The super-scaling Residual in Residual Dense Network described in <a href="https://arxiv.org/abs/1809.00219" target="_blank" rel="noopener">ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks</a> (Wang et al. 2018)</li>
<li>A multi-output version of the Keras VGG19 network for deep features extraction used in the perceptual loss</li>
<li>A custom discriminator network based on the one described in <a href="https://arxiv.org/abs/1609.04802" target="_blank" rel="noopener">Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network</a> (SRGANS, Ledig et al. 2017)</li>
</ul>
<p>Read the full documentation at: <a href="https://idealo.github.io/image-super-resolution/" target="_blank" rel="noopener">https://idealo.github.io/image-super-resolution/</a>.</p>
<p><img src="https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211017171103641.png" alt=""></p>
<h4 id="2-Waifu2x-Extension-GUI-5k"><a href="#2-Waifu2x-Extension-GUI-5k" class="headerlink" title="2. Waifu2x-Extension-GUI 5k"></a>2. <a href="https://github.com/AaronFeng753/Waifu2x-Extension-GUI" target="_blank" rel="noopener">Waifu2x-Extension-GUI</a> 5k</h4><p><img src="https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211017171647835.png" alt=""></p>
<h3 id="Survey"><a href="#Survey" class="headerlink" title="Survey"></a>Survey</h3><blockquote>
<p>Anwar S, Khan S, Barnes N. A deep journey into super-resolution: A survey[J]. ACM Computing Surveys (CSUR), 2020, 53(3): 1-34. cite 107 <a href="chrome-extension://ikhdkkncnoglghljlkmcimlnlhkeamad/pdf-viewer/web/viewer.html?file=https%3A%2F%2Farxiv.org%2Fpdf%2F1904.07523.pdf">pdf</a></p>
</blockquote>
<blockquote>
<p>Liu A, Liu Y, Gu J, et al. Blind image super-resolution: A survey and beyond[J]. arXiv preprint arXiv:2107.03055, 2021. cite 2, <a href="chrome-extension://ikhdkkncnoglghljlkmcimlnlhkeamad/pdf-viewer/web/viewer.html?file=https%3A%2F%2Farxiv.org%2Fpdf%2F2107.03055.pdf">pdf</a></p>
</blockquote>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script>
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        Author:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://liudongdong1.github.io" rel="external nofollow noreferrer">liudongdong1</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        Link:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://liudongdong1.github.io/2021/09/23/shi-jue-ai/video-understand/superresolution/">https://liudongdong1.github.io/2021/09/23/shi-jue-ai/video-understand/superresolution/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        Reprint policy:
                    </i>
                </span>
                <span class="reprint-info">
                    All articles in this blog are used except for special statements
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    reprint polocy. If reproduced, please indicate source
                    <a href="https://liudongdong1.github.io" target="_blank">liudongdong1</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>Copied successfully, please follow the reprint policy of this article</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">more</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/tags/VideoAnalyse/">
                                    <span class="chip bg-color">VideoAnalyse</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="qq,qzone,wechat,weibo,douban" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
                <style>
    #reward {
        margin: 40px 0;
        text-align: center;
    }

    #reward .reward-link {
        font-size: 1.4rem;
        line-height: 38px;
    }

    #reward .btn-floating:hover {
        box-shadow: 0 6px 12px rgba(0, 0, 0, 0.2), 0 5px 15px rgba(0, 0, 0, 0.2);
    }

    #rewardModal {
        width: 320px;
        height: 350px;
    }

    #rewardModal .reward-title {
        margin: 15px auto;
        padding-bottom: 5px;
    }

    #rewardModal .modal-content {
        padding: 10px;
    }

    #rewardModal .close {
        position: absolute;
        right: 15px;
        top: 15px;
        color: rgba(0, 0, 0, 0.5);
        font-size: 1.3rem;
        line-height: 20px;
        cursor: pointer;
    }

    #rewardModal .close:hover {
        color: #ef5350;
        transform: scale(1.3);
        -moz-transform:scale(1.3);
        -webkit-transform:scale(1.3);
        -o-transform:scale(1.3);
    }

    #rewardModal .reward-tabs {
        margin: 0 auto;
        width: 210px;
    }

    .reward-tabs .tabs {
        height: 38px;
        margin: 10px auto;
        padding-left: 0;
    }

    .reward-content ul {
        padding-left: 0 !important;
    }

    .reward-tabs .tabs .tab {
        height: 38px;
        line-height: 38px;
    }

    .reward-tabs .tab a {
        color: #fff;
        background-color: #ccc;
    }

    .reward-tabs .tab a:hover {
        background-color: #ccc;
        color: #fff;
    }

    .reward-tabs .wechat-tab .active {
        color: #fff !important;
        background-color: #22AB38 !important;
    }

    .reward-tabs .alipay-tab .active {
        color: #fff !important;
        background-color: #019FE8 !important;
    }

    .reward-tabs .reward-img {
        width: 210px;
        height: 210px;
    }
</style>

<div id="reward">
    <a href="#rewardModal" class="reward-link modal-trigger btn-floating btn-medium waves-effect waves-light red">赏</a>

    <!-- Modal Structure -->
    <div id="rewardModal" class="modal">
        <div class="modal-content">
            <a class="close modal-close"><i class="fas fa-times"></i></a>
            <h4 class="reward-title">你的赏识是我前进的动力</h4>
            <div class="reward-content">
                <div class="reward-tabs">
                    <ul class="tabs row">
                        <li class="tab col s6 alipay-tab waves-effect waves-light"><a href="#alipay">支付宝</a></li>
                        <li class="tab col s6 wechat-tab waves-effect waves-light"><a href="#wechat">微 信</a></li>
                    </ul>
                    <div id="alipay">
                        <img src="/medias/reward/alipay.jpg" class="reward-img" alt="支付宝打赏二维码">
                    </div>
                    <div id="wechat">
                        <img src="/medias/reward/wechat.png" class="reward-img" alt="微信打赏二维码">
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>

<script>
    $(function () {
        $('.tabs').tabs();
    });
</script>

            
        </div>
    </div>

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;Previous</div>
            <div class="card">
                <a href="/2021/10/01/aiot/framework/aiotframework/">
                    <div class="card-image">
                        
                        <img src="https://image.shutterstock.com/image-photo/digital-marketing-businessman-using-modern-260nw-1053868622.jpg" class="responsive-img" alt="AIOTFrameWork">
                        
                        <span class="card-title">AIOTFrameWork</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            
Resource
IoT 项目汇总：https://github.com/phodal/awesome-iot


        document.querySelectorAll('.github-emoji')
          
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2021-10-01
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/AIOT/" class="post-category">
                                    AIOT
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/IOT/">
                        <span class="chip bg-color">IOT</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                Next&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/2021/09/22/yu-yan-kuang-jia/framework/codereading/compreface/">
                    <div class="card-image">
                        
                        <img src="https://cdn.stocksnap.io/img-thumbs/280h/CJCMX14Z4F.jpg" class="responsive-img" alt="Compreface">
                        
                        <span class="card-title">Compreface</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            1. usage
Step 1. Install and run CompreFace using our Getting Started guide
Step 2.  sign up for the system and log in.

                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2021-09-22
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/%E8%AF%AD%E8%A8%80%E6%A1%86%E6%9E%B6/" class="post-category">
                                    语言框架
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/Framework/">
                        <span class="chip bg-color">Framework</span>
                    </a>
                    
                    <a href="/tags/Java/">
                        <span class="chip bg-color">Java</span>
                    </a>
                    
                    <a href="/tags/springcloud/">
                        <span class="chip bg-color">springcloud</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/libs/codeBlock/codeBlockFuction.js"></script>

<!-- 代码语言 -->

<script type="text/javascript" src="/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/libs/codeBlock/codeShrink.js"></script>


<!-- 代码块折行 -->

<style type="text/css">
code[class*="language-"], pre[class*="language-"] { white-space: pre !important; }
</style>


    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;TOC</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // modify the toc link href to support Chinese.
        let i = 0;
        let tocHeading = 'toc-heading-';
        $('#toc-content a').each(function () {
            $(this).attr('href', '#' + tocHeading + (++i));
        });

        // modify the heading title id to support Chinese.
        i = 0;
        $('#articleContent').children('h2, h3, h4').each(function () {
            $(this).attr('id', tocHeading + (++i));
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>

    

</main>




    <!-- <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="463294659"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/libs/aplayer/APlayer.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/meting@2/dist/Meting.min.js"></script>

    
    <div class="container row center-align" style="margin-bottom: 0px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            <span id="year">2019</span>
            <a href="https://liudongdong1.github.io" target="_blank">liudongdong</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            <br>
            
            &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                class="white-color">1206.4k</span>&nbsp;字
            
            
            
            
            
            
            <span id="busuanzi_container_site_pv">
                |&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;<span id="busuanzi_value_site_pv"
                    class="white-color"></span>&nbsp;次
            </span>
            
            
            <span id="busuanzi_container_site_uv">
                |&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;<span id="busuanzi_value_site_uv"
                    class="white-color"></span>&nbsp;人
            </span>
            
            <br>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/liudongdong1/" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:3463264078@qq.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>














    <a href="https://blog.csdn.net/liudongdong19/" class="tooltipped" target="_blank" data-tooltip="关注我的CSDN: https://blog.csdn.net/liudongdong19/" data-position="top" data-delay="50">
        <i class="fab fa-csdn">C</i>
    </a>





</div>
    </div>
</footer>

<div class="progress-bar"></div>
 -->

    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;Search</span>
            <input type="search" id="searchInput" name="s" placeholder="Please enter a search keyword"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script src="/js/search.js"></script>
<script type="text/javascript">
$(function () {
    searchFunc("/search.xml", 'searchInput', 'searchResult');
});
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script type="text/javascript" src="/js/CFS.Snow.min.js"></script>
    <!-- 点击爆灯效果 -->
    <canvas class="fireworks" style="position: fixed;left: 0;top: 0;z-index: 1; pointer-events: none;" ></canvas> 
    <script type="text/javascript" src="//cdn.bootcss.com/animejs/2.2.0/anime.min.js"></script> 
    <script type="text/javascript" src="/js/fireworks.js"></script>
    <!--动态线条背景-->
    <script type="text/javascript"
        color="122 103 238" opacity='0.7' zIndex="-2" count="200" src="//cdn.bootcss.com/canvas-nest.js/1.0.0/canvas-nest.min.js">
    </script>
    <!-- 天气 -->
    <!-- weather -->
    <!-- weather -->
    <script type="text/javascript">
         WIDGET = {FID: 'knAMQaFanP'}
    </script>
    <script type="text/javascript" src="https://apip.weatherdt.com/float/static/js/r.js?v=1111"></script>
    <script type="text/javascript" src="https://apip.weatherdt.com/float/static/js/r.js?v=1111"></script>
    <script src="/libs/materialize/materialize.min.js"></script>
    <script src="/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/libs/aos/aos.js"></script>
    <script src="/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/js/matery.js"></script>

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    
    <script async src="/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    

    
    
    <script type="text/javascript" size="150" alpha='0.6'
        zIndex="-1" src="/libs/background/ribbon-refresh.min.js" async="async"></script>
    

    

    
    <script src="/libs/instantpage/instantpage.js" type="module"></script>
    
    <!-- {% include '_custom/custom.swig' %} -->

</body>

</html>
