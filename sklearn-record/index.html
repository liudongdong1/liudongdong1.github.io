<!DOCTYPE html>
<html itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">
  <head>
    
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
    <meta name="robots" content="noodp" />
    <title>SkLearn Record - DAY By DAY</title><meta name="author" content="LiuDongdong">
<meta name="author-link" content="https://liudongdong1.github.io/">
<meta name="description" content="Supervised learning 1.1. Linear Models 1.2. Linear and Quadratic Discriminant Analysis 1.3. Kernel ridge regression 1.4. Support Vector Machines 1.5. Stochastic Gradient Descent 1.6. Nearest Neighbors 1.7. Gaussian Processes 1.8. Cross decomposition 1.9. Naive Bayes 1.10. Decision Trees 1.11. Ensemble methods 1.12. Multiclass and multioutput algorithms 1.13. Feature selection 1.14. Semi-supervised learning 1.15. Isotonic regression 1.16. Probability calibration 1.17. Neural network models (supervised) Unsupervised learning 2.1. Gaussian mixture models 2.2." /><meta name="keywords" content='SkLearn, math' /><meta itemprop="name" content="SkLearn Record">
<meta itemprop="description" content="Supervised learning 1.1. Linear Models 1.2. Linear and Quadratic Discriminant Analysis 1.3. Kernel ridge regression 1.4. Support Vector Machines 1.5. Stochastic Gradient Descent 1.6. Nearest Neighbors 1.7. Gaussian Processes 1.8. Cross decomposition 1.9. Naive Bayes 1.10. Decision Trees 1.11. Ensemble methods 1.12. Multiclass and multioutput algorithms 1.13. Feature selection 1.14. Semi-supervised learning 1.15. Isotonic regression 1.16. Probability calibration 1.17. Neural network models (supervised) Unsupervised learning 2.1. Gaussian mixture models 2.2."><meta itemprop="datePublished" content="2021-04-14T21:38:11+00:00" />
<meta itemprop="dateModified" content="2023-12-31T13:47:03+08:00" />
<meta itemprop="wordCount" content="9249"><meta itemprop="image" content="https://liudongdong1.github.io/logo.png"/>
<meta itemprop="keywords" content="SkLearn,math," /><meta property="og:title" content="SkLearn Record" />
<meta property="og:description" content="Supervised learning 1.1. Linear Models 1.2. Linear and Quadratic Discriminant Analysis 1.3. Kernel ridge regression 1.4. Support Vector Machines 1.5. Stochastic Gradient Descent 1.6. Nearest Neighbors 1.7. Gaussian Processes 1.8. Cross decomposition 1.9. Naive Bayes 1.10. Decision Trees 1.11. Ensemble methods 1.12. Multiclass and multioutput algorithms 1.13. Feature selection 1.14. Semi-supervised learning 1.15. Isotonic regression 1.16. Probability calibration 1.17. Neural network models (supervised) Unsupervised learning 2.1. Gaussian mixture models 2.2." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://liudongdong1.github.io/sklearn-record/" /><meta property="og:image" content="https://liudongdong1.github.io/logo.png"/><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2021-04-14T21:38:11+00:00" />
<meta property="article:modified_time" content="2023-12-31T13:47:03+08:00" />
<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://liudongdong1.github.io/logo.png"/>

<meta name="twitter:title" content="SkLearn Record"/>
<meta name="twitter:description" content="Supervised learning 1.1. Linear Models 1.2. Linear and Quadratic Discriminant Analysis 1.3. Kernel ridge regression 1.4. Support Vector Machines 1.5. Stochastic Gradient Descent 1.6. Nearest Neighbors 1.7. Gaussian Processes 1.8. Cross decomposition 1.9. Naive Bayes 1.10. Decision Trees 1.11. Ensemble methods 1.12. Multiclass and multioutput algorithms 1.13. Feature selection 1.14. Semi-supervised learning 1.15. Isotonic regression 1.16. Probability calibration 1.17. Neural network models (supervised) Unsupervised learning 2.1. Gaussian mixture models 2.2."/>
<meta name="application-name" content="DAY By DAY">
<meta name="apple-mobile-web-app-title" content="DAY By DAY"><meta name="theme-color" data-light="#f8f8f8" data-dark="#252627" content="#f8f8f8"><meta name="msapplication-TileColor" content="#da532c"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" />
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"><link rel="manifest" href="/site.webmanifest"><link rel="canonical" href="https://liudongdong1.github.io/sklearn-record/" /><link rel="prev" href="https://liudongdong1.github.io/sklearnvisualization/" /><link rel="next" href="https://liudongdong1.github.io/sklearn-evaluation/" /><link rel="stylesheet" href="/css/style.min.css"><link rel="stylesheet" href="/lib/fontawesome-free/all.min.css"><link rel="stylesheet" href="/lib/animate/animate.min.css"><script type="application/ld+json">
  {
    "@context": "http://schema.org",
    "@type": "BlogPosting",
    "headline": "SkLearn Record",
    "inLanguage": "zh-CN",
    "mainEntityOfPage": {
      "@type": "WebPage",
      "@id": "https:\/\/liudongdong1.github.io\/sklearn-record\/"
    },"genre": "posts","keywords": "SkLearn, math","wordcount":  9249 ,
    "url": "https:\/\/liudongdong1.github.io\/sklearn-record\/","datePublished": "2021-04-14T21:38:11+00:00","dateModified": "2023-12-31T13:47:03+08:00","license": "This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.","publisher": {
      "@type": "Organization",
      "name": "LiuDongdong","logo": "https:\/\/liudongdong1.github.io\/images\/person.png"},"author": {
        "@type": "Person",
        "name": "LiuDongdong"
      },"description": ""
  }
  </script></head>
  <body data-header-desktop="auto" data-header-mobile="auto"><script>(window.localStorage?.getItem('theme') ? localStorage.getItem('theme') === 'dark' : ('auto' === 'auto' ? window.matchMedia('(prefers-color-scheme: dark)').matches : 'auto' === 'dark')) && document.body.setAttribute('data-theme', 'dark');</script><div class="wrapper"><script type="text/javascript"
        async
        src="https://cdnjs.cloudflare.com/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$']],
    displayMath: [['$$','$$']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
});

MathJax.Hub.Queue(function() {
    
    
    
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>

<style>
code.has-jax {
    font: inherit;
    font-size: 100%;
    background: inherit;
    border: inherit;
    color: #515151;
}
</style>
<header class="desktop animate__faster" id="header-desktop">
  <div class="header-wrapper" data-github-corner="right">
    <div class="header-title">
      <a href="/" title="DAY By DAY"><img
    class="lazyload logo"
    src="/svg/loading.min.svg"
    data-src="/fixit.min.svg"
    data-srcset="/fixit.min.svg, /fixit.min.svg 1.5x, /fixit.min.svg 2x"
    data-sizes="auto"
    alt="DAY By DAY"
    title="DAY By DAY"/><span class="header-title-text"></span></a><span id="typeit-header-subtitle-desktop" class="typeit header-subtitle"></span></div>
    <nav>
      <ul class="menu"><li class="menu-item">
              <a
                class="menu-link"
                href="/posts/"
                
                
              ><i class="fa-solid fa-archive fa-fw fa-sm" aria-hidden="true"></i> 所有文章</a></li><li class="menu-item">
              <a
                class="menu-link"
                href="/categories/"
                
                
              ><i class="fa-solid fa-th fa-fw fa-sm" aria-hidden="true"></i> 分类</a></li><li class="menu-item">
              <a
                class="menu-link"
                href="/tags/"
                
                
              ><i class="fa-solid fa-tags fa-fw fa-sm" aria-hidden="true"></i> 标签</a></li><li class="menu-item">
              <a
                class="menu-link"
                href="/friends/"
                title="友情链接"
                
              ><i class="fa-solid fa-users fa-fw fa-sm" aria-hidden="true"></i> 友链</a></li><li class="menu-item">
              <a
                class="menu-link"
                href="/about/"
                
                
              ><i class="fa-solid fa-info-circle fa-fw fa-sm" aria-hidden="true"></i> 关于</a></li><li class="menu-item delimiter"></li><li class="menu-item language">
            <span role="button" aria-label="选择语言" title="选择语言">简体中文<i class="dropdown-icon fa-solid fa-chevron-down" aria-hidden="true"></i>
            </span>
            <ul class="sub-menu"><li class="menu-item">没有更多翻译</li></ul>
          </li><li class="menu-item search" id="search-desktop">
            <input type="text" placeholder="搜索文章标题或内容 ..." id="search-input-desktop">
            <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-desktop" title="搜索">
              <i class="fa-solid fa-search fa-fw" aria-hidden="true"></i>
            </a>
            <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-desktop" title="清空">
              <i class="fa-solid fa-times-circle fa-fw" aria-hidden="true"></i>
            </a>
            <span class="search-button search-loading" id="search-loading-desktop">
              <i class="fa-solid fa-spinner fa-fw fa-spin" aria-hidden="true"></i>
            </span>
          </li><li class="menu-item theme-switch" title="切换主题">
          <i class="fa-solid fa-adjust fa-fw" aria-hidden="true"></i>
        </li>
      </ul>
    </nav>
  </div>
</header><header class="mobile animate__faster" id="header-mobile">
  <div class="header-container">
    <div class="header-wrapper">
      <div class="header-title">
        <a href="/" title="DAY By DAY"><img
    class="lazyload logo"
    src="/svg/loading.min.svg"
    data-src="/fixit.min.svg"
    data-srcset="/fixit.min.svg, /fixit.min.svg 1.5x, /fixit.min.svg 2x"
    data-sizes="auto"
    alt="/fixit.min.svg"
    title="/fixit.min.svg"/><span class="header-title-text"></span></a><span id="typeit-header-subtitle-mobile" class="typeit header-subtitle"></span></div>
      <div class="menu-toggle" id="menu-toggle-mobile">
        <span></span><span></span><span></span>
      </div>
    </div>
    <nav>
      <ul class="menu" id="menu-mobile"><li class="search-wrapper">
            <div class="search mobile" id="search-mobile">
              <input type="text" placeholder="搜索文章标题或内容 ..." id="search-input-mobile">
              <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-mobile" title="搜索">
                <i class="fa-solid fa-search fa-fw" aria-hidden="true"></i>
              </a>
              <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-mobile" title="清空">
                <i class="fa-solid fa-times-circle fa-fw" aria-hidden="true"></i>
              </a>
              <span class="search-button search-loading" id="search-loading-mobile">
                <i class="fa-solid fa-spinner fa-fw fa-spin" aria-hidden="true"></i>
              </span>
            </div>
            <a href="javascript:void(0);" class="search-cancel" id="search-cancel-mobile">
              取消
            </a>
          </li><li
              class="menu-item"
            ><a
                  class="menu-link"
                  href="/posts/"
                  
                  
                ><i class="fa-solid fa-archive fa-fw fa-sm" aria-hidden="true"></i> 所有文章</a></li><li
              class="menu-item"
            ><a
                  class="menu-link"
                  href="/categories/"
                  
                  
                ><i class="fa-solid fa-th fa-fw fa-sm" aria-hidden="true"></i> 分类</a></li><li
              class="menu-item"
            ><a
                  class="menu-link"
                  href="/tags/"
                  
                  
                ><i class="fa-solid fa-tags fa-fw fa-sm" aria-hidden="true"></i> 标签</a></li><li
              class="menu-item"
            ><a
                  class="menu-link"
                  href="/friends/"
                  title="友情链接"
                  
                ><i class="fa-solid fa-users fa-fw fa-sm" aria-hidden="true"></i> 友链</a></li><li
              class="menu-item"
            ><a
                  class="menu-link"
                  href="/about/"
                  
                  
                ><i class="fa-solid fa-info-circle fa-fw fa-sm" aria-hidden="true"></i> 关于</a></li><li
              class="menu-item text-center"
            ><a
                  class="menu-link"
                  href="/"
                  title="GitHub"
                  
                ><i class='fa-brands fa-github fa-fw' aria-hidden='true'></i> </a></li><li class="menu-item theme-switch" title="切换主题">
          <i class="fa-solid fa-adjust fa-fw" aria-hidden="true"></i>
        </li><li class="menu-item language">
            <span role="button" aria-label="选择语言" title="选择语言">简体中文<i class="dropdown-icon fa-solid fa-chevron-down" aria-hidden="true"></i>
            </span>
            <select class="language-select" onchange="location = this.value;"><option disabled>没有更多翻译</option></select>
          </li></ul>
    </nav>
  </div>
</header><div class="search-dropdown desktop">
    <div id="search-dropdown-desktop"></div>
  </div>
  <div class="search-dropdown mobile">
    <div id="search-dropdown-mobile"></div>
  </div><main class="container" data-page-style="normal"><aside class="toc" id="toc-auto"><h2 class="toc-title">目录 <i class="toc-icon fa-solid fa-angle-down fa-fw"></i></h2>
      <div class="toc-content" id="toc-content-auto"></div></aside>

  <aside class="aside-custom" id="aside-sakana">
    

<div class="sakana-widget">
  <div class="sakana-item" id="takina-widget"></div>
  <div class="sakana-item" id="chisato-widget"></div>
</div>
<script>
  function initSakanaWidget() {
    const takina = SakanaWidget.getCharacter('takina')
    SakanaWidget.registerCharacter('takina-slow', takina);
    new SakanaWidget({
      character: 'takina-slow',
      controls: false,
      autoFit: true,
      stroke: {
        color: "#b4b4b4",
        width: 2
      }
    }).mount('#takina-widget');

    const chisato = SakanaWidget.getCharacter('chisato')
    SakanaWidget.registerCharacter('chisato-slow', chisato);
    new SakanaWidget({
      character: 'chisato-slow',
      controls: false,
      autoFit: true,
      stroke: {
        color: "#b4b4b4",
        width: 2
      }
    }).mount('#chisato-widget');
  }
</script>
<script async onload="initSakanaWidget()" src="https://cdn.jsdelivr.net/npm/sakana-widget@2.3.0/lib/sakana.min.js">
</script></aside>

  <article class="page single">
    <div class="header"><h1 class="single-title animate__animated animate__flipInX">
        <span>SkLearn Record</span>
      </h1></div><div class="post-meta">
      <div class="post-meta-line"><span class="post-author"><a href="https://liudongdong1.github.io/" title="作者"target="_blank" rel="external nofollow noopener noreferrer author" class="author"><img
    class="lazyload avatar"
    src="/svg/loading.min.svg"
    data-src="/images/study-avatar.png"
    data-srcset="/images/study-avatar.png, /images/study-avatar.png 1.5x, /images/study-avatar.png 2x"
    data-sizes="auto"
    alt="LiuDongdong"
    title="LiuDongdong" width="1024" height="1024"/>&nbsp;LiuDongdong</a></span>
          <span class="post-category">收录于 <a href="/categories/"><i class="fa-regular fa-folder fa-fw"></i>&nbsp;Categories</a>&ensp;<a href="/categories/demo/"><i class="fa-regular fa-folder fa-fw"></i>&nbsp;Demo</a></span></div>
      <div class="post-meta-line"><span title=2021-04-14&#32;21:38:11>
            <i class="fa-regular fa-calendar-alt fa-fw"></i>&nbsp;<time datetime="2021-04-14" >2021-04-14</time>
          </span>&nbsp;<i class="fa-solid fa-pencil-alt fa-fw"></i>&nbsp;约 9249 字&nbsp;
        <i class="fa-regular fa-clock fa-fw"></i>&nbsp;预计阅读 19 分钟&nbsp;<span id="busuanzi_container_page_pv" class="busuanzi_visitors comment-visitors" data-flag-title="SkLearn Record">
            <i class="fa-regular fa-eye fa-fw"></i>&nbsp;<span id="busuanzi_value_page_pv">-</span>&nbsp;次阅读
          </span>&nbsp;</div>
    </div><div class="featured-image"><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20210414084511.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20210414084511.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20210414084511.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20210414084511.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20210414084511.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20210414084511.png"/></div><div class="details toc" id="toc-static" kept="true">
        <div class="details-summary toc-title">
          <span>目录</span>
          <span><i class="details-icon fa-solid fa-angle-right"></i></span>
        </div>
        <div class="details-content toc-content" id="toc-content-static"><nav id="TableOfContents">
  <ul>
    <li>
      <ul>
        <li><a href="#1-svc">1. SVC</a></li>
        <li><a href="#2-kneighborsclassifierhttpsscikit-learnorgstablemodulesgeneratedsklearnneighborskneighborsclassifierhtmlhighlightkneighborsclassifiersklearnneighborskneighborsclassifier">2. <a href="https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html?highlight=kneighborsclassifier#sklearn.neighbors.KNeighborsClassifier">KNeighborsClassifier</a></a></li>
        <li><a href="#3-randomforestclassifierhttpsscikit-learnorgstablemodulesgeneratedsklearnensemblerandomforestclassifierhtmlhighlightrandomforestclassifiersklearnensemblerandomforestclassifier">3. <a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html?highlight=randomforestclassifier#sklearn.ensemble.RandomForestClassifier">RandomForestClassifier</a></a></li>
        <li><a href="#5-gradientboostingclassifierhttpsscikit-learnorgstablemodulesgeneratedsklearnensemblegradientboostingclassifierhtmlhighlightgradientboostingclassifiersklearnensemblegradientboostingclassifier">5. <a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html?highlight=gradientboostingclassifier#sklearn.ensemble.GradientBoostingClassifier">GradientBoostingClassifier</a></a></li>
        <li><a href="#6-gaussiannbhttpsscikit-learnorgstablemodulesgeneratedsklearnnaive_bayesgaussiannbhtmlhighlightgaussiannbsklearnnaive_bayesgaussiannb">6. <a href="https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html?highlight=gaussiannb#sklearn.naive_bayes.GaussianNB">GaussianNB</a></a></li>
      </ul>
    </li>
  </ul>
</nav></div>
      </div><div
      class="content"
      id="content"
      
      
    ><ul>
<li>Supervised learning
<ul>
<li><a href="https://scikit-learn.org/stable/modules/linear_model.html"target="_blank" rel="external nofollow noopener noreferrer">1.1. Linear Models<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></li>
<li><a href="https://scikit-learn.org/stable/modules/lda_qda.html"target="_blank" rel="external nofollow noopener noreferrer">1.2. Linear and Quadratic Discriminant Analysis<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></li>
<li><a href="https://scikit-learn.org/stable/modules/kernel_ridge.html"target="_blank" rel="external nofollow noopener noreferrer">1.3. Kernel ridge regression<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></li>
<li><a href="https://scikit-learn.org/stable/modules/svm.html"target="_blank" rel="external nofollow noopener noreferrer">1.4. Support Vector Machines<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></li>
<li><a href="https://scikit-learn.org/stable/modules/sgd.html"target="_blank" rel="external nofollow noopener noreferrer">1.5. Stochastic Gradient Descent<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></li>
<li><a href="https://scikit-learn.org/stable/modules/neighbors.html"target="_blank" rel="external nofollow noopener noreferrer">1.6. Nearest Neighbors<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></li>
<li><a href="https://scikit-learn.org/stable/modules/gaussian_process.html"target="_blank" rel="external nofollow noopener noreferrer">1.7. Gaussian Processes<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></li>
<li><a href="https://scikit-learn.org/stable/modules/cross_decomposition.html"target="_blank" rel="external nofollow noopener noreferrer">1.8. Cross decomposition<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></li>
<li><a href="https://scikit-learn.org/stable/modules/naive_bayes.html"target="_blank" rel="external nofollow noopener noreferrer">1.9. Naive Bayes<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></li>
<li><a href="https://scikit-learn.org/stable/modules/tree.html"target="_blank" rel="external nofollow noopener noreferrer">1.10. Decision Trees<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></li>
<li><a href="https://scikit-learn.org/stable/modules/ensemble.html"target="_blank" rel="external nofollow noopener noreferrer">1.11. Ensemble methods<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></li>
<li><a href="https://scikit-learn.org/stable/modules/multiclass.html"target="_blank" rel="external nofollow noopener noreferrer">1.12. Multiclass and multioutput algorithms<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></li>
<li><a href="https://scikit-learn.org/stable/modules/feature_selection.html"target="_blank" rel="external nofollow noopener noreferrer">1.13. Feature selection<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></li>
<li><a href="https://scikit-learn.org/stable/modules/semi_supervised.html"target="_blank" rel="external nofollow noopener noreferrer">1.14. Semi-supervised learning<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></li>
<li><a href="https://scikit-learn.org/stable/modules/isotonic.html"target="_blank" rel="external nofollow noopener noreferrer">1.15. Isotonic regression<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></li>
<li><a href="https://scikit-learn.org/stable/modules/calibration.html"target="_blank" rel="external nofollow noopener noreferrer">1.16. Probability calibration<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></li>
<li><a href="https://scikit-learn.org/stable/modules/neural_networks_supervised.html"target="_blank" rel="external nofollow noopener noreferrer">1.17. Neural network models (supervised)<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></li>
</ul>
</li>
<li>Unsupervised learning
<ul>
<li><a href="https://scikit-learn.org/stable/modules/mixture.html"target="_blank" rel="external nofollow noopener noreferrer">2.1. Gaussian mixture models<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></li>
<li><a href="https://scikit-learn.org/stable/modules/manifold.html"target="_blank" rel="external nofollow noopener noreferrer">2.2. Manifold learning<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></li>
<li><a href="https://scikit-learn.org/stable/modules/clustering.html"target="_blank" rel="external nofollow noopener noreferrer">2.3. Clustering<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></li>
<li><a href="https://scikit-learn.org/stable/modules/biclustering.html"target="_blank" rel="external nofollow noopener noreferrer">2.4. Biclustering<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></li>
<li><a href="https://scikit-learn.org/stable/modules/decomposition.html"target="_blank" rel="external nofollow noopener noreferrer">2.5. Decomposing signals in components (matrix factorization problems)<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></li>
<li><a href="https://scikit-learn.org/stable/modules/covariance.html"target="_blank" rel="external nofollow noopener noreferrer">2.6. Covariance estimation<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></li>
<li><a href="https://scikit-learn.org/stable/modules/outlier_detection.html"target="_blank" rel="external nofollow noopener noreferrer">2.7. Novelty and Outlier Detection<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></li>
<li><a href="https://scikit-learn.org/stable/modules/density.html"target="_blank" rel="external nofollow noopener noreferrer">2.8. Density Estimation<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></li>
<li><a href="https://scikit-learn.org/stable/modules/neural_networks_unsupervised.html"target="_blank" rel="external nofollow noopener noreferrer">2.9. Neural network models (unsupervised)<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></li>
</ul>
</li>
</ul>
<h3 id="1-svc">1. SVC</h3>
<blockquote>
<ul>
<li><a href="https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html?highlight=svc#sklearn.svm.LinearSVC"target="_blank" rel="external nofollow noopener noreferrer">sklearn.svm.LinearSVC<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></li>
<li><a href="https://scikit-learn.org/stable/modules/generated/sklearn.svm.NuSVC.html?highlight=svc#sklearn.svm.NuSVC"target="_blank" rel="external nofollow noopener noreferrer">sklearn.svm.NuSVC<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></li>
<li><a href="https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html?highlight=svc#sklearn.svm.SVC"target="_blank" rel="external nofollow noopener noreferrer">sklearn.svm.SVC<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></li>
</ul>
</blockquote>
<h4 id="11-函数介绍">1.1. 函数介绍</h4>
<p><code>sklearn.svm.``SVC</code>(*, <em>C=1.0</em>, <em>kernel=&lsquo;rbf&rsquo;</em>, <em>degree=3</em>, <em>gamma=&lsquo;scale&rsquo;</em>, <em>coef0=0.0</em>, <em>shrinking=True</em>, <em>probability=False</em>, <em>tol=0.001</em>, <em>cache_size=200</em>, <em>class_weight=None</em>, <em>verbose=False</em>, <em>max_iter=- 1</em>, <em>decision_function_shape=&lsquo;ovr&rsquo;</em>, <em>break_ties=False</em>, <em>random_state=None</em>)[<a href="https://github.com/scikit-learn/scikit-learn/blob/95119c13a/sklearn/svm/_classes.py#L443"target="_blank" rel="external nofollow noopener noreferrer">source]<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a><a href="https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html?highlight=svc#sklearn.svm.SVC"target="_blank" rel="external nofollow noopener noreferrer">¶<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<blockquote>
<p>The implementation is based on <code>libsvm</code>. The fit time scales at least quadratically with the number of samples and may be <code>impractical beyond tens of thousands of samples</code>. For large datasets consider using <a href="https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC"target="_blank" rel="external nofollow noopener noreferrer"><code>LinearSVC</code><i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a> or <a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier"target="_blank" rel="external nofollow noopener noreferrer"><code>SGDClassifier</code><i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a> instead, possibly after a <a href="https://scikit-learn.org/stable/modules/generated/sklearn.kernel_approximation.Nystroem.html#sklearn.kernel_approximation.Nystroem"target="_blank" rel="external nofollow noopener noreferrer"><code>Nystroem</code><i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a> transformer.</p>
</blockquote>
<ul>
<li>
<p>C : float, optional (default=1.0)</p>
<pre><code>误差项的惩罚参数，一般取值为10的n次幂，如10的-5次幂，10的-4次幂。。。。10的0次幂，10，1000,1000，在python中可以使用pow（10，n） n=-5~inf
C越大，相当于惩罚松弛变量，希望松弛变量接近0，即对误分类的惩罚增大，趋向于对训练集全分对的情况，这样会出现训练集测试时准确率很高，但泛化能力弱。
C值小，对误分类的惩罚减小，容错能力增强，泛化能力较强。
</code></pre>
</li>
<li>
<p>kernel : string, optional (default=’rbf’)</p>
<pre tabindex="0"><code>svc中指定的kernel类型。
可以是： ‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’ 或者自己指定。 默认使用‘rbf’ 
</code></pre></li>
<li>
<p>degree : int, optional (default=3)</p>
<pre tabindex="0"><code> 当指定kernel为 ‘poly’时，表示选择的多项式的最高次数，默认为三次多项式。
 若指定kernel不是‘poly’,则忽略，即该参数只对‘poly’有作用。
</code></pre></li>
<li>
<p>gamma : float, optional (default=’auto’)</p>
<pre tabindex="0"><code>当kernel为‘rbf’, ‘poly’或‘sigmoid’时的kernel系数。
如果不设置，默认为 ‘auto’ ，此时，kernel系数设置为：1/n_features
</code></pre></li>
<li>
<p>probability : boolean, optional (default=False)</p>
<pre tabindex="0"><code>是否采用概率估计。
必须在fit（）方法前使用，该方法的使用会降低运算速度，默认为False。
</code></pre></li>
<li>
<p>tol : float, optional (default=1e-3)</p>
<pre tabindex="0"><code>误差项达到指定值时则停止训练，默认为1e-3，即0.001。
</code></pre></li>
<li>
<p>max_iter : int, optional (default=-1)</p>
<pre tabindex="0"><code>强制设置最大迭代次数。
默认设置为-1，表示无穷大迭代次数。
Hard limit on iterations within solver, or -1 for no limit.
</code></pre></li>
<li>
<p>松弛变量：</p>
<pre tabindex="0"><code>若所研究的线性规划模型的约束条件全是小于类型，那么可以通过标准化过程引入M个非负的松弛变量。
松弛变量的引入常常是为了便于在更大的可行域内求解。若为0，则收敛到原有状态，若大于零，则约束松弛。
</code></pre></li>
</ul>
<h4 id="12-数学推导">1.2. 数学推导</h4>
<h3 id="2-kneighborsclassifierhttpsscikit-learnorgstablemodulesgeneratedsklearnneighborskneighborsclassifierhtmlhighlightkneighborsclassifiersklearnneighborskneighborsclassifier">2. <a href="https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html?highlight=kneighborsclassifier#sklearn.neighbors.KNeighborsClassifier"target="_blank" rel="external nofollow noopener noreferrer">KNeighborsClassifier<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></h3>
<h4 id="21-函数介绍">2.1. 函数介绍</h4>
<p><em>class</em> <code>sklearn.neighbors.``KNeighborsClassifier</code>(<em>n_neighbors=5</em>, ***, <em>weights=&lsquo;uniform&rsquo;</em>, <em>algorithm=&lsquo;auto&rsquo;</em>, <em>leaf_size=30</em>, <em>p=2</em>, <em>metric=&lsquo;minkowski&rsquo;</em>, <em>metric_params=None</em>, <em>n_jobs=None</em>, **<em>kwargs</em>)[<a href="https://github.com/scikit-learn/scikit-learn/blob/95119c13a/sklearn/neighbors/_classification.py#L24"target="_blank" rel="external nofollow noopener noreferrer">source]<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<ul>
<li>n_neighbors ： int，optional(default = 5)
默认情况下kneighbors查询使用的邻居数。就是k-NN的k的值，选取最近的k个点。</li>
<li>weights ： str或callable，可选(默认=‘uniform’)
默认是uniform，参数可以是uniform、distance，也可以是用户自己定义的函数。uniform是均等的权重，就说所有的邻近点的权重都是相等的。<code>distance是不均等的权重，距离近的点比距离远的点的影响大</code>。用户自定义的函数，接收距离的数组，返回一组维数相同的权重。</li>
<li>algorithm ： {‘auto’，‘ball_tree’，‘kd_tree’，‘brute’}，可选
<code>快速k近邻搜索算法</code>，默认参数为auto，可以理解为<code>算法自己决定合适的搜索算法</code>。除此之外，用户也可以自己指定搜索算法<code>ball_tree</code>、<code>kd_tree</code>、<code>brute方法</code>进行搜索，brute是蛮力搜索，也就是线性扫描，当训练集很大时，计算非常耗时。kd_tree，构造kd树存储数据以便对其进行快速检索的树形数据结构，kd树也就是数据结构中的二叉树。以中值切分构造的树，每个结点是一个超矩形，<code>在维数小于20时效率高</code>。<code>ball tree是为了克服kd树高纬失效而发明的</code>，其构造过程是以质心C和半径r分割样本空间，每个节点是一个超球体。</li>
<li>leaf_size ： int，optional(默认值= 30)
默认是30，这个是构造的kd树和ball树的大小。这个值的设置会影响树构建的速度和搜索速度，同样也影响着存储树所需的内存大小。需要根据问题的性质选择最优的大小。</li>
<li>p ： 整数，可选(默认= 2)
<code>距离度量公式</code>。在上小结，我们使用欧氏距离公式进行距离度量。除此之外，还有其他的度量方法，例如曼哈顿距离。这个参数默认为2，也就是默认使用欧式距离公式进行距离度量。也可以设置为1，使用曼哈顿距离公式进行距离度量。</li>
<li>metric ： 字符串或可调用，默认为’minkowski’
用于距离度量，默认度量是minkowski，也就是p=2的欧氏距离(欧几里德度量)。</li>
<li>n_jobs ： int或None，可选(默认=None)
并行处理设置。默认为1，临近点搜索并行工作数。如果为-1，那么CPU的所有cores都用于并行工作。</li>
</ul>
<h3 id="3-randomforestclassifierhttpsscikit-learnorgstablemodulesgeneratedsklearnensemblerandomforestclassifierhtmlhighlightrandomforestclassifiersklearnensemblerandomforestclassifier">3. <a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html?highlight=randomforestclassifier#sklearn.ensemble.RandomForestClassifier"target="_blank" rel="external nofollow noopener noreferrer">RandomForestClassifier<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></h3>
<h4 id="31-函数介绍">3.1. 函数介绍</h4>
<p><em>class</em> <code>sklearn.ensemble.``RandomForestClassifier</code>(<em>n_estimators=100</em>, ***, <em>criterion=&lsquo;gini&rsquo;</em>, <em>max_depth=None</em>, <em>min_samples_split=2</em>, <em>min_samples_leaf=1</em>, <em>min_weight_fraction_leaf=0.0</em>, <em>max_features=&lsquo;auto&rsquo;</em>, <em>max_leaf_nodes=None</em>, <em>min_impurity_decrease=0.0</em>, <em>min_impurity_split=None</em>, <em>bootstrap=True</em>, <em>oob_score=False</em>, <em>n_jobs=None</em>, <em>random_state=None</em>, <em>verbose=0</em>, <em>warm_start=False</em>, <em>class_weight=None</em>, <em>ccp_alpha=0.0</em>, <em>max_samples=None</em>)</p>
<blockquote>
<p>A random forest is a meta estimator that <code>fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting</code>. The sub-sample size is controlled with the <code>max_samples</code> parameter if <code>bootstrap=True</code> (default), otherwise the whole dataset is used to build each tree.</p>
</blockquote>
<ol>
<li>n_estimators：森林中<code>决策树的数量</code>。默认100</li>
<li>criterion：<code>分裂节点所用的标准</code>，可选“gini”, “entropy”，默认“gini”。</li>
<li>max_depth：<code>树的最大深度</code>。如果为None，则将节点展开，直到所有叶子都是纯净的(只有一个类)，或者直到所有叶子都包含少于min_samples_split个样本。默认是None。</li>
<li>min_samples_split：拆分内部节点所需的最少样本数：如果为int，则将min_samples_split视为最小值。如果为float，则min_samples_split是一个分数，而ceil（min_samples_split * n_samples）是每个拆分的最小样本数。默认是2。</li>
<li>min_samples_leaf：在叶节点处需要的最小样本数。仅在任何深度的分割点在左分支和右分支中的每个分支上至少留下min_samples_leaf个训练样本时，才考虑。这可能具有平滑模型的效果，尤其是在回归中。如果为int，则将min_samples_leaf视为最小值。如果为float，则min_samples_leaf是分数，而ceil（min_samples_leaf * n_samples）是每个节点的最小样本数。默认是1。</li>
<li>min_weight_fraction_leaf：在所有叶节点处（所有输入样本）的权重总和中的最小加权分数。如果未提供sample_weight，则样本的权重相等。</li>
<li>max_features：<code>寻找最佳分割时要考虑的特征数量</code>：如果为int，则在每个拆分中考虑max_features个特征。如果为float，则max_features是一个分数，并在每次拆分时考虑int（max_features * n_features）个特征。如果为“auto”，则max_features = sqrt（n_features）。如果为“ sqrt”，则max_features = sqrt（n_features）。如果为“ log2”，则max_features = log2（n_features）。如果为None，则max_features = n_features。注意：在找到至少一个有效的节点样本分区之前，分割的搜索不会停止，即使它需要有效检查多个max_features功能也是如此。</li>
<li>max_leaf_nodes：最大叶子节点数，整数，默认为None</li>
<li>min_impurity_decrease：如果分裂指标的减少量大于该值，则进行分裂。</li>
<li>min_impurity_split：决策树生长的最小纯净度。默认是0。自版本0.19起不推荐使用：不推荐使用min_impurity_split，而建议使用0.19中的min_impurity_decrease。min_impurity_split的默认值在0.23中已从1e-7更改为0，并将在0.25中删除。</li>
<li>bootstrap：<code>是否进行bootstrap操作</code>，bool。默认True。如果<code>bootstrap==True，将每次有放回地随机选取样本，只有在extra-trees中，bootstrap=False</code></li>
<li>oob_score：是否使用袋外样本来估计泛化精度。默认False。</li>
<li>n_jobs：并行计算数。默认是None。</li>
<li>random_state：<code>控制bootstrap的随机性以及选择样本的随机性</code>。</li>
<li>verbose：在拟合和预测时控制详细程度。默认是0。</li>
<li>warm_start：不常用</li>
<li>class_weight：每个类的权重，可以用字典的形式传入{class_label: weight}。如果选择了“balanced”，则输入的权重为n_samples / (n_classes * np.bincount(y))。</li>
<li>ccp_alpha：将选择成本复杂度最大且小于ccp_alpha的子树。默认情况下，不执行修剪。</li>
<li>max_samples：如果bootstrap为True，则从X抽取以训练每个基本分类器的样本数。如果为None（默认），则抽取X.shape [0]样本。如果为int，则抽取max_samples样本。如果为float，则抽取max_samples * X.shape [0]个样本。因此，max_samples应该在（0，1）中。是0.22版中的新功能。</li>
</ol>
<h4 id="4-decision-treehttpsscikit-learnorgstablemodulesgeneratedsklearntreedecisiontreeclassifierhtmlsklearntreedecisiontreeclassifier">4. <a href="https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier"target="_blank" rel="external nofollow noopener noreferrer">Decision Tree<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></h4>
<blockquote>
<table>
<thead>
<tr>
<th><a href="https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier"target="_blank" rel="external nofollow noopener noreferrer"><code>tree.DecisionTreeClassifier</code><i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>(*[, criterion, …])</th>
<th>A decision tree classifier.</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html#sklearn.tree.DecisionTreeRegressor"target="_blank" rel="external nofollow noopener noreferrer"><code>tree.DecisionTreeRegressor</code><i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>(*[, criterion, …])</td>
<td>A decision tree regressor.</td>
</tr>
<tr>
<td><a href="https://scikit-learn.org/stable/modules/generated/sklearn.tree.ExtraTreeClassifier.html#sklearn.tree.ExtraTreeClassifier"target="_blank" rel="external nofollow noopener noreferrer"><code>tree.ExtraTreeClassifier</code><i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>(*[, criterion, …])</td>
<td>An extremely randomized tree classifier.</td>
</tr>
<tr>
<td><a href="https://scikit-learn.org/stable/modules/generated/sklearn.tree.ExtraTreeRegressor.html#sklearn.tree.ExtraTreeRegressor"target="_blank" rel="external nofollow noopener noreferrer"><code>tree.ExtraTreeRegressor</code><i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>(*[, criterion, …])</td>
<td>An extremely randomized tree regressor.</td>
</tr>
</tbody>
</table>
</blockquote>
<h4 id="41-函数介绍">4.1. 函数介绍</h4>
<p><em>class</em> <code>sklearn.tree.``DecisionTreeClassifier</code>(***, <em>criterion=&lsquo;gini&rsquo;</em>, <em>splitter=&lsquo;best&rsquo;</em>, <em>max_depth=None</em>, <em>min_samples_split=2</em>, <em>min_samples_leaf=1</em>, <em>min_weight_fraction_leaf=0.0</em>, <em>max_features=None</em>, <em>random_state=None</em>, <em>max_leaf_nodes=None</em>, <em>min_impurity_decrease=0.0</em>, <em>min_impurity_split=None</em>, <em>class_weight=None</em>, <em>ccp_alpha=0.0</em>)[<a href="https://github.com/scikit-learn/scikit-learn/blob/95119c13a/sklearn/tree/_classes.py#L607"target="_blank" rel="external nofollow noopener noreferrer">source]<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<ul>
<li>criterion：<code>分裂节点所用的标准</code>，可选“gini”, “entropy”，默认“gini”。</li>
<li>splitter：用于在<code>每个节点上选择拆分的策略</code>。可选“best”, “random”，默认“best”。</li>
<li>max_depth：树的最大深度。如果为None，则将节点展开，直到所有叶子都是纯净的(只有一个类)，或者直到所有叶子都包含少于min_samples_split个样本。默认是None。</li>
<li>min_samples_split：<code>拆分内部节点所需的最少样本数</code>：如果为int，则将min_samples_split视为最小值。如果为float，则min_samples_split是一个分数，而ceil（min_samples_split * n_samples）是每个拆分的最小样本数。默认是2。</li>
<li>min_samples_leaf：在叶节点处需要的最小样本数。仅在任何深度的分割点在左分支和右分支中的每个分支上至少留下min_samples_leaf个训练样本时，才考虑。这可能具有平滑模型的效果，尤其是在回归中。如果为int，则将min_samples_leaf视为最小值。如果为float，则min_samples_leaf是分数，而ceil（min_samples_leaf * n_samples）是每个节点的最小样本数。默认是1。</li>
<li>min_weight_fraction_leaf：在所有叶节点处（所有输入样本）的权重总和中的最小加权分数。如果未提供sample_weight，则样本的权重相等。</li>
<li>max_features：<code>寻找最佳分割时要考虑的特征数量</code>：如果为int，则在每个拆分中考虑max_features个特征。如果为float，则max_features是一个分数，并在每次拆分时考虑int（max_features * n_features）个特征。如果为“auto”，则max_features = sqrt（n_features）。如果为“ sqrt”，则max_features = sqrt（n_features）。如果为“ log2”，则max_features = log2（n_features）。如果为None，则max_features = n_features。注意：在找到至少一个有效的节点样本分区之前，分割的搜索不会停止，即使它需要有效检查多个max_features功能也是如此。</li>
<li>random_state：随机种子，负责控制分裂特征的随机性，为整数。默认是None。</li>
<li>max_leaf_nodes：<code>最大叶子节点数</code>，整数，默认为None</li>
<li>min_impurity_decrease：如果<code>分裂指标的减少量大于该值</code>，则进行分裂。</li>
<li>min_impurity_split：决策树生长的最小纯净度。默认是0。自版本0.19起不推荐使用：不推荐使用min_impurity_split，而建议使用0.19中的min_impurity_decrease。min_impurity_split的默认值在0.23中已从1e-7更改为0，并将在0.25中删除。</li>
<li>class_weight：每个类的权重，可以用字典的形式传入{class_label: weight}。如果选择了“balanced”，则输入的权重为n_samples / (n_classes * np.bincount(y))。</li>
<li>presort：此参数已弃用，并将在v0.24中删除。</li>
<li>ccp_alpha：将选择成本复杂度最大且小于ccp_alpha的子树。默认情况下，不执行修剪。</li>
</ul>
<h4 id="43-示例代码">4.3. 示例代码</h4>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># -*- coding: utf-8 -*-</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 引入数据</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn <span style="color:#f92672">import</span> datasets
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> numpy <span style="color:#66d9ef">as</span> np
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>iris <span style="color:#f92672">=</span> datasets<span style="color:#f92672">.</span>load_iris()
</span></span><span style="display:flex;"><span>X <span style="color:#f92672">=</span> iris<span style="color:#f92672">.</span>data[:,[<span style="color:#ae81ff">2</span>,<span style="color:#ae81ff">3</span>]]
</span></span><span style="display:flex;"><span>y <span style="color:#f92672">=</span> iris<span style="color:#f92672">.</span>target
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;Class labels:&#34;</span>,np<span style="color:#f92672">.</span>unique(y))  <span style="color:#75715e">#打印分类类别的种类</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">## 画出决策边界图(只有在2个特征才能画出来)</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> matplotlib.pyplot <span style="color:#66d9ef">as</span> plt
</span></span><span style="display:flex;"><span><span style="color:#f92672">%</span>matplotlib inline
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> matplotlib.colors <span style="color:#f92672">import</span> ListedColormap
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">plot_decision_region</span>(X,y,classifier,resolution<span style="color:#f92672">=</span><span style="color:#ae81ff">0.02</span>):
</span></span><span style="display:flex;"><span>    markers <span style="color:#f92672">=</span> (<span style="color:#e6db74">&#39;s&#39;</span>,<span style="color:#e6db74">&#39;x&#39;</span>,<span style="color:#e6db74">&#39;o&#39;</span>,<span style="color:#e6db74">&#39;^&#39;</span>,<span style="color:#e6db74">&#39;v&#39;</span>)
</span></span><span style="display:flex;"><span>    colors <span style="color:#f92672">=</span> (<span style="color:#e6db74">&#39;red&#39;</span>,<span style="color:#e6db74">&#39;blue&#39;</span>,<span style="color:#e6db74">&#39;lightgreen&#39;</span>,<span style="color:#e6db74">&#39;gray&#39;</span>,<span style="color:#e6db74">&#39;cyan&#39;</span>)
</span></span><span style="display:flex;"><span>    cmap <span style="color:#f92672">=</span> ListedColormap(colors[:len(np<span style="color:#f92672">.</span>unique(y))])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># plot the decision surface</span>
</span></span><span style="display:flex;"><span>    x1_min,x1_max <span style="color:#f92672">=</span> X[:,<span style="color:#ae81ff">0</span>]<span style="color:#f92672">.</span>min()<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,X[:,<span style="color:#ae81ff">0</span>]<span style="color:#f92672">.</span>max()<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>    x2_min,x2_max <span style="color:#f92672">=</span> X[:,<span style="color:#ae81ff">1</span>]<span style="color:#f92672">.</span>min()<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,X[:,<span style="color:#ae81ff">1</span>]<span style="color:#f92672">.</span>max()<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>    xx1,xx2 <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>meshgrid(np<span style="color:#f92672">.</span>arange(x1_min,x1_max,resolution),
</span></span><span style="display:flex;"><span>                         np<span style="color:#f92672">.</span>arange(x2_min,x2_max,resolution))
</span></span><span style="display:flex;"><span>    Z <span style="color:#f92672">=</span> classifier<span style="color:#f92672">.</span>predict(np<span style="color:#f92672">.</span>array([xx1<span style="color:#f92672">.</span>ravel(),xx2<span style="color:#f92672">.</span>ravel()])<span style="color:#f92672">.</span>T)
</span></span><span style="display:flex;"><span>    Z <span style="color:#f92672">=</span> Z<span style="color:#f92672">.</span>reshape(xx1<span style="color:#f92672">.</span>shape)
</span></span><span style="display:flex;"><span>    plt<span style="color:#f92672">.</span>contourf(xx1,xx2,Z,alpha<span style="color:#f92672">=</span><span style="color:#ae81ff">0.3</span>,cmap<span style="color:#f92672">=</span>cmap)
</span></span><span style="display:flex;"><span>    plt<span style="color:#f92672">.</span>xlim(xx1<span style="color:#f92672">.</span>min(),xx1<span style="color:#f92672">.</span>max())
</span></span><span style="display:flex;"><span>    plt<span style="color:#f92672">.</span>ylim(xx2<span style="color:#f92672">.</span>min(),xx2<span style="color:#f92672">.</span>max())
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># plot class samples</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> idx,cl <span style="color:#f92672">in</span> enumerate(np<span style="color:#f92672">.</span>unique(y)):
</span></span><span style="display:flex;"><span>        plt<span style="color:#f92672">.</span>scatter(x<span style="color:#f92672">=</span>X[y<span style="color:#f92672">==</span>cl,<span style="color:#ae81ff">0</span>],
</span></span><span style="display:flex;"><span>                   y <span style="color:#f92672">=</span> X[y<span style="color:#f92672">==</span>cl,<span style="color:#ae81ff">1</span>],
</span></span><span style="display:flex;"><span>                   alpha<span style="color:#f92672">=</span><span style="color:#ae81ff">0.8</span>,
</span></span><span style="display:flex;"><span>                   c<span style="color:#f92672">=</span>colors[idx],
</span></span><span style="display:flex;"><span>                   marker <span style="color:#f92672">=</span> markers[idx],
</span></span><span style="display:flex;"><span>                   label<span style="color:#f92672">=</span>cl,
</span></span><span style="display:flex;"><span>                   edgecolors<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;black&#39;</span>)
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 切分训练数据和测试数据</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.model_selection <span style="color:#f92672">import</span> train_test_split
</span></span><span style="display:flex;"><span><span style="color:#75715e">## 30%测试数据，70%训练数据，stratify=y表示训练数据和测试数据具有相同的类别比例</span>
</span></span><span style="display:flex;"><span>X_train,X_test,y_train,y_test <span style="color:#f92672">=</span> train_test_split(X,y,test_size<span style="color:#f92672">=</span><span style="color:#ae81ff">0.3</span>,random_state<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>,stratify<span style="color:#f92672">=</span>y)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.preprocessing <span style="color:#f92672">import</span> StandardScaler
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>sc <span style="color:#f92672">=</span> StandardScaler()
</span></span><span style="display:flex;"><span><span style="color:#75715e">## 估算训练数据中的mu和sigma</span>
</span></span><span style="display:flex;"><span>sc<span style="color:#f92672">.</span>fit(X_train)
</span></span><span style="display:flex;"><span><span style="color:#75715e">## 使用训练数据中的mu和sigma对数据进行标准化</span>
</span></span><span style="display:flex;"><span>X_train_std <span style="color:#f92672">=</span> sc<span style="color:#f92672">.</span>transform(X_train)
</span></span><span style="display:flex;"><span>X_test_std <span style="color:#f92672">=</span> sc<span style="color:#f92672">.</span>transform(X_test)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">## 决策树分类器</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.tree <span style="color:#f92672">import</span> DecisionTreeClassifier
</span></span><span style="display:flex;"><span>tree <span style="color:#f92672">=</span> DecisionTreeClassifier(criterion<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;gini&#39;</span>,max_depth<span style="color:#f92672">=</span><span style="color:#ae81ff">4</span>,random_state<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>tree<span style="color:#f92672">.</span>fit(X_train_std,y_train)
</span></span><span style="display:flex;"><span>plot_decision_region(X_train_std,y_train,classifier<span style="color:#f92672">=</span>tree,resolution<span style="color:#f92672">=</span><span style="color:#ae81ff">0.02</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>xlabel(<span style="color:#e6db74">&#39;petal length [standardized]&#39;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>ylabel(<span style="color:#e6db74">&#39;petal width [standardized]&#39;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>legend(loc<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;upper left&#39;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>show()
</span></span><span style="display:flex;"><span>tree_fit<span style="color:#f92672">=</span>tree<span style="color:#f92672">.</span>fit(X_train_std,y_train)
</span></span><span style="display:flex;"><span>tree_fit<span style="color:#f92672">.</span>classes_  <span style="color:#75715e">#array([0, 1, 2])</span>
</span></span><span style="display:flex;"><span>tree_fit<span style="color:#f92672">.</span>feature_importances_  <span style="color:#75715e">#array([0.42708333, 0.57291667])</span>
</span></span><span style="display:flex;"><span>tree_fit<span style="color:#f92672">.</span>max_features_  <span style="color:#75715e">#2</span>
</span></span><span style="display:flex;"><span>tree_fit<span style="color:#f92672">.</span>n_classes_  <span style="color:#75715e">#3</span>
</span></span><span style="display:flex;"><span>tree_fit<span style="color:#f92672">.</span>n_features_  <span style="color:#75715e">#2</span>
</span></span><span style="display:flex;"><span>tree_fit<span style="color:#f92672">.</span>n_outputs_  <span style="color:#75715e">#1</span>
</span></span><span style="display:flex;"><span>tree_fit<span style="color:#f92672">.</span>tree_
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># conda install -c conda-forge pydotplus</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">## 决策树可视化</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> pydotplus <span style="color:#f92672">import</span> graph_from_dot_data
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.tree <span style="color:#f92672">import</span> export_graphviz
</span></span><span style="display:flex;"><span>dot_data <span style="color:#f92672">=</span> export_graphviz(tree,filled<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>,class_names<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#39;Setosa&#39;</span>,<span style="color:#e6db74">&#39;Versicolor&#39;</span>,<span style="color:#e6db74">&#39;Virginica&#39;</span>],
</span></span><span style="display:flex;"><span>                          feature_names<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#39;petal_length&#39;</span>,<span style="color:#e6db74">&#39;petal_width&#39;</span>],out_file<span style="color:#f92672">=</span><span style="color:#66d9ef">None</span>)
</span></span><span style="display:flex;"><span>graph <span style="color:#f92672">=</span> graph_from_dot_data(dot_data)
</span></span><span style="display:flex;"><span>graph<span style="color:#f92672">.</span>write_png(<span style="color:#e6db74">&#39;D:</span><span style="color:#ae81ff">\\</span><span style="color:#e6db74">Users</span><span style="color:#ae81ff">\\</span><span style="color:#e6db74">Desktop</span><span style="color:#ae81ff">\\</span><span style="color:#e6db74">一部二部文件</span><span style="color:#ae81ff">\\</span><span style="color:#e6db74">tree.png&#39;</span>)
</span></span></code></pre></div><h3 id="5-gradientboostingclassifierhttpsscikit-learnorgstablemodulesgeneratedsklearnensemblegradientboostingclassifierhtmlhighlightgradientboostingclassifiersklearnensemblegradientboostingclassifier">5. <a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html?highlight=gradientboostingclassifier#sklearn.ensemble.GradientBoostingClassifier"target="_blank" rel="external nofollow noopener noreferrer">GradientBoostingClassifier<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></h3>
<blockquote>
<ul>
<li><a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html?highlight=gradientboostingclassifier#sklearn.ensemble.GradientBoostingClassifier"target="_blank" rel="external nofollow noopener noreferrer">sklearn.ensemble.GradientBoostingClassifier<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></li>
<li><a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingClassifier.html?highlight=gradientboostingclassifier#sklearn.ensemble.HistGradientBoostingClassifier"target="_blank" rel="external nofollow noopener noreferrer">sklearn.ensemble.HistGradientBoostingClassifier<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></li>
<li><code>sklearn.ensemble</code>.GradientBoostingClassifier</li>
<li><code>sklearn.ensemble</code>.AdaBoostClassifier</li>
<li><a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingClassifier.html?highlight=gradientboostingclassifier"target="_blank" rel="external nofollow noopener noreferrer"><code>sklearn.ensemble</code>.HistGradientBoostingClassifier<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></li>
</ul>
</blockquote>
<h4 id="51-参数介绍">5.1. 参数介绍</h4>
<p><em>class</em> <code>sklearn.ensemble.``GradientBoostingClassifier</code>(***, <em>loss=&lsquo;deviance&rsquo;</em>, <em>learning_rate=0.1</em>, <em>n_estimators=100</em>, <em>subsample=1.0</em>, <em>criterion=&lsquo;friedman_mse&rsquo;</em>, <em>min_samples_split=2</em>, <em>min_samples_leaf=1</em>, <em>min_weight_fraction_leaf=0.0</em>, <em>max_depth=3</em>, <em>min_impurity_decrease=0.0</em>, <em>min_impurity_split=None</em>, <em>init=None</em>, <em>random_state=None</em>, <em>max_features=None</em>, <em>verbose=0</em>, <em>max_leaf_nodes=None</em>, <em>warm_start=False</em>, <em>validation_fraction=0.1</em>, <em>n_iter_no_change=None</em>, <em>tol=0.0001</em>, <em>ccp_alpha=0.0</em>)</p>
<ul>
<li><strong>n_estimators</strong>: 也就是弱学习器的最大迭代次数，或者说最大的弱学习器的个数。一般来说n_estimators太小，容易欠拟合，n_estimators太大，又容易过拟合，一般选择一个适中的数值。默认是100。在<code>实际调参的过程中，我们常常将n_estimators和下面介绍的参数learning_rate一起考虑</code>。</li>
<li><strong>learning_rate</strong>: 即每个弱学习器的权重缩减系数νν，也称作步长，在原理篇的正则化章节我们也讲到了，加上了正则化项，我们的强学习器的迭代公式为fk(x)=fk−1(x)+νhk(x)fk(x)=fk−1(x)+νhk(x)。νν的取值范围为0&lt;ν≤10&lt;ν≤1。对于同样的训练集拟合效果，较小的νν意味着我们需要更多的弱学习器的迭代次数。通常我们用步长和迭代最大次数一起来决定算法的拟合效果。所以这两个参数n_estimators和learning_rate要一起调参。一般来说，可以从一个小一点的νν开始调参，默认是1。</li>
<li><strong>subsample</strong>: 即我们在原理篇的正则化章节讲到的子采样，取值为(0,1]。注意这里的子采样和随机森林不一样，<code>随机森林使用的是放回抽样，而这里是不放回抽样</code>。如果取值为1，则全部样本都使用，等于没有使用子采样。如果取值小于1，则只有一部分样本会去做GBDT的决策树拟合。选择小于1的比例可以减少方差，即防止过拟合，但是会增加样本拟合的偏差，因此取值不能太低。推荐在<code>[0.5, 0.8]之间</code>，默认是1.0，即不使用子采样。</li>
<li><strong>init</strong>: 即我们的初始化的时候的弱学习器，拟合对应原理篇里面的f0(x)f0(x)，如果不输入，则用训练集样本来做样本集的初始化分类回归预测。否则用init参数提供的学习器做初始化分类回归预测。一般用在我们对数据有先验知识，或者之前做过一些拟合的时候，如果没有的话就不用管这个参数了。</li>
<li><strong>loss:</strong> 即我们GBDT算法中的损失函数。分类模型和回归模型的损失函数是不一样的。</li>
</ul>
<p>​               对于分类模型，有对数似然损失函数&quot;deviance&quot;和指数损失函数&quot;exponential&quot;两者输入选择。默认是对数似然损失函数&quot;deviance&quot;。在原理篇中对这些分类损失函数有详细的介绍。一般来说，推荐使用默认的&quot;deviance&quot;。它对二元分离和多元分类各自都有比较好的优化。而指数损失函数等于把我们带到了Adaboost算法。</p>
<p>​            对于回归模型，有均方差&quot;ls&quot;, 绝对损失&quot;lad&quot;, Huber损失&quot;huber&quot;和分位数损失“quantile”。默认是均方差&quot;ls&quot;。一般来说，如果数据的噪音点不多，用默认的均方差&quot;ls&quot;比较好。如果是噪音点较多，则推荐用抗噪音的损失函数&quot;huber&quot;。而如果我们需要对训练集进行分段预测的时候，则采用“quantile”。</p>
<ul>
<li>**alpha：**这个参数只有GradientBoostingRegressor有，当我们使用Huber损失&quot;huber&quot;和分位数损失“quantile”时，需要指定分位数的值。默认是0.9，如果噪音点较多，可以适当降低这个分位数的值。</li>
</ul>
<h3 id="6-gaussiannbhttpsscikit-learnorgstablemodulesgeneratedsklearnnaive_bayesgaussiannbhtmlhighlightgaussiannbsklearnnaive_bayesgaussiannb">6. <a href="https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html?highlight=gaussiannb#sklearn.naive_bayes.GaussianNB"target="_blank" rel="external nofollow noopener noreferrer">GaussianNB<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></h3>
<h4 id="61-函数介绍">6.1. 函数介绍</h4>
<p><em>class</em> <code>sklearn.naive_bayes.``GaussianNB</code>(***, <em>priors=None</em>, <em>var_smoothing=1e-09</em>)[<a href="https://github.com/scikit-learn/scikit-learn/blob/95119c13a/sklearn/naive_bayes.py#L118"target="_blank" rel="external nofollow noopener noreferrer">source]<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<blockquote>
<p>perform online updates to model parameters via <a href="https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html?highlight=gaussiannb#sklearn.naive_bayes.GaussianNB.partial_fit"target="_blank" rel="external nofollow noopener noreferrer"><code>partial_fit</code><i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>. For <a href="http://i.stanford.edu/pub/cstr/reports/cs/tr/79/773/CS-TR-79-773.pdf"target="_blank" rel="external nofollow noopener noreferrer">details<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a> on algorithm used to update feature means and variance online.</p>
</blockquote>
<ul>
<li>
<p><code>priors</code>:先验概率大小，如果没有给定，模型则根据样本数据自己计算（利用极大似然法）。</p>
</li>
<li>
<p><code>class_prior_</code>:每个样本的概率</p>
</li>
<li>
<p><code>class_count</code>:每个类别的样本数量</p>
</li>
<li>
<p><code>theta_</code>:每个类别中每个特征的均值</p>
</li>
<li>
<p><code>sigma_</code>:每个类别中每个特征的方差</p>
</li>
</ul>
<h4 id="62-多项式分布贝叶斯">6.2. <strong>多项式分布贝叶斯</strong></h4>
<h5 id="621-函数介绍">6.2.1. 函数介绍</h5>
<p><strong>class</strong> sklearn.naive_bayes.<strong>MultinomialNB</strong>(<strong>alpha</strong>=1.0, <strong>fit_prior</strong>=<strong>True</strong>, <strong>class_prior</strong>=<strong>None</strong>)</p>
<ul>
<li>
<p><code>alpha</code>:先验平滑因子，默认等于1，当等于1时表示拉普拉斯平滑。</p>
</li>
<li>
<p><code>fit_prior</code>:是否去学习类的先验概率，默认是True</p>
</li>
<li>
<p><code>class_prior</code>:各个类别的先验概率，如果没有指定，则模型会根据数据自动学习， 每个类别的先验概率相同，等于类标记总个数N分之一。</p>
</li>
<li>
<p><code>class_log_prior_</code>:每个类别平滑后的先验概率</p>
</li>
<li>
<p><code>intercept_</code>:是朴素贝叶斯对应的线性模型，其值和class_log_prior_相同<code>feature_log_prob_</code>:给定特征类别的对数概率(条件概率)。 特征的条件概率=（指定类下指定特征出现的次数+alpha）/（指定类下所有特征出现次数之和+类的可能取值个数*alpha）<code>coef_</code>: 是朴素贝叶斯对应的线性模型，其值和feature_log_prob相同</p>
</li>
<li>
<p><code>class_count_</code>: 训练样本中各类别对应的样本数</p>
</li>
<li>
<p><code>feature_count_</code>: 每个类别中各个特征出现的次数</p>
</li>
</ul>
<h4 id="63-bernoullinbhttpsscikit-learnorgstablemodulesgeneratedsklearnnaive_bayesbernoullinbhtmlhighlightbernoullinbsklearnnaive_bayesbernoullinb">6.3. <a href="https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html?highlight=bernoullinb#sklearn.naive_bayes.BernoulliNB"target="_blank" rel="external nofollow noopener noreferrer">BernoulliNB<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></h4>
<h5 id="631-函数介绍">6.3.1. 函数介绍</h5>
<blockquote>
<p>this classifier is suitable for discrete data. The difference is that while MultinomialNB works with occurrence counts, <code>BernoulliNB is designed for binary/boolean features</code>.</p>
</blockquote>
<p>class sklearn.naive_bayes.BernoulliNB(alpha=1.0, binarize=0.0, fit_prior=True, class_prior=None)</p>
<ul>
<li>
<p><code>alpha</code>:平滑因子，与多项式中的alpha一致。</p>
</li>
<li>
<p><code>binarize</code>:样本特征二值化的阈值，默认是0。如果不输入，则模型会认为所有特征都已经是二值化形式了；如果输入具体的值，则模型会把大于该值的部分归为一类，小于的归为另一类。</p>
</li>
<li>
<p><code>fit_prior</code>:是否去学习类的先验概率，默认是True</p>
</li>
<li>
<p><code>class_prior</code>:各个类别的先验概率，如果没有指定，则模型会根据数据自动学习， 每个类别的先验概率相同，等于类标记总个数N分之一。</p>
</li>
<li>
<p><code>class_log_prior_</code>:每个类别平滑后的先验对数概率。</p>
</li>
<li>
<p><code>feature_log_prob_</code>:给定特征类别的经验对数概率。</p>
</li>
<li>
<p><code>class_count_</code>:拟合过程中每个样本的数量。</p>
</li>
<li>
<p><code>feature_count_</code>:拟合过程中每个特征的数量。</p>
</li>
</ul>
<h4 id="64-multinomialnbhttpsscikit-learnorgstablemodulesgeneratedsklearnnaive_bayesmultinomialnbhtmlhighlightmultinomialnbsklearnnaive_bayesmultinomialnb">6.4. <a href="https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html?highlight=multinomialnb#sklearn.naive_bayes.MultinomialNB"target="_blank" rel="external nofollow noopener noreferrer">MultinomialNB<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></h4>
<h5 id="641-函数介绍httpsscikit-learnorgstablemodulesgeneratedsklearnnaive_bayesmultinomialnbhtmlhighlightmultinomialnbsklearnnaive_bayesmultinomialnb">6.4.1. <a href="https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html?highlight=multinomialnb#sklearn.naive_bayes.MultinomialNB"target="_blank" rel="external nofollow noopener noreferrer">函数介绍<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></h5>
<p><em>class</em> <code>sklearn.naive_bayes.``MultinomialNB</code>(***, <em>alpha=1.0</em>, <em>fit_prior=True</em>, <em>class_prior=None</em>)[<a href="https://github.com/scikit-learn/scikit-learn/blob/95119c13a/sklearn/naive_bayes.py#L669"target="_blank" rel="external nofollow noopener noreferrer">source]<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a><a href="https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html?highlight=multinomialnb#sklearn.naive_bayes.MultinomialNB"target="_blank" rel="external nofollow noopener noreferrer">
<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<blockquote>
<p>The multinomial Naive Bayes classifier is suitable for <code>classification with discrete features (e.g., word counts for text classification)</code>. The <code>multinomial distribution normally requires integer feature counts</code>. However, in practice, fractional counts such as tf-idf may also work.</p>
</blockquote>
<h4 id="65-adaboostclassifierhttpsscikit-learnorgstablemodulesgeneratedsklearnensembleadaboostclassifierhtmlhighlightadaboostclassifiersklearnensembleadaboostclassifier">6.5. <a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html?highlight=adaboostclassifier#sklearn.ensemble.AdaBoostClassifier"target="_blank" rel="external nofollow noopener noreferrer">AdaBoostClassifier<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></h4>
<h5 id="651-函数介绍">6.5.1. 函数介绍</h5>
<p><em>class</em> <code>sklearn.ensemble.``AdaBoostClassifier</code>(<em>base_estimator=None</em>, ***, <em>n_estimators=50</em>, <em>learning_rate=1.0</em>, <em>algorithm=&lsquo;SAMME.R&rsquo;</em>, <em>random_state=None</em>)[<a href="https://github.com/scikit-learn/scikit-learn/blob/95119c13a/sklearn/ensemble/_weight_boosting.py#L285"target="_blank" rel="external nofollow noopener noreferrer">source]<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<blockquote>
<p>a meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases.</p>
</blockquote>
<ul>
<li><strong>base_estimator：</strong> 可选参数，默认为DecisionTreeClassifier。理论上可以选择任何一个分类或者回归学习器，不过需要支持样本权重。我们常用的一般是CART决策树或者神经网络MLP。默认是决策树，即AdaBoostClassifier默认使用CART分类树DecisionTreeClassifier，而AdaBoostRegressor默认使用CART回归树DecisionTreeRegressor。另外有一个要注意的点是，如果我们选择的AdaBoostClassifier算法是SAMME.R，则我们的弱分类学习器还需要支持概率预测，也就是在scikit-learn中弱分类学习器对应的预测方法除了predict还需要有predict_proba。</li>
<li><strong>algorithm：</strong> 可选参数，默认为SAMME.R。scikit-learn实现了两种Adaboost分类算法，SAMME和SAMME.R。两者的主要区别是弱学习器权重的度量，SAMME使用对样本集分类效果作为弱学习器权重，而SAMME.R使用了对样本集分类的预测概率大小来作为弱学习器权重。由于SAMME.R使用了概率度量的连续值，迭代一般比SAMME快，因此AdaBoostClassifier的默认算法algorithm的值也是SAMME.R。我们一般使用默认的SAMME.R就够了，但是要注意的是使用了SAMME.R， 则弱分类学习器参数base_estimator必须限制使用支持概率预测的分类器。SAMME算法则没有这个限制。</li>
<li><strong>n_estimators：</strong> 整数型，可选参数，默认为50。弱学习器的最大迭代次数，或者说最大的弱学习器的个数。一般来说n_estimators太小，容易欠拟合，n_estimators太大，又容易过拟合，一般选择一个适中的数值。默认是50。在实际调参的过程中，我们常常将n_estimators和下面介绍的参数learning_rate一起考虑。</li>
<li><strong>learning_rate：</strong> 浮点型，可选参数，默认为1.0。每个弱学习器的权重缩减系数，取值范围为0到1，对于同样的训练集拟合效果，较小的v意味着我们需要更多的弱学习器的迭代次数。通常我们用步长和迭代最大次数一起来决定算法的拟合效果。所以这两个参数n_estimators和learning_rate要一起调参。一般来说，可以从一个小一点的v开始调参，默认是1。</li>
<li><strong>random_state：</strong> 整数型，可选参数，默认为None。如果RandomState的实例，random_state是随机数生成器; 如果None，则随机数生成器是由np.random使用的RandomState实例。</li>
</ul>
<h4 id="66-函数使用">6.6. 函数使用</h4>
<ul>
<li><code>fit(X,Y)</code>:在数据集(X,Y)上拟合模型。</li>
<li><code>partial_fit</code>(<em>X</em>, <em>y</em>, <em>classes=None</em>, <em>sample_weight=None</em>)[<a href="https://github.com/scikit-learn/scikit-learn/blob/95119c13a/sklearn/naive_bayes.py#L289"target="_blank" rel="external nofollow noopener noreferrer">source]<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a><a href="https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html?highlight=gaussiannb#sklearn.naive_bayes.GaussianNB.partial_fit"target="_blank" rel="external nofollow noopener noreferrer">
<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>： 当数据很大的时候</li>
<li><code>get_params()</code>:获取模型参数。</li>
<li><code>predict(X)</code>:对数据集X进行预测。</li>
<li><code>predict_log_proba(X)</code>:对数据集X预测，得到每个类别的概率对数值。<code>predict_proba(X)</code>:对数据集X预测，得到每个类别的概率。</li>
<li><code>score(X,Y)</code>:得到模型在数据集(X,Y)的得分情况。</li>
</ul>
</div>
<div class="post-footer" id="post-footer">
  <div class="post-info">
    <div class="post-info-line">
      <div class="post-info-mod">
        <span title=2023-12-31&#32;13:47:03>更新于 2023-12-31&nbsp;</span>
      </div><div class="post-info-license">
          <span><a rel="license external nofollow noopener noreferrer" href="https://creativecommons.org/licenses/by-nc/4.0/" target="_blank">CC BY-NC 4.0</a></span>
        </div></div>
    <div class="post-info-line">
      <div class="post-info-md"><span><a href="/sklearn-record/index.md" title="阅读原始文档" class="link-to-markdown">阅读原始文档</a></span><span><a href="https://liudongdong1.github.io/edit/master/content/posts%5c%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%5cSklearn%5cSkLearn%20Record.md" title="编辑此页"target="_blank" rel="external nofollow noopener noreferrer" class="link-to-edit">编辑此页</a></span></div>
      <div class="post-info-share">
        <span><a href="javascript:void(0);" title="分享到 Twitter" data-sharer="twitter" data-url="https://liudongdong1.github.io/sklearn-record/" data-title="SkLearn Record" data-hashtags="SkLearn,math"><i class="fa-brands fa-twitter fa-fw" aria-hidden="true"></i></a>
  <a href="javascript:void(0);" title="分享到 Facebook" data-sharer="facebook" data-url="https://liudongdong1.github.io/sklearn-record/" data-hashtag="SkLearn"><i class="fa-brands fa-facebook-square fa-fw" aria-hidden="true"></i></a>
  <a href="javascript:void(0);" title="分享到 微博" data-sharer="weibo" data-url="https://liudongdong1.github.io/sklearn-record/" data-title="SkLearn Record" data-image="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20210414084511.png"><i class="fa-brands fa-weibo fa-fw" aria-hidden="true"></i></a>
  </span>
      </div>
    </div>
  </div>

  <div class="post-info-more">
    <section class="post-tags"><i class="fa-solid fa-tags fa-fw" aria-hidden="true"></i>&nbsp;<a href="/tags/sklearn/">sklearn</a>,&nbsp;<a href="/tags/math/">math</a></section>
    <section>
      <span><a href="javascript:void(0);" onclick="window.history.back();">返回</a></span>&nbsp;|&nbsp;<span><a href="/">主页</a></span>
    </section>
  </div>

  <div class="post-nav"><a href="/sklearnvisualization/" class="prev" rel="prev" title="SkLearnVisualization"><i class="fa-solid fa-angle-left fa-fw" aria-hidden="true"></i>SkLearnVisualization</a>
      <a href="/sklearn-evaluation/" class="next" rel="next" title="SkLearn Evaluation">SkLearn Evaluation<i class="fa-solid fa-angle-right fa-fw" aria-hidden="true"></i></a></div>
</div>
</article></main><footer class="footer">
    <div class="footer-container"><div class="footer-line powered">由 <a href="https://gohugo.io/" target="_blank" rel="external nofollow noopener noreferrer" title="Hugo 0.118.2">Hugo</a> 强力驱动 | 主题 - <a href="https://github.com/hugo-fixit/FixIt" target="_blank" rel="external" title="FixIt v0.2.17-RC"><img class="fixit-icon" src="/fixit.min.svg" alt="FixIt logo" />&nbsp;FixIt</a>
        </div><div class="footer-line copyright" itemscope itemtype="http://schema.org/CreativeWork"><i class="fa-regular fa-copyright fa-fw" aria-hidden="true"></i>
            <span itemprop="copyrightYear">2020 - 2024</span><span class="author" itemprop="copyrightHolder">
              <a href="https://liudongdong1.github.io/"target="_blank" rel="external nofollow noopener noreferrer">LiuDongdong</a></span><span class="license footer-divider"><a rel="license external nofollow noopener noreferrer" href="https://creativecommons.org/licenses/by-nc/4.0/" target="_blank">CC BY-NC 4.0</a></span></div><div class="footer-line statistics"><span class="site-time" title='网站运行中 ...'><i class="fa-solid fa-heartbeat fa-fw animate-icon" aria-hidden="true"></i>&nbsp;<span class="run-times">网站运行中 ...</span></span></div><div class="footer-line ibruce">
          <span id="busuanzi_container_site_uv" title='总访客数'><i class="fa-regular fa-user fa-fw" aria-hidden="true"></i>&nbsp;<span id="busuanzi_value_site_uv"><i class="fa-solid fa-spinner fa-spin fa-fw" aria-hidden="true"></i></span></span><span id="busuanzi_container_site_pv" class="footer-divider" title='总访问量'><i class="fa-regular fa-eye fa-fw" aria-hidden="true"></i>&nbsp;<span id="busuanzi_value_site_pv"><i class="fa-solid fa-spinner fa-spin fa-fw" aria-hidden="true"></i></span></span>
        </div></div>
  </footer></div><div class="widgets"><div class="fixed-buttons animate__faster d-none"><div class="fixed-button back-to-top" role="button" aria-label="回到顶部"><i class="fa-solid fa-arrow-up fa-fw" aria-hidden="true"></i><span class="variant-numeric">0%</span>
        </div></div><a href="https://liudongdong1.github.io/" title="在 GitHub 上查看源代码"target="_blank" rel="external nofollow" class="github-corner right d-none-mobile"><svg viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><div id="mask"></div><div class="reading-progress-bar" style="left: 0;top: 0;--bg-progress: #0076ff;--bg-progress-dark: #fff;"></div><noscript>
    <div class="noscript-warning">FixIt 主题在启用 JavaScript 的情况下效果最佳。</div>
  </noscript>
</div><link rel="stylesheet" href="/lib/katex/katex.min.css"><link rel="stylesheet" href="/lib/cookieconsent/cookieconsent.min.css"><script src="/lib/autocomplete/autocomplete.min.js" defer></script><script src="/lib/algoliasearch/algoliasearch-lite.umd.min.js" defer></script><script src="/lib/lazysizes/lazysizes.min.js" async defer></script><script src="/lib/sharer/sharer.min.js" async defer></script><script src="/lib/typeit/index.umd.js" defer></script><script src="/lib/katex/katex.min.js" defer></script><script src="/lib/katex/auto-render.min.js" defer></script><script src="/lib/katex/copy-tex.min.js" defer></script><script src="/lib/katex/mhchem.min.js" defer></script><script src="/lib/cookieconsent/cookieconsent.min.js" defer></script><script src="/lib/pangu/pangu.min.js" defer></script><script src="/lib/cell-watermark/watermark.min.js" defer></script><script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async defer></script><script>window.config={"code":{"copyTitle":"复制到剪贴板","editLockTitle":"锁定可编辑代码块","editUnLockTitle":"解锁可编辑代码块","editable":true,"maxShownLines":10},"comment":{"enable":false},"cookieconsent":{"content":{"dismiss":"同意","link":"了解更多","message":"本网站使用 Cookies 来改善您的浏览体验。"},"enable":true,"palette":{"button":{"background":"#f0f0f0"},"popup":{"background":"#1aa3ff"}},"theme":"edgeless"},"data":{"typeit-header-subtitle-desktop":"\u003cspan style='font-family: MMT,\"沐目体\";'\u003e吾日三省吾身\u003c/span\u003e","typeit-header-subtitle-mobile":"\u003cspan style='font-family: MMT,\"沐目体\";'\u003e吾日三省吾身\u003c/span\u003e"},"enablePWA":true,"enablePangu":true,"math":{"delimiters":[{"display":true,"left":"$$","right":"$$"},{"display":true,"left":"\\[","right":"\\]"},{"display":true,"left":"\\begin{equation}","right":"\\end{equation}"},{"display":true,"left":"\\begin{equation*}","right":"\\end{equation*}"},{"display":true,"left":"\\begin{align}","right":"\\end{align}"},{"display":true,"left":"\\begin{align*}","right":"\\end{align*}"},{"display":true,"left":"\\begin{alignat}","right":"\\end{alignat}"},{"display":true,"left":"\\begin{alignat*}","right":"\\end{alignat*}"},{"display":true,"left":"\\begin{gather}","right":"\\end{gather}"},{"display":true,"left":"\\begin{CD}","right":"\\end{CD}"},{"display":false,"left":"$","right":"$"},{"display":false,"left":"\\(","right":"\\)"}],"strict":false},"search":{"algoliaAppID":"2R1K9SKLQZ","algoliaIndex":"index.zh-cn","algoliaSearchKey":"4a226aa1c5c98d6859e4d1386adb2bc7","highlightTag":"em","maxResultLength":10,"noResultsFound":"没有找到结果","snippetLength":50,"type":"algolia"},"siteTime":"2020-12-18T16:15:22+08:00","typeit":{"cursorChar":"|","cursorSpeed":1000,"data":{"typeit-header-subtitle-desktop":["typeit-header-subtitle-desktop"],"typeit-header-subtitle-mobile":["typeit-header-subtitle-mobile"]},"duration":-1,"speed":100},"watermark":{"appendto":".wrapper\u003emain","colspacing":30,"content":"\u003cimg class=\"fixit-icon\" src=\"/fixit.min.svg\" alt=\"FixIt logo\" /\u003e FixIt 主题","enable":true,"fontfamily":"inherit","fontsize":0.85,"height":21,"opacity":0.0125,"rotate":15,"rowspacing":60,"width":150}};</script><script src="/js/theme.min.js" defer></script><script src="/js/custom.min.js" defer></script></body>
</html>
