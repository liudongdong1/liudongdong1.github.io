<!DOCTYPE html>
<html itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">
  <head>
    
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
    <meta name="robots" content="noodp" />
    <title>Sign_Language_Introduce - DAY By DAY</title><meta name="author" content="LiuDongdong">
<meta name="author-link" content="https://liudongdong1.github.io/">
<meta name="description" content="手语是聋哑人士的主要沟通工具，它是利用手部和身体的动作来传达意义。虽然手语帮助它的使用者之间互相沟通，但聋哑人士与一般人的沟通却十分困难，这" /><meta name="keywords" content='CV, Sensing' /><meta itemprop="name" content="Sign_Language_Introduce">
<meta itemprop="description" content="手语是聋哑人士的主要沟通工具，它是利用手部和身体的动作来传达意义。虽然手语帮助它的使用者之间互相沟通，但聋哑人士与一般人的沟通却十分困难，这"><meta itemprop="datePublished" content="2020-07-13T09:47:19+00:00" />
<meta itemprop="dateModified" content="2023-09-28T23:36:00+08:00" />
<meta itemprop="wordCount" content="6729"><meta itemprop="image" content="/logo.png"/>
<meta itemprop="keywords" content="CV,Sensing," /><meta property="og:title" content="Sign_Language_Introduce" />
<meta property="og:description" content="手语是聋哑人士的主要沟通工具，它是利用手部和身体的动作来传达意义。虽然手语帮助它的使用者之间互相沟通，但聋哑人士与一般人的沟通却十分困难，这" />
<meta property="og:type" content="article" />
<meta property="og:url" content="liudongdong1.github.io/sign-language-introduce/" /><meta property="og:image" content="/logo.png"/><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2020-07-13T09:47:19+00:00" />
<meta property="article:modified_time" content="2023-09-28T23:36:00+08:00" />
<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="/logo.png"/>

<meta name="twitter:title" content="Sign_Language_Introduce"/>
<meta name="twitter:description" content="手语是聋哑人士的主要沟通工具，它是利用手部和身体的动作来传达意义。虽然手语帮助它的使用者之间互相沟通，但聋哑人士与一般人的沟通却十分困难，这"/>
<meta name="application-name" content="DAY By DAY">
<meta name="apple-mobile-web-app-title" content="DAY By DAY"><meta name="theme-color" data-light="#f8f8f8" data-dark="#252627" content="#f8f8f8"><meta name="msapplication-TileColor" content="#da532c"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" />
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"><link rel="manifest" href="/site.webmanifest"><link rel="canonical" href="liudongdong1.github.io/sign-language-introduce/" /><link rel="prev" href="liudongdong1.github.io/similaritymetric/" /><link rel="next" href="liudongdong1.github.io/siamesenetwork/" /><link rel="stylesheet" href="/liudongdong1.github.io/css/style.min.css"><link rel="stylesheet" href="/liudongdong1.github.io/lib/fontawesome-free/all.min.css"><link rel="stylesheet" href="/liudongdong1.github.io/lib/animate/animate.min.css"><script type="application/ld+json">
  {
    "@context": "http://schema.org",
    "@type": "BlogPosting",
    "headline": "Sign_Language_Introduce",
    "inLanguage": "zh-CN",
    "mainEntityOfPage": {
      "@type": "WebPage",
      "@id": "liudongdong1.github.io\/sign-language-introduce\/"
    },"genre": "posts","keywords": "CV, Sensing","wordcount":  6729 ,
    "url": "liudongdong1.github.io\/sign-language-introduce\/","datePublished": "2020-07-13T09:47:19+00:00","dateModified": "2023-09-28T23:36:00+08:00","license": "This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.","publisher": {
      "@type": "Organization",
      "name": "LiuDongdong","logo": "\/images\/person.png"},"author": {
        "@type": "Person",
        "name": "liudongdong1"
      },"description": ""
  }
  </script></head>
  <body data-header-desktop="auto" data-header-mobile="auto"><script>(window.localStorage?.getItem('theme') ? localStorage.getItem('theme') === 'dark' : ('auto' === 'auto' ? window.matchMedia('(prefers-color-scheme: dark)').matches : 'auto' === 'dark')) && document.body.setAttribute('data-theme', 'dark');</script><div class="wrapper"><header class="desktop animate__faster" id="header-desktop">
  <div class="header-wrapper" data-github-corner="right">
    <div class="header-title">
      <a href="liudongdong1.github.io/" title="DAY By DAY"><img
    class="lazyload logo"
    src="/liudongdong1.github.io/svg/loading.min.svg"
    data-src="/fixit.min.svg"
    data-srcset="/fixit.min.svg, /fixit.min.svg 1.5x, /fixit.min.svg 2x"
    data-sizes="auto"
    alt="DAY By DAY"
    title="DAY By DAY"/><span class="header-title-text"></span></a><span id="typeit-header-subtitle-desktop" class="typeit header-subtitle"></span></div>
    <nav>
      <ul class="menu"><li class="menu-item">
              <a
                class="menu-link"
                href="/liudongdong1.github.io/posts/"
                
                
              ><i class="fa-solid fa-archive fa-fw fa-sm" aria-hidden="true"></i> 所有文章</a></li><li class="menu-item">
              <a
                class="menu-link"
                href="/liudongdong1.github.io/categories/"
                
                
              ><i class="fa-solid fa-th fa-fw fa-sm" aria-hidden="true"></i> 分类</a></li><li class="menu-item">
              <a
                class="menu-link"
                href="/liudongdong1.github.io/tags/"
                
                
              ><i class="fa-solid fa-tags fa-fw fa-sm" aria-hidden="true"></i> 标签</a></li><li class="menu-item">
              <a
                class="menu-link"
                href="/liudongdong1.github.io/friends/"
                title="友情链接"
                
              ><i class="fa-solid fa-users fa-fw fa-sm" aria-hidden="true"></i> 友链</a></li><li class="menu-item">
              <a
                class="menu-link"
                href="/liudongdong1.github.io/about/"
                
                
              ><i class="fa-solid fa-info-circle fa-fw fa-sm" aria-hidden="true"></i> 关于</a></li><li class="menu-item delimiter"></li><li class="menu-item language">
            <span role="button" aria-label="选择语言" title="选择语言">简体中文<i class="dropdown-icon fa-solid fa-chevron-down" aria-hidden="true"></i>
            </span>
            <ul class="sub-menu"><li class="menu-item">没有更多翻译</li></ul>
          </li><li class="menu-item search" id="search-desktop">
            <input type="text" placeholder="搜索文章标题或内容 ..." id="search-input-desktop">
            <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-desktop" title="搜索">
              <i class="fa-solid fa-search fa-fw" aria-hidden="true"></i>
            </a>
            <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-desktop" title="清空">
              <i class="fa-solid fa-times-circle fa-fw" aria-hidden="true"></i>
            </a>
            <span class="search-button search-loading" id="search-loading-desktop">
              <i class="fa-solid fa-spinner fa-fw fa-spin" aria-hidden="true"></i>
            </span>
          </li><li class="menu-item theme-switch" title="切换主题">
          <i class="fa-solid fa-adjust fa-fw" aria-hidden="true"></i>
        </li>
      </ul>
    </nav>
  </div>
</header><header class="mobile animate__faster" id="header-mobile">
  <div class="header-container">
    <div class="header-wrapper">
      <div class="header-title">
        <a href="liudongdong1.github.io/" title="DAY By DAY"><img
    class="lazyload logo"
    src="/liudongdong1.github.io/svg/loading.min.svg"
    data-src="/fixit.min.svg"
    data-srcset="/fixit.min.svg, /fixit.min.svg 1.5x, /fixit.min.svg 2x"
    data-sizes="auto"
    alt="/fixit.min.svg"
    title="/fixit.min.svg"/><span class="header-title-text"></span></a><span id="typeit-header-subtitle-mobile" class="typeit header-subtitle"></span></div>
      <div class="menu-toggle" id="menu-toggle-mobile">
        <span></span><span></span><span></span>
      </div>
    </div>
    <nav>
      <ul class="menu" id="menu-mobile"><li class="search-wrapper">
            <div class="search mobile" id="search-mobile">
              <input type="text" placeholder="搜索文章标题或内容 ..." id="search-input-mobile">
              <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-mobile" title="搜索">
                <i class="fa-solid fa-search fa-fw" aria-hidden="true"></i>
              </a>
              <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-mobile" title="清空">
                <i class="fa-solid fa-times-circle fa-fw" aria-hidden="true"></i>
              </a>
              <span class="search-button search-loading" id="search-loading-mobile">
                <i class="fa-solid fa-spinner fa-fw fa-spin" aria-hidden="true"></i>
              </span>
            </div>
            <a href="javascript:void(0);" class="search-cancel" id="search-cancel-mobile">
              取消
            </a>
          </li><li
              class="menu-item"
            ><a
                  class="menu-link"
                  href="/liudongdong1.github.io/posts/"
                  
                  
                ><i class="fa-solid fa-archive fa-fw fa-sm" aria-hidden="true"></i> 所有文章</a></li><li
              class="menu-item"
            ><a
                  class="menu-link"
                  href="/liudongdong1.github.io/categories/"
                  
                  
                ><i class="fa-solid fa-th fa-fw fa-sm" aria-hidden="true"></i> 分类</a></li><li
              class="menu-item"
            ><a
                  class="menu-link"
                  href="/liudongdong1.github.io/tags/"
                  
                  
                ><i class="fa-solid fa-tags fa-fw fa-sm" aria-hidden="true"></i> 标签</a></li><li
              class="menu-item"
            ><a
                  class="menu-link"
                  href="/liudongdong1.github.io/friends/"
                  title="友情链接"
                  
                ><i class="fa-solid fa-users fa-fw fa-sm" aria-hidden="true"></i> 友链</a></li><li
              class="menu-item"
            ><a
                  class="menu-link"
                  href="/liudongdong1.github.io/about/"
                  
                  
                ><i class="fa-solid fa-info-circle fa-fw fa-sm" aria-hidden="true"></i> 关于</a></li><li
              class="menu-item text-center"
            ><a
                  class="menu-link"
                  href="https://liudongdong1.github.io/"
                  title="GitHub"
                  rel="noopener noreferrer" target="_blank"
                ><i class='fa-brands fa-github fa-fw' aria-hidden='true'></i> </a></li><li class="menu-item theme-switch" title="切换主题">
          <i class="fa-solid fa-adjust fa-fw" aria-hidden="true"></i>
        </li><li class="menu-item language">
            <span role="button" aria-label="选择语言" title="选择语言">简体中文<i class="dropdown-icon fa-solid fa-chevron-down" aria-hidden="true"></i>
            </span>
            <select class="language-select" onchange="location = this.value;"><option disabled>没有更多翻译</option></select>
          </li></ul>
    </nav>
  </div>
</header><div class="search-dropdown desktop">
    <div id="search-dropdown-desktop"></div>
  </div>
  <div class="search-dropdown mobile">
    <div id="search-dropdown-mobile"></div>
  </div><main class="container" data-page-style="normal"><aside class="toc" id="toc-auto"><h2 class="toc-title">目录 <i class="toc-icon fa-solid fa-angle-down fa-fw"></i></h2>
      <div class="toc-content" id="toc-content-auto"></div></aside>

  <aside class="aside-custom" id="aside-sakana">
    

<div class="sakana-widget">
  <div class="sakana-item" id="takina-widget"></div>
  <div class="sakana-item" id="chisato-widget"></div>
</div>
<script>
  function initSakanaWidget() {
    const takina = SakanaWidget.getCharacter('takina')
    SakanaWidget.registerCharacter('takina-slow', takina);
    new SakanaWidget({
      character: 'takina-slow',
      controls: false,
      autoFit: true,
      stroke: {
        color: "#b4b4b4",
        width: 2
      }
    }).mount('#takina-widget');

    const chisato = SakanaWidget.getCharacter('chisato')
    SakanaWidget.registerCharacter('chisato-slow', chisato);
    new SakanaWidget({
      character: 'chisato-slow',
      controls: false,
      autoFit: true,
      stroke: {
        color: "#b4b4b4",
        width: 2
      }
    }).mount('#chisato-widget');
  }
</script>
<script async onload="initSakanaWidget()" src="https://cdn.jsdelivr.net/npm/sakana-widget@2.3.0/lib/sakana.min.js">
</script></aside>

  <article class="page single">
    <div class="header"><h1 class="single-title animate__animated animate__flipInX">
        <span>Sign_Language_Introduce</span>
      </h1></div><div class="post-meta">
      <div class="post-meta-line"><span class="post-author"><span class="author"><i class="fa-solid fa-user-circle" aria-hidden="true"></i>
      liudongdong1</span></span>
          <span class="post-category">收录于 <a href="liudongdong1.github.io/categories/ai/"><i class="fa-regular fa-folder fa-fw"></i>&nbsp;AI</a></span></div>
      <div class="post-meta-line"><span title=2020-07-13&#32;09:47:19>
            <i class="fa-regular fa-calendar-alt fa-fw"></i>&nbsp;<time datetime="2020-07-13" >2020-07-13</time>
          </span>&nbsp;<i class="fa-solid fa-pencil-alt fa-fw"></i>&nbsp;约 6729 字&nbsp;
        <i class="fa-regular fa-clock fa-fw"></i>&nbsp;预计阅读 14 分钟&nbsp;<span id="busuanzi_container_page_pv" class="busuanzi_visitors comment-visitors" data-flag-title="Sign_Language_Introduce">
            <i class="fa-regular fa-eye fa-fw"></i>&nbsp;<span id="busuanzi_value_page_pv">-</span>&nbsp;次阅读
          </span>&nbsp;</div>
    </div><div class="featured-image"><img
    class="lazyload"
    src="/liudongdong1.github.io/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/clouds-in-sky-over-fields.jpg"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/clouds-in-sky-over-fields.jpg, https://gitee.com/github-25970295/blogImage/raw/master/img/clouds-in-sky-over-fields.jpg 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/clouds-in-sky-over-fields.jpg 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/clouds-in-sky-over-fields.jpg"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/clouds-in-sky-over-fields.jpg"/></div><div class="details toc" id="toc-static" kept="true">
        <div class="details-summary toc-title">
          <span>目录</span>
          <span><i class="details-icon fa-solid fa-angle-right"></i></span>
        </div>
        <div class="details-content toc-content" id="toc-content-static"><nav id="TableOfContents">
  <ul>
    <li><a href="#1-手势手语">1. 手势&amp;&amp;手语</a></li>
    <li><a href="#2-手语分类">2. 手语分类</a></li>
    <li><a href="#3-数据集">3. 数据集</a></li>
  </ul>

  <ul>
    <li>
      <ul>
        <li></li>
      </ul>
    </li>
  </ul>

  <ul>
    <li>
      <ul>
        <li></li>
      </ul>
    </li>
  </ul>

  <ul>
    <li>
      <ul>
        <li></li>
      </ul>
    </li>
  </ul>

  <ul>
    <li>
      <ul>
        <li></li>
      </ul>
    </li>
  </ul>

  <ul>
    <li>
      <ul>
        <li></li>
      </ul>
    </li>
  </ul>

  <ul>
    <li>
      <ul>
        <li></li>
      </ul>
    </li>
  </ul>

  <ul>
    <li>
      <ul>
        <li></li>
      </ul>
    </li>
  </ul>

  <ul>
    <li>
      <ul>
        <li></li>
      </ul>
    </li>
  </ul>

  <ul>
    <li>
      <ul>
        <li></li>
      </ul>
    </li>
  </ul>
</nav></div>
      </div><div
      class="content"
      id="content"
      
      
    ><blockquote>
<p>手语是聋哑人士的主要沟通工具，它是利用手部和身体的动作来传达意义。虽然手语帮助它的使用者之间互相沟通，但聋哑人士与一般人的沟通却十分困难，这个沟通障碍是源于大部分人不懂得手语。</p>
</blockquote>
<p><img
    class="lazyload"
    src="/liudongdong1.github.io/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210222232624578.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210222232624578.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210222232624578.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210222232624578.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210222232624578.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210222232624578.png"/></p>
<h2 id="1-手势手语">1. 手势&amp;&amp;手语</h2>
<ul>
<li><strong>手势</strong>：手的姿势 ，通常称作手势。它指的是人在运用手臂时，所出现的具体动作与体位。</li>
</ul>
<p><img
    class="lazyload"
    src="/liudongdong1.github.io/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200713095739826.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200713095739826.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200713095739826.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200713095739826.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200713095739826.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200713095739826.png"/></p>
<ul>
<li>**手语：**手语是用手势比量动作，根据手势的变化模拟形象或者音节以构成的一定意思或词语。</li>
</ul>
<p><img
    class="lazyload"
    src="/liudongdong1.github.io/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200713095832667.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200713095832667.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200713095832667.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200713095832667.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200713095832667.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200713095832667.png"/></p>
<h2 id="2-手语分类">2. 手语分类</h2>
<ul>
<li>单手手语； 双手手语</li>
<li>静态手语；动态手语</li>
<li>动态手语：
<ul>
<li>孤立词手语，包括 <strong>准备动作（Prestoke）、有效动作（Stocke）和结束动作（Poststoke）</strong> 三个部分。</li>
<li>连续手语识别。</li>
</ul>
</li>
</ul>
<h2 id="3-数据集">3. 数据集</h2>
<ul>
<li>
<p>著名的连续手语数据集是RWTH- PHOENIX-Weather
包含由 9 个人提供的45 760 个视频样本，其中包含 5 356 个与天气预报相关的句子、1200个德国手语词汇，大约占 52 GB的存储空间。</p>
</li>
<li>
<p>SIGNUM数据集：它包含由 25 个人提供的 33 210 个视频样本，其中包含 780 个句子、450 个德国手语词汇，每个句子包含 2 个至 11 个手语词汇不等。数据集大约占920 GB的存储空间。</p>
</li>
<li>
<p>波士顿大学的 ASLLVD：它包含由6 名手语者根据超过 3 300 个美国手语词汇提供的 9 800 个视频样本。</p>
</li>
<li>
<p><strong><a href="http://www-i6.informatik.rwth-aachen.de/aslr/database-rwth-boston-104.php"target="_blank" rel="external nofollow noopener noreferrer">RWTH-BOSTON-104<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></strong>:http://ww1.chalearn.org/resou/databases</p>
</li>
<li>
<p><a href="http://vipl.ict.ac.cn/homepage/ksl/data_ch.html"target="_blank" rel="external nofollow noopener noreferrer">中国手语 <strong>DEVISIGN</strong> 数据集<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>： 在微软亚洲研究院的赞助下建立的，旨在为世界范围内的研究者提供一个大型的词汇级的中国手语数据集，用于训练和评估他们的识别算法。目前，该数据集包含 4 414 个中国手语词汇，共包含 331 050 个 RGB-D 视频及对应的骨骼信息，由13名男性和17名女性提供</p>
</li>
<li>
<p>自己采集数据要求：</p>
<ul>
<li>图像采集设备要求：单目摄像机、双目摄像机、红外摄像机、深度摄像机（kinetic等）</li>
<li>图像的丰富性：同一种手势，不同背景、不同光照、不同角度、不同人多方面全面考虑；不同手势尽可能全的手语特征手型类种。</li>
</ul>
</li>
<li>
<p>ChaLearn LAP IsoGD Dataset：</p>
</li>
<li>
<p>RWTH-PHOENIXWeather 2014T dataset ：both gloss level annotations and spoken language translations for sign language videos that is of sufficient size and challenge for deep learning.f a parallel corpus of German sign language videos from 9 different signers, gloss-level annotations with a vocabulary of 1,066 different signs and translations into German spoken language with a vocabulary of 2,887 different words.   PHOENIX14T</p>
<ul>
<li><a href="https://www-i6.informatik.rwth-aachen.de/~koller/1miohands-data/"target="_blank" rel="external nofollow noopener noreferrer">https://www-i6.informatik.rwth-aachen.de/~koller/1miohands-data/<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></li>
</ul>
</li>
<li>
<p><strong>word-level ASL datasets</strong> ：2019年开源 , i.e. Purdue RVL-SLLL ASL Database [69], Boston ASLLVD [6] and RWTH-BOSTON-50</p>
</li>
<li>
<p><a href="https://www-i6.informatik.rwth-aachen.de/~koller/1miohands-data/"target="_blank" rel="external nofollow noopener noreferrer">https://www-i6.informatik.rwth-aachen.de/~koller/1miohands-data/<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
</li>
<li>
<p><a href="https://www-i6.informatik.rwth-aachen.de/~koller/RWTH-PHOENIX/"target="_blank" rel="external nofollow noopener noreferrer">https://www-i6.informatik.rwth-aachen.de/~koller/RWTH-PHOENIX/<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
</li>
</ul>
<hr>
<h1 id="paper-signcol">Paper: SignCol</h1>
<!-- raw HTML omitted -->
<h4 id="summary">Summary</h4>
<ol>
<li>Signs recognition software should be capable of handling eight different types of sign combinations,e.g. numbers, letters, words, and sentences, including hand gestures, head actions, facial expressions,etc.</li>
<li>SignCol can capture and store colored frames, depth frames, infrared frames, body index frames, coordinate mapped color-body frames, skeleton information of each frame and camera parameters simultaneously.</li>
<li><strong>Visual data capturing</strong>, connect to kinect2, capture and simultaneously save different information and data modalities of the gestures.</li>
<li><strong>Database managing&amp; statistics reporting</strong>;</li>
<li>SignLanguage parameters:
<ul>
<li><strong>Palm orientation</strong> Palm could face up, down, left, right, out and in. For example, consider baby vs. table.</li>
<li><strong>Hand Shape</strong> Shape of hands and ﬁngers are so useful for alphabetical letters and numbers. For example, consider I am Rita vs. My Rita. •</li>
<li><strong>Facial Expressions</strong> Head nodes, head shakes, eyebrows, nose, eyes, lips, and emotions can be attached to the sign and bring a new meaning. For example, consider you vs. is it you? whose difference is in the surprising form of the eyes and eyebrows. Similarly, the hand shape and movement for I’m late and I haven’t are same and just the face shape makes them different.</li>
<li><strong>Location</strong> Begin and end the sign at the correct position. Usually, signs originate from the body and terminate away or originate away from the body and terminate close to the body. For example, consider I’ll see you tomorrow. •</li>
<li><strong>Movement</strong> Different kinds of movements are usually arc, straight line, circle, alternating in and out, the twist of the wrist and ﬁnger ﬂick. In addition, in movement duration, the location, direction and also shape of the hands could change. For example, consider happy or enjoy.</li>
</ul>
</li>
<li>Language Category Divided:
<ul>
<li>cat1 – Number &lt; 10 – such as ’4’, ’8’</li>
<li>cat2 – Number &gt; 10 – such as ’16’, ’222’</li>
<li>cat3 – Alphabet Letter – such as ’A’, ’F’ •</li>
<li>cat4 – Word by a Sign – such as ’I’, ’My’, ’Mom’ •</li>
<li>cat5 – Word by Letters – such as ’Entropy’, ’Fourier’ •</li>
<li>cat6 –SentencebyWords(by concatenatedletters) –such as ’Entropy learning’, ’Fourier Transform’</li>
<li>cat7 – Sentence by Signs – such as ’I love you’</li>
<li>cat8 – Arbitrary sentence – such as ’Entropy of Mike’s image is high’</li>
</ul>
</li>
<li><a href="https://github.com/mohaEs/SignCol"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/mohaEs/SignCol<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>  star 5</li>
</ol>
<p><strong>level</strong>:   2020 <strong>The IEEE Winter Conference on Applications of Computer Vision</strong>
<strong>author</strong>: Dongxu Li (The Australian National University)
<strong>date</strong>: 2020
<strong>keyword</strong>:</p>
<ul>
<li>Sign Language</li>
</ul>
<hr>
<h1 id="paper-word-level-sign-language">Paper: Word-level Sign Language</h1>
<!-- raw HTML omitted -->
<h4 id="summary-1">Summary</h4>
<ol>
<li>introduce a new large-scale word-level ASL(WLASL) video dataset, containing more than 2000 words performed by over 100 signers.</li>
<li>implement and compare two different models,(i) holistic visual appearance based approach,(ii) 2D human pose based approach.</li>
<li>propose a novel pose-based temporal graph convolution networks(Pose-TGCN) that model spatial and temporal dependencies in human pose trajectories simultaneously, which has further boosted the performance of the pose-based method.</li>
<li>Pose-based and appearance-based models achieve comparable performance-based models achieve  comparable performances up to 62.63% at top-10 accuracy on 2000 words/glosses.</li>
<li>word-level sign language recognition (or “isolated sign language recognition”) and sentence-level sign language recognition (or “continuous sign language recognition”).</li>
</ol>
<h4 id="proble-statement">Proble Statement</h4>
<ul>
<li>The meaning of signs mainly depends on the <!-- raw HTML omitted --> combination of body motions, manual movements and head poses, and subtle differences<!-- raw HTML omitted --> may translate into different meanings</li>
<li>the vocabulary of signs in daily use is large and usually in the magnitude of thousands,( gesture recognition and action recognition only contains at most a few hundred categories).</li>
<li>a word in sign language may have multiple counterparts in natural languages depending on the context.</li>
</ul>
<p><img
    class="lazyload"
    src="/liudongdong1.github.io/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200715103232337.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200715103232337.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200715103232337.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200715103232337.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200715103232337.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200715103232337.png"/></p>
<h4 id="methods">Methods</h4>
<blockquote>
<p>For appearance-based methods, providing a base-line be re-training VGG backbone, and GRU as a representative for CNN, and provide a 3D CNN baseline using fine-tuned I3D, which perform better than VGG-GRU baseline. For pose-based methods,extract human poses from orignal videos and use them as input features,propose a novel pose-based model temporal graph convolutional network.reaching up to 62.63%.</p>
</blockquote>
<ul>
<li><strong>system overview</strong>:</li>
</ul>
<p>【DataSet Construction】</p>
<ul>
<li><strong>Collection:</strong>  construct a large-scale signer-independent ASL dataset from websites,like ASLU and ASL_LEX. ASL as well as tutorial on YouTube where a signer performs only one sign.</li>
<li>**Annotations: ** using some meta information to provide a gloss label for each video, including temporal boundary, body bounding box, signer annotation and sign dialect/variation annotaions.
<ul>
<li>Temporal boundary: indicate the start and the end frames of a sign.</li>
<li>Body Bounding-box: using YOLOv3 as  a person detection tool to identify body bounding-boxes of signers in videos, and use the largest bounding-box size to crop the person from video.</li>
<li>Signer Diversity: employ the face detector and the face dataset, and then compare the Euclidean distances among the face embeddings to identify the person,</li>
<li>Dialect Variation: Annotation: count the number of dialects and assign labels for different dialects automatically.</li>
</ul>
</li>
<li>Data Arrangement:</li>
</ul>
<blockquote>
<p>select top-K glosses with K = {100, 300, 1000, 2000}, and organize them to four subsets, named WLASL100, WLASL300, WLASL1000 and WLASL2000</p>
</blockquote>
<p><img
    class="lazyload"
    src="/liudongdong1.github.io/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200715103334772.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200715103334772.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200715103334772.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200715103334772.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200715103334772.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200715103334772.png"/></p>
<p><img
    class="lazyload"
    src="/liudongdong1.github.io/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200715104553269.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200715104553269.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200715104553269.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200715104553269.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200715104553269.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200715104553269.png"/></p>
<p><strong>【Method Comparison】Image-appearance</strong></p>
<ul>
<li><strong>Image-appearance based Baselines:</strong>  using VGG16 pretrained on ImageNet to extract spatial features and then feed the extracted features to a stacked GRU.</li>
</ul>
<blockquote>
<p>to avoid overfiting the training set, the hidden sizes of GRU for the four subsets are set to 64,96,128 and 256, the number of the stacked RNN layers in GRU is 2, and randomly select at most 50 consecutive frames from each video, using cross-entropy losses.</p>
</blockquote>
<ul>
<li><strong>3D Convolutional Networks:</strong>  employ the network of I3D as image-appearance base baseline, <!-- raw HTML omitted -->focusing on the hand shapes and orientations as well as arm movements.<!-- raw HTML omitted -->,to better capture the spatio-temporal information of signs.</li>
</ul>
<p><img
    class="lazyload"
    src="/liudongdong1.github.io/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200715105724235.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200715105724235.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200715105724235.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200715105724235.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200715105724235.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200715105724235.png"/></p>
<p><strong>【Method Comparison】Pose-based Baselines</strong></p>
<ul>
<li>
<p>two mainstream approaches of pose estimation: regressing the key-points and estimating key-point heatmaps followed by a non-maximal suppression techique.</p>
</li>
<li>
<p><strong>RNN-Methods:</strong>  employs RNN to model the temporal sequential information of the pose movements, and representation output by RNN is used for sign recognition.</p>
<blockquote>
<p>using Openpose to extract the keypoints of person, 55 body and hand 2D keypoints, and concatenate all the 2D coordinates of each joint as the input feature and feed it to a stacked GRU of 2 layers.</p>
</blockquote>
</li>
<li>
<p><strong>Temporal Graph Neural Networks:</strong>  models the spatial and temporal dependencies of the pose sequence.</p>
<ul>
<li>$$
X_{1:N}=[x_1,x_2,x_3,&hellip;x_N] \
x_i\epsilon R^k \
H_{n+1}=G_b(H_n)=\theta(A_nH_nW_n)
$$</li>
</ul>
</li>
</ul>
<blockquote>
<p>view human body as a fully-connected graph with K vertices and represent the edges in the graph as a weighted adjacency matric$A \epsilon R^{K*K}$</p>
</blockquote>
<p><img
    class="lazyload"
    src="/liudongdong1.github.io/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/20200829170303.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/20200829170303.png, https://gitee.com/github-25970295/blogImage/raw/master/img/20200829170303.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/20200829170303.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/20200829170303.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/20200829170303.png"/></p>
<h4 id="evaluation">Evaluation</h4>
<ul>
<li>
<p><strong>Data Pre-processing and Augmentation:</strong></p>
<ul>
<li>resize the frame to 256 pixels;</li>
<li>randomly crop a 224*224 patch from an input frame and apply a horizontal flipping with probability of 0.5</li>
<li>the video of 50 frames are randomly selected and the models are asked to predict labels based on only partial observations of the input video.</li>
<li>4:1:1 to split data</li>
</ul>
</li>
<li>
<p><strong>Evaluation Metric:</strong> using the mean scores of top-K classification accuracy with K={1,5,10}</p>
</li>
</ul>
<p><img
    class="lazyload"
    src="/liudongdong1.github.io/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200715111050347.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200715111050347.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200715111050347.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200715111050347.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200715111050347.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200715111050347.png"/></p>
<p><img
    class="lazyload"
    src="/liudongdong1.github.io/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200715111127599.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200715111127599.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200715111127599.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200715111127599.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200715111127599.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200715111127599.png"/></p>
<h4 id="notes">Notes</h4>
<ul>
<li><i class="fa-regular fa-check-square fa-fw" aria-hidden="true"></i> GRU &amp;&amp; LSTM</li>
</ul>
<p><strong>level</strong>: ECCV
<strong>author</strong>: Gyeongsik Moon(Seoul National University, Korea)  Shoou-I Yu <a href="blob:https://www.facebook.com/b18d4b2f-c5f4-4a45-8110-8eef7a151a5b">(FaceBook Reality Labs)</a>
<strong>date</strong>: 2020
<strong>keyword</strong>:</p>
<ul>
<li>handinteraction</li>
</ul>
<hr>
<h1 id="paper-interhand26m">Paper: InterHand2.6M</h1>
<!-- raw HTML omitted -->
<h4 id="summary-2">Summary</h4>
<ol>
<li>propose a large-scale high-resolution multi-view single and interacting hand sequences dataset, InterHand2.6M;</li>
<li>a baseline network, InterNet, for 3D interacting hand pose estimation from a single RGB image, estimate handedness, 2.5D hand pose, and right hand-relative left hand depth from a single RGB image;</li>
<li>show that single hand data is not enough, and interacting hand data is essential for accurate 3D interacting hand pose estimation;</li>
</ol>
<h4 id="proble-statement-1">Proble Statement</h4>
<ul>
<li>single hand scenarios have limitations in covering all realistic human hand postures for people often interact with each other to interact with each other people or objects;</li>
<li>the 2.5D hand pose consists of 2D pose in x- and y- axis and root joint-relative depth in z-axis, widely used in 3D human body and hand pose estimation from a single RGB image;</li>
<li>RootNet[16] obtain an absolute depth of the root joint to lift 2.5D right and left hand pose to 3D space;
<ul>
<li>the interNet predict right hand-relative left hand depth by leveraging the appearance of the interacting hand from the interacting hand from input image;</li>
</ul>
</li>
</ul>
<p>previous work:</p>
<ul>
<li><strong>Depth-based 3D single hand pose estimation:</strong>
<ul>
<li>fit a pre-defined hand model to the input depth image by minimizing hand-crafted cost functions, particle swarm optimization, iterative closest point&hellip;</li>
<li>directly localizes hand joints from an input depth map,
<ul>
<li>by estimating 2D heatmaps for each hand joint;</li>
<li>by estimating multi-view 2D heatmaps;</li>
</ul>
</li>
</ul>
</li>
<li><strong>RGB-based 3D single hand pose estimation:</strong>
<ul>
<li>[17] used an image-to-image translation model to generate a realistic hand pose dataset from a synthetic dataset.</li>
<li>[41] proposed deep generative models to learn latent space for hand;</li>
</ul>
</li>
<li><strong>3D interacting hand pose estimation:</strong>
<ul>
<li>[1] present a framework that outputs 3D hand pose and mesh from multi-view RGB sequences;</li>
<li>[37] by incorporating a physical model;</li>
</ul>
</li>
</ul>
<h4 id="methods-1">Methods</h4>
<ul>
<li><strong>system overview</strong>:</li>
</ul>
<p><img
    class="lazyload"
    src="/liudongdong1.github.io/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200915204915815.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200915204915815.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200915204915815.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200915204915815.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200915204915815.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200915204915815.png"/></p>
<p><img
    class="lazyload"
    src="/liudongdong1.github.io/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200915211644483.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200915211644483.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200915211644483.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200915211644483.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200915211644483.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200915211644483.png"/></p>
<p><img
    class="lazyload"
    src="/liudongdong1.github.io/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200915211931856.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200915211931856.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200915211931856.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200915211931856.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200915211931856.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200915211931856.png"/></p>
<p><strong>【Model one】Dataset Prepare</strong></p>
<ul>
<li><strong>Dataset release</strong></li>
</ul>
<blockquote>
<p>downsized 512×334 image resolution at 5 fps, and downsized 512×334 resolution at 30 fps</p>
<p>The annotation file includes camera type, subject index, camera index, bounding box, handedness, camera parameters, and 3D joint coordinates</p>
</blockquote>
<ul>
<li><strong>Single Hand Sequences:</strong>
<ol>
<li>neutral relaxed: the neutral hand pose. Hands in front of the chest, fingers do not touch, and palms face the side.</li>
<li>neutral rigid: the neutral hand pose with maximally extended fingers, muscles tense.</li>
<li>good luck: hand sign language with crossed index and middle fingers.</li>
<li>fake gun: hand gesture mimicking the gun.</li>
<li>star trek: hand gesture popularized by the television series Star Trek.</li>
<li>star trek extended thumb: “star trek” with extended thumb.</li>
<li>thumb up relaxed: hand sign language that means “good”, hand muscles relaxed.</li>
<li>thumb up normal: “thumb up”, hand muscles average tenseness.</li>
<li>thumb up rigid: “thumb up”, hand muscles very tense.</li>
<li>thumb tuck normal: similar to fist, but the thumb is hidden by other fingers.</li>
<li>thumb tuck rigid: “thumb tuck”, hand muscles very tense.</li>
<li>aokay: hand sign language that means “okay”, where palm faces the side.</li>
<li>aokay upright: “aokay” where palm faces the front.</li>
<li>surfer: the SHAKA sign.</li>
<li>rocker: hand gesture that represents rock and roll, where palm faces the side.</li>
<li>rocker front: the “rocker” where palm faces the front.</li>
<li>rocker back: the “rocker” where palm faces the back.</li>
<li>fist: fist hand pose.</li>
<li>fist rigid: fist with very tense hand muscles.</li>
<li>alligator closed: hand gesture mimicking the alligator with a closed mouth.</li>
<li>one count: hand sign language that represents “one.”</li>
<li>two count: hand sign language that represents “two.”</li>
<li>three count: hand sign language that represents “three.”</li>
<li>four count: hand sign language that represents “four.”</li>
<li>five count: hand sign language that represents “five.”</li>
<li>indextip: thumb and index fingertip are touching.</li>
<li>middletip: thumb and middle fingertip are touching.</li>
<li>ringtip: thumb and ring fingertip are touching.</li>
<li>pinkytip: thumb and pinky fingertip are touching.</li>
<li>palm up: has palm facing up.</li>
<li>finger spread relaxed: spread all fingers, hand muscles relaxed.</li>
<li>finger spread normal: spread all fingers, hand muscles average tenseness.</li>
<li>finger spread rigid: spread all fingers, hand muscles very tense.</li>
<li>capisce: hand sign language that represents “got it” in Italian.</li>
<li>claws: hand pose mimicking claws of animals.</li>
<li>peacock: hand pose mimicking peacock.</li>
<li>cup: hand pose mimicking a cup.</li>
<li>shakespeareyorick: hand pose from Yorick from Shakespeare’s play Hamlet.</li>
<li>dinosaur: hand pose mimicking a dinosaur.</li>
<li>middle finger: hand sign language that has an offensive meaning</li>
</ol>
</li>
</ul>
<p><img
    class="lazyload"
    src="/liudongdong1.github.io/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200916185336884.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200916185336884.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200916185336884.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200916185336884.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200916185336884.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200916185336884.png"/></p>
<p><img
    class="lazyload"
    src="/liudongdong1.github.io/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200916185414522.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200916185414522.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200916185414522.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200916185414522.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200916185414522.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200916185414522.png"/></p>
<p><img
    class="lazyload"
    src="/liudongdong1.github.io/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200916185437395.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200916185437395.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200916185437395.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200916185437395.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200916185437395.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200916185437395.png"/></p>
<ul>
<li><strong>ROM of single hand sequence</strong>  ROM represents conversational gestures with minimal instructions.
<ul>
<li>five count: count from one to five.</li>
<li>five countdown: count from five to one.</li>
<li>fingertip touch: thumb touch each fingertip.</li>
<li>relaxed wave: wrist relaxed, fingertips facing down and relaxed, wave.</li>
<li>fist wave: rotate wrist while hand in a fist shape.</li>
<li>prom wave: wave with fingers together.</li>
<li>palm down wave: wave hand with the palm facing down.</li>
<li>index finger wave: hand gesture that represents “no” sign.</li>
<li>palmer wave: palm down, scoop towards you, like petting an animal.</li>
<li>snap: snap middle finger and thumb.</li>
<li>finger wave: palm down, move fingers like playing the piano.</li>
<li>finger walk: mimicking a walking person by index and middle finger.</li>
<li>cash money: rub thumb on the index and middle fingertips.</li>
<li>snap all: snap each finger on the thumb.</li>
</ul>
</li>
</ul>
<p><img
    class="lazyload"
    src="/liudongdong1.github.io/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200916185530746.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200916185530746.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200916185530746.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200916185530746.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200916185530746.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200916185530746.png"/></p>
<ul>
<li><strong>Interacting hand sequences</strong>
<ul>
<li>right clasp left: two hands clasp each other, right hand is on top of the left hand.</li>
<li>left clasp right: two hands clasp each other, left hand is on top of the right hand.</li>
<li>fire gun: mimicking a gun with two hands together.</li>
<li>right fist cover left: right fist completely covers the left hand.</li>
<li>left fist cover right: left fist completely covers the right hand.</li>
<li>interlocked fingers: fingers of the right and left hands are interlocked.</li>
<li>pray: hand sign language that represents praying.</li>
<li>right fist over left: right fist is on top of the left fist.</li>
<li>left fist over right: left fist is on top of the right fist.</li>
<li>right babybird: mimicking caring a babybird with two hands, the right hand is placed at the bottom.</li>
<li>left babybird: mimicking caring a babybird with two hands, the left hand is placed at the bottom.</li>
<li>interlocked finger spread: fingers of the right and left hands are interlocked yet spread.</li>
<li>finger squeeze: squeeze all five fingers with the other hand.</li>
</ul>
</li>
</ul>
<p><img
    class="lazyload"
    src="/liudongdong1.github.io/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200916185555201.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200916185555201.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200916185555201.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200916185555201.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200916185555201.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200916185555201.png"/></p>
<ul>
<li><strong>ROM of Interacting hand sequence</strong>
<ul>
<li>palmerrub: rub palm of one hand with opposite hand’s thumb.</li>
<li>knuckle crack: crack each finger by having the opposite hand compress a bent finger.</li>
<li>golf claplor: light clap, left over right.</li>
<li>itsy bitsy spider: finger motion used when singing the children song “itsy bitsy spider”, like this (link).</li>
<li>finger noodle: fingers interlocked, palms facing opposite directions, wiggle middle fingers.</li>
<li>nontouch: two hands random motion, hands do not touch.</li>
<li>sarcastic clap: exaggerated, slow clap.</li>
<li>golf claprol: light clap, right over left.</li>
<li>evil thinker: wrist together, tap fingers together one at a time.</li>
<li>rock paper scissors: hold rock, then paper, then scissors.</li>
<li>hand scratch: using the opposite hand, lightly scratch palm then top of hand; switch and do the same with the other hand.</li>
<li>touch: two hands interacting randomly in a frame, touching.</li>
<li>pointing towards features: using the opposite index finger, point out features on the palm and back of the hand (trace lifelines/wrinkles, etc.).</li>
<li>interlocked thumb tiddle: interlock fingers, rotate thumbs around each other.</li>
<li>right finger count index point: using the right pointer finger, count up to five on the left hand, starting with the pinky.</li>
<li>left finger count index point: using left pointer finger, count up to five on the right hand, starting with the pinky.</li>
<li>single relaxed finger: this consists of a series of actions: (1) touch each fingertip to the center of the palm for the same hand, do this for both hands, (2) interlock fingers and press palms out, (3) with the opposite hand, hold wrist, (4) with the opposite hand, bend wrist down and back, (5) point at watch on both wrists, (6) circle wrists, (7) look at nails, and (8) point at yourself with thumbs then with index fingers</li>
</ul>
</li>
</ul>
<p><img
    class="lazyload"
    src="/liudongdong1.github.io/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200916185619762.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200916185619762.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200916185619762.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200916185619762.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200916185619762.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200916185619762.png"/></p>
<p><strong>【Module Two】Exact Methods</strong></p>
<p><img
    class="lazyload"
    src="/liudongdong1.github.io/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200915212634843.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200915212634843.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200915212634843.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200915212634843.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200915212634843.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200915212634843.png"/></p>
<ul>
<li>
<p><strong>Handedness estimation:</strong> takes the image feature F , using two fully-connected layers with hidden activation size 512 followed by the ReLU activation function, and the last is sigmoid activation function;</p>
</li>
<li>
<p><strong>2.5D right and left hand pose estimation:</strong>     用到时再看网络结构</p>
</li>
<li>
<p><strong>Right hand-relative left hand depth estimation:</strong> 用到时再看网络结构</p>
</li>
</ul>
<h4 id="notes-font-colororange去加强了解font">Notes <!-- raw HTML omitted -->去加强了解<!-- raw HTML omitted --></h4>
<ul>
<li><a href="https://github.com/facebookresearch/InterHand2.6M/releases"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/facebookresearch/InterHand2.6M/releases<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>  下载地址。</li>
<li>facebookAI lab:   <a href="https://v.qq.com/x/page/j0956p2juqp.html"target="_blank" rel="external nofollow noopener noreferrer">https://v.qq.com/x/page/j0956p2juqp.html<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></li>
</ul>
<p><img
    class="lazyload"
    src="/liudongdong1.github.io/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200916193851551.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200916193851551.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200916193851551.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200916193851551.png 2x"
    data-sizes="auto"
    alt="Oculus Connect"
    title="Oculus Connect"/></p>
<p><strong>level</strong>:
<strong>author</strong>: Nicolas Pugeault &amp; Richard Bowden
<strong>date</strong>: 2011
<strong>keyword</strong>:</p>
<ul>
<li>ASL</li>
</ul>
<blockquote>
<p>Pugeault, Nicolas, and Richard Bowden. &ldquo;Spelling it out: Real-time ASL fingerspelling recognition.&rdquo; <em>2011 IEEE International conference on computer vision workshops (ICCV workshops)</em>. IEEE, 2011.</p>
</blockquote>
<hr>
<h1 id="paper-spelling-it-out">Paper: Spelling It Out</h1>
<!-- raw HTML omitted -->
<ol>
<li>presents an interactive hand shape recognition user interface for ASL finger-spelling;</li>
<li>Hand-shapes corresponding to letters of the alphabet are characterized using appearance and depth images and classified using random forests;</li>
<li>extract feature from images and depth images;</li>
</ol>
<h4 id="methods-2">Methods</h4>
<ul>
<li><strong>Problem Formulation</strong>:
<ul>
<li>finger-spelling is single-handed, removes the difficulties of hands occluding one another;</li>
<li>some of the signs in the alphabet are visually very similar;</li>
<li>different persons perform signs in different way;</li>
<li>the differences between people&rsquo;s hands and natural dexterity leads to differences in the execution of signs between different signers;</li>
<li>need to run in real-time;</li>
</ul>
</li>
</ul>
<p><img
    class="lazyload"
    src="/liudongdong1.github.io/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200915084116179.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200915084116179.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200915084116179.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200915084116179.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200915084116179.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200915084116179.png"/></p>
<p><img
    class="lazyload"
    src="/liudongdong1.github.io/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200915084158379.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200915084158379.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200915084158379.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200915084158379.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200915084158379.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200915084158379.png"/></p>
<ul>
<li><strong>system overview</strong>:</li>
</ul>
<p><img
    class="lazyload"
    src="/liudongdong1.github.io/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200915083100309.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200915083100309.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200915083100309.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200915083100309.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200915083100309.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200915083100309.png"/></p>
<p><img
    class="lazyload"
    src="/liudongdong1.github.io/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200915083755524.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200915083755524.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200915083755524.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200915083755524.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200915083755524.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200915083755524.png"/></p>
<p><img
    class="lazyload"
    src="/liudongdong1.github.io/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200915084527995.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200915084527995.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200915084527995.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200915084527995.png 2x"
    data-sizes="auto"
    alt="features"
    title="features"/></p>
<p><img
    class="lazyload"
    src="/liudongdong1.github.io/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200915084833977.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200915084833977.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200915084833977.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200915084833977.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200915084833977.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200915084833977.png"/></p>
<p><img
    class="lazyload"
    src="/liudongdong1.github.io/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200915084945109.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200915084945109.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200915084945109.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200915084945109.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200915084945109.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200915084945109.png"/></p>
<h4 id="notes-font-colororange去加强了解font-1">Notes <!-- raw HTML omitted -->去加强了解<!-- raw HTML omitted --></h4>
<ul>
<li>http://www. youtube.com/watch?v=0tCGMhbTDmw</li>
</ul>
<p><strong>level</strong>:
<strong>author</strong>: Hamid Reza Vaezi Joze(Microsoft)
<strong>date</strong>:  2018
<strong>keyword</strong>:</p>
<ul>
<li>ASL</li>
</ul>
<blockquote>
<p>Joze, Hamid Reza Vaezi, and Oscar Koller. &ldquo;Ms-asl: A large-scale data set and benchmark for understanding american sign language.&rdquo; <em>arXiv preprint arXiv:1812.01053</em> (2018).</p>
</blockquote>
<hr>
<h1 id="paper-ms-asl">Paper: MS-ASL</h1>
<!-- raw HTML omitted -->
<h4 id="summary-3">Summary</h4>
<ol>
<li>propose the first real-life large-scale sign language data set comprising over 25000 annotated videos, covering of 1000 signs in challenging and unconstrained real-life recording conditions;</li>
<li>evaluate current state-of-the-art approaches: 2D-CNN-LSTM, body key point, CNN-LSTM-HMM and 3D-CNN as baseline;</li>
<li>propose I3D as a powerful and suitable architecture for sign language recognition;</li>
</ol>
<h4 id="research-objective">Research Objective</h4>
<ul>
<li><strong>Application Area</strong>:</li>
<li><strong>Purpose</strong>:</li>
</ul>
<h4 id="proble-statement-2">Proble Statement</h4>
<ul>
<li><strong>Sign Language Data sets:</strong>
<ul>
<li><strong>Purdue RVL-SLLL ASL dataset:</strong> contains 10 short stories with a vocabulary of 104 signs and total count of 1834 produced by 14 native signers in a lab environment under controlled lighting;</li>
<li><strong>RWTH-BOSTON-50\100\400:</strong> contain isolated language with a vocabulary of 50-104 signs. the 400 contains a vocabulary of 483 signs and also constitutes of continuous signing by 5 signers;</li>
<li><strong>Devisign:</strong> a chinese sign language data set featuring isolated single signs perform by 8 non-natives in a laboratory environment, covering a vocabulary of up to 2000 isolated signs and provides RGB with depth information in 24000 recordings.</li>
<li><strong>Finish S-pot:</strong> base on lexicon, covers 1211 isolated citation form signs that need to be spotted in 4328 continuous sign language videos.</li>
</ul>
</li>
</ul>
<p><img
    class="lazyload"
    src="/liudongdong1.github.io/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200915095247256.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200915095247256.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200915095247256.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200915095247256.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200915095247256.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200915095247256.png"/></p>
<h4 id="methods-3">Methods</h4>
<ul>
<li><strong>Dataset:</strong>
<ul>
<li>one sample video may include repetitive act of a distinct signs;</li>
<li>one word can sign differently in different dialects based on geographical regions;</li>
<li>includes large number of signers and is a signer independent data set;</li>
<li>they are larege visual variabilities in the videos such as background, lighting, clothing and camera view point;</li>
</ul>
</li>
</ul>
<p><img
    class="lazyload"
    src="/liudongdong1.github.io/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200915100200887.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200915100200887.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200915100200887.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200915100200887.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200915100200887.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200915100200887.png"/></p>
<p>【<strong>Evaluation Scheme</strong> 】using average per class accuracy; using average per class top-five accuracy;</p>
<blockquote>
<p>2D-CNN: used VGG16 network followed by an average pooling and LSTM layer of size of 256 with batch normalization, the final layer are a 512 hidden units followed by a fully connected layer for classification;</p>
<p>use googlenets as 2D-CNN with 2 bi-directional LSTM layers and 3-state HMM on Phoenix2014;</p>
</blockquote>
<blockquote>
<p>body key-points: extracted all the key-frame point for all sample , using 64 frames for time window;</p>
<ul>
<li>the input: 64*137*3 representing x,y coordinates and confidence values for the 137 key-points;</li>
<li>using co-occurrence network(HCN), to learn form 137 keypoints as well as per frame difference of them;</li>
</ul>
</blockquote>
<blockquote>
<p>3D-CNN: using C3D, and I3D network;</p>
</blockquote>
<p><img
    class="lazyload"
    src="/liudongdong1.github.io/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200915102058478.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200915102058478.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200915102058478.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200915102058478.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200915102058478.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200915102058478.png"/></p>
<p><img
    class="lazyload"
    src="/liudongdong1.github.io/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200915102149811.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200915102149811.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200915102149811.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200915102149811.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200915102149811.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200915102149811.png"/></p>
<p><img
    class="lazyload"
    src="/liudongdong1.github.io/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200915102239850.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200915102239850.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200915102239850.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200915102239850.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200915102239850.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200915102239850.png"/></p>
<p><img
    class="lazyload"
    src="/liudongdong1.github.io/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200915102321944.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200915102321944.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200915102321944.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200915102321944.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200915102321944.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200915102321944.png"/></p>
<h4 id="notes-font-colororange去加强了解font-2">Notes <!-- raw HTML omitted -->去加强了解<!-- raw HTML omitted --></h4>
<ul>
<li>如果使用跨源数据转化，则第二种方法可以使用；</li>
</ul>
<p><strong>level</strong>: CVPR
<strong>author</strong>:  Necati Cihan Camg¨oz(camgoz), Microsoft Munich (German)
<strong>date</strong>: 2020
<strong>keyword</strong>:</p>
<ul>
<li>sign languages</li>
</ul>
<hr>
<h1 id="paper-sign-language-transformers">Paper: Sign Language Transformers</h1>
<!-- raw HTML omitted -->
<h4 id="summary-4">Summary</h4>
<ol>
<li>introduce a novel transformer based architecture that jointly learns continuous sign language recognition and translation while being trainable in an end-to-end manner.</li>
<li>a novel multi-task formalization of CSLR and SLT which exploits the supervision power of glosses, without limiting the translation to spoken language.</li>
<li>the first successful application of transformers for CSLR and SLT which achieves state-of-art results in both recognition and translation accuracy, vastly outperforming all comparable previous approaches.</li>
<li>a broad range of new baseline results to guide future research in this field.</li>
</ol>
<h4 id="proble-statement-3">Proble Statement</h4>
<ul>
<li>such translation system requires several sub-tasks, <strong>Sign Segmentation, Sign Language Recognition and Understanding, Sign Language Translation</strong></li>
</ul>
<p>previous work:</p>
<ul>
<li>previous sequence-to-sequence base literature on SLT can be categorized into two groups:
<ul>
<li>utilized a state-of-art CSLR method to obtain sign glosses, and then used an attention-based text-to-text NMT model to learn the sign gloss to spoken language sentence translation.</li>
<li>focus on translation from sign video representation to spoken language with no intermediate representations, to learn $p(S|V)$ directly. With enough data and a sufficiently sophisticated network architecture, such model can theoretically realize end-to-end SLT with no need for human-interpretable information that act as a bottle-neck.</li>
</ul>
</li>
</ul>
<h4 id="methods-4">Methods</h4>
<ul>
<li>
<p><strong>Problem Formulation</strong>:</p>
</li>
<li>
<p><strong>system overview</strong>:</p>
</li>
</ul>
<p><img
    class="lazyload"
    src="/liudongdong1.github.io/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/20200827104715.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/20200827104715.png, https://gitee.com/github-25970295/blogImage/raw/master/img/20200827104715.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/20200827104715.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/20200827104715.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/20200827104715.png"/></p>
<p><img
    class="lazyload"
    src="/liudongdong1.github.io/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/20200827105016.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/20200827105016.png, https://gitee.com/github-25970295/blogImage/raw/master/img/20200827105016.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/20200827105016.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/20200827105016.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/20200827105016.png"/></p>
<p><img
    class="lazyload"
    src="/liudongdong1.github.io/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/20200827104944.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/20200827104944.png, https://gitee.com/github-25970295/blogImage/raw/master/img/20200827104944.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/20200827104944.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/20200827104944.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/20200827104944.png"/></p>
<p><strong>【Module One】Spatial and Word Embeddings</strong>
$$
m_u=WordEmdedding(w_u)\
f_t=SpatialEmbedding(I_t)
$$</p>
<blockquote>
<p>$m_u$  is the embedded represetation of the spoken language word $w_u$ and $f_t$ corresponds to the non-linear frame level spatial representation obtained from a CNN.</p>
</blockquote>
<p>$$
f_t&rsquo;=f_t+PositionalEncoding(t)\
m_u&rsquo;=m_u+PositionEncoding(u)
$$</p>
<blockquote>
<p>PositionalEncoding is a predefined function which produces a unique vector in the form of a phase shifted sine wave for each time step.</p>
</blockquote>
<p><strong>【Module Two】 Sign Language Recognition Transformers</strong>
$$
z_t=SLRT(f_t&rsquo;|f_{1:T}&rsquo;)
$$</p>
<blockquote>
<p>where $z_t$ denotes the  spatio-temporal representation of the frame $I_t$ which is generated by SLRT at time step t, given the spatial representations of all of the video frames,$f_{1:T}&rsquo;$'</p>
</blockquote>
<p><strong>【Module Three】 Sign language Translation Transformers</strong></p>
<h4 id="evaluation-1">Evaluation</h4>
<p><img
    class="lazyload"
    src="/liudongdong1.github.io/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/20200827111159.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/20200827111159.png, https://gitee.com/github-25970295/blogImage/raw/master/img/20200827111159.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/20200827111159.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/20200827111159.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/20200827111159.png"/></p>
<h4 id="notes-font-colororange去加强了解font-3">Notes <!-- raw HTML omitted -->去加强了解<!-- raw HTML omitted --></h4>
<ul>
<li><a href="https://github.com/neccam/slt"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/neccam/slt<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></li>
</ul>
<p><strong>level</strong>: arxiv
<strong>author</strong>: Kayo Yin
<strong>date</strong>: 2020
<strong>keyword</strong>:</p>
<ul>
<li>SLR, CSLR, SLT</li>
</ul>
<hr>
<h1 id="paper-slt-transformers">Paper: SLT Transformers</h1>
<!-- raw HTML omitted -->
<h4 id="summary-5">Summary</h4>
<ol>
<li>Using SLR system to extract sign language glosses from videos, and using translation system generates spoken language translations from the sign language glosses.</li>
<li><!-- raw HTML omitted -->Report a wide range of experimental results for various Transformer setups and introduce the use of Spatial-Temporal Multi-Cue networks in an ent-to-end SLT system with Transformer.<!-- raw HTML omitted --></li>
<li>Perform experiment on RWTH-PHOENIX-Weather 2014T, and ASLG-PC12, and improves on the current state-of-art by over 5-7 points.</li>
</ol>
<h4 id="proble-statement-4">Proble Statement</h4>
<ul>
<li><strong>Tokenization Problem:</strong> analyze videos of sign language to generate sign language glosses that capture the meaning of the sequence of different signs.</li>
</ul>
<p>【Previous work】</p>
<ul>
<li>CSLR: divide the task into three sub-tasks: alignmnet, single-gloss SLR, and sequence construction while others perform the task in an end-to-end fashion using DNN. <!-- raw HTML omitted -->Sequence to Sequence model that directly translate ASL glosses from ASLG_PC12 dataset without taking sign language data itself.<!-- raw HTML omitted --></li>
</ul>
<blockquote>
<p>approach to the problem as visual recognition task and ignores the underlying grammatical and linguistic structures unique to SL.</p>
</blockquote>
<h4 id="methods-5">Methods</h4>
<ul>
<li><strong>system overview</strong>:</li>
</ul>
<p><img
    class="lazyload"
    src="/liudongdong1.github.io/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200715162316328.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200715162316328.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200715162316328.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200715162316328.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200715162316328.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200715162316328.png"/></p>
<p>【Transformer Model】</p>
<p><img
    class="lazyload"
    src="/liudongdong1.github.io/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200715163955352.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200715163955352.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200715163955352.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200715163955352.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200715163955352.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200715163955352.png"/></p>
<h4 id="evaluation-2">Evaluation</h4>
<ul>
<li>Dataset:
<ul>
<li><strong>RWTH-PHOENIX-Weather 2014T</strong>: the only publicly available dataset with both gloss level annotations and spoken language translations for sign language videos that is of sufficient size and challenge for deep learning.</li>
<li><strong>ASLG-PC12</strong>: SLT on this dataset has only been performed using RNN-based sequence-to-sequence attention networks.</li>
</ul>
</li>
</ul>
<p><img
    class="lazyload"
    src="/liudongdong1.github.io/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200715164236920.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200715164236920.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200715164236920.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200715164236920.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200715164236920.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200715164236920.png"/></p>
<p><img
    class="lazyload"
    src="/liudongdong1.github.io/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200715212229107.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200715212229107.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200715212229107.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200715212229107.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200715212229107.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200715212229107.png"/></p>
<h4 id="conclusion">Conclusion</h4>
<ul>
<li>Perform the first thorough study of using the Transformer network for SLT and demonstrate how it outperforms previous NMT architectures for this task.</li>
<li>Make the first use of weight tying, transfer learning with spoken language data and ensemble learning SLT and report baseline results of Transformers in various setups.</li>
<li>Improve on the state-of-art results in German SLT on the RWTH-PHOENIX-Weather 2014T dataset for both sign language gloss to spoken language text translation and end-to-end sign language video to spoken language text translation, and in American SLT on the ASLG-PC12 dataset.</li>
<li>demonstrate how a Spatial-Temporal Multi-Cue network provides better end-to-end performance when use for CSLR in STL than previous approaches and even surpass translation using ground truth glosses.</li>
</ul>
<h4 id="notes-font-colororange去加强了解font-4">Notes <!-- raw HTML omitted -->去加强了解<!-- raw HTML omitted --></h4>
<ul>
<li><i class="fa-regular fa-check-square fa-fw" aria-hidden="true"></i> Sign Language Glossing:</li>
</ul>
<blockquote>
<p>Corresponds to transcribing sign language word-for-word by means of another written language, differing from translation as they merely indicate what each part in a sign language sentence mean, and does not form an appropriate sentence in the writtn language that signifies the same thing.</p>
</blockquote>
<ul>
<li><i class="fa-regular fa-square fa-fw" aria-hidden="true"></i> <a href="https://github.com/kayoyin/transformer-slt"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/kayoyin/transformer-slt<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></li>
<li><i class="fa-regular fa-square fa-fw" aria-hidden="true"></i> Open-NMT library</li>
</ul>
<p><strong>level</strong>: CCF_A   CVPR
<strong>author</strong>: Necati Cihan Camgoz1 (University of Surrey)
<strong>date</strong>: 2018
<strong>keyword</strong>:</p>
<ul>
<li>Sign Language</li>
</ul>
<hr>
<h1 id="paper-neural-sign-language-translation">Paper: Neural Sign Language Translation</h1>
<!-- raw HTML omitted -->
<h4 id="summary-6">Summary</h4>
<ol>
<li>Object at generate spoken language translations from sign language videos, taking into account the different word orders and grammar.</li>
<li>Formalize SLT in the framework of Neural Machine Translation(NMT) for both end-to-end and pretrained settings(using expert knowledge), to jointly learn the spatial representations, the underlying language model, and the mapping between sign and spoken language.</li>
<li>Evaluate on the PHOENIX-WEATHER 2014T datasets.</li>
</ol>
<h4 id="proble-statement-5">Proble Statement</h4>
<p>previous work:</p>
<ul>
<li>computer vision researchers adopted CTC and applied it to weakly labeled visual  problems, such as lip reading, action recognition, hand shape recognition and CSLR.</li>
<li>seq2seq along with Encoder-Decoder network architectures and the emergence of the NMT field.</li>
</ul>
<h4 id="methods-6">Methods</h4>
<ul>
<li>
<p><strong>Question Define:</strong> learn the conditional probability $p(y|x)$ of generating a spoken language sentence $y=(y_1,y_2,&hellip;,y_U)$with U number of words given a sign video $x=(x_1,x_2,&hellip;,x_T)$ with $T$ number of frames.</p>
<ul>
<li>the number of video frames is much more than the number of words in its spoken language tranlation.</li>
<li>the alignment between sing and spoken language sequences are usually unknown and non-monotonic.</li>
</ul>
</li>
<li>
<p><strong>system overview</strong>:</p>
</li>
</ul>
<p><img
    class="lazyload"
    src="/liudongdong1.github.io/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200716161409107.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200716161409107.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200716161409107.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200716161409107.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200716161409107.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200716161409107.png"/></p>
<p><img
    class="lazyload"
    src="/liudongdong1.github.io/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200716230416733.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200716230416733.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200716230416733.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200716230416733.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200716230416733.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200716230416733.png"/></p>
<ul>
<li>Spatial and Word Embedding: to transform the sparse one-hot vector representations. Given a sing video x, using 2D CNN learns  to extract non-linear frame level spatial representation as:</li>
</ul>
<p>$$
f_t=SpatialEmbedding(x_t)
$$</p>
<p>$x_t$: video frame. for word embedding, using a fully connected layer that learns a linear projection from one-hot vectors of spoken language words to a denser space as:   and $g_u$ is the embedded version of the spoken word $y_u$.
$$
g_u=WordEmbedding(y_u)
$$</p>
<ul>
<li>
<p>Tokenization Laryer: in NMT the input and output sequences can be tokenized at many different levels of complexity: characters, words, N-grams or phases.</p>
<ul>
<li>Low level tokenization schemes, like character level, allow smaller vocabularies to be used, but require long term relationships to be maintained.</li>
<li>High level tokenization makes the recognition problem far more difficult due to vastly increased vocabularies.</li>
<li>explore both &ldquo;frame level&rdquo; and &ldquo;gloss level&rdquo; input tokenization, with RNN-HMM to force alignment, the output tokenization is at the word level.</li>
</ul>
</li>
</ul>
<p>$$
z_{1,N}=Tokenization(f_{1:T})
$$</p>
<ul>
<li>**Attention-based Encoder-Decoder Networks: ** learn a mapping function $B(z_{1:N})-&gt;y$, which will maximize the probability $p(y|x)$ based on tokenized embeddings $z_{1:N}$ of a sign video x.
<ul>
<li>$o_n:$ the hidden state produced by recurrent unit n;</li>
<li>$o_{n+1}:$ a zero vector;</li>
<li>$z_n:$ reverse representations of a sequence.</li>
</ul>
</li>
</ul>
<p>$$
O_n=Encoder(z_n,o_{n+1})\
y_u,h_u=Decoder(g_{u-1},h_{u-1})
$$</p>
<p>​            By generating sentences word by word, the Decoder decomposes the conditional probability $p(y|x)$ into ordered conditional probabilities:
$$
p(y|x)=\prod_{u=1}^Up(y_u|y_{1:u-1},h_{sign})
$$</p>
<ul>
<li>
<p><strong>Attention Mechanisms:</strong> deal with information bottleneck caused by representing a whole sign language video with a fixed size vector.</p>
<ul>
<li>suffer from long term dependencies and vanishing gradients.</li>
<li>utilize attention mechanisms to provide additional information to decoding phase, to learn where to focus while generating each word, providing alignment of sign videos and spoken languages  sentences.</li>
<li>$c_u$: context vector</li>
<li>$u$: decoding step</li>
<li>$y_n^u:$ the attention weights, regarded as interpreted as the relevance of an encoder input $z_n$ to generating the word $y_u$.</li>
<li>$tanh(W[h_u;o_n])$: the attention vector.</li>
</ul>
<p>$$
c_u=\sum_{n=1}^Ny_n^uo_n\
For Alignment: y_n^u=exp(score(h_u,o_n))/\sum_{n&rsquo;=1}exp(score(h_u,o_{n&rsquo;}))\
score(h_u,o_n)=\begin{cases}h_u^TWo_n [Multiplication]\
V^Ttanh(W[h_u;o_n]) [Concatenation]
\end{cases}\
y_u,h_u=Decoder(g_{u-1},h_{u-1},a_{u-1})
$$</p>
</li>
</ul>
<h4 id="evaluation-3">Evaluation</h4>
<ul>
<li><strong>Environment</strong>:
<ul>
<li>Dataset: RWTH-PHOENIX-Weather 2014</li>
</ul>
</li>
</ul>
<p><img
    class="lazyload"
    src="/liudongdong1.github.io/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200717083519814.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200717083519814.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200717083519814.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200717083519814.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200717083519814.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200717083519814.png"/></p>
<ul>
<li><strong>Quantitative Experiments:</strong>
<ul>
<li>Gloss2Text: simulate having perfect SLR system as an intermediate tokenization.</li>
<li>sign2Text: covers the end-to-end pipeline translating directly from frame level sign language video into spoken language.</li>
<li>Sign2Gloss2Text: uses SLR system as tokenization layer to add intermediate supervision.</li>
</ul>
</li>
</ul>
<p><img
    class="lazyload"
    src="/liudongdong1.github.io/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200717085452987.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200717085452987.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200717085452987.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200717085452987.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200717085452987.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200717085452987.png"/></p>
<h4 id="conclusion-1">Conclusion</h4>
<ul>
<li>use state-of-the-art seq2seq based deep learning methods to learn: the spatio-temporal representation of the signs, the relation between these signs and how these sign map to the spoken or written language.</li>
<li>the first exploration of the video to text SLT problem.</li>
<li>the first publicly available video segments, gloss annotations and spoken language translations.</li>
<li>A broad range of baseline results on the new corpus including a range of different tokenization and attention schemes in addition to parameter recommendations.</li>
</ul>
<h4 id="notes-font-colororange去加强了解font-5">Notes <!-- raw HTML omitted -->去加强了解<!-- raw HTML omitted --></h4>
<ul>
<li><i class="fa-regular fa-square fa-fw" aria-hidden="true"></i> <a href="https://github.com/neccam/nslt"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/neccam/nslt<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></li>
<li><i class="fa-regular fa-check-square fa-fw" aria-hidden="true"></i> BLEU 介绍</li>
</ul>
<blockquote>
<p>BLEU(Bilingual Evaluation understudy)方法由IBM提出，这种方法认为如果熟译系统魏译文越接近人工翻翻译结果，那么它的翻译质量越高。所以，<!-- raw HTML omitted -->评测关键就在于如何定义系统译文与参考译文之间的相似度。<!-- raw HTML omitted -->BLEU 采用的方式是比较并统计共同出现的n元词的个数，即<!-- raw HTML omitted -->统计同时出现在系统译文和参考译文中的n元词的个数，最后把匹配到的n元词的数目除以系统译文的单词数目，得到评测结果。<!-- raw HTML omitted --></p>
<ul>
<li>$Count(n-gram)$是某个n元词在候选译文中的出现次数，而$MaxRefCount(n-gram)$是该n元词在参考译文中出现的最大次数。</li>
<li><img
    class="lazyload"
    src="/liudongdong1.github.io/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200717085120019.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200717085120019.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200717085120019.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200717085120019.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200717085120019.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200717085120019.png"/></li>
</ul>
<p>$$
Count_{clip}(n-gram)=min{Count(n-gram),MaxRefCount(n-gram)}\</p>
<p>BLEU=BP*exp(\sum_{n=1}^Nw_nlogP_n)\
BP=\begin{cases}1 if C&gt;r\
e^{1-r/c} if c\le r\end{cases}
$$</p>
</blockquote>
<ul>
<li><i class="fa-regular fa-check-square fa-fw" aria-hidden="true"></i> NIST评测方法介绍</li>
</ul>
<blockquote>
<p>NIST(National Institute of standards and Technology)方法是在BLEU方法上的一种改进。它并不是简单的将匹配的n—gram片段数目累加起来，而是求出每个n-gram的信息量(information)，然后累加起来再除以整个译文的n-gram片段数目。</p>
<p><img
    class="lazyload"
    src="/liudongdong1.github.io/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200717085318749.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200717085318749.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200717085318749.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200717085318749.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200717085318749.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200717085318749.png"/></p>
</blockquote>
<p><strong>level</strong>: CCF_A
<strong>author</strong>: Simon Alexanderson, Gustav Eje Henter, Taras Kucherenc
<strong>date</strong>: 2020,<br>
<strong>keyword</strong>:</p>
<ul>
<li>Motion Cpature; Animation; Neural Networks</li>
</ul>
<hr>
<h1 id="paper-style-controllable">Paper: Style-Controllable</h1>
<!-- raw HTML omitted -->
<h4 id="summary-7">Summary</h4>
<ol>
<li>Given a high-level input, the learned network translates these instructions into an appropriate sequence of body poses.</li>
<li>By adapting a deep learning-based motion synthesis method called MoGlow, we propose a new generative model for generating state-of-the-art realistic speech-driven gesticulation.</li>
<li>Produce a battery of different, yet plausible, gestures given the same input speech signal, and demonstrate the ability to exert directorial control over the output style, such as gesture level, speed, symmetry and spacial extent.</li>
</ol>
<h4 id="research-objective-1">Research Objective</h4>
<ul>
<li><strong>Application Area</strong>: Animation, crowd simulation, virtual agents, social robots.</li>
<li><strong>Purpose</strong>:</li>
</ul>
<h4 id="proble-statement-6">Proble Statement</h4>
<ul>
<li>Lack of coherence in gesture production-the same speech utterance is usually accompanied by different gestures from speaker to speaker and time to time.</li>
</ul>
<p>previous work:</p>
<ul>
<li><strong>Data-driven human body-motion generation:</strong>  locomotion, lip movements, and head motion.
<ul>
<li>large variation in the output given the same control.</li>
</ul>
</li>
<li><strong>Deterministic and Probabilistic gesture generation:</strong>  this article are capable of generating unseen gestures.
<ul>
<li>Hasegawa et al.[HKS 18] designed  a speech-driven neural network producing 3D motion sequences.</li>
<li>Kucherenko et al.[KHH19] representation learning for the motion, achieving smoother gestures.</li>
<li>Yoon et al.[YKJ 19] <!-- raw HTML omitted -->using seq2seq on TED-talk data to map text transcriptions to 2D gestures.<!-- raw HTML omitted --></li>
<li>Ahuja et al. [AMMS 19] conditioned pose prediction not only on the audio of the agent, but also on the audio and pose of the interlocutor.</li>
<li>Sadoughi&amp; Busso [SB19] <!-- raw HTML omitted --> used a probabilistic graphical model for mapping speech to gestures, but only experimented on three hand gesture and two head motions.</li>
</ul>
</li>
<li><strong>Style Control:</strong> different levels of animated motion control.
<ul>
<li>Normoyle et al. [NLK*13] used motion editing to identify links between motion statistics and the emotion intensities recognised by human obervers.</li>
</ul>
</li>
<li><strong>Probabilistic generative sequence models:</strong></li>
</ul>
<h4 id="methods-7">Methods</h4>
<ul>
<li><strong>system overview</strong>:</li>
</ul>
<p><img
    class="lazyload"
    src="/liudongdong1.github.io/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200717173653914.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200717173653914.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200717173653914.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200717173653914.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200717173653914.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200717173653914.png"/></p>
<p>【Function 1】Normalising flows and glow</p>
<blockquote>
<p>learn the multi-dimensional next-step distribution of poses $X$ in a stationary autoregressive model of pose sequences $x=[x_1,x_2,x_3,&hellip;,x_t]$ using normalising flows.</p>
</blockquote>
<p>$$
x=f(z)=f_1(f_2(&hellip;f_N(z)))\
z_n(x)=f_n^{-1}(&hellip;f_1^{-1}(x))\
z_0(x)=x;\
z_n(x)=z;
$$</p>
<p>【Function 2】MoGlow for gesture generation</p>
<blockquote>
<p>By using Glow to describe the next-step distribution in an autoregressive model, it also adds control over the output and uses recurrent neural networks for the long-term memory across time.</p>
<ul>
<li>$x_{t-i:t-1}$: the previous poses.</li>
<li>$c_t$: a current control signal.</li>
</ul>
<p><img
    class="lazyload"
    src="/liudongdong1.github.io/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200717205409444.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200717205409444.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200717205409444.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200717205409444.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200717205409444.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200717205409444.png"/></p>
</blockquote>
<p><img
    class="lazyload"
    src="/liudongdong1.github.io/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200717174844348.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200717174844348.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200717174844348.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200717174844348.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200717174844348.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200717174844348.png"/></p>
<h4 id="evaluation-4">Evaluation</h4>
<ul>
<li><strong>Dataset</strong>  :
<ul>
<li><a href="trinityspeechgesture.scss.tcd.ie">Trinty Gesture Dataset:</a> joint speech and gestures, consists of 244 minutes of motion capture and audio of one male actor speaking spontaneously on different topics. the actor&rsquo;s movements were captured with a 20-camera Vicon system and solved to a skeleton with 69 joints.</li>
</ul>
</li>
</ul>
<h4 id="conclusion-2">Conclusion</h4>
<ul>
<li>Adapting MoGlow to speech-driven gesture systhesis.</li>
<li>Adding a framework for high-level control over gesturing style.</li>
<li>Evaluating the use of these methods for probabilistic gesture systhesis.</li>
</ul>
<h4 id="notes-font-colororange去加强了解font-6">Notes <!-- raw HTML omitted -->去加强了解<!-- raw HTML omitted --></h4>
<ul>
<li><i class="fa-regular fa-square fa-fw" aria-hidden="true"></i> github.com/simonalexanderson/StyleGestures</li>
<li><i class="fa-regular fa-check-square fa-fw" aria-hidden="true"></i> Normalizing Flows: <a href="https://gebob19.github.io/normalizing-flows/"target="_blank" rel="external nofollow noopener noreferrer">https://gebob19.github.io/normalizing-flows/<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></li>
</ul>
<blockquote>
<p>Normalizing flows learn an invertible mapping $f:x-&gt;z$, where $X$ is our data distribution and $Z$ is a chosen laten-distribution.</p>
<p>Normalizing flows are part of the generative model family, which includes Variational Autoencoders(VAEs), and generative adversarial networks. Once get the mapping $f$, generate data by sampling $z~p_z$ and then applying the inverse transformation, $f^{-1}(z)=x_{gen}$</p>
<ul>
<li>Advantage:
<ul>
<li>NFs optimize the exact log-likelihood of the data
<ul>
<li>VAEs optimize the lower bound</li>
<li>GANs learn to fool a discriminator network.</li>
</ul>
</li>
<li>NFs infer exact latent-variable values $z$, which are useful for downstream tasks
<ul>
<li>VAE infers a distribution over latent-variable values.</li>
<li>GANs don&rsquo;t have a latent-distribution</li>
</ul>
</li>
<li>Potential for memory savings, with NFs gradient computations scaling constant to their depth.
<ul>
<li>VAE&rsquo;s and GAN&rsquo;s gradient  computations scale linearly to their depth</li>
</ul>
</li>
<li>NFs require only an encoder to be learned.
<ul>
<li>VAEs require encoder and decoder networks</li>
<li>GANs require generative and discriminative networks.</li>
</ul>
</li>
</ul>
</li>
</ul>
</blockquote>
</div>
<div class="post-footer" id="post-footer">
  <div class="post-info">
    <div class="post-info-line">
      <div class="post-info-mod">
        <span title=2023-09-28&#32;23:36:00>更新于 2023-09-28&nbsp;</span>
      </div><div class="post-info-license">
          <span><a rel="license external nofollow noopener noreferrer" href="https://creativecommons.org/licenses/by-nc/4.0/" target="_blank">CC BY-NC 4.0</a></span>
        </div></div>
    <div class="post-info-line">
      <div class="post-info-md"><span><a href="liudongdong1.github.io/sign-language-introduce/index.md" title="阅读原始文档" class="link-to-markdown">阅读原始文档</a></span><span><a href="https://liudongdong1.github.io/edit/master/content/posts%5c%e8%a7%86%e8%a7%89%e8%bf%90%e5%8a%a8%5cDataGlove%5cSign-Language-Introduce.md" title="编辑此页"target="_blank" rel="external nofollow noopener noreferrer" class="link-to-edit">编辑此页</a></span></div>
      <div class="post-info-share">
        <span><a href="javascript:void(0);" title="分享到 Twitter" data-sharer="twitter" data-url="liudongdong1.github.io/sign-language-introduce/" data-title="Sign_Language_Introduce" data-hashtags="CV,Sensing"><i class="fa-brands fa-twitter fa-fw" aria-hidden="true"></i></a>
  <a href="javascript:void(0);" title="分享到 Facebook" data-sharer="facebook" data-url="liudongdong1.github.io/sign-language-introduce/" data-hashtag="CV"><i class="fa-brands fa-facebook-square fa-fw" aria-hidden="true"></i></a>
  <a href="javascript:void(0);" title="分享到 微博" data-sharer="weibo" data-url="liudongdong1.github.io/sign-language-introduce/" data-title="Sign_Language_Introduce" data-image="https://gitee.com/github-25970295/blogImage/raw/master/img/clouds-in-sky-over-fields.jpg"><i class="fa-brands fa-weibo fa-fw" aria-hidden="true"></i></a>
  </span>
      </div>
    </div>
  </div>

  <div class="post-info-more">
    <section class="post-tags"><i class="fa-solid fa-tags fa-fw" aria-hidden="true"></i>&nbsp;<a href="liudongdong1.github.io/tags/cv/">CV</a>,&nbsp;<a href="liudongdong1.github.io/tags/sensing/">Sensing</a></section>
    <section>
      <span><a href="javascript:void(0);" onclick="window.history.back();">返回</a></span>&nbsp;|&nbsp;<span><a href="liudongdong1.github.io/">主页</a></span>
    </section>
  </div>

  <div class="post-nav"><a href="liudongdong1.github.io/similaritymetric/" class="prev" rel="prev" title="SimilarityMetric"><i class="fa-solid fa-angle-left fa-fw" aria-hidden="true"></i>SimilarityMetric</a>
      <a href="liudongdong1.github.io/siamesenetwork/" class="next" rel="next" title="SiameseNetwork">SiameseNetwork<i class="fa-solid fa-angle-right fa-fw" aria-hidden="true"></i></a></div>
</div>
</article></main><footer class="footer">
    <div class="footer-container"><div class="footer-line powered">由 <a href="https://gohugo.io/" target="_blank" rel="external nofollow noopener noreferrer" title="Hugo 0.118.2">Hugo</a> 强力驱动 | 主题 - <a href="https://github.com/hugo-fixit/FixIt" target="_blank" rel="external" title="FixIt v0.2.17-RC"><img class="fixit-icon" src="/liudongdong1.github.io/fixit.min.svg" alt="FixIt logo" />&nbsp;FixIt</a>
        </div><div class="footer-line copyright" itemscope itemtype="http://schema.org/CreativeWork"><i class="fa-regular fa-copyright fa-fw" aria-hidden="true"></i>
            <span itemprop="copyrightYear">2020 - 2023</span><span class="author" itemprop="copyrightHolder">
              <a href="https://liudongdong1.github.io/"target="_blank" rel="external nofollow noopener noreferrer">LiuDongdong</a></span><span class="license footer-divider"><a rel="license external nofollow noopener noreferrer" href="https://creativecommons.org/licenses/by-nc/4.0/" target="_blank">CC BY-NC 4.0</a></span></div><div class="footer-line statistics"><span class="site-time" title='网站运行中 ...'><i class="fa-solid fa-heartbeat fa-fw animate-icon" aria-hidden="true"></i>&nbsp;<span class="run-times">网站运行中 ...</span></span></div><div class="footer-line ibruce">
          <span id="busuanzi_container_site_uv" title='总访客数'><i class="fa-regular fa-user fa-fw" aria-hidden="true"></i>&nbsp;<span id="busuanzi_value_site_uv"><i class="fa-solid fa-spinner fa-spin fa-fw" aria-hidden="true"></i></span></span><span id="busuanzi_container_site_pv" class="footer-divider" title='总访问量'><i class="fa-regular fa-eye fa-fw" aria-hidden="true"></i>&nbsp;<span id="busuanzi_value_site_pv"><i class="fa-solid fa-spinner fa-spin fa-fw" aria-hidden="true"></i></span></span>
        </div></div>
  </footer></div><div class="widgets"><div class="fixed-buttons animate__faster d-none"><div class="fixed-button back-to-top" role="button" aria-label="回到顶部"><i class="fa-solid fa-arrow-up fa-fw" aria-hidden="true"></i><span class="variant-numeric">0%</span>
        </div></div><a href="https://liudongdong1.github.io/" title="在 GitHub 上查看源代码"target="_blank" rel="external nofollow" class="github-corner right d-none-mobile"><svg viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><div id="mask"></div><div class="reading-progress-bar" style="left: 0;top: 0;--bg-progress: #0076ff;--bg-progress-dark: #fff;"></div><noscript>
    <div class="noscript-warning">FixIt 主题在启用 JavaScript 的情况下效果最佳。</div>
  </noscript>
</div><link rel="stylesheet" href="/liudongdong1.github.io/lib/katex/katex.min.css"><link rel="stylesheet" href="/liudongdong1.github.io/lib/cookieconsent/cookieconsent.min.css"><script src="/liudongdong1.github.io/lib/autocomplete/autocomplete.min.js" defer></script><script src="/liudongdong1.github.io/lib/algoliasearch/algoliasearch-lite.umd.min.js" defer></script><script src="/liudongdong1.github.io/lib/lazysizes/lazysizes.min.js" async defer></script><script src="/liudongdong1.github.io/lib/sharer/sharer.min.js" async defer></script><script src="/liudongdong1.github.io/lib/typeit/index.umd.js" defer></script><script src="/liudongdong1.github.io/lib/katex/katex.min.js" defer></script><script src="/liudongdong1.github.io/lib/katex/auto-render.min.js" defer></script><script src="/liudongdong1.github.io/lib/katex/copy-tex.min.js" defer></script><script src="/liudongdong1.github.io/lib/katex/mhchem.min.js" defer></script><script src="/liudongdong1.github.io/lib/cookieconsent/cookieconsent.min.js" defer></script><script src="/liudongdong1.github.io/lib/pangu/pangu.min.js" defer></script><script src="/liudongdong1.github.io/lib/cell-watermark/watermark.min.js" defer></script><script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async defer></script><script>window.config={"code":{"copyTitle":"复制到剪贴板","editLockTitle":"锁定可编辑代码块","editUnLockTitle":"解锁可编辑代码块","editable":true,"maxShownLines":10},"comment":{"enable":false},"cookieconsent":{"content":{"dismiss":"同意","link":"了解更多","message":"本网站使用 Cookies 来改善您的浏览体验。"},"enable":true,"palette":{"button":{"background":"#f0f0f0"},"popup":{"background":"#1aa3ff"}},"theme":"edgeless"},"data":{"typeit-header-subtitle-desktop":"\u003cspan style='font-family: MMT,\"沐目体\";'\u003e吾日三省吾身\u003c/span\u003e","typeit-header-subtitle-mobile":"\u003cspan style='font-family: MMT,\"沐目体\";'\u003e吾日三省吾身\u003c/span\u003e"},"enablePWA":true,"enablePangu":true,"math":{"delimiters":[{"display":true,"left":"$$","right":"$$"},{"display":true,"left":"\\[","right":"\\]"},{"display":true,"left":"\\begin{equation}","right":"\\end{equation}"},{"display":true,"left":"\\begin{equation*}","right":"\\end{equation*}"},{"display":true,"left":"\\begin{align}","right":"\\end{align}"},{"display":true,"left":"\\begin{align*}","right":"\\end{align*}"},{"display":true,"left":"\\begin{alignat}","right":"\\end{alignat}"},{"display":true,"left":"\\begin{alignat*}","right":"\\end{alignat*}"},{"display":true,"left":"\\begin{gather}","right":"\\end{gather}"},{"display":true,"left":"\\begin{CD}","right":"\\end{CD}"},{"display":false,"left":"$","right":"$"},{"display":false,"left":"\\(","right":"\\)"}],"strict":false},"search":{"algoliaAppID":"2R1K9SKLQZ","algoliaIndex":"index.zh-cn","algoliaSearchKey":"4a226aa1c5c98d6859e4d1386adb2bc7","highlightTag":"em","maxResultLength":10,"noResultsFound":"没有找到结果","snippetLength":50,"type":"algolia"},"siteTime":"2020-12-18T16:15:22+08:00","typeit":{"cursorChar":"|","cursorSpeed":1000,"data":{"typeit-header-subtitle-desktop":["typeit-header-subtitle-desktop"],"typeit-header-subtitle-mobile":["typeit-header-subtitle-mobile"]},"duration":-1,"speed":100},"watermark":{"appendto":".wrapper\u003emain","colspacing":30,"content":"\u003cimg class=\"fixit-icon\" src=\"/fixit.min.svg\" alt=\"FixIt logo\" /\u003e FixIt 主题","enable":true,"fontfamily":"inherit","fontsize":0.85,"height":21,"opacity":0.0125,"rotate":15,"rowspacing":60,"width":150}};</script><script src="/liudongdong1.github.io/js/theme.min.js" defer></script><script src="/liudongdong1.github.io/js/custom.min.js" defer></script></body>
</html>
