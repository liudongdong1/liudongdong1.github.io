<!DOCTYPE html>
<html itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">
  <head>
    
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
    <meta name="robots" content="noodp" />
    <title>Multi-Sense - DAY By DAY</title><meta name="author" content="LiuDongdong">
<meta name="author-link" content="https://liudongdong1.github.io/">
<meta name="description" content="level: IEEE Robotics and automation letters date: &lsquo;2019,10&rsquo; keyword: Deep learning in robotics and automation,action segmentation,ergonomic safety. Paper: Ergonomic Risk predition we present a first of its kind end-to-end deep learning system for ergonomic risk assessment during indoor object manipulation using camera videos. Our learning system is based on action segmentation*, where an action class (with a corresponding risk label) is predicted for every video frame. The REBA model assigns" /><meta name="keywords" content='AIOT' /><meta itemprop="name" content="Multi-Sense">
<meta itemprop="description" content="level: IEEE Robotics and automation letters date: &lsquo;2019,10&rsquo; keyword: Deep learning in robotics and automation,action segmentation,ergonomic safety. Paper: Ergonomic Risk predition we present a first of its kind end-to-end deep learning system for ergonomic risk assessment during indoor object manipulation using camera videos. Our learning system is based on action segmentation*, where an action class (with a corresponding risk label) is predicted for every video frame. The REBA model assigns"><meta itemprop="datePublished" content="2020-06-24T19:30:32+00:00" />
<meta itemprop="dateModified" content="2023-09-28T22:55:53+08:00" />
<meta itemprop="wordCount" content="2134"><meta itemprop="image" content="https://liudongdong1.github.io/logo.png"/>
<meta itemprop="keywords" content="AIOT," /><meta property="og:title" content="Multi-Sense" />
<meta property="og:description" content="level: IEEE Robotics and automation letters date: &lsquo;2019,10&rsquo; keyword: Deep learning in robotics and automation,action segmentation,ergonomic safety. Paper: Ergonomic Risk predition we present a first of its kind end-to-end deep learning system for ergonomic risk assessment during indoor object manipulation using camera videos. Our learning system is based on action segmentation*, where an action class (with a corresponding risk label) is predicted for every video frame. The REBA model assigns" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://liudongdong1.github.io/multi-sense/" /><meta property="og:image" content="https://liudongdong1.github.io/logo.png"/><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2020-06-24T19:30:32+00:00" />
<meta property="article:modified_time" content="2023-09-28T22:55:53+08:00" />
<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://liudongdong1.github.io/logo.png"/>

<meta name="twitter:title" content="Multi-Sense"/>
<meta name="twitter:description" content="level: IEEE Robotics and automation letters date: &lsquo;2019,10&rsquo; keyword: Deep learning in robotics and automation,action segmentation,ergonomic safety. Paper: Ergonomic Risk predition we present a first of its kind end-to-end deep learning system for ergonomic risk assessment during indoor object manipulation using camera videos. Our learning system is based on action segmentation*, where an action class (with a corresponding risk label) is predicted for every video frame. The REBA model assigns"/>
<meta name="application-name" content="DAY By DAY">
<meta name="apple-mobile-web-app-title" content="DAY By DAY"><meta name="theme-color" data-light="#f8f8f8" data-dark="#252627" content="#f8f8f8"><meta name="msapplication-TileColor" content="#da532c"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" />
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"><link rel="manifest" href="/site.webmanifest"><link rel="canonical" href="https://liudongdong1.github.io/multi-sense/" /><link rel="prev" href="https://liudongdong1.github.io/bluepaperrecord/" /><link rel="next" href="https://liudongdong1.github.io/imu-trajectory/" /><link rel="stylesheet" href="/css/style.min.css"><link rel="stylesheet" href="/lib/fontawesome-free/all.min.css"><link rel="stylesheet" href="/lib/animate/animate.min.css"><script type="application/ld+json">
  {
    "@context": "http://schema.org",
    "@type": "BlogPosting",
    "headline": "Multi-Sense",
    "inLanguage": "zh-CN",
    "mainEntityOfPage": {
      "@type": "WebPage",
      "@id": "https:\/\/liudongdong1.github.io\/multi-sense\/"
    },"genre": "posts","keywords": "AIOT","wordcount":  2134 ,
    "url": "https:\/\/liudongdong1.github.io\/multi-sense\/","datePublished": "2020-06-24T19:30:32+00:00","dateModified": "2023-09-28T22:55:53+08:00","license": "This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.","publisher": {
      "@type": "Organization",
      "name": "LiuDongdong","logo": "https:\/\/liudongdong1.github.io\/images\/person.png"},"author": {
        "@type": "Person",
        "name": "liudongdong1"
      },"description": ""
  }
  </script></head>
  <body data-header-desktop="auto" data-header-mobile="auto"><script>(window.localStorage?.getItem('theme') ? localStorage.getItem('theme') === 'dark' : ('auto' === 'auto' ? window.matchMedia('(prefers-color-scheme: dark)').matches : 'auto' === 'dark')) && document.body.setAttribute('data-theme', 'dark');</script><div class="wrapper"><script type="text/javascript"
        async
        src="https://cdnjs.cloudflare.com/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$']],
    displayMath: [['$$','$$']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
});

MathJax.Hub.Queue(function() {
    
    
    
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>

<style>
code.has-jax {
    font: inherit;
    font-size: 100%;
    background: inherit;
    border: inherit;
    color: #515151;
}
</style>
<header class="desktop animate__faster" id="header-desktop">
  <div class="header-wrapper" data-github-corner="right">
    <div class="header-title">
      <a href="/" title="DAY By DAY"><img
    class="lazyload logo"
    src="/svg/loading.min.svg"
    data-src="/fixit.min.svg"
    data-srcset="/fixit.min.svg, /fixit.min.svg 1.5x, /fixit.min.svg 2x"
    data-sizes="auto"
    alt="DAY By DAY"
    title="DAY By DAY"/><span class="header-title-text"></span></a><span id="typeit-header-subtitle-desktop" class="typeit header-subtitle"></span></div>
    <nav>
      <ul class="menu"><li class="menu-item">
              <a
                class="menu-link"
                href="/posts/"
                
                
              ><i class="fa-solid fa-archive fa-fw fa-sm" aria-hidden="true"></i> 所有文章</a></li><li class="menu-item">
              <a
                class="menu-link"
                href="/categories/"
                
                
              ><i class="fa-solid fa-th fa-fw fa-sm" aria-hidden="true"></i> 分类</a></li><li class="menu-item">
              <a
                class="menu-link"
                href="/tags/"
                
                
              ><i class="fa-solid fa-tags fa-fw fa-sm" aria-hidden="true"></i> 标签</a></li><li class="menu-item">
              <a
                class="menu-link"
                href="/friends/"
                title="友情链接"
                
              ><i class="fa-solid fa-users fa-fw fa-sm" aria-hidden="true"></i> 友链</a></li><li class="menu-item">
              <a
                class="menu-link"
                href="/about/"
                
                
              ><i class="fa-solid fa-info-circle fa-fw fa-sm" aria-hidden="true"></i> 关于</a></li><li class="menu-item delimiter"></li><li class="menu-item language">
            <span role="button" aria-label="选择语言" title="选择语言">简体中文<i class="dropdown-icon fa-solid fa-chevron-down" aria-hidden="true"></i>
            </span>
            <ul class="sub-menu"><li class="menu-item">没有更多翻译</li></ul>
          </li><li class="menu-item search" id="search-desktop">
            <input type="text" placeholder="搜索文章标题或内容 ..." id="search-input-desktop">
            <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-desktop" title="搜索">
              <i class="fa-solid fa-search fa-fw" aria-hidden="true"></i>
            </a>
            <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-desktop" title="清空">
              <i class="fa-solid fa-times-circle fa-fw" aria-hidden="true"></i>
            </a>
            <span class="search-button search-loading" id="search-loading-desktop">
              <i class="fa-solid fa-spinner fa-fw fa-spin" aria-hidden="true"></i>
            </span>
          </li><li class="menu-item theme-switch" title="切换主题">
          <i class="fa-solid fa-adjust fa-fw" aria-hidden="true"></i>
        </li>
      </ul>
    </nav>
  </div>
</header><header class="mobile animate__faster" id="header-mobile">
  <div class="header-container">
    <div class="header-wrapper">
      <div class="header-title">
        <a href="/" title="DAY By DAY"><img
    class="lazyload logo"
    src="/svg/loading.min.svg"
    data-src="/fixit.min.svg"
    data-srcset="/fixit.min.svg, /fixit.min.svg 1.5x, /fixit.min.svg 2x"
    data-sizes="auto"
    alt="/fixit.min.svg"
    title="/fixit.min.svg"/><span class="header-title-text"></span></a><span id="typeit-header-subtitle-mobile" class="typeit header-subtitle"></span></div>
      <div class="menu-toggle" id="menu-toggle-mobile">
        <span></span><span></span><span></span>
      </div>
    </div>
    <nav>
      <ul class="menu" id="menu-mobile"><li class="search-wrapper">
            <div class="search mobile" id="search-mobile">
              <input type="text" placeholder="搜索文章标题或内容 ..." id="search-input-mobile">
              <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-mobile" title="搜索">
                <i class="fa-solid fa-search fa-fw" aria-hidden="true"></i>
              </a>
              <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-mobile" title="清空">
                <i class="fa-solid fa-times-circle fa-fw" aria-hidden="true"></i>
              </a>
              <span class="search-button search-loading" id="search-loading-mobile">
                <i class="fa-solid fa-spinner fa-fw fa-spin" aria-hidden="true"></i>
              </span>
            </div>
            <a href="javascript:void(0);" class="search-cancel" id="search-cancel-mobile">
              取消
            </a>
          </li><li
              class="menu-item"
            ><a
                  class="menu-link"
                  href="/posts/"
                  
                  
                ><i class="fa-solid fa-archive fa-fw fa-sm" aria-hidden="true"></i> 所有文章</a></li><li
              class="menu-item"
            ><a
                  class="menu-link"
                  href="/categories/"
                  
                  
                ><i class="fa-solid fa-th fa-fw fa-sm" aria-hidden="true"></i> 分类</a></li><li
              class="menu-item"
            ><a
                  class="menu-link"
                  href="/tags/"
                  
                  
                ><i class="fa-solid fa-tags fa-fw fa-sm" aria-hidden="true"></i> 标签</a></li><li
              class="menu-item"
            ><a
                  class="menu-link"
                  href="/friends/"
                  title="友情链接"
                  
                ><i class="fa-solid fa-users fa-fw fa-sm" aria-hidden="true"></i> 友链</a></li><li
              class="menu-item"
            ><a
                  class="menu-link"
                  href="/about/"
                  
                  
                ><i class="fa-solid fa-info-circle fa-fw fa-sm" aria-hidden="true"></i> 关于</a></li><li
              class="menu-item text-center"
            ><a
                  class="menu-link"
                  href="/"
                  title="GitHub"
                  
                ><i class='fa-brands fa-github fa-fw' aria-hidden='true'></i> </a></li><li class="menu-item theme-switch" title="切换主题">
          <i class="fa-solid fa-adjust fa-fw" aria-hidden="true"></i>
        </li><li class="menu-item language">
            <span role="button" aria-label="选择语言" title="选择语言">简体中文<i class="dropdown-icon fa-solid fa-chevron-down" aria-hidden="true"></i>
            </span>
            <select class="language-select" onchange="location = this.value;"><option disabled>没有更多翻译</option></select>
          </li></ul>
    </nav>
  </div>
</header><div class="search-dropdown desktop">
    <div id="search-dropdown-desktop"></div>
  </div>
  <div class="search-dropdown mobile">
    <div id="search-dropdown-mobile"></div>
  </div><main class="container" data-page-style="normal"><aside class="toc" id="toc-auto"><h2 class="toc-title">目录 <i class="toc-icon fa-solid fa-angle-down fa-fw"></i></h2>
      <div class="toc-content" id="toc-content-auto"></div></aside>

  <aside class="aside-custom" id="aside-sakana">
    

<div class="sakana-widget">
  <div class="sakana-item" id="takina-widget"></div>
  <div class="sakana-item" id="chisato-widget"></div>
</div>
<script>
  function initSakanaWidget() {
    const takina = SakanaWidget.getCharacter('takina')
    SakanaWidget.registerCharacter('takina-slow', takina);
    new SakanaWidget({
      character: 'takina-slow',
      controls: false,
      autoFit: true,
      stroke: {
        color: "#b4b4b4",
        width: 2
      }
    }).mount('#takina-widget');

    const chisato = SakanaWidget.getCharacter('chisato')
    SakanaWidget.registerCharacter('chisato-slow', chisato);
    new SakanaWidget({
      character: 'chisato-slow',
      controls: false,
      autoFit: true,
      stroke: {
        color: "#b4b4b4",
        width: 2
      }
    }).mount('#chisato-widget');
  }
</script>
<script async onload="initSakanaWidget()" src="https://cdn.jsdelivr.net/npm/sakana-widget@2.3.0/lib/sakana.min.js">
</script></aside>

  <article class="page single">
    <div class="header"><h1 class="single-title animate__animated animate__flipInX">
        <span>Multi-Sense</span>
      </h1></div><div class="post-meta">
      <div class="post-meta-line"><span class="post-author"><span class="author"><i class="fa-solid fa-user-circle" aria-hidden="true"></i>
      liudongdong1</span></span>
          <span class="post-category">收录于 <a href="/categories/"><i class="fa-regular fa-folder fa-fw"></i>&nbsp;Categories</a>&ensp;<a href="/categories/aiot/"><i class="fa-regular fa-folder fa-fw"></i>&nbsp;AIOT</a></span></div>
      <div class="post-meta-line"><span title=2020-06-24&#32;19:30:32>
            <i class="fa-regular fa-calendar-alt fa-fw"></i>&nbsp;<time datetime="2020-06-24" >2020-06-24</time>
          </span>&nbsp;<i class="fa-solid fa-pencil-alt fa-fw"></i>&nbsp;约 2134 字&nbsp;
        <i class="fa-regular fa-clock fa-fw"></i>&nbsp;预计阅读 5 分钟&nbsp;<span id="busuanzi_container_page_pv" class="busuanzi_visitors comment-visitors" data-flag-title="Multi-Sense">
            <i class="fa-regular fa-eye fa-fw"></i>&nbsp;<span id="busuanzi_value_page_pv">-</span>&nbsp;次阅读
          </span>&nbsp;</div>
    </div><div class="featured-image"><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/apple-iphone-smartphone-technology-1.jpg"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/apple-iphone-smartphone-technology-1.jpg, https://gitee.com/github-25970295/blogImage/raw/master/img/apple-iphone-smartphone-technology-1.jpg 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/apple-iphone-smartphone-technology-1.jpg 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/apple-iphone-smartphone-technology-1.jpg"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/apple-iphone-smartphone-technology-1.jpg"/></div><div class="details toc" id="toc-static" kept="true">
        <div class="details-summary toc-title">
          <span>目录</span>
          <span><i class="details-icon fa-solid fa-angle-right"></i></span>
        </div>
        <div class="details-content toc-content" id="toc-content-static"><nav id="TableOfContents">
  <ul>
    <li><a href="#paper-unimodal-dynamic-handgesture">Paper: Unimodal Dynamic HandGesture</a>
      <ul>
        <li></li>
      </ul>
    </li>
  </ul>

  <ul>
    <li>
      <ul>
        <li></li>
      </ul>
    </li>
  </ul>
</nav></div>
      </div><div
      class="content"
      id="content"
      
      
    ><p><strong>level</strong>: IEEE Robotics and automation letters
<strong>date</strong>: &lsquo;2019,10&rsquo;
<strong>keyword</strong>:</p>
<ul>
<li>Deep learning in robotics and automation,action segmentation,ergonomic safety.</li>
</ul>
<h1 id="paper-ergonomic-risk-predition">Paper: Ergonomic Risk predition</h1>
<!-- raw HTML omitted -->
<p>we present a first of its kind <em>end-to-end</em> deep learning  system for ergonomic risk assessment during indoor object manipulation using camera videos. Our learning system is based on action segmentation*, where an action class (with a corresponding risk label) is predicted for every video frame.</p>
<p>The REBA model assigns scores to the human poses, within a range of 1–15, on a frame-by-frame basis by accounting for the joints motions and angles, load conditions, and activity repetitions. An action with an overall score of less than 3 is labeled as ergonomically safe, a score between 3–7 is deemed to be medium risk that requires monitoring, and every other action is considered high risk that needs attention.</p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191101153604076.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191101153604076.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191101153604076.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191101153604076.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191101153604076.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191101153604076.png"/></p>
<hr>
<p><strong>level</strong>: International Conference on Open Source System and Technology(ICOSST)
<strong>author</strong>:
<strong>date</strong>:
<strong>keyword</strong>:</p>
<ul>
<li>.Smart Home,Android, RaspiberryPi ,OpenCV</li>
</ul>
<hr>
<h1 id="paper">Paper:</h1>
<!-- raw HTML omitted -->
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191101155544019.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191101155544019.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191101155544019.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191101155544019.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191101155544019.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191101155544019.png"/></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191101155623730.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191101155623730.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191101155623730.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191101155623730.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191101155623730.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191101155623730.png"/></p>
<h1 id="paper-hauar">Paper： Hauar</h1>
<p><strong>home automation using action recognition</strong>： using action recognition to  fully automate the home appliances. We recognize the three  actions of a person (sitting, standing and lying) along with the  recognition of an empty room.  使用了PIR Motion 传感器</p>
<h1 id="paper-hybrid-user-action-prediction">Paper： Hybrid user action prediction</h1>
<p>Hybrid user action prediction system for automated home using association rules and ontology：  based on the frequent pattern (FP)-growth and ontology graphs for home automation systems. Their proposed system simulates the human prediction actions by adding common sense data by utilizing the advantages of the ontology graph and the FP-growth to find a better solution in predicting home user actions for automated systems .使用了室内开关数据预测，关联分析，马尔可夫状态转换，聚类</p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191101161301257.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191101161301257.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191101161301257.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191101161301257.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191101161301257.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191101161301257.png"/></p>
<hr>
<p><strong>level</strong>:IEEE access Special section on mobile multimedia for healthcare
<strong>author</strong>: M.Shamim Hossain
<strong>date</strong>:2017
<strong>keyword</strong>:</p>
<ul>
<li>.wireless sensors,inhome activities</li>
</ul>
<hr>
<h1 id="paper-smarthomemonitor">Paper: SmartHomeMonitor</h1>
<!-- raw HTML omitted -->
<p>no previous research has considered automatically segmenting data during the process of data acquisition.       humans may perform two or more actionsconcurently</p>
<p>​    The  proposed technique defifines the annotation process as an optimization problem in which each incoming action is modeled to increase the probability of assigning a given set of actions  to a specifific activity. Hidden Markov Model (HMM) and Conditional Random Field (CRF) are applied to model the joint probability and features of activities in terms of actions.</p>
<p>(1) modeling activity actions as a set of states and transitions using HMM, (2) modeling a transition feature function that embeds temporal and spatial relations among consecutive actions, and (3) defifining the segmentation problem as an optimization problem to minimize</p>
<p>This paper focuses only on data segmentation, in which an agent must decide the</p>
<p>size of the block of actions that represents an activity.</p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191101163430182.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191101163430182.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191101163430182.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191101163430182.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191101163430182.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191101163430182.png"/></p>
<h1 id="paper-1">Paper:</h1>
<p><strong>Multi-Task Deep Learning for Pedestrian detection ,action recognition and Time to cross prediction:</strong> on pedestrian detection and pedestrian action recognition but also on estimating if the pedestrian’s action presents a risky situation according to time to cross the street. We propose 1) a pedestrian detection and action recognition component based, on RetinaNet;   2) an estimation of the time to cross the street for multiple pedestrians using a recurrent neural network. For each pedestrian, the recurrent network estimates the pedestrian’s action intention in order to predict the time to cross the street. We based our experiments on the JAAD dataset, and show that integrating multiple pedestrian action tags for the detection part when merge with a recurrent neural network (LSTM) allows a signifificant performance improvement.</p>
<h1 id="paper-skeleton-basedonlineactpre">Paper: Skeleton-basedOnlineActPre</h1>
<p><strong>Skeleton-Based Online Action Prediction Using Scale Selection Network</strong>:</p>
<p>Action Prediction, Scale Selection, Sliding Window, Dilated Convolution, Skeleton Data.  online action prediction in streaming 3D skeleton sequences. A dilated convolutional network is introduced to model the motion dynamics in temporal dimension via a sliding window over the temporal axis. Since there are signifificant temporal scale variations in the observed part of the ongoing action at different time steps, a novel window scale selection method is proposed to make our network focus on the performed part of the ongoing action and try to suppress the possible incoming interference from the previous actions at each step.</p>
<h1 id="papertask-oriented-grasping">Paper:Task-Oriented Grasping</h1>
<p><strong>Learning Task-Oriented Grasping for Tool Manipulation from Simulated Self-Supervision</strong>:</p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191104134717938.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191104134717938.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191104134717938.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191104134717938.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191104134717938.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191104134717938.png"/></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191104134753308.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191104134753308.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191104134753308.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191104134753308.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191104134753308.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191104134753308.png"/></p>
<p><strong>level</strong>: CCF_A    CVPR
<strong>author</strong>: MahdiAbavisani
<strong>date</strong>: 2019
<strong>keyword</strong>:</p>
<ul>
<li>hand gesture recognitioln</li>
</ul>
<hr>
<h2 id="paper-unimodal-dynamic-handgesture">Paper: Unimodal Dynamic HandGesture</h2>
<!-- raw HTML omitted -->
<h4 id="summary">Summary</h4>
<ol>
<li>present an efficient approach for leveraging the knowledge from multiple modalities in training unimodal 3D convolutional neural networks for the task of dynamic hand gesture recognition.</li>
<li>dedicate separate networks per available modality and enforce them to collaborate and learn to develop networks with common semantics and better representations</li>
</ol>
<h4 id="research-objective">Research Objective</h4>
<ul>
<li><strong>Application Area</strong>: human-computer interaction, sign language recognition, gaming and virtual reality control</li>
<li><strong>Purpose</strong>:  multimodal learning and unimodal testing.</li>
</ul>
<h4 id="proble-statement">Proble Statement</h4>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200312135338379.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200312135338379.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200312135338379.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200312135338379.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200312135338379.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200312135338379.png"/></p>
<p>previous work:</p>
<ul>
<li>most hand gesture recognition methods exploit multiple sensors such as visible RGB cameras, depth camera or compute an extra modality like optical flow.</li>
<li>Dynamic hand gesture recognition: same to video analysis approaches, derive properties such as appearance, motion cues, or body skeleton to perform classification
<ul>
<li>3D-CNN-base hand gesture recognition methods, Multi-sensor system: fuses streams of data from multiple sensors including short-range radar, color and depth sensors for recongition.</li>
<li>ResC3D combines multimodal data and expoits an attention model</li>
</ul>
</li>
<li>Transfer Learning:  an agent is independently trained on a source task, then another agent uses the knowledge of the source agent by repurposing the learned features or transferring them to improve its learning on a target task.</li>
<li></li>
</ul>
<h4 id="methods">Methods</h4>
<ul>
<li><strong>Problem Formulation</strong>:</li>
</ul>
<p>the stream of data is available in M modalities， and there are M classifier networks with similar architectures that classify based on their corresponding input, we aim to improve the learning process by transferring the knowledge of different modalities.</p>
<ul>
<li><strong>system overview</strong>:</li>
</ul>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200312140717210.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200312140717210.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200312140717210.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200312140717210.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200312140717210.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200312140717210.png"/></p>
<p>【Qustion 1】how to alignment the spatiotemporal semantic multi-modal data?</p>
<ul>
<li>assume that different modalities of the input videos are aligned over the time and spatial positions, the networks are expected to have the same understanding and share semantics for spatial positions and frame of the input videos across the different modalities.</li>
</ul>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200312141904565.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200312141904565.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200312141904565.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200312141904565.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200312141904565.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200312141904565.png"/></p>
<p>【Qustion 2】 how to avoid Negative Transfer throw multi-modal data?</p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200312142405938.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200312142405938.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200312142405938.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200312142405938.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200312142405938.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200312142405938.png"/></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://cdn.pixabay.com/photo/2015/06/24/16/36/office-820390__340.jpgimage-20200312142343590.png"
    data-srcset="https://cdn.pixabay.com/photo/2015/06/24/16/36/office-820390__340.jpgimage-20200312142343590.png, https://cdn.pixabay.com/photo/2015/06/24/16/36/office-820390__340.jpgimage-20200312142343590.png 1.5x, https://cdn.pixabay.com/photo/2015/06/24/16/36/office-820390__340.jpgimage-20200312142343590.png 2x"
    data-sizes="auto"
    alt="https://cdn.pixabay.com/photo/2015/06/24/16/36/office-820390__340.jpgimage-20200312142343590.png"
    title="https://cdn.pixabay.com/photo/2015/06/24/16/36/office-820390__340.jpgimage-20200312142343590.png"/></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200312142431917.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200312142431917.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200312142431917.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200312142431917.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200312142431917.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200312142431917.png"/></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200312142521221.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200312142521221.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200312142521221.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200312142521221.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200312142521221.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200312142521221.png"/></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200312142536425.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200312142536425.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200312142536425.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200312142536425.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200312142536425.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200312142536425.png"/></p>
<h4 id="evaluation">Evaluation</h4>
<ul>
<li><strong>Environment</strong>:
<ul>
<li>Dataset:
<ul>
<li>VIVA hand gesture dataset: for studying natural human activities in real-world driving settings, 19 hand gesture classes collected from 8 subjects.</li>
<li>EgoGesture Dataset: for the task of egocentric gesture recognition, contains 24161 hand gesture clips of 83 classes of gestures performed by 50 subjects, including both static and dynamic gesture.   <!-- raw HTML omitted -->重点了解下这个数据集<!-- raw HTML omitted --></li>
<li>NVGestures datasets: multiple sensors and from multiple viewpoints for studying human-computer interfaces, contains 1532 dynamic hand gestures inside a car simulator with artificial lighting conditions.<!-- raw HTML omitted -->重点了解下这个数据集<!-- raw HTML omitted --></li>
</ul>
</li>
</ul>
</li>
<li><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200312143418786.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200312143418786.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200312143418786.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200312143418786.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200312143418786.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200312143418786.png"/></li>
<li><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200312143443999.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200312143443999.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200312143443999.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200312143443999.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200312143443999.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200312143443999.png"/></li>
</ul>
<h4 id="conclusion">Conclusion</h4>
<ul>
<li>propose a new framework for single modality networks in dynamic hand gesture recognition task to learn from multiple modalities</li>
<li>introduce the SSA loss to share the knowledge of single modality networks</li>
<li>develop the focal regularization parameter for avoiding negative transfer.</li>
</ul>
<h4 id="notes-font-colororange去加强了解font">Notes <!-- raw HTML omitted -->去加强了解<!-- raw HTML omitted --></h4>
<ul>
<li>Multi-sensorsystemfordriver’shand-gesturerecognition. InAutomaticFaceandGestureRecognition(FG)</li>
<li>Deep multimodal learning: A survey on recent advances and trends</li>
<li>Online detection and classiﬁcation of dynamic hand gestures with recurrent 3d convolutional neural network</li>
<li>Multimodal gesture recognition based on the resc3d network</li>
<li>ImageNet+Kinectics pre-trained networks</li>
<li>Quo vadis, action recognition? a new model and the kinetics dataset</li>
<li>该论文没有代码，介绍了模型实现但是现在无法复现。</li>
<li>VGG16+LSTM [13]                          这些网络模型都是可以学习使用的</li>
<li>C3D+LSTM+RLSTM [8]</li>
<li>I3D</li>
<li>C3D</li>
<li>VGG16</li>
<li>HOG+HOG2 [29]</li>
</ul>
<p><strong>level</strong>: <em>IEEE international conference and workshops on automatic face and gesture recognition</em>
<strong>author</strong>: Pavlo Molchanov (NVIDIA Research,)
<strong>date</strong>:  2015
<strong>keyword</strong>:</p>
<ul>
<li>hand gesture understand</li>
</ul>
<hr>
<h1 id="paper-multi-sensor-system">Paper: Multi-sensor System</h1>
<!-- raw HTML omitted -->
<h4 id="summary-1">Summary</h4>
<ol>
<li>using short-range radar, a color camera, and a depth camera which together make the system robust against variable lighting conditions.</li>
<li>present a jointly calibrate the radar and depth sensors.</li>
<li>employ concolutional deep neural networks to fuse data from multiple sensors and to classify the gestures.(10 different gestured indoors and outdoors in a car)</li>
</ol>
<h4 id="research-objective-1">Research Objective</h4>
<ul>
<li><strong>Purpose</strong>:  fuse multi-modal data to recognise the hand gesture.</li>
</ul>
<h4 id="proble-statement-1">Proble Statement</h4>
<ul>
<li>color sensors are ineffective under low-light conditions at night</li>
<li>commodity depth cameras that typically use projected IR signals are ineffective under direct bright sunlight.</li>
<li>suffer from from the presence of harsh shadows and hand self-occlusion</li>
<li>micro-Doppler signatures of acoustic signals have also been developed ,while acoustical sensors for gesture recognition are not directly applicable inside vehicles because of the presence of significant ambient acoustical noise.</li>
</ul>
<h4 id="methods-1">Methods</h4>
<ul>
<li><strong>system overview</strong>:</li>
</ul>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200313165655037.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200313165655037.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200313165655037.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200313165655037.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200313165655037.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200313165655037.png"/></p>
<ul>
<li>build a prototype radar system, with an operational range of &lt;1m, the system measures the range(z) and angular velocity(v) of moving objects in the scene, and estimates their azimuth(x) 方位角and elevation海拔(y) angles.  (FMCW)[27, 28] 重点了解雷达特性</li>
</ul>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200313170714490.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200313170714490.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200313170714490.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200313170714490.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200313170714490.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200313170714490.png"/></p>
<p>【Radar Relative】</p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200313170714490.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200313170714490.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200313170714490.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200313170714490.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200313170714490.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200313170714490.png"/></p>
<p>【Calibration】</p>
<ul>
<li>assume a rigid transfermation exists between the optical imaging centers of the radar and depth sensors.</li>
<li>experiment: concurrently observe 3D coordinates of the center of a moving spherical ball of radius 3cm with both sensor. Using linear-squares optimization.</li>
</ul>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200313171052341.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200313171052341.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200313171052341.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200313171052341.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200313171052341.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200313171052341.png"/></p>
<p>【Gesture Detection and classifier】</p>
<ul>
<li>assume that a true gesture occurs only when the radar detects signiﬁcant motion, i.e., with velocity above a conﬁgurable threshold (0.05m/s), roughly in the center of the FOV of the UI。</li>
<li>The duration of a true gesture is assumed to be between 0.3 and 3 seconds. The gesture ends when no motion is observed by the radar continuously for 0.5 seconds.</li>
<li>normalize the depth values of the detected hand region to the range of [0,1], and generate a mask for hand region. And conert RGB image of the hand to a single grayscale image with values in the range[0,1]</li>
</ul>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200313171445019.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200313171445019.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200313171445019.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200313171445019.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200313171445019.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200313171445019.png"/></p>
<ul>
<li>temporally normalize the gestures to 60 frames by re-sampling them via nearest neibor interpolation.</li>
</ul>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200313171637448.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200313171637448.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200313171637448.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200313171637448.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200313171637448.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200313171637448.png"/></p>
<h4 id="evaluation-1">Evaluation</h4>
<ul>
<li><strong>Environment</strong>:
<ul>
<li>Dataset:</li>
</ul>
</li>
</ul>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200313171706749.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200313171706749.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200313171706749.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200313171706749.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200313171706749.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200313171706749.png"/></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200313171715796.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200313171715796.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200313171715796.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200313171715796.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200313171715796.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200313171715796.png"/></p>
<h4 id="conclusion-1">Conclusion</h4>
<ul>
<li>a novel multi-sensor gesture recognition system that effectively combines imaging and radar sensors</li>
<li>use of the radar sensor for dynamic gesture segmentation, recognition,and reduced power consumption</li>
<li>demonstration of a real-time illumination robust geture interface for the challenging use case of vehicles.</li>
</ul>
<h4 id="notes-font-colororange去加强了解font-1">Notes <!-- raw HTML omitted -->去加强了解<!-- raw HTML omitted --></h4>
<ul>
<li>Voronoi diagram ：根据点集划分的区域到点的距离最近的特点，其在地理学、气象学、结晶学、航天、核物理学、机器人等领域具有广泛的应用。如在障碍物点集中，规避障碍寻找最佳路径。<img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200313181217743.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200313181217743.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200313181217743.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200313181217743.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200313181217743.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200313181217743.png"/></li>
<li>FMCW ：即调频连续波。FMCW技术和脉冲雷达技术是两种在高精度雷达测距中使用的技术。其基本原理为发射波为高频连续波，其频率随时间按照三角波规律变化。接收的回波频率与发射的频率变化规律相同，都是三角波规律，只是有一个时间差，利用这个微小的时间差可计算出目标距离。</li>
<li>“Monopulse range-doppler FMCW radar signal processing for spatial localization of moving targets</li>
<li><a href="https://zhuanlan.zhihu.com/p/77474295"target="_blank" rel="external nofollow noopener noreferrer">https://zhuanlan.zhihu.com/p/77474295<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></li>
<li><a href="https://training.eeworld.com.cn/TI/show/course/4132"target="_blank" rel="external nofollow noopener noreferrer">https://training.eeworld.com.cn/TI/show/course/4132<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>  FMCW 学习链接</li>
</ul>
</div>
<div class="post-footer" id="post-footer">
  <div class="post-info">
    <div class="post-info-line">
      <div class="post-info-mod">
        <span title=2023-09-28&#32;22:55:53>更新于 2023-09-28&nbsp;</span>
      </div><div class="post-info-license">
          <span><a rel="license external nofollow noopener noreferrer" href="https://creativecommons.org/licenses/by-nc/4.0/" target="_blank">CC BY-NC 4.0</a></span>
        </div></div>
    <div class="post-info-line">
      <div class="post-info-md"><span><a href="/multi-sense/index.md" title="阅读原始文档" class="link-to-markdown">阅读原始文档</a></span><span><a href="https://liudongdong1.github.io/edit/master/content/posts%5cAIOT%5cMultiSensing%5cMulti-Sense.md" title="编辑此页"target="_blank" rel="external nofollow noopener noreferrer" class="link-to-edit">编辑此页</a></span></div>
      <div class="post-info-share">
        <span><a href="javascript:void(0);" title="分享到 Twitter" data-sharer="twitter" data-url="https://liudongdong1.github.io/multi-sense/" data-title="Multi-Sense" data-hashtags="AIOT"><i class="fa-brands fa-twitter fa-fw" aria-hidden="true"></i></a>
  <a href="javascript:void(0);" title="分享到 Facebook" data-sharer="facebook" data-url="https://liudongdong1.github.io/multi-sense/" data-hashtag="AIOT"><i class="fa-brands fa-facebook-square fa-fw" aria-hidden="true"></i></a>
  <a href="javascript:void(0);" title="分享到 微博" data-sharer="weibo" data-url="https://liudongdong1.github.io/multi-sense/" data-title="Multi-Sense" data-image="https://gitee.com/github-25970295/blogImage/raw/master/img/apple-iphone-smartphone-technology-1.jpg"><i class="fa-brands fa-weibo fa-fw" aria-hidden="true"></i></a>
  </span>
      </div>
    </div>
  </div>

  <div class="post-info-more">
    <section class="post-tags"><i class="fa-solid fa-tags fa-fw" aria-hidden="true"></i>&nbsp;<a href="/tags/aiot/">AIOT</a></section>
    <section>
      <span><a href="javascript:void(0);" onclick="window.history.back();">返回</a></span>&nbsp;|&nbsp;<span><a href="/">主页</a></span>
    </section>
  </div>

  <div class="post-nav"><a href="/bluepaperrecord/" class="prev" rel="prev" title="BlueTooth Paper"><i class="fa-solid fa-angle-left fa-fw" aria-hidden="true"></i>BlueTooth Paper</a>
      <a href="/imu-trajectory/" class="next" rel="next" title="IMU Trajectory">IMU Trajectory<i class="fa-solid fa-angle-right fa-fw" aria-hidden="true"></i></a></div>
</div>
</article></main><footer class="footer">
    <div class="footer-container"><div class="footer-line powered">由 <a href="https://gohugo.io/" target="_blank" rel="external nofollow noopener noreferrer" title="Hugo 0.118.2">Hugo</a> 强力驱动 | 主题 - <a href="https://github.com/hugo-fixit/FixIt" target="_blank" rel="external" title="FixIt v0.2.17-RC"><img class="fixit-icon" src="/fixit.min.svg" alt="FixIt logo" />&nbsp;FixIt</a>
        </div><div class="footer-line copyright" itemscope itemtype="http://schema.org/CreativeWork"><i class="fa-regular fa-copyright fa-fw" aria-hidden="true"></i>
            <span itemprop="copyrightYear">2020 - 2023</span><span class="author" itemprop="copyrightHolder">
              <a href="https://liudongdong1.github.io/"target="_blank" rel="external nofollow noopener noreferrer">LiuDongdong</a></span><span class="license footer-divider"><a rel="license external nofollow noopener noreferrer" href="https://creativecommons.org/licenses/by-nc/4.0/" target="_blank">CC BY-NC 4.0</a></span></div><div class="footer-line statistics"><span class="site-time" title='网站运行中 ...'><i class="fa-solid fa-heartbeat fa-fw animate-icon" aria-hidden="true"></i>&nbsp;<span class="run-times">网站运行中 ...</span></span></div><div class="footer-line ibruce">
          <span id="busuanzi_container_site_uv" title='总访客数'><i class="fa-regular fa-user fa-fw" aria-hidden="true"></i>&nbsp;<span id="busuanzi_value_site_uv"><i class="fa-solid fa-spinner fa-spin fa-fw" aria-hidden="true"></i></span></span><span id="busuanzi_container_site_pv" class="footer-divider" title='总访问量'><i class="fa-regular fa-eye fa-fw" aria-hidden="true"></i>&nbsp;<span id="busuanzi_value_site_pv"><i class="fa-solid fa-spinner fa-spin fa-fw" aria-hidden="true"></i></span></span>
        </div></div>
  </footer></div><div class="widgets"><div class="fixed-buttons animate__faster d-none"><div class="fixed-button back-to-top" role="button" aria-label="回到顶部"><i class="fa-solid fa-arrow-up fa-fw" aria-hidden="true"></i><span class="variant-numeric">0%</span>
        </div></div><a href="https://liudongdong1.github.io/" title="在 GitHub 上查看源代码"target="_blank" rel="external nofollow" class="github-corner right d-none-mobile"><svg viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><div id="mask"></div><div class="reading-progress-bar" style="left: 0;top: 0;--bg-progress: #0076ff;--bg-progress-dark: #fff;"></div><noscript>
    <div class="noscript-warning">FixIt 主题在启用 JavaScript 的情况下效果最佳。</div>
  </noscript>
</div><link rel="stylesheet" href="/lib/katex/katex.min.css"><link rel="stylesheet" href="/lib/cookieconsent/cookieconsent.min.css"><script src="/lib/autocomplete/autocomplete.min.js" defer></script><script src="/lib/algoliasearch/algoliasearch-lite.umd.min.js" defer></script><script src="/lib/lazysizes/lazysizes.min.js" async defer></script><script src="/lib/sharer/sharer.min.js" async defer></script><script src="/lib/typeit/index.umd.js" defer></script><script src="/lib/katex/katex.min.js" defer></script><script src="/lib/katex/auto-render.min.js" defer></script><script src="/lib/katex/copy-tex.min.js" defer></script><script src="/lib/katex/mhchem.min.js" defer></script><script src="/lib/cookieconsent/cookieconsent.min.js" defer></script><script src="/lib/pangu/pangu.min.js" defer></script><script src="/lib/cell-watermark/watermark.min.js" defer></script><script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async defer></script><script>window.config={"code":{"copyTitle":"复制到剪贴板","editLockTitle":"锁定可编辑代码块","editUnLockTitle":"解锁可编辑代码块","editable":true,"maxShownLines":10},"comment":{"enable":false},"cookieconsent":{"content":{"dismiss":"同意","link":"了解更多","message":"本网站使用 Cookies 来改善您的浏览体验。"},"enable":true,"palette":{"button":{"background":"#f0f0f0"},"popup":{"background":"#1aa3ff"}},"theme":"edgeless"},"data":{"typeit-header-subtitle-desktop":"\u003cspan style='font-family: MMT,\"沐目体\";'\u003e吾日三省吾身\u003c/span\u003e","typeit-header-subtitle-mobile":"\u003cspan style='font-family: MMT,\"沐目体\";'\u003e吾日三省吾身\u003c/span\u003e"},"enablePWA":true,"enablePangu":true,"math":{"delimiters":[{"display":true,"left":"$$","right":"$$"},{"display":true,"left":"\\[","right":"\\]"},{"display":true,"left":"\\begin{equation}","right":"\\end{equation}"},{"display":true,"left":"\\begin{equation*}","right":"\\end{equation*}"},{"display":true,"left":"\\begin{align}","right":"\\end{align}"},{"display":true,"left":"\\begin{align*}","right":"\\end{align*}"},{"display":true,"left":"\\begin{alignat}","right":"\\end{alignat}"},{"display":true,"left":"\\begin{alignat*}","right":"\\end{alignat*}"},{"display":true,"left":"\\begin{gather}","right":"\\end{gather}"},{"display":true,"left":"\\begin{CD}","right":"\\end{CD}"},{"display":false,"left":"$","right":"$"},{"display":false,"left":"\\(","right":"\\)"}],"strict":false},"search":{"algoliaAppID":"2R1K9SKLQZ","algoliaIndex":"index.zh-cn","algoliaSearchKey":"4a226aa1c5c98d6859e4d1386adb2bc7","highlightTag":"em","maxResultLength":10,"noResultsFound":"没有找到结果","snippetLength":50,"type":"algolia"},"siteTime":"2020-12-18T16:15:22+08:00","typeit":{"cursorChar":"|","cursorSpeed":1000,"data":{"typeit-header-subtitle-desktop":["typeit-header-subtitle-desktop"],"typeit-header-subtitle-mobile":["typeit-header-subtitle-mobile"]},"duration":-1,"speed":100},"watermark":{"appendto":".wrapper\u003emain","colspacing":30,"content":"\u003cimg class=\"fixit-icon\" src=\"/fixit.min.svg\" alt=\"FixIt logo\" /\u003e FixIt 主题","enable":true,"fontfamily":"inherit","fontsize":0.85,"height":21,"opacity":0.0125,"rotate":15,"rowspacing":60,"width":150}};</script><script src="/js/theme.min.js" defer></script><script src="/js/custom.min.js" defer></script></body>
</html>
