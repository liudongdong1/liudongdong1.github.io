<!DOCTYPE html>
<html itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">
  <head>
    
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
    <meta name="robots" content="noodp" />
    <title>MediaPipe - DAY By DAY</title><meta name="author" content="LiuDongdong">
<meta name="author-link" content="https://liudongdong1.github.io/">
<meta name="description" content="MediaPipe is the simplest way for researchers and developers to build world-class ML solutions and applications for mobile, desktop/cloud, web and IoT devices. 1. Introduce End-to-End acceleration: built-in fast ML inference and processing accelerated even on common hardware Build one, deploy anywhere: Unified solution works across Android, iOS, desktop/cloud, web and IoT Ready-to-use solutions: cutting-edge ML solutions demonstrating full power of the framework Free and Open Source 2.PaperReading level: CCF_A" /><meta name="keywords" content='Mediapipe' /><meta itemprop="name" content="MediaPipe">
<meta itemprop="description" content="MediaPipe is the simplest way for researchers and developers to build world-class ML solutions and applications for mobile, desktop/cloud, web and IoT devices. 1. Introduce End-to-End acceleration: built-in fast ML inference and processing accelerated even on common hardware Build one, deploy anywhere: Unified solution works across Android, iOS, desktop/cloud, web and IoT Ready-to-use solutions: cutting-edge ML solutions demonstrating full power of the framework Free and Open Source 2.PaperReading level: CCF_A"><meta itemprop="datePublished" content="2020-08-24T21:59:57+00:00" />
<meta itemprop="dateModified" content="2023-12-31T13:47:03+08:00" />
<meta itemprop="wordCount" content="3425"><meta itemprop="image" content="https://liudongdong1.github.io/logo.png"/>
<meta itemprop="keywords" content="Mediapipe," /><meta property="og:title" content="MediaPipe" />
<meta property="og:description" content="MediaPipe is the simplest way for researchers and developers to build world-class ML solutions and applications for mobile, desktop/cloud, web and IoT devices. 1. Introduce End-to-End acceleration: built-in fast ML inference and processing accelerated even on common hardware Build one, deploy anywhere: Unified solution works across Android, iOS, desktop/cloud, web and IoT Ready-to-use solutions: cutting-edge ML solutions demonstrating full power of the framework Free and Open Source 2.PaperReading level: CCF_A" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://liudongdong1.github.io/mediapipe/" /><meta property="og:image" content="https://liudongdong1.github.io/logo.png"/><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2020-08-24T21:59:57+00:00" />
<meta property="article:modified_time" content="2023-12-31T13:47:03+08:00" />
<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://liudongdong1.github.io/logo.png"/>

<meta name="twitter:title" content="MediaPipe"/>
<meta name="twitter:description" content="MediaPipe is the simplest way for researchers and developers to build world-class ML solutions and applications for mobile, desktop/cloud, web and IoT devices. 1. Introduce End-to-End acceleration: built-in fast ML inference and processing accelerated even on common hardware Build one, deploy anywhere: Unified solution works across Android, iOS, desktop/cloud, web and IoT Ready-to-use solutions: cutting-edge ML solutions demonstrating full power of the framework Free and Open Source 2.PaperReading level: CCF_A"/>
<meta name="application-name" content="DAY By DAY">
<meta name="apple-mobile-web-app-title" content="DAY By DAY"><meta name="theme-color" data-light="#f8f8f8" data-dark="#252627" content="#f8f8f8"><meta name="msapplication-TileColor" content="#da532c"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" />
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"><link rel="manifest" href="/site.webmanifest"><link rel="canonical" href="https://liudongdong1.github.io/mediapipe/" /><link rel="prev" href="https://liudongdong1.github.io/visionnlpcommend/" /><link rel="next" href="https://liudongdong1.github.io/anchorintroduce/" /><link rel="stylesheet" href="/css/style.min.css"><link rel="stylesheet" href="/lib/fontawesome-free/all.min.css"><link rel="stylesheet" href="/lib/animate/animate.min.css"><script type="application/ld+json">
  {
    "@context": "http://schema.org",
    "@type": "BlogPosting",
    "headline": "MediaPipe",
    "inLanguage": "zh-CN",
    "mainEntityOfPage": {
      "@type": "WebPage",
      "@id": "https:\/\/liudongdong1.github.io\/mediapipe\/"
    },"genre": "posts","keywords": "Mediapipe","wordcount":  3425 ,
    "url": "https:\/\/liudongdong1.github.io\/mediapipe\/","datePublished": "2020-08-24T21:59:57+00:00","dateModified": "2023-12-31T13:47:03+08:00","license": "This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.","publisher": {
      "@type": "Organization",
      "name": "LiuDongdong","logo": "https:\/\/liudongdong1.github.io\/images\/person.png"},"author": {
        "@type": "Person",
        "name": "liudongdong1"
      },"description": ""
  }
  </script></head>
  <body data-header-desktop="auto" data-header-mobile="auto"><script>(window.localStorage?.getItem('theme') ? localStorage.getItem('theme') === 'dark' : ('auto' === 'auto' ? window.matchMedia('(prefers-color-scheme: dark)').matches : 'auto' === 'dark')) && document.body.setAttribute('data-theme', 'dark');</script><div class="wrapper"><script type="text/javascript"
        async
        src="https://cdnjs.cloudflare.com/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$']],
    displayMath: [['$$','$$']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
});

MathJax.Hub.Queue(function() {
    
    
    
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>

<style>
code.has-jax {
    font: inherit;
    font-size: 100%;
    background: inherit;
    border: inherit;
    color: #515151;
}
</style>
<header class="desktop animate__faster" id="header-desktop">
  <div class="header-wrapper" data-github-corner="right">
    <div class="header-title">
      <a href="/" title="DAY By DAY"><img
    class="lazyload logo"
    src="/svg/loading.min.svg"
    data-src="/fixit.min.svg"
    data-srcset="/fixit.min.svg, /fixit.min.svg 1.5x, /fixit.min.svg 2x"
    data-sizes="auto"
    alt="DAY By DAY"
    title="DAY By DAY"/><span class="header-title-text"></span></a><span id="typeit-header-subtitle-desktop" class="typeit header-subtitle"></span></div>
    <nav>
      <ul class="menu"><li class="menu-item">
              <a
                class="menu-link"
                href="/posts/"
                
                
              ><i class="fa-solid fa-archive fa-fw fa-sm" aria-hidden="true"></i> 所有文章</a></li><li class="menu-item">
              <a
                class="menu-link"
                href="/categories/"
                
                
              ><i class="fa-solid fa-th fa-fw fa-sm" aria-hidden="true"></i> 分类</a></li><li class="menu-item">
              <a
                class="menu-link"
                href="/tags/"
                
                
              ><i class="fa-solid fa-tags fa-fw fa-sm" aria-hidden="true"></i> 标签</a></li><li class="menu-item">
              <a
                class="menu-link"
                href="/friends/"
                title="友情链接"
                
              ><i class="fa-solid fa-users fa-fw fa-sm" aria-hidden="true"></i> 友链</a></li><li class="menu-item">
              <a
                class="menu-link"
                href="/about/"
                
                
              ><i class="fa-solid fa-info-circle fa-fw fa-sm" aria-hidden="true"></i> 关于</a></li><li class="menu-item delimiter"></li><li class="menu-item language">
            <span role="button" aria-label="选择语言" title="选择语言">简体中文<i class="dropdown-icon fa-solid fa-chevron-down" aria-hidden="true"></i>
            </span>
            <ul class="sub-menu"><li class="menu-item">没有更多翻译</li></ul>
          </li><li class="menu-item search" id="search-desktop">
            <input type="text" placeholder="搜索文章标题或内容 ..." id="search-input-desktop">
            <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-desktop" title="搜索">
              <i class="fa-solid fa-search fa-fw" aria-hidden="true"></i>
            </a>
            <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-desktop" title="清空">
              <i class="fa-solid fa-times-circle fa-fw" aria-hidden="true"></i>
            </a>
            <span class="search-button search-loading" id="search-loading-desktop">
              <i class="fa-solid fa-spinner fa-fw fa-spin" aria-hidden="true"></i>
            </span>
          </li><li class="menu-item theme-switch" title="切换主题">
          <i class="fa-solid fa-adjust fa-fw" aria-hidden="true"></i>
        </li>
      </ul>
    </nav>
  </div>
</header><header class="mobile animate__faster" id="header-mobile">
  <div class="header-container">
    <div class="header-wrapper">
      <div class="header-title">
        <a href="/" title="DAY By DAY"><img
    class="lazyload logo"
    src="/svg/loading.min.svg"
    data-src="/fixit.min.svg"
    data-srcset="/fixit.min.svg, /fixit.min.svg 1.5x, /fixit.min.svg 2x"
    data-sizes="auto"
    alt="/fixit.min.svg"
    title="/fixit.min.svg"/><span class="header-title-text"></span></a><span id="typeit-header-subtitle-mobile" class="typeit header-subtitle"></span></div>
      <div class="menu-toggle" id="menu-toggle-mobile">
        <span></span><span></span><span></span>
      </div>
    </div>
    <nav>
      <ul class="menu" id="menu-mobile"><li class="search-wrapper">
            <div class="search mobile" id="search-mobile">
              <input type="text" placeholder="搜索文章标题或内容 ..." id="search-input-mobile">
              <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-mobile" title="搜索">
                <i class="fa-solid fa-search fa-fw" aria-hidden="true"></i>
              </a>
              <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-mobile" title="清空">
                <i class="fa-solid fa-times-circle fa-fw" aria-hidden="true"></i>
              </a>
              <span class="search-button search-loading" id="search-loading-mobile">
                <i class="fa-solid fa-spinner fa-fw fa-spin" aria-hidden="true"></i>
              </span>
            </div>
            <a href="javascript:void(0);" class="search-cancel" id="search-cancel-mobile">
              取消
            </a>
          </li><li
              class="menu-item"
            ><a
                  class="menu-link"
                  href="/posts/"
                  
                  
                ><i class="fa-solid fa-archive fa-fw fa-sm" aria-hidden="true"></i> 所有文章</a></li><li
              class="menu-item"
            ><a
                  class="menu-link"
                  href="/categories/"
                  
                  
                ><i class="fa-solid fa-th fa-fw fa-sm" aria-hidden="true"></i> 分类</a></li><li
              class="menu-item"
            ><a
                  class="menu-link"
                  href="/tags/"
                  
                  
                ><i class="fa-solid fa-tags fa-fw fa-sm" aria-hidden="true"></i> 标签</a></li><li
              class="menu-item"
            ><a
                  class="menu-link"
                  href="/friends/"
                  title="友情链接"
                  
                ><i class="fa-solid fa-users fa-fw fa-sm" aria-hidden="true"></i> 友链</a></li><li
              class="menu-item"
            ><a
                  class="menu-link"
                  href="/about/"
                  
                  
                ><i class="fa-solid fa-info-circle fa-fw fa-sm" aria-hidden="true"></i> 关于</a></li><li
              class="menu-item text-center"
            ><a
                  class="menu-link"
                  href="/"
                  title="GitHub"
                  
                ><i class='fa-brands fa-github fa-fw' aria-hidden='true'></i> </a></li><li class="menu-item theme-switch" title="切换主题">
          <i class="fa-solid fa-adjust fa-fw" aria-hidden="true"></i>
        </li><li class="menu-item language">
            <span role="button" aria-label="选择语言" title="选择语言">简体中文<i class="dropdown-icon fa-solid fa-chevron-down" aria-hidden="true"></i>
            </span>
            <select class="language-select" onchange="location = this.value;"><option disabled>没有更多翻译</option></select>
          </li></ul>
    </nav>
  </div>
</header><div class="search-dropdown desktop">
    <div id="search-dropdown-desktop"></div>
  </div>
  <div class="search-dropdown mobile">
    <div id="search-dropdown-mobile"></div>
  </div><main class="container" data-page-style="normal"><aside class="toc" id="toc-auto"><h2 class="toc-title">目录 <i class="toc-icon fa-solid fa-angle-down fa-fw"></i></h2>
      <div class="toc-content" id="toc-content-auto"></div></aside>

  <aside class="aside-custom" id="aside-sakana">
    

<div class="sakana-widget">
  <div class="sakana-item" id="takina-widget"></div>
  <div class="sakana-item" id="chisato-widget"></div>
</div>
<script>
  function initSakanaWidget() {
    const takina = SakanaWidget.getCharacter('takina')
    SakanaWidget.registerCharacter('takina-slow', takina);
    new SakanaWidget({
      character: 'takina-slow',
      controls: false,
      autoFit: true,
      stroke: {
        color: "#b4b4b4",
        width: 2
      }
    }).mount('#takina-widget');

    const chisato = SakanaWidget.getCharacter('chisato')
    SakanaWidget.registerCharacter('chisato-slow', chisato);
    new SakanaWidget({
      character: 'chisato-slow',
      controls: false,
      autoFit: true,
      stroke: {
        color: "#b4b4b4",
        width: 2
      }
    }).mount('#chisato-widget');
  }
</script>
<script async onload="initSakanaWidget()" src="https://cdn.jsdelivr.net/npm/sakana-widget@2.3.0/lib/sakana.min.js">
</script></aside>

  <article class="page single">
    <div class="header"><h1 class="single-title animate__animated animate__flipInX">
        <span>MediaPipe</span>
      </h1></div><div class="post-meta">
      <div class="post-meta-line"><span class="post-author"><span class="author"><i class="fa-solid fa-user-circle" aria-hidden="true"></i>
      liudongdong1</span></span>
          <span class="post-category">收录于 <a href="/categories/"><i class="fa-regular fa-folder fa-fw"></i>&nbsp;Categories</a>&ensp;<a href="/categories/framework/"><i class="fa-regular fa-folder fa-fw"></i>&nbsp;Framework</a></span></div>
      <div class="post-meta-line"><span title=2020-08-24&#32;21:59:57>
            <i class="fa-regular fa-calendar-alt fa-fw"></i>&nbsp;<time datetime="2020-08-24" >2020-08-24</time>
          </span>&nbsp;<i class="fa-solid fa-pencil-alt fa-fw"></i>&nbsp;约 3425 字&nbsp;
        <i class="fa-regular fa-clock fa-fw"></i>&nbsp;预计阅读 7 分钟&nbsp;<span id="busuanzi_container_page_pv" class="busuanzi_visitors comment-visitors" data-flag-title="MediaPipe">
            <i class="fa-regular fa-eye fa-fw"></i>&nbsp;<span id="busuanzi_value_page_pv">-</span>&nbsp;次阅读
          </span>&nbsp;</div>
    </div><div class="featured-image"><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20200715085515063.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20200715085515063.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20200715085515063.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20200715085515063.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20200715085515063.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20200715085515063.png"/></div><div class="details toc" id="toc-static" kept="true">
        <div class="details-summary toc-title">
          <span>目录</span>
          <span><i class="details-icon fa-solid fa-angle-right"></i></span>
        </div>
        <div class="details-content toc-content" id="toc-content-static"><nav id="TableOfContents">
  <ul>
    <li><a href="#paper-mediapipe">Paper: MediaPipe</a>
      <ul>
        <li></li>
      </ul>
    </li>
    <li><a href="#paper-blazepose">Paper: BlazePose</a>
      <ul>
        <li></li>
      </ul>
    </li>
    <li><a href="#paper-mediapipe-hands">Paper: MediaPipe Hands</a>
      <ul>
        <li></li>
      </ul>
    </li>
    <li><a href="#paper-pupiltracking">Paper: PupilTracking</a>
      <ul>
        <li></li>
      </ul>
    </li>
    <li><a href="#3-案例">3. 案例</a>
      <ul>
        <li><a href="#31-video-reframing">3.1. Video Reframing</a></li>
        <li><a href="#32-real-time-3d-object-detection"><strong>3.2.</strong> Real-Time 3D Object Detection</a></li>
        <li><a href="#33-afred-camera">3.3. Afred Camera</a></li>
        <li><a href="#34-iris-tracking-and-depth-estimation">3.4. Iris Tracking and Depth Estimation</a></li>
      </ul>
    </li>
  </ul>
</nav></div>
      </div><div
      class="content"
      id="content"
      
      
    ><blockquote>
<p><a href="https://google.github.io/mediapipe/"target="_blank" rel="external nofollow noopener noreferrer">MediaPipe<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a> is the simplest way for researchers and developers to build world-class ML solutions and applications for mobile, desktop/cloud, web and IoT devices.</p>
</blockquote>
<h1 id="1-introduce">1. Introduce</h1>
<ol>
<li><strong>End-to-End acceleration</strong>: <em>built-in fast ML inference and processing accelerated even on common hardware</em></li>
<li><strong>Build one, deploy anywhere</strong>: <em>Unified solution works across Android, iOS, desktop/cloud, web and IoT</em></li>
<li><strong>Ready-to-use solutions:</strong> cutting-edge ML solutions demonstrating full power of the framework</li>
<li><strong>Free and Open Source</strong></li>
</ol>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200824115410.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200824115410.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200824115410.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200824115410.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200824115410.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200824115410.png"/></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200824115501.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200824115501.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200824115501.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200824115501.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200824115501.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200824115501.png"/></p>
<h1 id="2paperreading">2.PaperReading</h1>
<p><strong>level</strong>: CCF_A CVPR
<strong>author</strong>:  <a href="mailto:Mediapipe@google.com">Mediapipe@google.com</a>
<strong>date</strong>: 2019.6.14
<strong>keyword</strong>:</p>
<ul>
<li>Perception, Framework</li>
</ul>
<hr>
<h2 id="paper-mediapipe">Paper: MediaPipe</h2>
<!-- raw HTML omitted -->
<h4 id="summary">Summary</h4>
<ol>
<li>propose a framework consists of three main parts:
<ul>
<li>inference from sensory data;</li>
<li>a set of tools for performance evaluation;</li>
<li>a collection of re-usable inference and processing components called calculators.</li>
</ul>
</li>
<li>MediaPipe is targeted towards applications in the audio/video processing domain and not limited to the scope of modeling the performance of concurrent systems.</li>
</ol>
<h4 id="research-objective">Research Objective</h4>
<h4 id="proble-statement">Proble Statement</h4>
<ul>
<li>select and develop corresponding machine learning algorithms and models;</li>
<li>build a series of prototypes and demos;</li>
<li>balance resource consumption against the quality of the solutions;</li>
<li>identify and mitigate problematic cases;</li>
</ul>
<blockquote>
<p>Modifying a perception application to incorporate additional processing steps or inference models can be difficult due to excessive coupling between steps;</p>
<p>different platforms consuming time and involves optimizing inference and processing steps to run correctly and efficiently on a target device.</p>
</blockquote>
<h4 id="system-overview">System overview</h4>
<blockquote>
<p>MediaPipe allows to prototype a pipeline incrementally as a directed graph of components where each component is a calculator; The graph is specified using <strong>GraphConfig</strong> protocol buffer and then run using a Graph object; the calculators are connected by data Stream, each Stream represents a time-series of data Packets;</p>
</blockquote>
<p>【<strong>Module one</strong>】 <strong>Component</strong></p>
<ul>
<li><strong>Packets：</strong> consists of a numeric timestamp and a shared pointer to an immutable payload.</li>
<li><strong>Streams</strong>: carries a sequence of packets whose timestamps must be monotonically increasing. Each input stream receives a separate copy of the packets from  an output stream, and maintains its own queue to allow the receiving node to consume the packets.</li>
<li><strong>Side packets:</strong> a side-packets connection between nodes carries a single packet with an unspecified timestamp.</li>
<li><strong>Calculators:</strong> a calculator may receive zero or more output streams or its packets, comprise of four essential methods: <strong>GetContract(),Open(),Process(),Close()</strong>;</li>
<li><strong>Graph:</strong> the context of all processing, contains a collection of nodes joined by directed connections along which packets can flow, some constraints are as follows:
<ul>
<li>each stream and side packet must be produced by one source;</li>
<li>the type of an input stream/side packet must be compatible with the type of the output stream/side packet to which it is connected;</li>
<li>each node&rsquo;s connections are compatible with its contract.</li>
</ul>
</li>
<li><strong>GraphConfig:</strong> a specification that describes the topology and functionality of the graph</li>
</ul>
<h4 id="implementation">Implementation</h4>
<blockquote>
<p>scheduling logic and powerful synchronization primitives to process time-series in a customizable fashion.</p>
</blockquote>
<p>【Scheduling】</p>
<ul>
<li>each graph has at least one scheduler queue, each scheduler has exactly one executor, nodes are statically assigned to a queue.</li>
<li>each node has a scheduling state, <strong>not ready, ready, running</strong>;</li>
<li>when a node becomes ready for execution, a task is added to the corresponding scheduler queue, the nodes are topologically sorted and assigned a priority based on the graph&rsquo;s layout;</li>
</ul>
<p>【<strong>Synchronization</strong>】</p>
<blockquote>
<p>mediapipe graph execution is decentralized: there is no global clock, and different nodes can process data from different timestamps at the same time;</p>
</blockquote>
<ul>
<li>the packets pushed into a given stream must have monotonically increasing timestamps;</li>
<li>each stream has a timestamp bound, which is the lowest possible timestamp allowed for a new packet on the stream.</li>
</ul>
<p>【<strong>Input policies</strong>】</p>
<blockquote>
<p>Synchronization is handled locally on each node, using input policy specified by the node.</p>
</blockquote>
<ul>
<li>if packets with the same timestamp are provided on multiple input streams, they will always be processed together regardless of their arrival order in real time;</li>
<li>input set are processed in strictly ascending timestamp order;</li>
<li>no packets are dropped, and the processing is fully deterministic;</li>
<li>the node becomes ready to process data as soon as possible given the guarantees above.</li>
</ul>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200831085500.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200831085500.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200831085500.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200831085500.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200831085500.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200831085500.png"/></p>
<p>【<strong>Flow Control</strong>】</p>
<blockquote>
<p>packets may be generated faster than they can be process, flow control is necessary to keep resource usage under control;</p>
</blockquote>
<ul>
<li>a simple back-pressure system: throttles the execution of upstream nodes when the packets buffered on a stream reach limit; by maintaining deterministic behavior and includes a deadlock avoidance system that relaxes configured limits;</li>
<li>a richer node-based system: consists of inserting special nodes which can drop packets according to real-time constraints;</li>
</ul>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200831090000.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200831090000.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200831090000.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200831090000.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200831090000.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200831090000.png"/></p>
<p>【GPU support】</p>
<p>【Opaque buffer type】</p>
<p>【OpenGL support】</p>
<h4 id="tools">Tools</h4>
<ul>
<li><strong>Tracker</strong>：follow individual packets across a graph and records timing events along the way, recording a <strong>TraceEvent</strong> structure with several data fields event_time, packet_timestamp, packet_data_id, node_id, and stream_id;</li>
<li><strong>Visualizer</strong>: help to understand the topology and overall behavior of their pipelines:
<ul>
<li>Timeline View</li>
<li>Graph view</li>
</ul>
</li>
</ul>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200831090454.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200831090454.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200831090454.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200831090454.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200831090454.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200831090454.png"/></p>
<h4 id="experiment">Experiment</h4>
<ul>
<li>Object Detection</li>
</ul>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200831090533.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200831090533.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200831090533.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200831090533.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200831090533.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200831090533.png"/></p>
<blockquote>
<ul>
<li>In the detection branch, a frame-selection node ﬁrst selects frames to go through detection based on limiting frequency or scene-change analysis, and passes them to the detector while dropping the irrelevant frames.</li>
<li>The objectdetection node consumes an ML model and the associated label map as input side packets, performs ML inference on the incoming selected frames using an inference engine (e.g., [12] or [2]) and outputs detection results.</li>
<li>the tracking branch updates earlier detections and advances their locations to the current camera frame.</li>
<li>the detection-merging node compares results and merges them with detections from earlier frames removing duplicate results based on their location in the frame and/or class proximity.</li>
</ul>
</blockquote>
<ul>
<li>FaceLandmark
<ul>
<li>demultiplexing node splits the packets in the input stream into interleaving subsets of packets, with subset going into a separate output stream;</li>
</ul>
</li>
</ul>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200831091200.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200831091200.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200831091200.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200831091200.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200831091200.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200831091200.png"/></p>
<h4 id="notes-font-colororange去加强了解font">Notes <!-- raw HTML omitted -->去加强了解<!-- raw HTML omitted --></h4>
<ul>
<li>Beam[1]; Apache beam: An advanced uniﬁed programming model.</li>
<li>Dataflow[5];  Thedataﬂowmodel: Apracticalapproach to balancing correctness, latency, and cost in massive-scale, unbounded,out-of-orderdataprocessing</li>
<li>Gstream[8];  https: //gstreamer.freedesktop.org/,</li>
<li>opencv4.0(graph api[9])；OpenCV Graph API. Intel Corporation, 2018.</li>
</ul>
<p><strong>level</strong>:
<strong>author</strong>: Valentin Bazarevsky (google research)
<strong>date</strong>: 2020
<strong>keyword</strong>:</p>
<ul>
<li>Pose estimation</li>
</ul>
<blockquote>
<p>Bazarevsky, Valentin, Ivan Grishchenko, Karthik Raveendran, Tyler Zhu, Fan Zhang, and Matthias Grundmann. &ldquo;BlazePose: On-device Real-time Body Pose tracking.&rdquo; <em>arXiv preprint arXiv:2006.10204</em> (2020).</p>
</blockquote>
<hr>
<h2 id="paper-blazepose">Paper: BlazePose</h2>
<!-- raw HTML omitted -->
<ol>
<li>blazepose, a lightweight convolutional neural network architecture for human pose estimation that is tailored for real-time inference on mobile devices.</li>
<li>produces 33 body keypoints for a single person and runs at over 30    frames per second on a Pixel 2 phone.</li>
</ol>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200821200224.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200821200224.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200821200224.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200821200224.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200821200224.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200821200224.png"/></p>
<h4 id="research-objective-1">Research Objective</h4>
<ul>
<li><strong>Application Area</strong>: fitness tracking ; sign language recognition; Yoga;</li>
<li><strong>Purpose</strong>:  estimate human pose from images or video with edge devices.</li>
</ul>
<h4 id="methods">Methods</h4>
<ul>
<li><strong>system overview</strong>:</li>
</ul>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200821195628.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200821195628.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200821195628.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200821195628.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200821195628.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200821195628.png"/></p>
<blockquote>
<p>a lightweight body pose detector followed by a pose tracker network, the tracker predicts keypoint coordinates, the presence of the person on the current frame, and the refined region of interest for the current frame, when the tracker indicates that there is no human present, we re-run the detector network on the next frame.</p>
</blockquote>
<p>【Person Detector】use a fast on-device face-detector as a proxy for a person detector. the middle point between the person&rsquo;s hips, the size of the circle circumscribing the whole person, and incline(the angle between the lines connecting the two mid-shoulder and mid-hip points).</p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200821212805.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200821212805.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200821212805.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200821212805.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200821212805.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200821212805.png"/></p>
<blockquote>
<ul>
<li>
<p>use the heatmap and offset loss only in the training stage and remove the corresponding output layers from the model before running the inference. use the heatmap to supervise the lightweight embedding,</p>
</li>
<li>
<p>stack a tiny encoder-decoder heatmap-based network and subsequent regression encoder network.</p>
</li>
<li>
<p>utilize skip-connections between all the stages of the network to achieve a balance between high and low-level features.</p>
</li>
<li>
<p>for invisible points, simulate occlusions during training and introduce a per-point visibility classifier that indicates whether a particular point is occluded and if the position prediction is deemed inaccurate.</p>
</li>
</ul>
</blockquote>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200821213938.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200821213938.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200821213938.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200821213938.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200821213938.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200821213938.png"/></p>
<h4 id="notes-font-colororange去加强了解font-1">Notes <!-- raw HTML omitted -->去加强了解<!-- raw HTML omitted --></h4>
<ul>
<li><i class="fa-regular fa-square fa-fw" aria-hidden="true"></i> 运行代码，学习代码中模型结构，以及数据格式</li>
</ul>
<p><strong>level</strong>:
<strong>author</strong>: Fan Zhang(google research)
<strong>date</strong>: 2020
<strong>keyword</strong>:</p>
<ul>
<li>hand pose estimation;</li>
</ul>
<blockquote>
<p>Zhang, Fan, et al. &ldquo;MediaPipe Hands: On-device Real-time Hand Tracking.&rdquo; <em>arXiv preprint arXiv:2006.10214</em> (2020).</p>
</blockquote>
<hr>
<h2 id="paper-mediapipe-hands">Paper: MediaPipe Hands</h2>
<!-- raw HTML omitted -->
<ol>
<li>the pipeline consists of two models:
<ul>
<li>a palm detector, that is providing a bounding box of a hand to</li>
<li>a hand landmark model, that is predicting the hand skeleton.</li>
</ul>
</li>
<li>an efficient two-stage hand tracking pipeline that can track multiple hands in real-time on mobile devices.</li>
<li>a hand pose estimation model that is capable of predicting 2.5D hand pose with only RGB input.</li>
<li>an open source hand tracking pipelines as a ready-to-go solution on variety of platforms, including android, ios, web, and desktop PCs.</li>
</ol>
<h4 id="methods-1">Methods</h4>
<ul>
<li><strong>system overview</strong>:</li>
</ul>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200821214746.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200821214746.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200821214746.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200821214746.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200821214746.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200821214746.png"/></p>
<p><strong>【BlazePalm Detector】</strong></p>
<blockquote>
<ul>
<li>work across a variety of hand sizes with a large scale span</li>
<li>be able to detect occluded and self-occluded hands</li>
<li>the hands is dynamic, lack of contrast patterns.</li>
</ul>
</blockquote>
<ol>
<li>train a palm detector instead of a hand detector;</li>
<li>use an encoder-decoder feature extractro similar to FPN for larger scene-context awareness even for small objects.</li>
<li>minimize the focal loss during training to support a large amount of anchors resulting from the high scale variance.</li>
</ol>
<p><strong>【Hand Landmark Model】</strong></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200821220035.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200821220035.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200821220035.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200821220035.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200821220035.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200821220035.png"/></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200821220256.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200821220256.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200821220256.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200821220256.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200821220256.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200821220256.png"/></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200821220320.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200821220320.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200821220320.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200821220320.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200821220320.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200821220320.png"/></p>
<h4 id="notes-font-colororange去加强了解font-2">Notes <!-- raw HTML omitted -->去加强了解<!-- raw HTML omitted --></h4>
<blockquote>
<p>2.5D游戏仅仅是在2D游戏基础上把视角横向旋转了45度。2.5D视角带来的最核心的问题是每个图片和其他图片之间的遮挡关系如何处理，才能更符合人类对3D世界的常识性认知呢，也就是用2D的方式来模拟3D。2D游戏的做法很简单粗暴。2D游戏世界中每一个物件都会用一个2维坐标来表示其位置，x表示其横向位置，y表示其纵向位置。<strong>当一个物件的y值越小，也就是其越靠近画面底部，则渲染顺序越靠后</strong>。就像一个画家在Photoshop上作画一样，离相机越近的图层要越后面画，才能盖住离相机远的图层，所以画家要从远到近地画。3D游戏的渲染，简单来说可以理解为将三维数据在二维平面上做投影的过程。所以所谓的3D游戏，呈现在玩家面前依然是一个二维的画面，三维空间中的物件移动表现在二维画面上，也就是二维坐标位置的移动而已。θ就是相机的俯仰角,投影线段 = 3D线段 * sin(俯仰角)。CD长度=3D世界中正方形CD长度 * sin(俯仰角)。遍历三角函数查找表，只有sin(30°)的分子分母都为整数，也就是说只有30°这个角度有可能让长宽都为整数，具体可参看尼文定理：。因此人们通常说的斜45度视角游戏只是人们通过臆测而给2.5D游戏取得俗名，准确来说我们应该称这类游戏叫做斜30度视角游戏。或者可以采用另一种对斜45度视角游戏的解释，斜45度指的是相机水平方向上（围绕世界空间Y轴）的旋转角度。</p>
<p><a href="http://matov.me/isometric-toolset/"target="_blank" rel="external nofollow noopener noreferrer">http://matov.me/isometric-toolset/<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>  能不能将2.5D坐标转化为3D坐标。</p>
</blockquote>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201101104102167.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201101104102167.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201101104102167.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201101104102167.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201101104102167.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201101104102167.png"/></p>
<ul>
<li><i class="fa-regular fa-check-square fa-fw" aria-hidden="true"></i> multi-handDetection pipeline</li>
<li><strong>multi_hand_detection_gpu</strong></li>
</ul>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20200916212815466.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20200916212815466.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20200916212815466.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20200916212815466.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20200916212815466.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20200916212815466.png"/></p>
<ul>
<li><strong>multi_hand_landmark_gpu</strong></li>
</ul>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20200916214124663.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20200916214124663.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20200916214124663.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20200916214124663.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20200916214124663.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20200916214124663.png"/></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20200916214222259.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20200916214222259.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20200916214222259.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20200916214222259.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20200916214222259.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20200916214222259.png"/></p>
<ul>
<li><strong>multi_hand_renderer_gpu</strong></li>
</ul>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20200916221843111.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20200916221843111.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20200916221843111.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20200916221843111.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20200916221843111.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20200916221843111.png"/></p>
<p><strong>level</strong>:  CVPR
<strong>author</strong>: Artsiom Ablavatski
<strong>date</strong>: 2020
<strong>keyword</strong>:</p>
<ul>
<li>iris tracking</li>
</ul>
<hr>
<h2 id="paper-pupiltracking">Paper: PupilTracking</h2>
<!-- raw HTML omitted -->
<h4 id="summary-1">Summary</h4>
<ol>
<li>present a simple, real-time approach for pupil tracking from live video on mobile devices.</li>
<li>consists of two new component: a tiny neural network that predicts positions of the pupils in 2D, and a displacement-based estimation of pupil blend shape coefficients.</li>
<li>this methods can be used to accurately control the pupil movements of a virtual puppet, and lends liveliness and energy to it, run 50FPs on model phones.</li>
<li>detects 5 points of the pupil, outer iris circle and eye contour for each eye;</li>
</ol>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200831094224.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200831094224.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200831094224.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200831094224.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200831094224.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200831094224.png"/></p>
<h4 id="system-overview-1">System Overview</h4>
<p>【<strong>Neural network based eye landmarks</strong>】</p>
<blockquote>
<p>using a tiny neural network combined face mesh model to produces additional higher quality landmarks.</p>
</blockquote>
<ul>
<li>combine the corresponding landmarks (16 points of eye contour)from the face estimation pipeline with those from the eye reﬁnement network by <!-- raw HTML omitted -->replacing the x,y coordinates of the former while leaving z untouched<!-- raw HTML omitted -->.</li>
<li>extend the face mesh with 5 pupil landmarks(pupil center and  4 points of outer iris circle ), with z coordinate set to the average of the z coordinate of the eye corners.</li>
</ul>
<p>【<strong>Displacement-based pupil blend shape estimation</strong>】基于位移的瞳孔混合变形估计</p>
<ul>
<li>refine the mesh to predict 4 blend shapes for the pupils:<!-- raw HTML omitted --> pupils pointing outwards, inwards, upwards and downwards respectively.<!-- raw HTML omitted --></li>
<li>by combine 3 displacement to obtain scalar value in the range of [0,1] for each pupil blend shape.  <!-- raw HTML omitted --> 不明白这一步达到的效果是什么？<!-- raw HTML omitted --></li>
</ul>
<blockquote>
<p>for the pupil pointing inwards, using the vertex of the pupil and vertex of eye corner, and measure the displacement $D_{current}$ between these two vertices and compare it to two empirically derived displacements $D_{neutral}$ , the displacement with the minimum activation of the blend shape and $D_{activated}$ the displacement measured using maximum activation of the blend shape.</p>
</blockquote>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200831100400.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200831100400.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200831100400.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200831100400.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200831100400.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200831100400.png"/></p>
<p>【<strong>Real-time heuristics calibratin</strong>】</p>
<blockquote>
<p>the initial displacements are empirically estimated based on the representative face mesh dataset, but unable to model all person-specific variations.</p>
<p>employ the standard score calculation algorithm with a few modificaitons, the main idea of the filter is to check the displacement on every iteration and add it to a circular buffer of the trusted displacement if it falls within the specified confidence interval, and the calibrated displacement is calculated as an average of the trusted displacement, the standard deviation of these trusted displacements is used as the confidence interval in the next iteration.</p>
</blockquote>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200831102522.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200831102522.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200831102522.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200831102522.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200831102522.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200831102522.png"/></p>
<h4 id="evaluation">Evaluation</h4>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200831102637.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200831102637.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200831102637.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200831102637.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200831102637.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200831102637.png"/></p>
<h4 id="notes-font-colororange去加强了解font-3">Notes <!-- raw HTML omitted -->去加强了解<!-- raw HTML omitted --></h4>
<ul>
<li>human face model predicts a 468 vertex mesh [4]  Real-time facial surface geometry from monocular video on mobile gpus.</li>
<li>[1] predicts 5 locations in 2D(pupil center , 4 points of outer iris circle, and 16 points of eye contour)  Blazeface: Sub-milli second neural face detection on mobile gpus. arXiv preprint arXiv:1907.05047, 2019. 2</li>
<li>[8]Mnasnet: Platform-aware neural architecture search for mobile</li>
</ul>
<h2 id="3-案例">3. 案例</h2>
<h3 id="31-video-reframing">3.1. Video Reframing</h3>
<p><a href="http://ai.googleblog.com/2020/02/autoflip-open-source-framework-for.html"target="_blank" rel="external nofollow noopener noreferrer">AutoFlip: An Open Source Framework for Intelligent Video Reframing<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<blockquote>
<p>AutoFlip provides a fully automatic solution to smart video reframing, making use of state-of-the-art ML-enabled object detection and tracking technologies to intelligently understand video content. AutoFlip detects changes in the composition that signify scene changes in order to isolate scenes for processing. Within each shot, video analysis is used to identify salient content before the scene is reframed by selecting a camera mode and path optimized for the contents.</p>
</blockquote>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200825153825.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200825153825.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200825153825.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200825153825.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200825153825.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200825153825.png"/></p>
<ul>
<li><strong>Shot (Scene) Detection</strong>:</li>
</ul>
<blockquote>
<p>A scene or shot is a continuous sequence of video without cuts (or jumps). To detect the occurrence of a shot change, AutoFlip computes the color histogram of each frame and compares this with prior frames. If the distribution of frame colors changes at a different rate than a sliding historical window, a shot change is signaled. AutoFlip buffers the video until the scene is complete before making reframing decisions, in order to optimize the reframing for  entire scene.</p>
</blockquote>
<ul>
<li><strong>Video Content Analysis:</strong> utilize deep  learning-based object detection models to find interesting, salient content in the frame.</li>
<li><strong>Reframing:</strong></li>
</ul>
<blockquote>
<p>AutoFlip automatically chooses an optimal refremingn strategy, stationary, paining or tracking, depending on the way obects behave during the scene.</p>
</blockquote>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/jitter_combined.gif"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/jitter_combined.gif, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/jitter_combined.gif 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/jitter_combined.gif 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/jitter_combined.gif"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/jitter_combined.gif"/></p>
<blockquote>
<p><strong>Top:</strong> Camera paths resulting from following the bounding boxes from frame-to-frame. <strong>Bottom:</strong> Final smoothed camera paths generated using <a href="https://en.wikipedia.org/wiki/Norm_%28mathematics%29#Euclidean_norm"target="_blank" rel="external nofollow noopener noreferrer">Euclidean-norm<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a> path formation. <strong>Left:</strong> Scene in which objects are moving around, requiring a tracking camera path. <strong>Right:</strong> Scene where objects stay close to the same position; a stationary camera covers the content for the full duration of the scene.</p>
</blockquote>
<h3 id="32-real-time-3d-object-detection"><strong>3.2.</strong> Real-Time 3D Object Detection</h3>
<blockquote>
<p>robotics, self-driving vehicles, image retrieval, and augmented reality. <a href="https://arxiv.org/abs/2003.03522"target="_blank" rel="external nofollow noopener noreferrer">built a single-stage model<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a> to predict the pose and physical size of an object from a single RGB image</p>
</blockquote>
<ul>
<li><strong>Real-World 3D Training Data</strong>: With the arrival of <a href="https://developers.google.com/ar"target="_blank" rel="external nofollow noopener noreferrer">ARCore<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a> and <a href="https://developer.apple.com/augmented-reality/"target="_blank" rel="external nofollow noopener noreferrer">ARKit<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>, <a href="https://arinsider.co/2019/05/13/arcore-reaches-400-million-devices/"target="_blank" rel="external nofollow noopener noreferrer">hundreds of millions<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a> of smartphones now have AR capabilities and the ability to capture additional information during an AR session, including the camera pose, sparse <a href="https://en.wikipedia.org/wiki/Point_cloud"target="_blank" rel="external nofollow noopener noreferrer">3D point clouds<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>, estimated lighting, and planar surfaces.</li>
<li><strong>AR Synthetic Data Generation:</strong> AR Synthetic Data Generation, places virtual objects into scenes that have AR session data, which allows us to leverage camera poses, detected planar surfaces, and estimated lighting to generate placements that are physically probable and with lighting that matches the scene.</li>
</ul>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200825170636.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200825170636.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200825170636.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200825170636.png 2x"
    data-sizes="auto"
    alt="Network architecture and post-processing for 3D object detection."
    title="Network architecture and post-processing for 3D object detection."/></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200825170655.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200825170655.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200825170655.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200825170655.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200825170655.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200825170655.png"/></p>
<blockquote>
<p>Sample results of our network — [<strong>left</strong>] original 2D image with estimated bounding boxes, [<strong>middle</strong>] object detection by Gaussian distribution, [<strong>right</strong>] predicted segmentation mask.</p>
</blockquote>
<h3 id="33-afred-camera">3.3. Afred Camera</h3>
<blockquote>
<p>users are able to turn their spare phones into security cameras and monitors directly, which allows them to watch their homes, shops, pets anytime.</p>
</blockquote>
<ul>
<li><strong>Moving Object Detection:</strong>  continuously uses the device&rsquo;s camera to monitor a target scene, once detected recording the video and send notifications to the device owner.</li>
<li><strong>Low-light Detection and Low-light Filter:</strong> calculate the average luminance of the scene, and conditionally process the incoming frames to intensify the brightness of the pixel.</li>
<li>**Motion Detection: ** by calculating the difference between two frames with some additional tricks that take the movements detected in a few frames</li>
<li><strong>Area of Interest:</strong> manually mask out the area where they don&rsquo;t want the camera to see.</li>
</ul>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200825173215.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200825173215.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200825173215.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200825173215.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200825173215.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200825173215.png"/></p>
<h3 id="34-iris-tracking-and-depth-estimation">3.4. Iris Tracking and Depth Estimation</h3>
<blockquote>
<p>A wide range of real-world applications, including computational photography (e.g., <a href="https://ai.googleblog.com/2019/12/improvements-to-portrait-mode-on-google.html"target="_blank" rel="external nofollow noopener noreferrer">portrait mode<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a> and glint reflections) and <a href="https://ai.googleblog.com/2019/03/real-time-ar-self-expression-with.html"target="_blank" rel="external nofollow noopener noreferrer">augmented reality effects<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a> (e.g., virtual avatars) rely on estimating eye position by tracking the iris</p>
</blockquote>
<blockquote>
<p>Iris tracking is a challenging task to solve on mobile devices, due to limited computing resources, variable light conditions and the presence of occlusions, such as hair or people squinting. Often, sophisticated specialized hardware is employed, limiting the range of devices on which the solution could be applied.</p>
</blockquote>
<ul>
<li>Depth-from-Iris from a single Image: <!-- raw HTML omitted -->by relying on the fact that the horizontal iris diameter of the human eye remains roughly constant at 11.7+-0.5 mm across a wide population, along with some simple geometric arguments.</li>
</ul>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200825174114.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200825174114.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200825174114.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200825174114.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200825174114.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/20200825174114.png"/></p>
</div>
<div class="post-footer" id="post-footer">
  <div class="post-info">
    <div class="post-info-line">
      <div class="post-info-mod">
        <span title=2023-12-31&#32;13:47:03>更新于 2023-12-31&nbsp;</span>
      </div><div class="post-info-license">
          <span><a rel="license external nofollow noopener noreferrer" href="https://creativecommons.org/licenses/by-nc/4.0/" target="_blank">CC BY-NC 4.0</a></span>
        </div></div>
    <div class="post-info-line">
      <div class="post-info-md"><span><a href="/mediapipe/index.md" title="阅读原始文档" class="link-to-markdown">阅读原始文档</a></span><span><a href="https://liudongdong1.github.io/edit/master/content/posts%5c%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%5cOpenSource%5cMediaPipe.md" title="编辑此页"target="_blank" rel="external nofollow noopener noreferrer" class="link-to-edit">编辑此页</a></span></div>
      <div class="post-info-share">
        <span><a href="javascript:void(0);" title="分享到 Twitter" data-sharer="twitter" data-url="https://liudongdong1.github.io/mediapipe/" data-title="MediaPipe" data-hashtags="Mediapipe"><i class="fa-brands fa-twitter fa-fw" aria-hidden="true"></i></a>
  <a href="javascript:void(0);" title="分享到 Facebook" data-sharer="facebook" data-url="https://liudongdong1.github.io/mediapipe/" data-hashtag="Mediapipe"><i class="fa-brands fa-facebook-square fa-fw" aria-hidden="true"></i></a>
  <a href="javascript:void(0);" title="分享到 微博" data-sharer="weibo" data-url="https://liudongdong1.github.io/mediapipe/" data-title="MediaPipe" data-image="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20200715085515063.png"><i class="fa-brands fa-weibo fa-fw" aria-hidden="true"></i></a>
  </span>
      </div>
    </div>
  </div>

  <div class="post-info-more">
    <section class="post-tags"><i class="fa-solid fa-tags fa-fw" aria-hidden="true"></i>&nbsp;<a href="/tags/mediapipe/">Mediapipe</a></section>
    <section>
      <span><a href="javascript:void(0);" onclick="window.history.back();">返回</a></span>&nbsp;|&nbsp;<span><a href="/">主页</a></span>
    </section>
  </div>

  <div class="post-nav"><a href="/visionnlpcommend/" class="prev" rel="prev" title="VisionNLPCommend"><i class="fa-solid fa-angle-left fa-fw" aria-hidden="true"></i>VisionNLPCommend</a>
      <a href="/anchorintroduce/" class="next" rel="next" title="AnchorIntroduce">AnchorIntroduce<i class="fa-solid fa-angle-right fa-fw" aria-hidden="true"></i></a></div>
</div>
</article></main><footer class="footer">
    <div class="footer-container"><div class="footer-line powered">由 <a href="https://gohugo.io/" target="_blank" rel="external nofollow noopener noreferrer" title="Hugo 0.118.2">Hugo</a> 强力驱动 | 主题 - <a href="https://github.com/hugo-fixit/FixIt" target="_blank" rel="external" title="FixIt v0.2.17-RC"><img class="fixit-icon" src="/fixit.min.svg" alt="FixIt logo" />&nbsp;FixIt</a>
        </div><div class="footer-line copyright" itemscope itemtype="http://schema.org/CreativeWork"><i class="fa-regular fa-copyright fa-fw" aria-hidden="true"></i>
            <span itemprop="copyrightYear">2020 - 2024</span><span class="author" itemprop="copyrightHolder">
              <a href="https://liudongdong1.github.io/"target="_blank" rel="external nofollow noopener noreferrer">LiuDongdong</a></span><span class="license footer-divider"><a rel="license external nofollow noopener noreferrer" href="https://creativecommons.org/licenses/by-nc/4.0/" target="_blank">CC BY-NC 4.0</a></span></div><div class="footer-line statistics"><span class="site-time" title='网站运行中 ...'><i class="fa-solid fa-heartbeat fa-fw animate-icon" aria-hidden="true"></i>&nbsp;<span class="run-times">网站运行中 ...</span></span></div><div class="footer-line ibruce">
          <span id="busuanzi_container_site_uv" title='总访客数'><i class="fa-regular fa-user fa-fw" aria-hidden="true"></i>&nbsp;<span id="busuanzi_value_site_uv"><i class="fa-solid fa-spinner fa-spin fa-fw" aria-hidden="true"></i></span></span><span id="busuanzi_container_site_pv" class="footer-divider" title='总访问量'><i class="fa-regular fa-eye fa-fw" aria-hidden="true"></i>&nbsp;<span id="busuanzi_value_site_pv"><i class="fa-solid fa-spinner fa-spin fa-fw" aria-hidden="true"></i></span></span>
        </div></div>
  </footer></div><div class="widgets"><div class="fixed-buttons animate__faster d-none"><div class="fixed-button back-to-top" role="button" aria-label="回到顶部"><i class="fa-solid fa-arrow-up fa-fw" aria-hidden="true"></i><span class="variant-numeric">0%</span>
        </div></div><a href="https://liudongdong1.github.io/" title="在 GitHub 上查看源代码"target="_blank" rel="external nofollow" class="github-corner right d-none-mobile"><svg viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><div id="mask"></div><div class="reading-progress-bar" style="left: 0;top: 0;--bg-progress: #0076ff;--bg-progress-dark: #fff;"></div><noscript>
    <div class="noscript-warning">FixIt 主题在启用 JavaScript 的情况下效果最佳。</div>
  </noscript>
</div><link rel="stylesheet" href="/lib/katex/katex.min.css"><link rel="stylesheet" href="/lib/cookieconsent/cookieconsent.min.css"><script src="/lib/autocomplete/autocomplete.min.js" defer></script><script src="/lib/algoliasearch/algoliasearch-lite.umd.min.js" defer></script><script src="/lib/lazysizes/lazysizes.min.js" async defer></script><script src="/lib/sharer/sharer.min.js" async defer></script><script src="/lib/typeit/index.umd.js" defer></script><script src="/lib/katex/katex.min.js" defer></script><script src="/lib/katex/auto-render.min.js" defer></script><script src="/lib/katex/copy-tex.min.js" defer></script><script src="/lib/katex/mhchem.min.js" defer></script><script src="/lib/cookieconsent/cookieconsent.min.js" defer></script><script src="/lib/pangu/pangu.min.js" defer></script><script src="/lib/cell-watermark/watermark.min.js" defer></script><script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async defer></script><script>window.config={"code":{"copyTitle":"复制到剪贴板","editLockTitle":"锁定可编辑代码块","editUnLockTitle":"解锁可编辑代码块","editable":true,"maxShownLines":10},"comment":{"enable":false},"cookieconsent":{"content":{"dismiss":"同意","link":"了解更多","message":"本网站使用 Cookies 来改善您的浏览体验。"},"enable":true,"palette":{"button":{"background":"#f0f0f0"},"popup":{"background":"#1aa3ff"}},"theme":"edgeless"},"data":{"typeit-header-subtitle-desktop":"\u003cspan style='font-family: MMT,\"沐目体\";'\u003e吾日三省吾身\u003c/span\u003e","typeit-header-subtitle-mobile":"\u003cspan style='font-family: MMT,\"沐目体\";'\u003e吾日三省吾身\u003c/span\u003e"},"enablePWA":true,"enablePangu":true,"math":{"delimiters":[{"display":true,"left":"$$","right":"$$"},{"display":true,"left":"\\[","right":"\\]"},{"display":true,"left":"\\begin{equation}","right":"\\end{equation}"},{"display":true,"left":"\\begin{equation*}","right":"\\end{equation*}"},{"display":true,"left":"\\begin{align}","right":"\\end{align}"},{"display":true,"left":"\\begin{align*}","right":"\\end{align*}"},{"display":true,"left":"\\begin{alignat}","right":"\\end{alignat}"},{"display":true,"left":"\\begin{alignat*}","right":"\\end{alignat*}"},{"display":true,"left":"\\begin{gather}","right":"\\end{gather}"},{"display":true,"left":"\\begin{CD}","right":"\\end{CD}"},{"display":false,"left":"$","right":"$"},{"display":false,"left":"\\(","right":"\\)"}],"strict":false},"search":{"algoliaAppID":"2R1K9SKLQZ","algoliaIndex":"index.zh-cn","algoliaSearchKey":"4a226aa1c5c98d6859e4d1386adb2bc7","highlightTag":"em","maxResultLength":10,"noResultsFound":"没有找到结果","snippetLength":50,"type":"algolia"},"siteTime":"2020-12-18T16:15:22+08:00","typeit":{"cursorChar":"|","cursorSpeed":1000,"data":{"typeit-header-subtitle-desktop":["typeit-header-subtitle-desktop"],"typeit-header-subtitle-mobile":["typeit-header-subtitle-mobile"]},"duration":-1,"speed":100},"watermark":{"appendto":".wrapper\u003emain","colspacing":30,"content":"\u003cimg class=\"fixit-icon\" src=\"/fixit.min.svg\" alt=\"FixIt logo\" /\u003e FixIt 主题","enable":true,"fontfamily":"inherit","fontsize":0.85,"height":21,"opacity":0.0125,"rotate":15,"rowspacing":60,"width":150}};</script><script src="/js/theme.min.js" defer></script><script src="/js/custom.min.js" defer></script></body>
</html>
