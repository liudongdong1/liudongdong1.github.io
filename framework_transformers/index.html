<!doctype html><html itemscope itemtype=http://schema.org/WebPage lang=zh-CN><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=2"><meta name=robots content="noodp"><title>Framework_Transformers - DAY By DAY</title><meta name=author content="LiuDongdong"><meta name=author-link content="https://liudongdong1.github.io/"><meta name=description content="provides thousands of pretrained models to perform tasks on texts such as classification, information extraction, question answering, summarization, translation, text generation and more. [code] tutorial Transformers is backed by the three most popular deep learning libraries&ndash;Jax, PyTorch, and TensorFlow. Masked word completion with BERT Name Entity Recognition with Electra Text generation with GPT-2 Natural Language Inference with RoBERTa Summarization with BART Question answering with DistilBERT Translation with T5 1. Example"><meta name=keywords content="NLP"><meta itemprop=name content="Framework_Transformers"><meta itemprop=description content="provides thousands of pretrained models to perform tasks on texts such as classification, information extraction, question answering, summarization, translation, text generation and more. [code] tutorial Transformers is backed by the three most popular deep learning libraries&ndash;Jax, PyTorch, and TensorFlow. Masked word completion with BERT Name Entity Recognition with Electra Text generation with GPT-2 Natural Language Inference with RoBERTa Summarization with BART Question answering with DistilBERT Translation with T5 1. Example"><meta itemprop=datePublished content="2021-05-13T21:31:56+00:00"><meta itemprop=dateModified content="2023-09-24T17:00:05+08:00"><meta itemprop=wordCount content="552"><meta itemprop=image content="/logo.png"><meta itemprop=keywords content="NLP,"><meta property="og:title" content="Framework_Transformers"><meta property="og:description" content="provides thousands of pretrained models to perform tasks on texts such as classification, information extraction, question answering, summarization, translation, text generation and more. [code] tutorial Transformers is backed by the three most popular deep learning libraries&ndash;Jax, PyTorch, and TensorFlow. Masked word completion with BERT Name Entity Recognition with Electra Text generation with GPT-2 Natural Language Inference with RoBERTa Summarization with BART Question answering with DistilBERT Translation with T5 1. Example"><meta property="og:type" content="article"><meta property="og:url" content="liudongdong1.github.io/framework_transformers/"><meta property="og:image" content="/logo.png"><meta property="article:section" content="posts"><meta property="article:published_time" content="2021-05-13T21:31:56+00:00"><meta property="article:modified_time" content="2023-09-24T17:00:05+08:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="/logo.png"><meta name=twitter:title content="Framework_Transformers"><meta name=twitter:description content="provides thousands of pretrained models to perform tasks on texts such as classification, information extraction, question answering, summarization, translation, text generation and more. [code] tutorial Transformers is backed by the three most popular deep learning libraries&ndash;Jax, PyTorch, and TensorFlow. Masked word completion with BERT Name Entity Recognition with Electra Text generation with GPT-2 Natural Language Inference with RoBERTa Summarization with BART Question answering with DistilBERT Translation with T5 1. Example"><meta name=application-name content="DAY By DAY"><meta name=apple-mobile-web-app-title content="DAY By DAY"><meta name=theme-color data-light=#f8f8f8 data-dark=#252627 content="#f8f8f8"><meta name=msapplication-TileColor content="#da532c"><link rel="shortcut icon" type=image/x-icon href=/favicon.ico><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link rel=manifest href=/site.webmanifest><link rel=canonical href=liudongdong1.github.io/framework_transformers/><link rel=prev href=liudongdong1.github.io/observemode/><link rel=next href=liudongdong1.github.io/facademode/><link rel=stylesheet href=/liudongdong1.github.io/css/style.min.css><link rel=stylesheet href=/liudongdong1.github.io/lib/fontawesome-free/all.min.css><link rel=stylesheet href=/liudongdong1.github.io/lib/animate/animate.min.css><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"Framework_Transformers","inLanguage":"zh-CN","mainEntityOfPage":{"@type":"WebPage","@id":"liudongdong1.github.io\/framework_transformers\/"},"genre":"posts","keywords":"NLP","wordcount":552,"url":"liudongdong1.github.io\/framework_transformers\/","datePublished":"2021-05-13T21:31:56+00:00","dateModified":"2023-09-24T17:00:05+08:00","license":"This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.","publisher":{"@type":"Organization","name":"LiuDongdong","logo":"\/images\/person.png"},"author":{"@type":"Person","name":"liudongdong1"},"description":""}</script></head><body data-header-desktop=auto data-header-mobile=auto><script>(window.localStorage?.getItem("theme")?localStorage.getItem("theme")==="dark":"auto"==="auto"?window.matchMedia("(prefers-color-scheme: dark)").matches:"auto"==="dark")&&document.body.setAttribute("data-theme","dark")</script><div class=wrapper><header class="desktop animate__faster" id=header-desktop><div class=header-wrapper data-github-corner=right><div class=header-title><a href=liudongdong1.github.io/ title="DAY By DAY"><img class="lazyload logo" src=/liudongdong1.github.io/svg/loading.min.svg data-src=/fixit.min.svg data-srcset="/fixit.min.svg, /fixit.min.svg 1.5x, /fixit.min.svg 2x" data-sizes=auto alt="DAY By DAY" title="DAY By DAY"><span class=header-title-text></span></a><span id=typeit-header-subtitle-desktop class="typeit header-subtitle"></span></div><nav><ul class=menu><li class=menu-item><a class=menu-link href=/liudongdong1.github.io/posts/><i class="fa-solid fa-archive fa-fw fa-sm" aria-hidden=true></i> 所有文章</a></li><li class=menu-item><a class=menu-link href=/liudongdong1.github.io/categories/><i class="fa-solid fa-th fa-fw fa-sm" aria-hidden=true></i> 分类</a></li><li class=menu-item><a class=menu-link href=/liudongdong1.github.io/tags/><i class="fa-solid fa-tags fa-fw fa-sm" aria-hidden=true></i> 标签</a></li><li class=menu-item><a class=menu-link href=/liudongdong1.github.io/friends/ title=友情链接><i class="fa-solid fa-users fa-fw fa-sm" aria-hidden=true></i> 友链</a></li><li class=menu-item><a class=menu-link href=/liudongdong1.github.io/about/><i class="fa-solid fa-info-circle fa-fw fa-sm" aria-hidden=true></i> 关于</a></li><li class="menu-item delimiter"></li><li class="menu-item language"><span role=button aria-label=选择语言 title=选择语言>简体中文<i class="dropdown-icon fa-solid fa-chevron-down" aria-hidden=true></i></span><ul class=sub-menu><li class=menu-item>没有更多翻译</li></ul></li><li class="menu-item search" id=search-desktop><input type=text placeholder="搜索文章标题或内容 ..." id=search-input-desktop>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-desktop title=搜索><i class="fa-solid fa-search fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); class="search-button search-clear" id=search-clear-desktop title=清空><i class="fa-solid fa-times-circle fa-fw" aria-hidden=true></i></a>
<span class="search-button search-loading" id=search-loading-desktop><i class="fa-solid fa-spinner fa-fw fa-spin" aria-hidden=true></i></span></li><li class="menu-item theme-switch" title=切换主题><i class="fa-solid fa-adjust fa-fw" aria-hidden=true></i></li></ul></nav></div></header><header class="mobile animate__faster" id=header-mobile><div class=header-container><div class=header-wrapper><div class=header-title><a href=liudongdong1.github.io/ title="DAY By DAY"><img class="lazyload logo" src=/liudongdong1.github.io/svg/loading.min.svg data-src=/fixit.min.svg data-srcset="/fixit.min.svg, /fixit.min.svg 1.5x, /fixit.min.svg 2x" data-sizes=auto alt=/fixit.min.svg title=/fixit.min.svg><span class=header-title-text></span></a><span id=typeit-header-subtitle-mobile class="typeit header-subtitle"></span></div><div class=menu-toggle id=menu-toggle-mobile><span></span><span></span><span></span></div></div><nav><ul class=menu id=menu-mobile><li class=search-wrapper><div class="search mobile" id=search-mobile><input type=text placeholder="搜索文章标题或内容 ..." id=search-input-mobile>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-mobile title=搜索><i class="fa-solid fa-search fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); class="search-button search-clear" id=search-clear-mobile title=清空><i class="fa-solid fa-times-circle fa-fw" aria-hidden=true></i></a>
<span class="search-button search-loading" id=search-loading-mobile><i class="fa-solid fa-spinner fa-fw fa-spin" aria-hidden=true></i></span></div><a href=javascript:void(0); class=search-cancel id=search-cancel-mobile>取消</a></li><li class=menu-item><a class=menu-link href=/liudongdong1.github.io/posts/><i class="fa-solid fa-archive fa-fw fa-sm" aria-hidden=true></i> 所有文章</a></li><li class=menu-item><a class=menu-link href=/liudongdong1.github.io/categories/><i class="fa-solid fa-th fa-fw fa-sm" aria-hidden=true></i> 分类</a></li><li class=menu-item><a class=menu-link href=/liudongdong1.github.io/tags/><i class="fa-solid fa-tags fa-fw fa-sm" aria-hidden=true></i> 标签</a></li><li class=menu-item><a class=menu-link href=/liudongdong1.github.io/friends/ title=友情链接><i class="fa-solid fa-users fa-fw fa-sm" aria-hidden=true></i> 友链</a></li><li class=menu-item><a class=menu-link href=/liudongdong1.github.io/about/><i class="fa-solid fa-info-circle fa-fw fa-sm" aria-hidden=true></i> 关于</a></li><li class="menu-item text-center"><a class=menu-link href=https://liudongdong1.github.io/ title=GitHub rel="noopener noreferrer" target=_blank><i class='fa-brands fa-github fa-fw' aria-hidden=true></i></a></li><li class="menu-item theme-switch" title=切换主题><i class="fa-solid fa-adjust fa-fw" aria-hidden=true></i></li><li class="menu-item language"><span role=button aria-label=选择语言 title=选择语言>简体中文<i class="dropdown-icon fa-solid fa-chevron-down" aria-hidden=true></i></span>
<select class=language-select onchange="location=this.value"><option disabled>没有更多翻译</option></select></li></ul></nav></div></header><div class="search-dropdown desktop"><div id=search-dropdown-desktop></div></div><div class="search-dropdown mobile"><div id=search-dropdown-mobile></div></div><main class=container data-page-style=normal><aside class=toc id=toc-auto><h2 class=toc-title>目录 <i class="toc-icon fa-solid fa-angle-down fa-fw"></i></h2><div class=toc-content id=toc-content-auto></div></aside><aside class=aside-custom id=aside-sakana><div class=sakana-widget><div class=sakana-item id=takina-widget></div><div class=sakana-item id=chisato-widget></div></div><script>function initSakanaWidget(){const e=SakanaWidget.getCharacter("takina");SakanaWidget.registerCharacter("takina-slow",e),new SakanaWidget({character:"takina-slow",controls:!1,autoFit:!0,stroke:{color:"#b4b4b4",width:2}}).mount("#takina-widget");const t=SakanaWidget.getCharacter("chisato");SakanaWidget.registerCharacter("chisato-slow",t),new SakanaWidget({character:"chisato-slow",controls:!1,autoFit:!0,stroke:{color:"#b4b4b4",width:2}}).mount("#chisato-widget")}</script><script async onload=initSakanaWidget() src=https://cdn.jsdelivr.net/npm/sakana-widget@2.3.0/lib/sakana.min.js></script></aside><article class="page single"><div class=header><h1 class="single-title animate__animated animate__flipInX"><span>Framework_Transformers</span></h1></div><div class=post-meta><div class=post-meta-line><span class=post-author><span class=author><i class="fa-solid fa-user-circle" aria-hidden=true></i>
liudongdong1</span></span>
<span class=post-category>收录于 <a href=liudongdong1.github.io/categories/><i class="fa-regular fa-folder fa-fw"></i>&nbsp;Categories</a>&ensp;<a href=liudongdong1.github.io/categories/nlp/><i class="fa-regular fa-folder fa-fw"></i>&nbsp;NLP</a></span></div><div class=post-meta-line><span title="2021-05-13 21:31:56"><i class="fa-regular fa-calendar-alt fa-fw"></i>&nbsp;<time datetime=2021-05-13>2021-05-13</time>
</span>&nbsp;<i class="fa-solid fa-pencil-alt fa-fw"></i>&nbsp;约 552 字&nbsp;
<i class="fa-regular fa-clock fa-fw"></i>&nbsp;预计阅读 2 分钟&nbsp;<span id=busuanzi_container_page_pv class="busuanzi_visitors comment-visitors" data-flag-title=Framework_Transformers>
<i class="fa-regular fa-eye fa-fw"></i>&nbsp;<span id=busuanzi_value_page_pv>-</span>&nbsp;次阅读
</span>&nbsp;</div></div><div class=featured-image><img class=lazyload src=/liudongdong1.github.io/svg/loading.min.svg data-src=https://cdn.stocksnap.io/img-thumbs/280h/DYRNHWZGAN.jpg data-srcset="https://cdn.stocksnap.io/img-thumbs/280h/DYRNHWZGAN.jpg, https://cdn.stocksnap.io/img-thumbs/280h/DYRNHWZGAN.jpg 1.5x, https://cdn.stocksnap.io/img-thumbs/280h/DYRNHWZGAN.jpg 2x" data-sizes=auto alt=https://cdn.stocksnap.io/img-thumbs/280h/DYRNHWZGAN.jpg title=https://cdn.stocksnap.io/img-thumbs/280h/DYRNHWZGAN.jpg></div><div class="details toc" id=toc-static kept=true><div class="details-summary toc-title"><span>目录</span>
<span><i class="details-icon fa-solid fa-angle-right"></i></span></div><div class="details-content toc-content" id=toc-content-static><nav id=TableOfContents><ul><li><ul><li><a href=#1-example-code>1. Example code</a></li><li><a href=#2-next_word_predictionhttpsgithubcomrenatoviolinnext_word_prediction>2. <a href=https://github.com/renatoviolin/next_word_prediction>next_word_prediction</a></a></li></ul></li></ul></nav></div></div><div class=content id=content><blockquote><p>provides thousands of pretrained models to perform tasks on texts such as classification, information extraction, question answering, summarization, translation, text generation and more. <a href=https://github.com/huggingface/transformers#quick-tour target=_blank rel="external nofollow noopener noreferrer">[code]<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden=true></i></a> <a href=https://huggingface.co/transformers/ target=_blank rel="external nofollow noopener noreferrer">tutorial<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden=true></i></a></p><p>Transformers is backed by the three most popular deep learning libraries&ndash;Jax, PyTorch, and TensorFlow.</p><ul><li>Masked word completion with BERT</li><li>Name Entity Recognition with Electra</li><li>Text generation with GPT-2</li><li>Natural Language Inference with RoBERTa</li><li>Summarization with BART</li><li>Question answering with DistilBERT</li><li>Translation with T5</li></ul></blockquote><h3 id=1-example-code>1. Example code</h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> transformers <span style=color:#f92672>import</span> pipeline
</span></span><span style=display:flex><span><span style=color:#75715e># Allocate a pipeline for sentiment-analysis</span>
</span></span><span style=display:flex><span>classifier <span style=color:#f92672>=</span> pipeline(<span style=color:#e6db74>&#39;sentiment-analysis&#39;</span>)
</span></span><span style=display:flex><span>classifier(<span style=color:#e6db74>&#39;We are very happy to introduce pipeline to the transformers repository.&#39;</span>)
</span></span><span style=display:flex><span>[{<span style=color:#e6db74>&#39;label&#39;</span>: <span style=color:#e6db74>&#39;POSITIVE&#39;</span>, <span style=color:#e6db74>&#39;score&#39;</span>: <span style=color:#ae81ff>0.9996980428695679</span>}]
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> transformers <span style=color:#f92672>import</span> pipeline
</span></span><span style=display:flex><span><span style=color:#75715e># Allocate a pipeline for question-answering</span>
</span></span><span style=display:flex><span>question_answerer <span style=color:#f92672>=</span> pipeline(<span style=color:#e6db74>&#39;question-answering&#39;</span>)
</span></span><span style=display:flex><span>question_answerer({
</span></span><span style=display:flex><span><span style=color:#f92672>...</span>     <span style=color:#e6db74>&#39;question&#39;</span>: <span style=color:#e6db74>&#39;What is the name of the repository ?&#39;</span>,
</span></span><span style=display:flex><span><span style=color:#f92672>...</span>     <span style=color:#e6db74>&#39;context&#39;</span>: <span style=color:#e6db74>&#39;Pipeline has been included in the huggingface/transformers repository&#39;</span>
</span></span><span style=display:flex><span><span style=color:#f92672>...</span> })
</span></span><span style=display:flex><span>{<span style=color:#e6db74>&#39;score&#39;</span>: <span style=color:#ae81ff>0.30970096588134766</span>, <span style=color:#e6db74>&#39;start&#39;</span>: <span style=color:#ae81ff>34</span>, <span style=color:#e6db74>&#39;end&#39;</span>: <span style=color:#ae81ff>58</span>, <span style=color:#e6db74>&#39;answer&#39;</span>: <span style=color:#e6db74>&#39;huggingface/transformers&#39;</span>}
</span></span></code></pre></div><ul><li>equal tensorflow code</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> transformers <span style=color:#f92672>import</span> AutoTokenizer, TFAutoModel
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>tokenizer <span style=color:#f92672>=</span> AutoTokenizer<span style=color:#f92672>.</span>from_pretrained(<span style=color:#e6db74>&#34;bert-base-uncased&#34;</span>)  <span style=color:#75715e># 下载预训练模型</span>
</span></span><span style=display:flex><span>model <span style=color:#f92672>=</span> TFAutoModel<span style=color:#f92672>.</span>from_pretrained(<span style=color:#e6db74>&#34;bert-base-uncased&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>inputs <span style=color:#f92672>=</span> tokenizer(<span style=color:#e6db74>&#34;Hello world!&#34;</span>, return_tensors<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;tf&#34;</span>)  <span style=color:#75715e># returns a dictionary string to list of ints. It contains the ids of the tokens</span>
</span></span><span style=display:flex><span>outputs <span style=color:#f92672>=</span> model(<span style=color:#f92672>**</span>inputs)  <span style=color:#75715e>#使用模型</span>
</span></span></code></pre></div><h3 id=2-next_word_predictionhttpsgithubcomrenatoviolinnext_word_prediction>2. <a href=https://github.com/renatoviolin/next_word_prediction target=_blank rel="external nofollow noopener noreferrer">next_word_prediction<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden=true></i></a></h3><p><img class=lazyload src=/liudongdong1.github.io/svg/loading.min.svg data-src=https://github.com/renatoviolin/next_word_prediction/raw/master/word_prediction.gif data-srcset="https://github.com/renatoviolin/next_word_prediction/raw/master/word_prediction.gif, https://github.com/renatoviolin/next_word_prediction/raw/master/word_prediction.gif 1.5x, https://github.com/renatoviolin/next_word_prediction/raw/master/word_prediction.gif 2x" data-sizes=auto alt="Word prediction" title="Word prediction"></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># %%</span>
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> string
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> transformers <span style=color:#f92672>import</span> BertTokenizer, BertForMaskedLM
</span></span><span style=display:flex><span>bert_tokenizer <span style=color:#f92672>=</span> BertTokenizer<span style=color:#f92672>.</span>from_pretrained(<span style=color:#e6db74>&#39;bert-base-uncased&#39;</span>)
</span></span><span style=display:flex><span>bert_model <span style=color:#f92672>=</span> BertForMaskedLM<span style=color:#f92672>.</span>from_pretrained(<span style=color:#e6db74>&#39;bert-base-uncased&#39;</span>)<span style=color:#f92672>.</span>eval()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> transformers <span style=color:#f92672>import</span> XLNetTokenizer, XLNetLMHeadModel
</span></span><span style=display:flex><span>xlnet_tokenizer <span style=color:#f92672>=</span> XLNetTokenizer<span style=color:#f92672>.</span>from_pretrained(<span style=color:#e6db74>&#39;xlnet-base-cased&#39;</span>)
</span></span><span style=display:flex><span>xlnet_model <span style=color:#f92672>=</span> XLNetLMHeadModel<span style=color:#f92672>.</span>from_pretrained(<span style=color:#e6db74>&#39;xlnet-base-cased&#39;</span>)<span style=color:#f92672>.</span>eval()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> transformers <span style=color:#f92672>import</span> XLMRobertaTokenizer, XLMRobertaForMaskedLM
</span></span><span style=display:flex><span>xlmroberta_tokenizer <span style=color:#f92672>=</span> XLMRobertaTokenizer<span style=color:#f92672>.</span>from_pretrained(<span style=color:#e6db74>&#39;xlm-roberta-base&#39;</span>)
</span></span><span style=display:flex><span>xlmroberta_model <span style=color:#f92672>=</span> XLMRobertaForMaskedLM<span style=color:#f92672>.</span>from_pretrained(<span style=color:#e6db74>&#39;xlm-roberta-base&#39;</span>)<span style=color:#f92672>.</span>eval()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> transformers <span style=color:#f92672>import</span> BartTokenizer, BartForConditionalGeneration
</span></span><span style=display:flex><span>bart_tokenizer <span style=color:#f92672>=</span> BartTokenizer<span style=color:#f92672>.</span>from_pretrained(<span style=color:#e6db74>&#39;facebook/bart-large&#39;</span>)
</span></span><span style=display:flex><span>bart_model <span style=color:#f92672>=</span> BartForConditionalGeneration<span style=color:#f92672>.</span>from_pretrained(<span style=color:#e6db74>&#39;facebook/bart-large&#39;</span>)<span style=color:#f92672>.</span>eval()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> transformers <span style=color:#f92672>import</span> ElectraTokenizer, ElectraForMaskedLM
</span></span><span style=display:flex><span>electra_tokenizer <span style=color:#f92672>=</span> ElectraTokenizer<span style=color:#f92672>.</span>from_pretrained(<span style=color:#e6db74>&#39;google/electra-small-generator&#39;</span>)
</span></span><span style=display:flex><span>electra_model <span style=color:#f92672>=</span> ElectraForMaskedLM<span style=color:#f92672>.</span>from_pretrained(<span style=color:#e6db74>&#39;google/electra-small-generator&#39;</span>)<span style=color:#f92672>.</span>eval()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> transformers <span style=color:#f92672>import</span> RobertaTokenizer, RobertaForMaskedLM
</span></span><span style=display:flex><span>roberta_tokenizer <span style=color:#f92672>=</span> RobertaTokenizer<span style=color:#f92672>.</span>from_pretrained(<span style=color:#e6db74>&#39;roberta-base&#39;</span>)
</span></span><span style=display:flex><span>roberta_model <span style=color:#f92672>=</span> RobertaForMaskedLM<span style=color:#f92672>.</span>from_pretrained(<span style=color:#e6db74>&#39;roberta-base&#39;</span>)<span style=color:#f92672>.</span>eval()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>top_k <span style=color:#f92672>=</span> <span style=color:#ae81ff>10</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>decode</span>(tokenizer, pred_idx, top_clean):
</span></span><span style=display:flex><span>    ignore_tokens <span style=color:#f92672>=</span> string<span style=color:#f92672>.</span>punctuation <span style=color:#f92672>+</span> <span style=color:#e6db74>&#39;[PAD]&#39;</span>
</span></span><span style=display:flex><span>    tokens <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> w <span style=color:#f92672>in</span> pred_idx:
</span></span><span style=display:flex><span>        token <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;&#39;</span><span style=color:#f92672>.</span>join(tokenizer<span style=color:#f92672>.</span>decode(w)<span style=color:#f92672>.</span>split())
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> token <span style=color:#f92672>not</span> <span style=color:#f92672>in</span> ignore_tokens:
</span></span><span style=display:flex><span>            tokens<span style=color:#f92672>.</span>append(token<span style=color:#f92672>.</span>replace(<span style=color:#e6db74>&#39;##&#39;</span>, <span style=color:#e6db74>&#39;&#39;</span>))
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> <span style=color:#e6db74>&#39;</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>&#39;</span><span style=color:#f92672>.</span>join(tokens[:top_clean])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>encode</span>(tokenizer, text_sentence, add_special_tokens<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>):
</span></span><span style=display:flex><span>    text_sentence <span style=color:#f92672>=</span> text_sentence<span style=color:#f92672>.</span>replace(<span style=color:#e6db74>&#39;&lt;mask&gt;&#39;</span>, tokenizer<span style=color:#f92672>.</span>mask_token)
</span></span><span style=display:flex><span>    <span style=color:#75715e># if &lt;mask&gt; is the last token, append a &#34;.&#34; so that models dont predict punctuation.</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> tokenizer<span style=color:#f92672>.</span>mask_token <span style=color:#f92672>==</span> text_sentence<span style=color:#f92672>.</span>split()[<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>]:
</span></span><span style=display:flex><span>        text_sentence <span style=color:#f92672>+=</span> <span style=color:#e6db74>&#39; .&#39;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    input_ids <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tensor([tokenizer<span style=color:#f92672>.</span>encode(text_sentence, add_special_tokens<span style=color:#f92672>=</span>add_special_tokens)])
</span></span><span style=display:flex><span>    mask_idx <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>where(input_ids <span style=color:#f92672>==</span> tokenizer<span style=color:#f92672>.</span>mask_token_id)[<span style=color:#ae81ff>1</span>]<span style=color:#f92672>.</span>tolist()[<span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> input_ids, mask_idx
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>get_all_predictions</span>(text_sentence, top_clean<span style=color:#f92672>=</span><span style=color:#ae81ff>5</span>):
</span></span><span style=display:flex><span>    <span style=color:#75715e># ========================= BERT =================================</span>
</span></span><span style=display:flex><span>    print(text_sentence)
</span></span><span style=display:flex><span>    input_ids, mask_idx <span style=color:#f92672>=</span> encode(bert_tokenizer, text_sentence)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>with</span> torch<span style=color:#f92672>.</span>no_grad():
</span></span><span style=display:flex><span>        predict <span style=color:#f92672>=</span> bert_model(input_ids)[<span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>    bert <span style=color:#f92672>=</span> decode(bert_tokenizer, predict[<span style=color:#ae81ff>0</span>, mask_idx, :]<span style=color:#f92672>.</span>topk(top_k)<span style=color:#f92672>.</span>indices<span style=color:#f92672>.</span>tolist(), top_clean)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># ========================= XLNET LARGE =================================</span>
</span></span><span style=display:flex><span>    input_ids, mask_idx <span style=color:#f92672>=</span> encode(xlnet_tokenizer, text_sentence, <span style=color:#66d9ef>False</span>)
</span></span><span style=display:flex><span>    perm_mask <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>zeros((<span style=color:#ae81ff>1</span>, input_ids<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>1</span>], input_ids<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>1</span>]), dtype<span style=color:#f92672>=</span>torch<span style=color:#f92672>.</span>float)
</span></span><span style=display:flex><span>    perm_mask[:, :, mask_idx] <span style=color:#f92672>=</span> <span style=color:#ae81ff>1.0</span>  <span style=color:#75715e># Previous tokens don&#39;t see last token</span>
</span></span><span style=display:flex><span>    target_mapping <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>zeros((<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, input_ids<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>1</span>]), dtype<span style=color:#f92672>=</span>torch<span style=color:#f92672>.</span>float)  <span style=color:#75715e># Shape [1, 1, seq_length] =&gt; let&#39;s predict one token</span>
</span></span><span style=display:flex><span>    target_mapping[<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, mask_idx] <span style=color:#f92672>=</span> <span style=color:#ae81ff>1.0</span>  <span style=color:#75715e># Our first (and only) prediction will be the last token of the sequence (the masked token)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>with</span> torch<span style=color:#f92672>.</span>no_grad():
</span></span><span style=display:flex><span>        predict <span style=color:#f92672>=</span> xlnet_model(input_ids, perm_mask<span style=color:#f92672>=</span>perm_mask, target_mapping<span style=color:#f92672>=</span>target_mapping)[<span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>    xlnet <span style=color:#f92672>=</span> decode(xlnet_tokenizer, predict[<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, :]<span style=color:#f92672>.</span>topk(top_k)<span style=color:#f92672>.</span>indices<span style=color:#f92672>.</span>tolist(), top_clean)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># ========================= XLM ROBERTA BASE =================================</span>
</span></span><span style=display:flex><span>    input_ids, mask_idx <span style=color:#f92672>=</span> encode(xlmroberta_tokenizer, text_sentence, add_special_tokens<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>with</span> torch<span style=color:#f92672>.</span>no_grad():
</span></span><span style=display:flex><span>        predict <span style=color:#f92672>=</span> xlmroberta_model(input_ids)[<span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>    xlm <span style=color:#f92672>=</span> decode(xlmroberta_tokenizer, predict[<span style=color:#ae81ff>0</span>, mask_idx, :]<span style=color:#f92672>.</span>topk(top_k)<span style=color:#f92672>.</span>indices<span style=color:#f92672>.</span>tolist(), top_clean)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># ========================= BART =================================</span>
</span></span><span style=display:flex><span>    input_ids, mask_idx <span style=color:#f92672>=</span> encode(bart_tokenizer, text_sentence, add_special_tokens<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>with</span> torch<span style=color:#f92672>.</span>no_grad():
</span></span><span style=display:flex><span>        predict <span style=color:#f92672>=</span> bart_model(input_ids)[<span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>    bart <span style=color:#f92672>=</span> decode(bart_tokenizer, predict[<span style=color:#ae81ff>0</span>, mask_idx, :]<span style=color:#f92672>.</span>topk(top_k)<span style=color:#f92672>.</span>indices<span style=color:#f92672>.</span>tolist(), top_clean)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># ========================= ELECTRA =================================</span>
</span></span><span style=display:flex><span>    input_ids, mask_idx <span style=color:#f92672>=</span> encode(electra_tokenizer, text_sentence, add_special_tokens<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>with</span> torch<span style=color:#f92672>.</span>no_grad():
</span></span><span style=display:flex><span>        predict <span style=color:#f92672>=</span> electra_model(input_ids)[<span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>    electra <span style=color:#f92672>=</span> decode(electra_tokenizer, predict[<span style=color:#ae81ff>0</span>, mask_idx, :]<span style=color:#f92672>.</span>topk(top_k)<span style=color:#f92672>.</span>indices<span style=color:#f92672>.</span>tolist(), top_clean)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># ========================= ROBERTA =================================</span>
</span></span><span style=display:flex><span>    input_ids, mask_idx <span style=color:#f92672>=</span> encode(roberta_tokenizer, text_sentence, add_special_tokens<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>with</span> torch<span style=color:#f92672>.</span>no_grad():
</span></span><span style=display:flex><span>        predict <span style=color:#f92672>=</span> roberta_model(input_ids)[<span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>    roberta <span style=color:#f92672>=</span> decode(roberta_tokenizer, predict[<span style=color:#ae81ff>0</span>, mask_idx, :]<span style=color:#f92672>.</span>topk(top_k)<span style=color:#f92672>.</span>indices<span style=color:#f92672>.</span>tolist(), top_clean)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> {<span style=color:#e6db74>&#39;bert&#39;</span>: bert,
</span></span><span style=display:flex><span>            <span style=color:#e6db74>&#39;xlnet&#39;</span>: xlnet,
</span></span><span style=display:flex><span>            <span style=color:#e6db74>&#39;xlm&#39;</span>: xlm,
</span></span><span style=display:flex><span>            <span style=color:#e6db74>&#39;bart&#39;</span>: bart,
</span></span><span style=display:flex><span>            <span style=color:#e6db74>&#39;electra&#39;</span>: electra,
</span></span><span style=display:flex><span>            <span style=color:#e6db74>&#39;roberta&#39;</span>: roberta}
</span></span></code></pre></div></div><div class=post-footer id=post-footer><div class=post-info><div class=post-info-line><div class=post-info-mod><span title="2023-09-24 17:00:05">更新于 2023-09-24&nbsp;</span></div><div class=post-info-license><span><a rel="license external nofollow noopener noreferrer" href=https://creativecommons.org/licenses/by-nc/4.0/ target=_blank>CC BY-NC 4.0</a></span></div></div><div class=post-info-line><div class=post-info-md><span><a href=liudongdong1.github.io/framework_transformers/index.md title=阅读原始文档 class=link-to-markdown>阅读原始文档</a></span><span><a href=https://liudongdong1.github.io/edit/master/content/posts%5c%e8%87%aa%e7%84%b6%e8%af%ad%e8%a8%80%5cFramework%5cFramework_Transformers.md title=编辑此页 target=_blank rel="external nofollow noopener noreferrer" class=link-to-edit>编辑此页</a></span></div><div class=post-info-share><span><a href=javascript:void(0); title="分享到 Twitter" data-sharer=twitter data-url=liudongdong1.github.io/framework_transformers/ data-title=Framework_Transformers data-hashtags=NLP><i class="fa-brands fa-twitter fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); title="分享到 Facebook" data-sharer=facebook data-url=liudongdong1.github.io/framework_transformers/ data-hashtag=NLP><i class="fa-brands fa-facebook-square fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); title="分享到 微博" data-sharer=weibo data-url=liudongdong1.github.io/framework_transformers/ data-title=Framework_Transformers data-image=https://cdn.stocksnap.io/img-thumbs/280h/DYRNHWZGAN.jpg><i class="fa-brands fa-weibo fa-fw" aria-hidden=true></i></a></span></div></div></div><div class=post-info-more><section class=post-tags><i class="fa-solid fa-tags fa-fw" aria-hidden=true></i>&nbsp;<a href=liudongdong1.github.io/tags/nlp/>NLP</a></section><section><span><a href=javascript:void(0); onclick=window.history.back()>返回</a></span>&nbsp;|&nbsp;<span><a href=liudongdong1.github.io/>主页</a></span></section></div><div class=post-nav><a href=liudongdong1.github.io/observemode/ class=prev rel=prev title=ObserveMode><i class="fa-solid fa-angle-left fa-fw" aria-hidden=true></i>ObserveMode</a>
<a href=liudongdong1.github.io/facademode/ class=next rel=next title=FacadeMode>FacadeMode<i class="fa-solid fa-angle-right fa-fw" aria-hidden=true></i></a></div></div></article></main><footer class=footer><div class=footer-container><div class="footer-line powered">由 <a href=https://gohugo.io/ target=_blank rel="external nofollow noopener noreferrer" title="Hugo 0.118.2">Hugo</a> 强力驱动 | 主题 - <a href=https://github.com/hugo-fixit/FixIt target=_blank rel=external title="FixIt v0.2.17-RC"><img class=fixit-icon src=/liudongdong1.github.io/fixit.min.svg alt="FixIt logo">&nbsp;FixIt</a></div><div class="footer-line copyright" itemscope itemtype=http://schema.org/CreativeWork><i class="fa-regular fa-copyright fa-fw" aria-hidden=true></i>
<span itemprop=copyrightYear>2020 - 2023</span><span class=author itemprop=copyrightHolder>
<a href=https://liudongdong1.github.io/ target=_blank rel="external nofollow noopener noreferrer">LiuDongdong</a></span><span class="license footer-divider"><a rel="license external nofollow noopener noreferrer" href=https://creativecommons.org/licenses/by-nc/4.0/ target=_blank>CC BY-NC 4.0</a></span></div><div class="footer-line statistics"><span class=site-time title='网站运行中 ...'><i class="fa-solid fa-heartbeat fa-fw animate-icon" aria-hidden=true></i>&nbsp;<span class=run-times>网站运行中 ...</span></span></div><div class="footer-line ibruce"><span id=busuanzi_container_site_uv title=总访客数><i class="fa-regular fa-user fa-fw" aria-hidden=true></i>&nbsp;<span id=busuanzi_value_site_uv><i class="fa-solid fa-spinner fa-spin fa-fw" aria-hidden=true></i></span></span><span id=busuanzi_container_site_pv class=footer-divider title=总访问量><i class="fa-regular fa-eye fa-fw" aria-hidden=true></i>&nbsp;<span id=busuanzi_value_site_pv><i class="fa-solid fa-spinner fa-spin fa-fw" aria-hidden=true></i></span></span></div></div></footer></div><div class=widgets><div class="fixed-buttons animate__faster d-none"><div class="fixed-button back-to-top" role=button aria-label=回到顶部><i class="fa-solid fa-arrow-up fa-fw" aria-hidden=true></i><span class=variant-numeric>0%</span></div></div><a href=https://liudongdong1.github.io/ title="在 GitHub 上查看源代码" target=_blank rel="external nofollow" class="github-corner right d-none-mobile"><svg viewBox="0 0 250 250" aria-hidden="true"><path d="M0 0 115 115h15l12 27L250 250V0z"/><path d="M128.3 109C113.8 99.7 119 89.6 119 89.6 122 82.7 120.5 78.6 120.5 78.6 119.2 72 123.4 76.3 123.4 76.3 127.3 80.9 125.5 87.3 125.5 87.3 122.9 97.6 130.6 101.9 134.4 103.2" fill="currentcolor" style="transform-origin:130px 106px" class="octo-arm"/><path d="M115 115C114.9 115.1 118.7 116.5 119.8 115.4l13.9-13.8C136.9 99.2 139.9 98.4 142.2 98.6 133.8 88 127.5 74.4 143.8 58 148.5 53.4 154 51.2 159.7 51 160.3 49.4 163.2 43.6 171.4 40.1 171.4 40.1 176.1 42.5 178.8 56.2 183.1 58.6 187.2 61.8 190.9 65.4 194.5 69 197.7 73.2 200.1 77.6 213.8 80.2 216.3 84.9 216.3 84.9 212.7 93.1 206.9 96 205.4 96.6 205.1 102.4 203 107.8 198.3 112.5 181.9 128.9 168.3 122.5 157.7 114.1 157.9 116.9 156.7 120.9 152.7 124.9L141 136.5C139.8 137.7 141.6 141.9 141.8 141.8z" fill="currentcolor" class="octo-body"/></svg></a><div id=mask></div><div class=reading-progress-bar style=left:0;top:0;--bg-progress:#0076ff;--bg-progress-dark:#fff></div><noscript><div class=noscript-warning>FixIt 主题在启用 JavaScript 的情况下效果最佳。</div></noscript></div><link rel=stylesheet href=/liudongdong1.github.io/lib/katex/katex.min.css><link rel=stylesheet href=/liudongdong1.github.io/lib/cookieconsent/cookieconsent.min.css><script src=/liudongdong1.github.io/lib/autocomplete/autocomplete.min.js defer></script><script src=/liudongdong1.github.io/lib/algoliasearch/algoliasearch-lite.umd.min.js defer></script><script src=/liudongdong1.github.io/lib/lazysizes/lazysizes.min.js async defer></script><script src=/liudongdong1.github.io/lib/sharer/sharer.min.js async defer></script><script src=/liudongdong1.github.io/lib/typeit/index.umd.js defer></script><script src=/liudongdong1.github.io/lib/katex/katex.min.js defer></script><script src=/liudongdong1.github.io/lib/katex/auto-render.min.js defer></script><script src=/liudongdong1.github.io/lib/katex/copy-tex.min.js defer></script><script src=/liudongdong1.github.io/lib/katex/mhchem.min.js defer></script><script src=/liudongdong1.github.io/lib/cookieconsent/cookieconsent.min.js defer></script><script src=/liudongdong1.github.io/lib/pangu/pangu.min.js defer></script><script src=/liudongdong1.github.io/lib/cell-watermark/watermark.min.js defer></script><script src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js async defer></script><script>window.config={code:{copyTitle:"复制到剪贴板",editLockTitle:"锁定可编辑代码块",editUnLockTitle:"解锁可编辑代码块",editable:!0,maxShownLines:10},comment:{enable:!1},cookieconsent:{content:{dismiss:"同意",link:"了解更多",message:"本网站使用 Cookies 来改善您的浏览体验。"},enable:!0,palette:{button:{background:"#f0f0f0"},popup:{background:"#1aa3ff"}},theme:"edgeless"},data:{"typeit-header-subtitle-desktop":`<span style='font-family: MMT,"沐目体";'>吾日三省吾身</span>`,"typeit-header-subtitle-mobile":`<span style='font-family: MMT,"沐目体";'>吾日三省吾身</span>`},enablePWA:!0,enablePangu:!0,math:{delimiters:[{display:!0,left:"$$",right:"$$"},{display:!0,left:"\\[",right:"\\]"},{display:!0,left:"\\begin{equation}",right:"\\end{equation}"},{display:!0,left:"\\begin{equation*}",right:"\\end{equation*}"},{display:!0,left:"\\begin{align}",right:"\\end{align}"},{display:!0,left:"\\begin{align*}",right:"\\end{align*}"},{display:!0,left:"\\begin{alignat}",right:"\\end{alignat}"},{display:!0,left:"\\begin{alignat*}",right:"\\end{alignat*}"},{display:!0,left:"\\begin{gather}",right:"\\end{gather}"},{display:!0,left:"\\begin{CD}",right:"\\end{CD}"},{display:!1,left:"$",right:"$"},{display:!1,left:"\\(",right:"\\)"}],strict:!1},search:{algoliaAppID:"2R1K9SKLQZ",algoliaIndex:"index.zh-cn",algoliaSearchKey:"4a226aa1c5c98d6859e4d1386adb2bc7",highlightTag:"em",maxResultLength:10,noResultsFound:"没有找到结果",snippetLength:50,type:"algolia"},siteTime:"2020-12-18T16:15:22+08:00",typeit:{cursorChar:"|",cursorSpeed:1e3,data:{"typeit-header-subtitle-desktop":["typeit-header-subtitle-desktop"],"typeit-header-subtitle-mobile":["typeit-header-subtitle-mobile"]},duration:-1,speed:100},watermark:{appendto:".wrapper>main",colspacing:30,content:'<img class="fixit-icon" src="/fixit.min.svg" alt="FixIt logo" /> FixIt 主题',enable:!0,fontfamily:"inherit",fontsize:.85,height:21,opacity:.0125,rotate:15,rowspacing:60,width:150}}</script><script src=/liudongdong1.github.io/js/theme.min.js defer></script><script src=/liudongdong1.github.io/js/custom.min.js defer></script></body></html>