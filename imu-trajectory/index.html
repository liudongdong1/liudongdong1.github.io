<!DOCTYPE html>
<html itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">
  <head>
    
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
    <meta name="robots" content="noodp" />
    <title>IMU Trajectory - DAY By DAY</title><meta name="author" content="LiuDongdong">
<meta name="author-link" content="https://liudongdong1.github.io/">
<meta name="description" content="Advantages of IMU : (1) energy-efficient, capable of running 24h a day without draining a battery; (2) works any where even inside a bag or a pocket(get device acc); Disadvantage: small sensor errors or biases explode quickly in the double integration process. In Augmented Reality applications(eg., apple ARKit, Google ARCore, Microsoft HoloLens), IMU augments Slam by resolving scale ambiguities and providing motion cues in the absence of visual features. UAVs," /><meta name="keywords" content='IMU, Sensing, CV' /><meta itemprop="name" content="IMU Trajectory">
<meta itemprop="description" content="Advantages of IMU : (1) energy-efficient, capable of running 24h a day without draining a battery; (2) works any where even inside a bag or a pocket(get device acc); Disadvantage: small sensor errors or biases explode quickly in the double integration process. In Augmented Reality applications(eg., apple ARKit, Google ARCore, Microsoft HoloLens), IMU augments Slam by resolving scale ambiguities and providing motion cues in the absence of visual features. UAVs,"><meta itemprop="datePublished" content="2020-06-25T10:09:10+00:00" />
<meta itemprop="dateModified" content="2023-09-28T23:29:54+08:00" />
<meta itemprop="wordCount" content="3364"><meta itemprop="image" content="/logo.png"/>
<meta itemprop="keywords" content="IMU,Sensing,CV," /><meta property="og:title" content="IMU Trajectory" />
<meta property="og:description" content="Advantages of IMU : (1) energy-efficient, capable of running 24h a day without draining a battery; (2) works any where even inside a bag or a pocket(get device acc); Disadvantage: small sensor errors or biases explode quickly in the double integration process. In Augmented Reality applications(eg., apple ARKit, Google ARCore, Microsoft HoloLens), IMU augments Slam by resolving scale ambiguities and providing motion cues in the absence of visual features. UAVs," />
<meta property="og:type" content="article" />
<meta property="og:url" content="liudongdong1.github.io/imu-trajectory/" /><meta property="og:image" content="/logo.png"/><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2020-06-25T10:09:10+00:00" />
<meta property="article:modified_time" content="2023-09-28T23:29:54+08:00" />
<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="/logo.png"/>

<meta name="twitter:title" content="IMU Trajectory"/>
<meta name="twitter:description" content="Advantages of IMU : (1) energy-efficient, capable of running 24h a day without draining a battery; (2) works any where even inside a bag or a pocket(get device acc); Disadvantage: small sensor errors or biases explode quickly in the double integration process. In Augmented Reality applications(eg., apple ARKit, Google ARCore, Microsoft HoloLens), IMU augments Slam by resolving scale ambiguities and providing motion cues in the absence of visual features. UAVs,"/>
<meta name="application-name" content="DAY By DAY">
<meta name="apple-mobile-web-app-title" content="DAY By DAY"><meta name="theme-color" data-light="#f8f8f8" data-dark="#252627" content="#f8f8f8"><meta name="msapplication-TileColor" content="#da532c"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" />
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"><link rel="manifest" href="/site.webmanifest"><link rel="canonical" href="liudongdong1.github.io/imu-trajectory/" /><link rel="prev" href="liudongdong1.github.io/multi-sense/" /><link rel="next" href="liudongdong1.github.io/docker-command/" /><link rel="stylesheet" href="/liudongdong1.github.io/css/style.min.css"><link rel="stylesheet" href="/liudongdong1.github.io/lib/fontawesome-free/all.min.css"><link rel="stylesheet" href="/liudongdong1.github.io/lib/animate/animate.min.css"><script type="application/ld+json">
  {
    "@context": "http://schema.org",
    "@type": "BlogPosting",
    "headline": "IMU Trajectory",
    "inLanguage": "zh-CN",
    "mainEntityOfPage": {
      "@type": "WebPage",
      "@id": "liudongdong1.github.io\/imu-trajectory\/"
    },"genre": "posts","keywords": "IMU, Sensing, CV","wordcount":  3364 ,
    "url": "liudongdong1.github.io\/imu-trajectory\/","datePublished": "2020-06-25T10:09:10+00:00","dateModified": "2023-09-28T23:29:54+08:00","license": "This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.","publisher": {
      "@type": "Organization",
      "name": "LiuDongdong","logo": "\/images\/person.png"},"author": {
        "@type": "Person",
        "name": "liudongdong1"
      },"description": ""
  }
  </script></head>
  <body data-header-desktop="auto" data-header-mobile="auto"><script>(window.localStorage?.getItem('theme') ? localStorage.getItem('theme') === 'dark' : ('auto' === 'auto' ? window.matchMedia('(prefers-color-scheme: dark)').matches : 'auto' === 'dark')) && document.body.setAttribute('data-theme', 'dark');</script><div class="wrapper"><header class="desktop animate__faster" id="header-desktop">
  <div class="header-wrapper" data-github-corner="right">
    <div class="header-title">
      <a href="liudongdong1.github.io/" title="DAY By DAY"><img
    class="lazyload logo"
    src="/liudongdong1.github.io/svg/loading.min.svg"
    data-src="/fixit.min.svg"
    data-srcset="/fixit.min.svg, /fixit.min.svg 1.5x, /fixit.min.svg 2x"
    data-sizes="auto"
    alt="DAY By DAY"
    title="DAY By DAY"/><span class="header-title-text"></span></a><span id="typeit-header-subtitle-desktop" class="typeit header-subtitle"></span></div>
    <nav>
      <ul class="menu"><li class="menu-item">
              <a
                class="menu-link"
                href="/liudongdong1.github.io/posts/"
                
                
              ><i class="fa-solid fa-archive fa-fw fa-sm" aria-hidden="true"></i> 所有文章</a></li><li class="menu-item">
              <a
                class="menu-link"
                href="/liudongdong1.github.io/categories/"
                
                
              ><i class="fa-solid fa-th fa-fw fa-sm" aria-hidden="true"></i> 分类</a></li><li class="menu-item">
              <a
                class="menu-link"
                href="/liudongdong1.github.io/tags/"
                
                
              ><i class="fa-solid fa-tags fa-fw fa-sm" aria-hidden="true"></i> 标签</a></li><li class="menu-item">
              <a
                class="menu-link"
                href="/liudongdong1.github.io/friends/"
                title="友情链接"
                
              ><i class="fa-solid fa-users fa-fw fa-sm" aria-hidden="true"></i> 友链</a></li><li class="menu-item">
              <a
                class="menu-link"
                href="/liudongdong1.github.io/about/"
                
                
              ><i class="fa-solid fa-info-circle fa-fw fa-sm" aria-hidden="true"></i> 关于</a></li><li class="menu-item delimiter"></li><li class="menu-item language">
            <span role="button" aria-label="选择语言" title="选择语言">简体中文<i class="dropdown-icon fa-solid fa-chevron-down" aria-hidden="true"></i>
            </span>
            <ul class="sub-menu"><li class="menu-item">没有更多翻译</li></ul>
          </li><li class="menu-item search" id="search-desktop">
            <input type="text" placeholder="搜索文章标题或内容 ..." id="search-input-desktop">
            <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-desktop" title="搜索">
              <i class="fa-solid fa-search fa-fw" aria-hidden="true"></i>
            </a>
            <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-desktop" title="清空">
              <i class="fa-solid fa-times-circle fa-fw" aria-hidden="true"></i>
            </a>
            <span class="search-button search-loading" id="search-loading-desktop">
              <i class="fa-solid fa-spinner fa-fw fa-spin" aria-hidden="true"></i>
            </span>
          </li><li class="menu-item theme-switch" title="切换主题">
          <i class="fa-solid fa-adjust fa-fw" aria-hidden="true"></i>
        </li>
      </ul>
    </nav>
  </div>
</header><header class="mobile animate__faster" id="header-mobile">
  <div class="header-container">
    <div class="header-wrapper">
      <div class="header-title">
        <a href="liudongdong1.github.io/" title="DAY By DAY"><img
    class="lazyload logo"
    src="/liudongdong1.github.io/svg/loading.min.svg"
    data-src="/fixit.min.svg"
    data-srcset="/fixit.min.svg, /fixit.min.svg 1.5x, /fixit.min.svg 2x"
    data-sizes="auto"
    alt="/fixit.min.svg"
    title="/fixit.min.svg"/><span class="header-title-text"></span></a><span id="typeit-header-subtitle-mobile" class="typeit header-subtitle"></span></div>
      <div class="menu-toggle" id="menu-toggle-mobile">
        <span></span><span></span><span></span>
      </div>
    </div>
    <nav>
      <ul class="menu" id="menu-mobile"><li class="search-wrapper">
            <div class="search mobile" id="search-mobile">
              <input type="text" placeholder="搜索文章标题或内容 ..." id="search-input-mobile">
              <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-mobile" title="搜索">
                <i class="fa-solid fa-search fa-fw" aria-hidden="true"></i>
              </a>
              <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-mobile" title="清空">
                <i class="fa-solid fa-times-circle fa-fw" aria-hidden="true"></i>
              </a>
              <span class="search-button search-loading" id="search-loading-mobile">
                <i class="fa-solid fa-spinner fa-fw fa-spin" aria-hidden="true"></i>
              </span>
            </div>
            <a href="javascript:void(0);" class="search-cancel" id="search-cancel-mobile">
              取消
            </a>
          </li><li
              class="menu-item"
            ><a
                  class="menu-link"
                  href="/liudongdong1.github.io/posts/"
                  
                  
                ><i class="fa-solid fa-archive fa-fw fa-sm" aria-hidden="true"></i> 所有文章</a></li><li
              class="menu-item"
            ><a
                  class="menu-link"
                  href="/liudongdong1.github.io/categories/"
                  
                  
                ><i class="fa-solid fa-th fa-fw fa-sm" aria-hidden="true"></i> 分类</a></li><li
              class="menu-item"
            ><a
                  class="menu-link"
                  href="/liudongdong1.github.io/tags/"
                  
                  
                ><i class="fa-solid fa-tags fa-fw fa-sm" aria-hidden="true"></i> 标签</a></li><li
              class="menu-item"
            ><a
                  class="menu-link"
                  href="/liudongdong1.github.io/friends/"
                  title="友情链接"
                  
                ><i class="fa-solid fa-users fa-fw fa-sm" aria-hidden="true"></i> 友链</a></li><li
              class="menu-item"
            ><a
                  class="menu-link"
                  href="/liudongdong1.github.io/about/"
                  
                  
                ><i class="fa-solid fa-info-circle fa-fw fa-sm" aria-hidden="true"></i> 关于</a></li><li
              class="menu-item text-center"
            ><a
                  class="menu-link"
                  href="https://liudongdong1.github.io/"
                  title="GitHub"
                  rel="noopener noreferrer" target="_blank"
                ><i class='fa-brands fa-github fa-fw' aria-hidden='true'></i> </a></li><li class="menu-item theme-switch" title="切换主题">
          <i class="fa-solid fa-adjust fa-fw" aria-hidden="true"></i>
        </li><li class="menu-item language">
            <span role="button" aria-label="选择语言" title="选择语言">简体中文<i class="dropdown-icon fa-solid fa-chevron-down" aria-hidden="true"></i>
            </span>
            <select class="language-select" onchange="location = this.value;"><option disabled>没有更多翻译</option></select>
          </li></ul>
    </nav>
  </div>
</header><div class="search-dropdown desktop">
    <div id="search-dropdown-desktop"></div>
  </div>
  <div class="search-dropdown mobile">
    <div id="search-dropdown-mobile"></div>
  </div><main class="container" data-page-style="normal"><aside class="toc" id="toc-auto"><h2 class="toc-title">目录 <i class="toc-icon fa-solid fa-angle-down fa-fw"></i></h2>
      <div class="toc-content" id="toc-content-auto"></div></aside>

  <aside class="aside-custom" id="aside-sakana">
    

<div class="sakana-widget">
  <div class="sakana-item" id="takina-widget"></div>
  <div class="sakana-item" id="chisato-widget"></div>
</div>
<script>
  function initSakanaWidget() {
    const takina = SakanaWidget.getCharacter('takina')
    SakanaWidget.registerCharacter('takina-slow', takina);
    new SakanaWidget({
      character: 'takina-slow',
      controls: false,
      autoFit: true,
      stroke: {
        color: "#b4b4b4",
        width: 2
      }
    }).mount('#takina-widget');

    const chisato = SakanaWidget.getCharacter('chisato')
    SakanaWidget.registerCharacter('chisato-slow', chisato);
    new SakanaWidget({
      character: 'chisato-slow',
      controls: false,
      autoFit: true,
      stroke: {
        color: "#b4b4b4",
        width: 2
      }
    }).mount('#chisato-widget');
  }
</script>
<script async onload="initSakanaWidget()" src="https://cdn.jsdelivr.net/npm/sakana-widget@2.3.0/lib/sakana.min.js">
</script></aside>

  <article class="page single">
    <div class="header"><h1 class="single-title animate__animated animate__flipInX">
        <span>IMU Trajectory</span>
      </h1></div><div class="post-meta">
      <div class="post-meta-line"><span class="post-author"><span class="author"><i class="fa-solid fa-user-circle" aria-hidden="true"></i>
      liudongdong1</span></span>
          <span class="post-category">收录于 <a href="liudongdong1.github.io/categories/"><i class="fa-regular fa-folder fa-fw"></i>&nbsp;Categories</a>&ensp;<a href="liudongdong1.github.io/categories/aiot/"><i class="fa-regular fa-folder fa-fw"></i>&nbsp;AIOT</a></span></div>
      <div class="post-meta-line"><span title=2020-06-25&#32;10:09:10>
            <i class="fa-regular fa-calendar-alt fa-fw"></i>&nbsp;<time datetime="2020-06-25" >2020-06-25</time>
          </span>&nbsp;<i class="fa-solid fa-pencil-alt fa-fw"></i>&nbsp;约 3364 字&nbsp;
        <i class="fa-regular fa-clock fa-fw"></i>&nbsp;预计阅读 7 分钟&nbsp;<span id="busuanzi_container_page_pv" class="busuanzi_visitors comment-visitors" data-flag-title="IMU Trajectory">
            <i class="fa-regular fa-eye fa-fw"></i>&nbsp;<span id="busuanzi_value_page_pv">-</span>&nbsp;次阅读
          </span>&nbsp;</div>
    </div><div class="featured-image"><img
    class="lazyload"
    src="/liudongdong1.github.io/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/armature-4103639__340.webp"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/armature-4103639__340.webp, https://gitee.com/github-25970295/blogImage/raw/master/img/armature-4103639__340.webp 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/armature-4103639__340.webp 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/armature-4103639__340.webp"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/armature-4103639__340.webp"/></div><div class="details toc" id="toc-static" kept="true">
        <div class="details-summary toc-title">
          <span>目录</span>
          <span><i class="details-icon fa-solid fa-angle-right"></i></span>
        </div>
        <div class="details-content toc-content" id="toc-content-static"><nav id="TableOfContents">
  <ul>
    <li><a href="#paper-fusing-wearable-imus">Paper: Fusing Wearable IMUs</a>
      <ul>
        <li></li>
      </ul>
    </li>
    <li><a href="#paper-imutube">Paper: IMUTube</a>
      <ul>
        <li></li>
      </ul>
    </li>
  </ul>

  <ul>
    <li><a href="#paper-error-reduction-for-imu">Paper: Error reduction for IMU</a>
      <ul>
        <li></li>
      </ul>
    </li>
    <li><a href="#paper-ronin">Paper: RoNIN</a>
      <ul>
        <li></li>
      </ul>
    </li>
    <li><a href="#paper-ridi">Paper: RIDI</a>
      <ul>
        <li></li>
      </ul>
    </li>
    <li><a href="#paper-ai-imu">Paper: AI-IMU</a>
      <ul>
        <li></li>
      </ul>
    </li>
    <li><a href="#paper-multi-person-motion-forecasting">Paper: Multi-Person Motion Forecasting</a>
      <ul>
        <li></li>
      </ul>
    </li>
    <li><a href="#paper-handwriting-trajectory">Paper: Handwriting Trajectory</a>
      <ul>
        <li></li>
      </ul>
    </li>
  </ul>
</nav></div>
      </div><div
      class="content"
      id="content"
      
      
    ><blockquote>
<p>Advantages of IMU : (1) energy-efficient, capable of running 24h a day without draining a battery; (2) works any where even inside a bag or a pocket(get device acc); Disadvantage: small sensor errors or biases explode quickly in the double integration process.</p>
<p>In Augmented Reality applications(eg., apple ARKit, Google ARCore, Microsoft HoloLens), IMU augments Slam by resolving scale ambiguities and providing motion cues in the absence of visual features. UAVs, automous cars, humanoid robots, and smart vacuum cleaners are other emerging domains, utilizing IMUs for enhanced navigation, control, and beyond.</p>
</blockquote>
<h1 id="1-pose-estimation-relative">1. Pose Estimation Relative</h1>
<p><strong>author</strong>: Zhe Zhang( Southeast University), Chunyu Wang(Microsoft Research Asia)
<strong>date</strong>: 2020, 4.10
<strong>keyword</strong>:</p>
<ul>
<li>3D pose estimation</li>
</ul>
<hr>
<h2 id="paper-fusing-wearable-imus">Paper: Fusing Wearable IMUs</h2>
<!-- raw HTML omitted -->
<h4 id="summary">Summary</h4>
<ol>
<li>present a geometric approach to reinforce the visual features of each pair of joints based on the IMU,  improving the 2D pose estimation accuracy especially when one joint is occluded.</li>
<li>fit the multi-view 2D poses to the 3D space by an orientation Regularized pictorial Structure Model which jointly minimizes the projection error between the 3D and 2D poses along with the discrepancy between the 3D pose and IMU orientations.</li>
</ol>
<p><img
    class="lazyload"
    src="/liudongdong1.github.io/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200612163514924.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200612163514924.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200612163514924.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200612163514924.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200612163514924.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200612163514924.png"/></p>
<h4 id="research-objective">Research Objective</h4>
<ul>
<li><strong>Purpose</strong>:  solve the occlusion problem in estimating 3D poses from images.</li>
</ul>
<h4 id="proble-statement">Proble Statement</h4>
<ul>
<li>instead of estimating 3D poses or pose embeddings from images and IMUs separately and then fusing them in a late stage, we propose to fuse IMUs and image features in a very early stage with the aid of 3D geometry.</li>
<li>in 3D pose estimation step, we levarage IMUs in the pictorial structure model.</li>
</ul>
<p>previous work:</p>
<ul>
<li>previous pose estimation use the limb length prior to prevent from generating abnormal 3D poses, and the prior is fixed for the same person and doesn&rsquo;t change over time.</li>
<li><strong>Image-based:</strong>
<ul>
<li>model/optimization based: defines the 3D parametric human body model and optimizes its parameters to minimize the discrepancy between model projections and extracted image features.</li>
<li>Supervised learning: learning a mapping from images to 3D pose , lack of abundant ground truth 3D poses, not aware of their absolute locations in the world coordinate system.</li>
<li>two-step mothods: first estimate 2D poses in each camera view, and recovers the 3D pose in a world coordinate system with camera paremeters.</li>
</ul>
</li>
<li><strong>IMUs-based:</strong>  suffer from drifting over time
<ul>
<li>Slyper et al. Tautges et al. propose to reconstruct human pose from 5 accelerometers by retrieving pre-recorded poses with similar accelerations from a databases.</li>
<li>Roetenberg et al. use 17 IMUs equipped with 3D accelerometers, gyroscopes and magnetometers and all the measurements are fused using Kalman Filter, to get the pose of the subject.</li>
</ul>
</li>
<li><strong>Images+IMUs-based</strong>:
<ul>
<li>estimate 3D human pose by minimizing an energy function which is related to both IMUs and image feature.</li>
<li>estimate 3D poses separately from the images and IMUs and then combine them to get the final estimation.</li>
</ul>
</li>
</ul>
<h4 id="methods">Methods</h4>
<ul>
<li><strong>Procedure</strong>:</li>
</ul>
<blockquote>
<p>introduce 2D Orientation Regularized Network to jointly estimate 2D poses for multi-view images. Using IMU orientations as a structural prior to mutually fuse the image features of each pair of joints linked by IMUs.</p>
<p>estimate 3D pose from multi-view 2D poses(heatmaps) by a Pictorial Structure Model, it jointly minimizes the projection error between the 3D and 2D poses</p>
</blockquote>
<ul>
<li><strong>system overview</strong>:</li>
</ul>
<p><img
    class="lazyload"
    src="/liudongdong1.github.io/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200612171943527.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200612171943527.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200612171943527.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200612171943527.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200612171943527.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200612171943527.png"/></p>
<p>【Qustion 1】<!-- raw HTML omitted --> 关键部分不理解,具体细节后面遇到再说<!-- raw HTML omitted --></p>
<h4 id="evaluation">Evaluation</h4>
<ul>
<li><strong>Environment</strong>:
<ul>
<li>Dataset:</li>
</ul>
</li>
</ul>
<p><img
    class="lazyload"
    src="/liudongdong1.github.io/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200612172211707.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200612172211707.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200612172211707.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200612172211707.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200612172211707.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200612172211707.png"/></p>
<h4 id="notes-font-colororange去加强了解font">Notes <!-- raw HTML omitted -->去加强了解<!-- raw HTML omitted --></h4>
<ul>
<li><a href="https://github.com/CHUNYUWANG/imu-human-pose-pytorch"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/CHUNYUWANG/imu-human-pose-pytorch<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></li>
<li>Daniel Roetenberg, Henk Luinge, and Per Slycke. Xsens mvn: full 6dof human motion tracking using miniature inertial sensors. Xsens Motion Technologies BV, Tech. Rep, 1, 2009.</li>
</ul>
<p><strong>level</strong>: on arXiv, don;t know the meeting.
<strong>author</strong>: Hyeokhyen Kwon(Georgia Tech), Catherine Tong(University of Oxford)
<strong>date</strong>: 2018,6
<strong>keyword</strong>:</p>
<ul>
<li>Ubiquitous and Mobile computing, Data Collection, Activity Recognition.</li>
</ul>
<hr>
<h2 id="paper-imutube">Paper: IMUTube</h2>
<!-- raw HTML omitted -->
<h4 id="summary-1">Summary</h4>
<ol>
<li>introduce IMUTube, an automated processing pipeline that integrated existing computer vision and signal processing techniques to convert videos of human activity into virtual streams of IMU data.</li>
<li>processing pipline:
<ul>
<li>applies standard pose tracking and 3D scene understanding techniques to estimate full 3D human motion from video segment that captures a target activity.</li>
<li>translates the visual tracking information into IMU that are placed on dedicated body position.</li>
<li>adapts the virtual IMU data towards the target domain through distribution matching.</li>
<li>derives activity recognizers from the generated virtual sensor data, potentially enriched with small amounts of real sensor data.</li>
</ul>
</li>
</ol>
<h4 id="research-objective-1">Research Objective</h4>
<ul>
<li><strong>Application Area</strong>: HAR( behavioral analysis like user authentication, healthcare, and tracking everyday activities)</li>
<li><strong>Purpose</strong>:   aim at harvesting existing video data from large-scale repositories, such as YouTube and automatically generate data for virtual, IMUs that will then used for deriving sensor-based human activity recognition system.</li>
</ul>
<h4 id="proble-statement-1">Proble Statement</h4>
<ul>
<li>the lack of large-scale, labeled data sets impedes progress in developing robust and generalized predictive models for  on-body sensor-based human activity recognition.</li>
<li>sensor data collection is expensive and the annotation is time-consuming and error-prone.</li>
</ul>
<h4 id="challenges">Challenges:</h4>
<ol>
<li>the datasets needs to be curated and filtered towards the actual activities of interest;</li>
<li>even though video data capture the same information about activities in principle, sophisticated preprocessing is required to match the source and target sensing domains;</li>
<li>the opportunistic use of activity videos requires adaptations to account for contextual factors such as multiple scene changes, rapid camera orientation changes, scale of the performer in the far sight, or multiple background people not involved in the activity;&lt;font color</li>
<li>new forms of features and activity recognition models will need to be designed to overcome the short-comings of learning from video-sourced motion information for eventual IMU-based inference.</li>
</ol>
<h4 id="methods-1">Methods</h4>
<ul>
<li><strong>system overview</strong>:</li>
</ul>
<p><img
    class="lazyload"
    src="/liudongdong1.github.io/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200625110657529.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200625110657529.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200625110657529.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200625110657529.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200625110657529.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200625110657529.png"/></p>
<p>【Module 1】Motion Estimatioln for 3D joints</p>
<blockquote>
<ol>
<li>estimate 2D pose skeletons for potentially multiple people in a scene using OpenPose.</li>
<li>lift each 2D pose to 3D pose by estimating the depth information using <!-- raw HTML omitted -->VideoPose3D model [56].<!-- raw HTML omitted --></li>
<li>apply SORT tracking algorithm[7] to track each person across the vedio sequence.<!-- raw HTML omitted --> 去了解<!-- raw HTML omitted --></li>
<li>interpolate and smooth missing or noise keypoints using KF algorithm.</li>
</ol>
</blockquote>
<p><img
    class="lazyload"
    src="/liudongdong1.github.io/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200625134748743.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200625134748743.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200625134748743.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200625134748743.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200625134748743.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200625134748743.png"/></p>
<p>【Module 2】Global Body Tracking in 3D</p>
<blockquote>
<p>To extract global 3D scene information from the 2D video to track a person&rsquo;s movement in the whole scene.</p>
<ol>
<li>3D localization in each 2D frame.</li>
<li>the camera viewpoint changes(ego-motion) between subsequence 3D scenes</li>
</ol>
</blockquote>
<ul>
<li>3D Pose Calibration:
<ul>
<li>using Pnp algorthm to calculate perspective projection between corresponding 3D and 2D keypoints.[33]</li>
<li>estimate the camera intrinsic parameters from video using DeepCalib model[8].</li>
</ul>
</li>
<li>Estimate camera egomotion： potential viewpoint changes across frames.
<ul>
<li>Camera ego-motion estimation from one viewpoint to another requies 3D point clouds of both scenes. To create 3D point of a scene requires two information: 1. the depth map(using DepthWild model[22]); 2. camera intrinsic parameters</li>
</ul>
</li>
</ul>
<p><img
    class="lazyload"
    src="/liudongdong1.github.io/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200625135916080.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200625135916080.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200625135916080.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200625135916080.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200625135916080.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200625135916080.png"/></p>
<p>【Module 3】Generating Virtual Sensor Data</p>
<ul>
<li>extract 3D motion information for each person in a video.</li>
<li>To tack the orientation change of person joint from the perspective of the body coordinates.</li>
</ul>
<h1 id="2-navigation-relative">2. Navigation Relative</h1>
<p><strong>level</strong>: Institufe of physic publishing
<strong>author</strong>: J Gao(University of Nottingham)
<strong>date</strong>: 2002,12,5</p>
<p><strong>keyword</strong>:</p>
<ul>
<li>IMU, Navigation, Trajectory</li>
</ul>
<hr>
<h2 id="paper-error-reduction-for-imu">Paper: Error reduction for IMU</h2>
<!-- raw HTML omitted -->
<h4 id="summary-2">Summary</h4>
<p><strong>【Knowledge one】Category of the errors for IMU system</strong></p>
<ul>
<li>Inertial sensor errors, including sensor bias error, cross-axis coupling and scale factor error;</li>
<li>misalignment error, due to the misalignment angle along the sensitive axis an gravity acceleration component will be sensed as a part of acceleration</li>
<li>computational process errors, such as numerical integration error.</li>
</ul>
<p><img
    class="lazyload"
    src="/liudongdong1.github.io/svg/loading.min.svg"
    data-src="C:/Users/dell/AppData/Roaming/Typora/typora-user-images/image-20200620085340958.png"
    data-srcset="C:/Users/dell/AppData/Roaming/Typora/typora-user-images/image-20200620085340958.png, C:/Users/dell/AppData/Roaming/Typora/typora-user-images/image-20200620085340958.png 1.5x, C:/Users/dell/AppData/Roaming/Typora/typora-user-images/image-20200620085340958.png 2x"
    data-sizes="auto"
    alt="image-20200620085340958"
    title="image-20200620085340958"/></p>
<blockquote>
<p>waveform numerical integration methods, such as cumulative rectangle, trapezoidal and Simpson integration methods.</p>
</blockquote>
<p><strong>【Knowledge two】Velocity waveform reconstruction</strong></p>
<blockquote>
<p>a high sampling rate should be used when data acquisition is performed to reduce the sampling interval and thus reduce the integrationerror. However,this will result in more noise in the measured data and increase the calculation burden.</p>
</blockquote>
<p><img
    class="lazyload"
    src="/liudongdong1.github.io/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200620090253185.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200620090253185.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200620090253185.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200620090253185.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200620090253185.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200620090253185.png"/></p>
<p><strong>【Knowledge three】Waveform distortion</strong></p>
<p><img
    class="lazyload"
    src="/liudongdong1.github.io/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200620090545825.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200620090545825.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200620090545825.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200620090545825.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200620090545825.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200620090545825.png"/></p>
<p><strong>【Knowledge four】 Velocity waveform correction</strong></p>
<blockquote>
<p>Fit the distortion of the velocity by a polynomial.     Trujillo and Carter</p>
<p>the physical properties and boundary conditions of machine motion the velocity waveform can be corrected by second-order or first-order polynomials.</p>
</blockquote>
<p>$$
v_c(t)=v(t)+b_1t^2+b_2t+b_3 \
x_c(t)=x(t)+b_1/3t^3+b_2/2t^2+b_3t+b_4
$$</p>
<p>$$
v_c(t)=v(t)+b_1t+b_2 \
x_c(t)=x(t)+b_1/2t^2+b_2t+b_3
$$</p>
<p>assuming $v(0)=0,v(T)=0,x(0)=0,x(T)!=0$
$$
v_c(t)=v(t)+1/T[v(0)-v(T)]-v(0)  \
x_c(t)=x(t)+1/(2T)[v(0)-v(T)]t^2-v(0)t-x(0)
$$
<img
    class="lazyload"
    src="/liudongdong1.github.io/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200620092722253.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200620092722253.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200620092722253.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200620092722253.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200620092722253.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200620092722253.png"/></p>
<p><img
    class="lazyload"
    src="/liudongdong1.github.io/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200620093339022.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200620093339022.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200620093339022.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200620093339022.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200620093339022.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200620093339022.png"/></p>
<p><strong>keyword</strong>:</p>
<ul>
<li>IMU, Navigation, Trajectory</li>
</ul>
<hr>
<h2 id="paper-ronin">Paper: RoNIN</h2>
<!-- raw HTML omitted -->
<h4 id="summary-3">Summary</h4>
<ol>
<li>a new benchmark containing more than 40h of IMU sensor data from 100 human subjects with ground-truth 3D trajectory under natural human motions</li>
<li>novel neural inertial navigation architectures, making significant improvements for challenging motion cases</li>
<li>qualitative and quantitative evaluations of the competing methods over three inertial navigation benchmarks.</li>
</ol>
<h4 id="proble-statement-2">Proble Statement</h4>
<ul>
<li><strong>Physics-based:</strong> IMU double integration. (Sensor biases explode quickly in the double integration process)
<ul>
<li>foot mounted IMU with zero speed update, the sensor bias can be corrected subject to a constraint that the velocity must become zero whenever foot touches the ground</li>
</ul>
</li>
<li><strong>Heuristic:</strong> human motion are highly repetitive
<ul>
<li>step counting: (1) the IMU is rigidly attached to body. (2) the motion direction is fixed with respect to IMU. (3) the distance of travel is proportional to the number of foot-steps.</li>
<li>PCA[10] or frequency domain analysis[13] to infer motion direction.</li>
</ul>
</li>
<li><strong>Data-driven priors:</strong>
<ul>
<li>RIDI focuses on regressing velocity vectors in a device coordinate frame, while rely on traditional sensor fusion methods to estimate device orientation.</li>
</ul>
</li>
</ul>
<h4 id="methods-2">Methods</h4>
<ul>
<li><strong>system overview</strong>:</li>
</ul>
<p><img
    class="lazyload"
    src="/liudongdong1.github.io/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200612111925446.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200612111925446.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200612111925446.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200612111925446.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200612111925446.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200612111925446.png"/></p>
<p>【Qustion 1】how to get body heading information?</p>
<blockquote>
<p>the body orientation differs from the device orientation arbitrarily depending on how one carries a phone.   Assuming the headings of the tracking phone with an constant offset introduced by the misalignment of the harness, and ask the subject to walk straight for the first five seconds, then estimate this offset angle as the difference between the average motion heading and the tracking phone&rsquo;s heading.</p>
</blockquote>
<p>【Qustion 2】RoNIN network to regress the heading direction? <!-- raw HTML omitted --> don&rsquo;t understand<!-- raw HTML omitted --></p>
<blockquote>
<p>RoNIN seeks to regress a velocity vector given an IMU sensor histroy with two key design principles,(1) Coordinate frame normalization defining the input and output feature space;(2) robust velocity losses improving the signal-to-noise-ratio even with noisy regression targets.</p>
</blockquote>
<ul>
<li>
<p><strong>Coordinate frame normalization:</strong></p>
<blockquote>
<p>uses a heading-agnostic coordinate frame, any coordinate frame whose Z axis is aligned with gravity.</p>
<p>IMU data is transformed into the same HACF by the device orientation and the same horizontal rotation.</p>
</blockquote>
</li>
</ul>
<p><img
    class="lazyload"
    src="/liudongdong1.github.io/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200612160431127.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200612160431127.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200612160431127.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200612160431127.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200612160431127.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200612160431127.png"/></p>
<h4 id="evaluation-1">Evaluation</h4>
<ul>
<li><strong>Environment</strong>:</li>
</ul>
<p><img
    class="lazyload"
    src="/liudongdong1.github.io/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200612155100851.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200612155100851.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200612155100851.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200612155100851.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200612155100851.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200612155100851.png"/></p>
<p><img
    class="lazyload"
    src="/liudongdong1.github.io/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200612162530065.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200612162530065.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200612162530065.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200612162530065.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200612162530065.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200612162530065.png"/>
<strong>date</strong>: 2017
<strong>keyword</strong>:</p>
<ul>
<li>Smartphones, Trajectory, IMU</li>
</ul>
<hr>
<h2 id="paper-ridi">Paper: RIDI</h2>
<!-- raw HTML omitted -->
<h4 id="summary-4">Summary</h4>
<ol>
<li>proposes a novel data-driven approach for inertial navigation, which learns to estimate trajectories of natural human motions.</li>
<li>based obervation: human motions are repetitive and consist of a few major modes(standing, walking, turning).</li>
<li>regresses a velocity vector from the history of linear accelerations and angular velocities, then corrects low-frequency bias in the linear accelerations, which are integrated twice to estimate position.</li>
</ol>
<p>previous work:</p>
<ul>
<li><strong>V-slame</strong>: (1) a camera must have a clear light-of-sight under well-lit environments all the time.(2) the recording and processing of the video data quickly drain a battery.</li>
</ul>
<h4 id="methods-3">Methods</h4>
<ul>
<li><strong>system overview</strong>: use the IMU to estimate trajectories of natural human motions</li>
</ul>
<p><img
    class="lazyload"
    src="/liudongdong1.github.io/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200612101206414.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200612101206414.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200612101206414.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200612101206414.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200612101206414.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200612101206414.png"/></p>
<p>【Algorithm】</p>
<blockquote>
<p>First, regresses a velocity vector from angular velocities and linear accelerations( accelerometer readings minus gravity).</p>
<p>Second, RIDI estimates low-frequency corrections in the linear accelerations so that their integrated velocities math the regressed values.</p>
</blockquote>
<p>【Question 1】how to learning to regress volocities?</p>
<blockquote>
<p>transform the device poses( world Coor) and angular velocities and linear acc( device Coor) into s, and apply Gaussian smoothing to supress high-frequency.</p>
<p>concatenate smoothed angular velocities and linear acc from the past 200 frames to construct a 1200 dimensional feature vector</p>
</blockquote>
<p><img
    class="lazyload"
    src="/liudongdong1.github.io/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200612103657344.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200612103657344.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200612103657344.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200612103657344.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200612103657344.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200612103657344.png"/></p>
<p>【Question 2】how to correcting acceleration errors?</p>
<blockquote>
<p>Error Sources: IMU readings, system gravities, system rotations, intract in a complex way.</p>
<p>Predicted velocities to provide effective cues in removing sensor noises and biases.</p>
</blockquote>
<h4 id="evaluation-2">Evaluation</h4>
<ul>
<li><strong>Environment</strong>:
<ul>
<li>Dataset:  six human subjects with four popular smartphone placements, and relative ground-truth.</li>
</ul>
</li>
</ul>
<h4 id="conclusion">Conclusion</h4>
<ul>
<li>the first to integrate sophisticated machine learning techniques with inertial navigation.</li>
<li>database of IMU sensor measurements and 3D motion trajectories across multiple human subjects and multiple device placements.( six human, four kinds placements, various type motions including walking forward/backward, side motion, acceleration/deceleration)</li>
</ul>
<p><img
    class="lazyload"
    src="/liudongdong1.github.io/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200612105303609.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200612105303609.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200612105303609.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200612105303609.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200612105303609.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200612105303609.png"/>
<strong>author</strong>:Martin Brossard（MINES Paris Tech)
<strong>date</strong>: 2019
<strong>keyword</strong>:</p>
<ul>
<li>AI, Trajectory, IMU</li>
</ul>
<hr>
<h2 id="paper-ai-imu">Paper: AI-IMU</h2>
<!-- raw HTML omitted -->
<h4 id="summary-5">Summary</h4>
<ul>
<li>propose a novel accurate method for dead-reckoning of wheeled vehicles based only on an Inertial Measurement Unit.</li>
<li>for intelligent vehicles, robust and accurate dead-reckoning based on IMU may prove useful to correlate feeds from imaging sensors, to safely navigate through obstructions, or for safe emergency stops in the extreme case of exteroceptive sensors failure.</li>
<li><!-- raw HTML omitted --> using Kalman filter and use of deep neural networks to dynamically adapt the noise parameters of the filter.<!-- raw HTML omitted --></li>
<li>tested on KITTI odometry dataset, this article estimates 3D position, velocity, orientation of the vehicle and self-calibrates the IMU biases, and achieve on average a 1.10% translational error and the algorithm competes with top-ranked methods.</li>
<li>the code:https://github.com/mbrossar/ai-imu-dr</li>
</ul>
<p><img
    class="lazyload"
    src="/liudongdong1.github.io/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200624220627710.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200624220627710.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200624220627710.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200624220627710.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200624220627710.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200624220627710.png"/></p>
<p><strong>level</strong>: UbiComp/ISWC'19
<strong>author</strong>:Yasuo Katsuhara(Frontier Research Center,Toyata Motor Corp)
<strong>date</strong>: 2019
<strong>keyword</strong>:</p>
<ul>
<li>AI, Trajectory, IMU</li>
</ul>
<hr>
<h2 id="paper-multi-person-motion-forecasting">Paper: Multi-Person Motion Forecasting</h2>
<!-- raw HTML omitted -->
<h4 id="summary-6">Summary</h4>
<ol>
<li>propose a multi-person motion forecasting system by using IMU motion captures to overcome these difficulties simultaneously.</li>
</ol>
<h4 id="research-objective-2">Research Objective</h4>
<ul>
<li><strong>Application Area</strong>: Human-centered computing, body motion forecasting; IMU based motion capture; sports and entertainment; anomaly motion prediction for factory workers and drivers;</li>
<li><strong>Purpose</strong>:</li>
</ul>
<h4 id="proble-statement-3">Proble Statement</h4>
<ul>
<li>camera and optical based methods have to take into account the environmental settings and occlusion problems</li>
<li>previous studies don&rsquo;t consider plural persons.</li>
</ul>
<p>previous work:</p>
<ul>
<li>employed cameras and optical motion captures to measure the joint positions of person, and predicted them about 0.5s before by using DNN.</li>
</ul>
<h4 id="methods-4">Methods</h4>
<ul>
<li>
<p><strong>Problem Formulation</strong>:</p>
</li>
<li>
<p><strong>system overview</strong>:</p>
</li>
</ul>
<p>【Module 1】Data Collection</p>
<ul>
<li>
<p>using Perception Neuron2.0 to collect three dimensianal position(x,y,z) of 21 body joints. sampleing interval was 50 ms</p>
<p><img
    class="lazyload"
    src="/liudongdong1.github.io/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200624223907049.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200624223907049.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200624223907049.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200624223907049.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200624223907049.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200624223907049.png"/></p>
</li>
</ul>
<p>【Module 2】Data Processing</p>
<ul>
<li>employed Horiuchi&rsquo;s manner[2], in each frame capture 21 3d joints position and center of gravity.</li>
<li>to forecast the positions after 0.5s, the predicted position vector $z_t$ is define by the 3D position of t+10 frames.</li>
<li>using 3-layer neural networks to forcaset the position.</li>
<li><!-- raw HTML omitted --> to predict standing up and walking acitvities<!-- raw HTML omitted --></li>
</ul>
<p><img
    class="lazyload"
    src="/liudongdong1.github.io/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200624225012923.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200624225012923.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200624225012923.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200624225012923.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200624225012923.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200624225012923.png"/></p>
<p><img
    class="lazyload"
    src="/liudongdong1.github.io/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200624225645127.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200624225645127.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200624225645127.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200624225645127.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200624225645127.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200624225645127.png"/></p>
<p><img
    class="lazyload"
    src="/liudongdong1.github.io/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200624225813310.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200624225813310.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200624225813310.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200624225813310.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200624225813310.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200624225813310.png"/></p>
<p><img
    class="lazyload"
    src="/liudongdong1.github.io/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200624225901410.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200624225901410.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200624225901410.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200624225901410.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200624225901410.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200624225901410.png"/></p>
<h4 id="notes-font-colororange去加强了解font-1">Notes <!-- raw HTML omitted -->去加强了解<!-- raw HTML omitted --></h4>
<ul>
<li>Paper 7: a mixed reality martial arts training system with RGB camera.</li>
</ul>
<p><strong>level</strong>: Proceedings of the 2020 IEEE/SICE International Symposium on System Integration, Hawaii,USA
<strong>author</strong>: Wataru Takano
<strong>date</strong>: 2020, 1-12</p>
<hr>
<p>Paper: Sentence Generation</p>
<!-- raw HTML omitted -->
<h4 id="summary-7">Summary</h4>
<ol>
<li>presents a probabilistic approach toward integrating human whole-body motions with natural language.</li>
<li><!-- raw HTML omitted --> human whole-body motions in daily life are recorded by IMU and subsequently encoded into motion symbols. Sentences are manually attached to the human motion primitives for their annotation by probabilistic graphical models.<!-- raw HTML omitted --></li>
<li><!-- raw HTML omitted --> One probabilistic model trains the lining of motion symbols to words, and the other represents sentence structure as word sequences<!-- raw HTML omitted --></li>
<li>translating human whole-body motions into descriptions, where multiple words are associated from the human motions by the first model and the second model searches for syntactically consistent sentences consisting of associated words.</li>
<li>seventeen IMU sensors are attached to a performer, the sesulting sensor data are transformed to positions of 34 virtual markers attached to the performer, and human whole body posture is expressed by a feature vector whose elements are positions of the virtual makers in a trunk coordinate system.</li>
</ol>
<h4 id="httpsgiteecomgithub-25970295blogimagerawmasterimgimage-20200624232127699png"><img
    class="lazyload"
    src="/liudongdong1.github.io/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200624232127699.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200624232127699.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200624232127699.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200624232127699.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200624232127699.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200624232127699.png"/></h4>
<p><img
    class="lazyload"
    src="/liudongdong1.github.io/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200624232815685.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200624232815685.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200624232815685.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200624232815685.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200624232815685.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200624232815685.png"/></p>
<p><img
    class="lazyload"
    src="/liudongdong1.github.io/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200624232753572.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200624232753572.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200624232753572.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200624232753572.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200624232753572.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200624232753572.png"/></p>
<h4 id="contribution">Contribution</h4>
<ul>
<li>recorded many human whole-body daily life motions by using IMU sensors, by annotated these human motions, we create large datesets.</li>
<li>present a framework for combining human whole body motions with sentences, Mappings between human motions and relevant words, and transitioning among words in sentences are represented by probabilistic graphical models.</li>
<li>the methods computes three probabilities: the probability of observed human motion being generated by motion symbols, probability of words being associated with those motion symbols, and probability of word transitions.</li>
</ul>
<p><strong>level</strong>: IEEE Transactions on Emerging Topic in Computational Intellegence.
<strong>author</strong>: Tse-Uu Pan( Student Member,IEEE), National Cheng Kung University ,Tainan
<strong>date</strong>: 2019.6
<strong>keyword</strong>:</p>
<ul>
<li>IMU, Trajectory</li>
</ul>
<hr>
<h2 id="paper-handwriting-trajectory">Paper: Handwriting Trajectory</h2>
<!-- raw HTML omitted -->
<h4 id="summary-8">Summary</h4>
<ol>
<li>propose a trajectory reconstruction method based on low-cost IMU in smartphones.</li>
<li>intrinsic bias and random noise usually cause unreliable IMU signals, filtering methods are utilized to reduce high- or low-frequency noises of signal.</li>
<li>extract multiple features from IMU signals and train a movement detection model based on LDA.</li>
<li>recognize the handwritten letter by constructing the trajectory.</li>
</ol>
<h4 id="research-objective-3">Research Objective</h4>
<ul>
<li><strong>Application Area</strong>: positioning, various HCI applications(human activity recognition, gait detection, patient resuscitation, sports sciences)</li>
<li><strong>Purpose</strong>:</li>
</ul>
<h4 id="proble-statement-4">Proble Statement</h4>
<ul>
<li>the writing space is often too small for many handwriting applications.</li>
<li>previous IMU based focus on large motion gesture/activity, can&rsquo;t deal with subtle interactions in VR.</li>
</ul>
<p>previous work:</p>
<ul>
<li>infrared sensors, ultrasonic sensors, tiny cameras to record the trajectory. <!-- raw HTML omitted --> high power-consumed<!-- raw HTML omitted --></li>
<li>Noitom Ltd. develop a product, Perception Neuron, a set of IMU sensors worn on a human body to record human movement and then control animation of virtual human in virtual space based on the recorded data.</li>
<li>Thalmic Labs introduced a wearable device named Myo, equipped with eight sEMG sensors and one IMU sensor to measure the trajectory of hand movements and hand gestures.</li>
<li>Wacon Inkling used an external sensor to sense ultrasound and infrared to reconstruct entire trajectory.</li>
<li>Livescrible Echo introduced a digital pen and a notebook to reconstruct the trajectory of handwritting by using invisible points on the notebook with which the pen could locate itself by sensing the invisible points.</li>
<li>Sperbel et al.[11] proposed a vision-based digital pen to reconstruct trajectory of handwriting.</li>
<li>Xie et al.[20] presented an accelerometer-based smart ring and used a similarity matching-based smart ring and used a similarity matching-based extensible algorithm to recognize basic and complex gestures.</li>
<li>Agrawal et al. [25] proposed a method to reconstruct the trajectory of the handwriting by using the build-in acc in a phone.</li>
<li>Yang et al.[26] proposed a mechanism ZVC to reduce the accumulative error of IMUs when reconstructing the trajectory.</li>
<li>Wang et al.[27] proposed an attitude error compensation method and a mechanism Multi-Axis Dynamic switch to discard some signals of noise, drift, and the users&rsquo; trembles by setting an appropriate threshold.</li>
</ul>
<h4 id="methods-5">Methods</h4>
<ul>
<li><strong>system overview</strong>:</li>
</ul>
<p>【Module 1】Signal Preprocessing</p>
<ul>
<li>Sensor  Calibration and Digital-Analog Conversion: ZGO: the offset between zero and the output of a stationary IMU. The sensitivity is related to the full-scare range and the resolution of the IMUs.</li>
</ul>
<p>$$
a=(Raw_{acc}-ZGO)/sensitivity_acc
$$</p>
<p>$$
w=(Raw_{gyr}-ZRO)/sensitivity_{gyr}
$$</p>
<p><img
    class="lazyload"
    src="/liudongdong1.github.io/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200625091630619.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200625091630619.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200625091630619.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200625091630619.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200625091630619.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200625091630619.png"/></p>
<p><img
    class="lazyload"
    src="/liudongdong1.github.io/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200625091647837.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200625091647837.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200625091647837.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200625091647837.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200625091647837.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200625091647837.png"/></p>
<ul>
<li>Smoothing Filter: using exponential weighted moving average(EWMA) filter.</li>
</ul>
<p>$$
EWMA(t)=\frac{Data_i+(1-\epsilon)Data_{t-1}+(1-\epsilon)^2Data_{t-2}+&hellip;}{1+(1-\epsilon)+(1-\epsilon)^2}+&hellip;
$$</p>
<p>【Module 2】Trajectory Reconstruction</p>
<ul>
<li>
<p>Attitude Estimation&amp;Coordinate Transformation:<img
    class="lazyload"
    src="/liudongdong1.github.io/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200625092320299.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200625092320299.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200625092320299.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200625092320299.png 2x"
    data-sizes="auto"
    alt="Coordinate Systems"
    title="Coordinate Systems"/></p>
<p><img
    class="lazyload"
    src="/liudongdong1.github.io/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200625092412445.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200625092412445.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200625092412445.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200625092412445.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200625092412445.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200625092412445.png"/></p>
</li>
<li>
<p>Gravity Erasion: to get the linear-acc</p>
</li>
<li>
<p>Movement Detection:</p>
</li>
</ul>
<blockquote>
<p>segment the signals using sliding window and extract features for each segment, and using LDA to classify a segment into moving or stationary signals, and morphological operations are then applied to smooth the classification results obtained by LDA.</p>
</blockquote>
<p><img
    class="lazyload"
    src="/liudongdong1.github.io/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200625092926820.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200625092926820.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200625092926820.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200625092926820.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200625092926820.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200625092926820.png"/></p>
<p><img
    class="lazyload"
    src="/liudongdong1.github.io/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200625092950136.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200625092950136.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200625092950136.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200625092950136.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200625092950136.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200625092950136.png"/></p>
<ul>
<li>Displacement Calculation: $x(t)=x(t-1)+v(t)*dt$
$$
offset=\frac{v(t_{end}-v{t_{end}-1})}{t_{end}-t_{begin}}\
v_c(t)=v_m(t)+(t-t_{begin}+1)*offset
$$</li>
</ul>
<p>【<strong>Module 3】Trajectory Recognize</strong></p>
<ul>
<li><strong>Sequence-Based Recognition</strong>: using orientation of short-term trajectory displacement, for a trajectory composed of k sampled points {(x0,y0),(x1,y1),&hellip;,(xk−1,yk−1)}, segment the trajectory into 20 short-term trajectories with equal sampled points,the nth segment can be presented by:
$$
C(n)head=(x_{\frac{k}{20}<em>n},y_{\frac{k}{20}<em>n})\
C(n)tail=(x_{\frac{k}{20}</em>(n+1)-1},y_{\frac{k}{20}</em>(n+1)-1})
$$</li>
</ul>
<p><img
    class="lazyload"
    src="/liudongdong1.github.io/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200625094307301.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200625094307301.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200625094307301.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200625094307301.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200625094307301.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200625094307301.png"/></p>
<p><img
    class="lazyload"
    src="/liudongdong1.github.io/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200625093913283.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200625093913283.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200625093913283.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200625093913283.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200625093913283.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200625093913283.png"/></p>
<blockquote>
<p>using above orientation to denote each segment to get the sequence and using DTW or HMM methods to classify.</p>
</blockquote>
<p><img
    class="lazyload"
    src="/liudongdong1.github.io/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200625093123952.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200625093123952.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200625093123952.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200625093123952.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200625093123952.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200625093123952.png"/></p>
<ul>
<li><strong>Image-Based Reconstruction</strong></li>
<li><strong>LSTM-Based Time Sequence Methods</strong></li>
</ul>
<h4 id="conclusion-1">Conclusion</h4>
<ul>
<li>design a framework which can not only reconstruct handwritting trajectory but also recognize the constructed letter based on low-cost IMU.</li>
<li>to overcome the intrinsic drift and the high-degree noise, a reset switch method is designed to reduce the accumulated error caused by the IMU sensor.</li>
</ul>
</div>
<div class="post-footer" id="post-footer">
  <div class="post-info">
    <div class="post-info-line">
      <div class="post-info-mod">
        <span title=2023-09-28&#32;23:29:54>更新于 2023-09-28&nbsp;</span>
      </div><div class="post-info-license">
          <span><a rel="license external nofollow noopener noreferrer" href="https://creativecommons.org/licenses/by-nc/4.0/" target="_blank">CC BY-NC 4.0</a></span>
        </div></div>
    <div class="post-info-line">
      <div class="post-info-md"><span><a href="liudongdong1.github.io/imu-trajectory/index.md" title="阅读原始文档" class="link-to-markdown">阅读原始文档</a></span><span><a href="https://liudongdong1.github.io/edit/master/content/posts%5cAIOT%5cSensors%5cIMU-Trajectory.md" title="编辑此页"target="_blank" rel="external nofollow noopener noreferrer" class="link-to-edit">编辑此页</a></span></div>
      <div class="post-info-share">
        <span><a href="javascript:void(0);" title="分享到 Twitter" data-sharer="twitter" data-url="liudongdong1.github.io/imu-trajectory/" data-title="IMU Trajectory" data-hashtags="IMU,Sensing,CV"><i class="fa-brands fa-twitter fa-fw" aria-hidden="true"></i></a>
  <a href="javascript:void(0);" title="分享到 Facebook" data-sharer="facebook" data-url="liudongdong1.github.io/imu-trajectory/" data-hashtag="IMU"><i class="fa-brands fa-facebook-square fa-fw" aria-hidden="true"></i></a>
  <a href="javascript:void(0);" title="分享到 微博" data-sharer="weibo" data-url="liudongdong1.github.io/imu-trajectory/" data-title="IMU Trajectory" data-image="https://gitee.com/github-25970295/blogImage/raw/master/img/armature-4103639__340.webp"><i class="fa-brands fa-weibo fa-fw" aria-hidden="true"></i></a>
  </span>
      </div>
    </div>
  </div>

  <div class="post-info-more">
    <section class="post-tags"><i class="fa-solid fa-tags fa-fw" aria-hidden="true"></i>&nbsp;<a href="liudongdong1.github.io/tags/imu/">IMU</a>,&nbsp;<a href="liudongdong1.github.io/tags/sensing/">Sensing</a>,&nbsp;<a href="liudongdong1.github.io/tags/cv/">CV</a></section>
    <section>
      <span><a href="javascript:void(0);" onclick="window.history.back();">返回</a></span>&nbsp;|&nbsp;<span><a href="liudongdong1.github.io/">主页</a></span>
    </section>
  </div>

  <div class="post-nav"><a href="liudongdong1.github.io/multi-sense/" class="prev" rel="prev" title="Multi-Sense"><i class="fa-solid fa-angle-left fa-fw" aria-hidden="true"></i>Multi-Sense</a>
      <a href="liudongdong1.github.io/docker-command/" class="next" rel="next" title="Docker-Command">Docker-Command<i class="fa-solid fa-angle-right fa-fw" aria-hidden="true"></i></a></div>
</div>
</article></main><footer class="footer">
    <div class="footer-container"><div class="footer-line powered">由 <a href="https://gohugo.io/" target="_blank" rel="external nofollow noopener noreferrer" title="Hugo 0.118.2">Hugo</a> 强力驱动 | 主题 - <a href="https://github.com/hugo-fixit/FixIt" target="_blank" rel="external" title="FixIt v0.2.17-RC"><img class="fixit-icon" src="/liudongdong1.github.io/fixit.min.svg" alt="FixIt logo" />&nbsp;FixIt</a>
        </div><div class="footer-line copyright" itemscope itemtype="http://schema.org/CreativeWork"><i class="fa-regular fa-copyright fa-fw" aria-hidden="true"></i>
            <span itemprop="copyrightYear">2020 - 2023</span><span class="author" itemprop="copyrightHolder">
              <a href="https://liudongdong1.github.io/"target="_blank" rel="external nofollow noopener noreferrer">LiuDongdong</a></span><span class="license footer-divider"><a rel="license external nofollow noopener noreferrer" href="https://creativecommons.org/licenses/by-nc/4.0/" target="_blank">CC BY-NC 4.0</a></span></div><div class="footer-line statistics"><span class="site-time" title='网站运行中 ...'><i class="fa-solid fa-heartbeat fa-fw animate-icon" aria-hidden="true"></i>&nbsp;<span class="run-times">网站运行中 ...</span></span></div><div class="footer-line ibruce">
          <span id="busuanzi_container_site_uv" title='总访客数'><i class="fa-regular fa-user fa-fw" aria-hidden="true"></i>&nbsp;<span id="busuanzi_value_site_uv"><i class="fa-solid fa-spinner fa-spin fa-fw" aria-hidden="true"></i></span></span><span id="busuanzi_container_site_pv" class="footer-divider" title='总访问量'><i class="fa-regular fa-eye fa-fw" aria-hidden="true"></i>&nbsp;<span id="busuanzi_value_site_pv"><i class="fa-solid fa-spinner fa-spin fa-fw" aria-hidden="true"></i></span></span>
        </div></div>
  </footer></div><div class="widgets"><div class="fixed-buttons animate__faster d-none"><div class="fixed-button back-to-top" role="button" aria-label="回到顶部"><i class="fa-solid fa-arrow-up fa-fw" aria-hidden="true"></i><span class="variant-numeric">0%</span>
        </div></div><a href="https://liudongdong1.github.io/" title="在 GitHub 上查看源代码"target="_blank" rel="external nofollow" class="github-corner right d-none-mobile"><svg viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><div id="mask"></div><div class="reading-progress-bar" style="left: 0;top: 0;--bg-progress: #0076ff;--bg-progress-dark: #fff;"></div><noscript>
    <div class="noscript-warning">FixIt 主题在启用 JavaScript 的情况下效果最佳。</div>
  </noscript>
</div><link rel="stylesheet" href="/liudongdong1.github.io/lib/katex/katex.min.css"><link rel="stylesheet" href="/liudongdong1.github.io/lib/cookieconsent/cookieconsent.min.css"><script src="/liudongdong1.github.io/lib/autocomplete/autocomplete.min.js" defer></script><script src="/liudongdong1.github.io/lib/algoliasearch/algoliasearch-lite.umd.min.js" defer></script><script src="/liudongdong1.github.io/lib/lazysizes/lazysizes.min.js" async defer></script><script src="/liudongdong1.github.io/lib/sharer/sharer.min.js" async defer></script><script src="/liudongdong1.github.io/lib/typeit/index.umd.js" defer></script><script src="/liudongdong1.github.io/lib/katex/katex.min.js" defer></script><script src="/liudongdong1.github.io/lib/katex/auto-render.min.js" defer></script><script src="/liudongdong1.github.io/lib/katex/copy-tex.min.js" defer></script><script src="/liudongdong1.github.io/lib/katex/mhchem.min.js" defer></script><script src="/liudongdong1.github.io/lib/cookieconsent/cookieconsent.min.js" defer></script><script src="/liudongdong1.github.io/lib/pangu/pangu.min.js" defer></script><script src="/liudongdong1.github.io/lib/cell-watermark/watermark.min.js" defer></script><script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async defer></script><script>window.config={"code":{"copyTitle":"复制到剪贴板","editLockTitle":"锁定可编辑代码块","editUnLockTitle":"解锁可编辑代码块","editable":true,"maxShownLines":10},"comment":{"enable":false},"cookieconsent":{"content":{"dismiss":"同意","link":"了解更多","message":"本网站使用 Cookies 来改善您的浏览体验。"},"enable":true,"palette":{"button":{"background":"#f0f0f0"},"popup":{"background":"#1aa3ff"}},"theme":"edgeless"},"data":{"typeit-header-subtitle-desktop":"\u003cspan style='font-family: MMT,\"沐目体\";'\u003e吾日三省吾身\u003c/span\u003e","typeit-header-subtitle-mobile":"\u003cspan style='font-family: MMT,\"沐目体\";'\u003e吾日三省吾身\u003c/span\u003e"},"enablePWA":true,"enablePangu":true,"math":{"delimiters":[{"display":true,"left":"$$","right":"$$"},{"display":true,"left":"\\[","right":"\\]"},{"display":true,"left":"\\begin{equation}","right":"\\end{equation}"},{"display":true,"left":"\\begin{equation*}","right":"\\end{equation*}"},{"display":true,"left":"\\begin{align}","right":"\\end{align}"},{"display":true,"left":"\\begin{align*}","right":"\\end{align*}"},{"display":true,"left":"\\begin{alignat}","right":"\\end{alignat}"},{"display":true,"left":"\\begin{alignat*}","right":"\\end{alignat*}"},{"display":true,"left":"\\begin{gather}","right":"\\end{gather}"},{"display":true,"left":"\\begin{CD}","right":"\\end{CD}"},{"display":false,"left":"$","right":"$"},{"display":false,"left":"\\(","right":"\\)"}],"strict":false},"search":{"algoliaAppID":"2R1K9SKLQZ","algoliaIndex":"index.zh-cn","algoliaSearchKey":"4a226aa1c5c98d6859e4d1386adb2bc7","highlightTag":"em","maxResultLength":10,"noResultsFound":"没有找到结果","snippetLength":50,"type":"algolia"},"siteTime":"2020-12-18T16:15:22+08:00","typeit":{"cursorChar":"|","cursorSpeed":1000,"data":{"typeit-header-subtitle-desktop":["typeit-header-subtitle-desktop"],"typeit-header-subtitle-mobile":["typeit-header-subtitle-mobile"]},"duration":-1,"speed":100},"watermark":{"appendto":".wrapper\u003emain","colspacing":30,"content":"\u003cimg class=\"fixit-icon\" src=\"/fixit.min.svg\" alt=\"FixIt logo\" /\u003e FixIt 主题","enable":true,"fontfamily":"inherit","fontsize":0.85,"height":21,"opacity":0.0125,"rotate":15,"rowspacing":60,"width":150}};</script><script src="/liudongdong1.github.io/js/theme.min.js" defer></script><script src="/liudongdong1.github.io/js/custom.min.js" defer></script></body>
</html>
