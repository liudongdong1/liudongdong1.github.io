<!DOCTYPE HTML>
<html lang="en">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="sparkRelative, AIOT,Space&amp;Temporal Sequence Analysis,SpringBoot,liudongdong1,cloud">
    <meta name="description" content="">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>sparkRelative | DaybyDay</title>
    <link rel="icon" type="image/png" href="/favicon.png">

    <link rel="stylesheet" type="text/css" href="/libs/awesome/css/all.css">
    <link rel="stylesheet" type="text/css" href="/libs/materialize/materialize.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/aos/aos.css">
    <link rel="stylesheet" type="text/css" href="/libs/animate/animate.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/lightGallery/css/lightgallery.min.css">
    <link rel="stylesheet" type="text/css" href="/css/matery.css">
    <link rel="stylesheet" type="text/css" href="/css/my.css">

    <script src="/libs/jquery/jquery.min.js"></script>

<meta name="generator" content="Hexo 4.2.0"><link rel="alternate" href="/atom.xml" title="DaybyDay" type="application/atom+xml">
<link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css"></head>


<body>
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/" class="waves-effect waves-light">
                    
                    <img src="/medias/logo.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">DaybyDay</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>Index</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>Tags</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>Categories</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>Archives</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/" class="waves-effect waves-light">

      
      <i class="fas fa-user-circle" style="zoom: 0.6;"></i>
      
      <span>About</span>
      <i class="fas fa-chevron-down" aria-hidden="true" style="zoom: 0.6;"></i>
    </a>
    <ul class="sub-nav menus_item_child ">
      
      <li>
        <a href="/about">
          
          <i class="fas fa-user-circle" style="margin-top: -20px; zoom: 0.6;"></i>
          
          <span>about</span>
        </a>
      </li>
      
      <li>
        <a href="/resume">
          
          <i class="fa fa-user-secret" style="margin-top: -20px; zoom: 0.6;"></i>
          
          <span>resume</span>
        </a>
      </li>
      
    </ul>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/friends" class="waves-effect waves-light">
      
      <i class="fas fa-address-book" style="zoom: 0.6;"></i>
      
      <span>Friends</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/gallery" class="waves-effect waves-light">
      
      <i class="fas fa-camera" style="zoom: 0.6;"></i>
      
      <span>Galleries</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="Search" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/medias/logo.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">DaybyDay</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			Index
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			Tags
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			Categories
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			Archives
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="javascript:;">
			
				<i class="fa-fw fas fa-user-circle"></i>
			
			About
			<span class="m-icon"><i class="fas fa-chevron-right"></i></span>
		</a>
            <ul  style="background:  ;" >
              
                <li>

                  <a href="/about " style="margin-left:75px">
				  
				   <i class="fa fas fa-user-circle" style="position: absolute;left:50px" ></i>
			      
		          <span>about</span>
                  </a>
                </li>
              
                <li>

                  <a href="/resume " style="margin-left:75px">
				  
				   <i class="fa fa fa-user-secret" style="position: absolute;left:50px" ></i>
			      
		          <span>resume</span>
                  </a>
                </li>
              
            </ul>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/friends" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-address-book"></i>
			
			Friends
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/gallery" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-camera"></i>
			
			Galleries
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/liudongdong1" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/liudongdong1" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://gitee.com/github-25970295/blogImage/raw/master/img/20210501132438.png')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">sparkRelative</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <link rel="stylesheet" href="/libs/tocbot/tocbot.css">
<style>
    #articleContent h1::before,
    #articleContent h2::before,
    #articleContent h3::before,
    #articleContent h4::before,
    #articleContent h5::before,
    #articleContent h6::before {
        display: block;
        content: " ";
        height: 100px;
        margin-top: -100px;
        visibility: hidden;
    }

    #articleContent :focus {
        outline: none;
    }

    .toc-fixed {
        position: fixed;
        top: 64px;
    }

    .toc-widget {
        width: 345px;
        padding-left: 20px;
    }

    .toc-widget .toc-title {
        margin: 35px 0 15px 0;
        padding-left: 17px;
        font-size: 1.5rem;
        font-weight: bold;
        line-height: 1.5rem;
    }

    .toc-widget ol {
        padding: 0;
        list-style: none;
    }

    #toc-content {
        height: calc(100vh - 250px);
        overflow: auto;
    }

    #toc-content ol {
        padding-left: 10px;
    }

    #toc-content ol li {
        padding-left: 10px;
    }

    #toc-content .toc-link:hover {
        color: #42b983;
        font-weight: 700;
        text-decoration: underline;
    }

    #toc-content .toc-link::before {
        background-color: transparent;
        max-height: 25px;

        position: absolute;
        right: 23.5vw;
        display: block;
    }

    .toc-fixed .toc-link::before{
        position: fixed!important;/*当toc的位置改为fixed时，.toc-link::before也要改为fixed*/
    }

    #toc-content .is-active-link {
        color: #42b983;
    }

    #toc-content .is-active-link::before {
        background-color: #42b983;
    }

    #floating-toc-btn {
        position: fixed;
        right: 15px;
        bottom: 76px;
        padding-top: 15px;
        margin-bottom: 0;
        z-index: 998;
    }

    #floating-toc-btn .btn-floating {
        width: 48px;
        height: 48px;
    }

    #floating-toc-btn .btn-floating i {
        line-height: 48px;
        font-size: 1.4rem;
    }
</style>
<div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/tags/spark/">
                                <span class="chip bg-color">spark</span>
                            </a>
                        
                            <a href="/tags/stream/">
                                <span class="chip bg-color">stream</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/categories/Framewrok/" class="post-category">
                                Framewrok
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>Publish Date:&nbsp;&nbsp;
                    2020-01-13
                </div>
                

                <!-- 
                    <i class="fa fa-pencil"></i> Author: liudongdong1
                  -->

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>Update Date:&nbsp;&nbsp;
                    2021-05-14
                </div>
                

                <!-- 
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>Word Count:&nbsp;&nbsp;
                    7.8k
                </div>
                 -->

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>Read Times:&nbsp;&nbsp;
                    44 Min
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>Read Count:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">
        <div class="card-content article-card-content">
            <div id="articleContent">
                <h4 id="1-部署方式"><a href="#1-部署方式" class="headerlink" title="1. 部署方式"></a>1. 部署方式</h4><h5 id="1-1-Local-模式"><a href="#1-1-Local-模式" class="headerlink" title="1.1. Local 模式"></a>1.1. Local 模式</h5><ul>
<li><strong>Local模式</strong>就是运行在一台计算机上的模式，通常用于在本机上测试，当不设置master参数的值时，默认此模式，具体有以下几种设置master的方式。<ol>
<li>local：所有计算都运行在一个线程当中，没有任何并行计算。</li>
<li>local[n]：指定使用n个线程来运行计算。</li>
<li>local[*]：按照CPU的最多核数来设置线程数。</li>
</ol>
</li>
</ul>
<h5 id="1-2-Standalone-模式"><a href="#1-2-Standalone-模式" class="headerlink" title="1.2.  Standalone 模式"></a>1.2.  Standalone 模式</h5><ul>
<li><strong>Standalone</strong>：独立模式，Spark原生的简单集群管理器，自带完整的服务，可单独部署到一个集群中，无需依赖任何其他资源管理系统，使用Standalone可以很方便地搭建一个集群；</li>
</ul>
<blockquote>
<p>Standalone集群有四个重要运行机制:</p>
<ul>
<li>Master: 是一个进程，主要负责资源的调度和分配，并进行集群的监控等职责</li>
<li>Worker: 是一个进程，可以启动其他的进程和线程(Executor)；同时用自己的内存存储RDD的某些partition</li>
<li>Driver：是一个进程，我们编写的Spark应用程序就运行在Driver上</li>
<li>Executor: 是一个进程，一个Worker可以运行多个Executor,Executor通过启动多个线程(task)来执行对RDD的partition进行并行计算。</li>
</ul>
</blockquote>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210219095555023.png" alt="standalone-client"></p>
<figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Run on a Spark standalone cluster <span class="keyword">in</span> client deploy mode</span></span><br><span class="line">./bin/spark-submit \</span><br><span class="line">  --class org.apache.spark.examples.SparkPi \</span><br><span class="line">  --master spark://207.184.161.138:7077 \</span><br><span class="line">  --deploy-mode client \</span><br><span class="line">  --executor-memory 20G \</span><br><span class="line">--total-executor-cores 100 \</span><br><span class="line">/path/to/examples.jar \</span><br><span class="line">1000</span><br></pre></td></tr></tbody></table></figure>

<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210219095555023.png" alt="standalone-cluster"></p>
<figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Run on a Spark standalone cluster <span class="keyword">in</span> cluster deploy mode with supervise</span></span><br><span class="line">./bin/spark-submit \</span><br><span class="line">  --class org.apache.spark.examples.SparkPi \</span><br><span class="line">  --master spark://207.184.161.138:7077 \</span><br><span class="line">  --deploy-mode cluster \</span><br><span class="line">  --supervise \</span><br><span class="line">  --executor-memory 20G \</span><br><span class="line">  --total-executor-cores 100 \</span><br><span class="line">  /path/to/examples.jar \</span><br><span class="line">  1000</span><br></pre></td></tr></tbody></table></figure>

<h5 id="1-3-Apache-Mesos模式"><a href="#1-3-Apache-Mesos模式" class="headerlink" title="1.3. Apache Mesos模式"></a>1.3. Apache Mesos模式</h5><ul>
<li><strong>Apache Mesos</strong>：一个强大的分布式资源管理框架，它允许多种不同的框架部署在其上，包括yarn；</li>
</ul>
<h5 id="1-4-Hadoop-Yarn模式"><a href="#1-4-Hadoop-Yarn模式" class="headerlink" title="1.4. Hadoop Yarn模式"></a>1.4. Hadoop Yarn模式</h5><ul>
<li><strong>Hadoop YARN</strong>：统一的资源管理机制，在上面可以运行多套计算框架，如map reduce、storm等，根据driver在集群中的位置不同，分为yarn client和yarn cluster。</li>
<li>俩种模式区别：</li>
</ul>
<blockquote>
<p>　①. 在于driver端启动在本地(client)，还是在Yarn集群内部的AM中(cluster)。</p>
<p>　②. client提交作业的进程是不能停止的，否则作业就挂了；cluster提交作业后就断开了，因为driver运行在AM中。</p>
<p>　③. client提交的作业，日志在客户端看不到，因为作业运行在yarn上，可以通过 yarn logs -applicationId <application_id> 查看。</application_id></p>
<p>　④. Cluster适合生产环境，Client适合交互和调试。</p>
</blockquote>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210219101359930.png" alt="Client"></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210219101430442.png" alt="cluster"></p>
<table>
<thead>
<tr>
<th>Master URL（主节点参数）</th>
<th>Meaning（含义）</th>
</tr>
</thead>
<tbody><tr>
<td>local</td>
<td>在本地运行，只有一个工作进程，无并行计算能力。</td>
</tr>
<tr>
<td>local[K]</td>
<td>在本地运行，有K个工作进程，通常设置K为机器的CPU核心数量。</td>
</tr>
<tr>
<td>local[*]</td>
<td>在本地运行，工作进程数量等于机器的CPU核心数量。</td>
</tr>
</tbody></table>
<table>
<thead>
<tr>
<th>Master URL（主节点参数）</th>
<th>Meaning（含义）</th>
</tr>
</thead>
<tbody><tr>
<td>spark://HOST:PORT</td>
<td>以Standalone模式运行，这是Spark自身提供的集群运行模式， 默认端口号: 7077。</td>
</tr>
<tr>
<td>mesos://HOST:PORT</td>
<td>在Mesos集群上运行，Driver进程和Worker进程运行在Mesos集群上， 部署模式必须使用固定值:–deploy-mode cluster</td>
</tr>
</tbody></table>
<table>
<thead>
<tr>
<th>Master URL（主节点参数）</th>
<th>Meaning（含义）</th>
</tr>
</thead>
<tbody><tr>
<td>yarn-client</td>
<td>在Yarn集群上运行，Driver进程在本地，Executor进程在Yarn集群上， 部署模式必须使用固定值:–deploy-mode client。 Yarn集群地址必须在HADOOP_CONF_DIR or YARN_CONF_DIR变量里定义。</td>
</tr>
<tr>
<td>yarn-cluster</td>
<td>在Yarn集群上运行，Driver进程在Yarn集群上，Work进程也在Yarn集群上， 部署模式必须使用固定值:–deploy-mode cluster。 Yarn集群地址必须在HADOOP_CONF_DIR or YARN_CONF_DIR变量里定义。</td>
</tr>
</tbody></table>
<h4 id="2-Demo"><a href="#2-Demo" class="headerlink" title="2. Demo"></a>2. Demo</h4><h5 id="2-1-文本处理"><a href="#2-1-文本处理" class="headerlink" title="2.1. 文本处理"></a>2.1. 文本处理</h5><ul>
<li><a href="https://cloud.tencent.com/developer/article/1096712" target="_blank" rel="noopener">将旧金山犯罪记录（San Francisco Crime Description）分类到33个类目中</a></li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SQLContext</span><br><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkContext</span><br><span class="line"><span class="comment"># 利用spark的csv库直接载入csv格式的数据</span></span><br><span class="line">sc = SparkContext()</span><br><span class="line">sqlContext = SQLContext(sc)</span><br><span class="line">data = sqlContext.read.format(<span class="string">'com.databricks.spark.csv'</span>).options(header=<span class="string">'true'</span>,                                                             inferschema=<span class="string">'true'</span>).load(<span class="string">'train.csv'</span>)</span><br><span class="line"><span class="comment"># 选10000条数据集，减少运行时间</span></span><br><span class="line">data = data.sample(<span class="literal">False</span>, <span class="number">0.01</span>, <span class="number">100</span>)</span><br><span class="line">print(data.count())</span><br><span class="line"><span class="comment"># 除去一些不要的列，并展示前五行</span></span><br><span class="line">drop_list = [<span class="string">'Dates'</span>, <span class="string">'DayOfWeek'</span>, <span class="string">'PdDistrict'</span>, <span class="string">'Resolution'</span>, <span class="string">'Address'</span>, <span class="string">'X'</span>, <span class="string">'Y'</span>]</span><br><span class="line">data = data.select([column <span class="keyword">for</span> column <span class="keyword">in</span> data.columns <span class="keyword">if</span> column <span class="keyword">not</span> <span class="keyword">in</span> drop_list])</span><br><span class="line">data.show(<span class="number">5</span>)</span><br><span class="line">data.printSchema()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 包含数量最多的20类犯罪</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> col</span><br><span class="line">data.groupBy(<span class="string">'Category'</span>).count().orderBy(col(<span class="string">'count'</span>).desc()).show()</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> pyspark.ml.feature <span class="keyword">import</span> RegexTokenizer, StopWordsRemover, CountVectorizer</span><br><span class="line"><span class="keyword">from</span> pyspark.ml.classification <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="comment"># 正则切分单词</span></span><br><span class="line"><span class="comment"># inputCol:输入字段名</span></span><br><span class="line"><span class="comment"># outputCol:输出字段名</span></span><br><span class="line">regexTokenizer = RegexTokenizer(inputCol=<span class="string">'Descript'</span>, outputCol=<span class="string">'words'</span>, pattern=<span class="string">'\\W'</span>)</span><br><span class="line"><span class="comment"># 停用词</span></span><br><span class="line">add_stopwords = [<span class="string">'http'</span>, <span class="string">'https'</span>, <span class="string">'amp'</span>, <span class="string">'rt'</span>, <span class="string">'t'</span>, <span class="string">'c'</span>, <span class="string">'the'</span>]</span><br><span class="line">stopwords_remover = StopWordsRemover(inputCol=<span class="string">'words'</span>, outputCol=<span class="string">'filtered'</span>).setStopWords(add_stopwords)</span><br><span class="line"><span class="comment"># 构建词频向量</span></span><br><span class="line">count_vectors = CountVectorizer(inputCol=<span class="string">'filtered'</span>, outputCol=<span class="string">'features'</span>, vocabSize=<span class="number">10000</span>, minDF=<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> pyspark.ml <span class="keyword">import</span> Pipeline</span><br><span class="line"><span class="keyword">from</span> pyspark.ml.feature <span class="keyword">import</span> OneHotEncoder, StringIndexer, VectorAssembler</span><br><span class="line">label_stringIdx = StringIndexer(inputCol=<span class="string">'Category'</span>, outputCol=<span class="string">'label'</span>)</span><br><span class="line">pipeline = Pipeline(stages=[regexTokenizer, stopwords_remover, count_vectors, label_stringIdx])</span><br><span class="line"><span class="comment"># fit the pipeline to training documents</span></span><br><span class="line">pipeline_fit = pipeline.fit(data)</span><br><span class="line">dataset = pipeline_fit.transform(data)</span><br><span class="line">dataset.show(<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># set seed for reproducibility</span></span><br><span class="line"><span class="comment"># 数据集划分训练集和测试集，比例7:3， 设置随机种子100</span></span><br><span class="line">(trainingData, testData) = dataset.randomSplit([<span class="number">0.7</span>, <span class="number">0.3</span>], seed=<span class="number">100</span>)</span><br><span class="line">print(<span class="string">'Training Dataset Count:{}'</span>.format(trainingData.count()))</span><br><span class="line">print(<span class="string">'Test Dataset Count:{}'</span>.format(testData.count()))</span><br><span class="line"></span><br><span class="line">start_time = time.time()</span><br><span class="line">lr = LogisticRegression(maxIter=<span class="number">20</span>, regParam=<span class="number">0.3</span>, elasticNetParam=<span class="number">0</span>)</span><br><span class="line">lrModel = lr.fit(trainingData)</span><br><span class="line">predictions = lrModel.transform(testData)</span><br><span class="line"><span class="comment"># 过滤prediction类别为0数据集</span></span><br><span class="line">predictions.filter(predictions[<span class="string">'prediction'</span>] == <span class="number">0</span>).select(<span class="string">'Descript'</span>, <span class="string">'Category'</span>, <span class="string">'probability'</span>, <span class="string">'label'</span>, <span class="string">'prediction'</span>).orderBy(<span class="string">'probability'</span>, accending=<span class="literal">False</span>).show(n=<span class="number">10</span>, truncate=<span class="number">30</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> pyspark.ml.evaluation <span class="keyword">import</span> MulticlassClassificationEvaluator</span><br><span class="line"><span class="comment"># predictionCol: 预测列的名称</span></span><br><span class="line">evaluator = MulticlassClassificationEvaluator(predictionCol=<span class="string">'prediction'</span>)</span><br><span class="line"><span class="comment"># 预测准确率</span></span><br><span class="line">print(evaluator.evaluate(predictions))</span><br><span class="line">end_time = time.time()</span><br><span class="line">print(end_time - start_time)</span><br><span class="line"></span><br><span class="line"><span class="comment">#以TF-ID作为特征，利用逻辑回归进行分类</span></span><br><span class="line"><span class="keyword">from</span> pyspark.ml.feature <span class="keyword">import</span> HashingTF, IDF</span><br><span class="line">start_time = time.time()</span><br><span class="line"><span class="comment"># numFeatures: 最大特征数</span></span><br><span class="line">hashingTF = HashingTF(inputCol=<span class="string">'filtered'</span>, outputCol=<span class="string">'rawFeatures'</span>, numFeatures=<span class="number">10000</span>)</span><br><span class="line"><span class="comment"># minDocFreq：过滤的最少文档数量</span></span><br><span class="line">idf = IDF(inputCol=<span class="string">'rawFeatures'</span>, outputCol=<span class="string">'features'</span>, minDocFreq=<span class="number">5</span>)</span><br><span class="line">pipeline = Pipeline(stages=[regexTokenizer, stopwords_remover, hashingTF, idf, label_stringIdx])</span><br><span class="line">pipeline_fit = pipeline.fit(data)</span><br><span class="line">dataset = pipeline_fit.transform(data)</span><br><span class="line">(trainingData, testData) = dataset.randomSplit([<span class="number">0.7</span>, <span class="number">0.3</span>], seed=<span class="number">100</span>)</span><br><span class="line"></span><br><span class="line">lr = LogisticRegression(maxIter=<span class="number">20</span>, regParam=<span class="number">0.3</span>, elasticNetParam=<span class="number">0</span>)</span><br><span class="line">lr_model = lr.fit(trainingData)</span><br><span class="line">predictions = lr_model.transform(testData)</span><br><span class="line">predictions.filter(predictions[<span class="string">'prediction'</span>] == <span class="number">0</span>).select(<span class="string">'Descript'</span>, <span class="string">'Category'</span>, <span class="string">'probability'</span>, <span class="string">'label'</span>, <span class="string">'prediction'</span>).\</span><br><span class="line">orderBy(<span class="string">'probability'</span>, ascending=<span class="literal">False</span>).show(n=<span class="number">10</span>, truncate=<span class="number">30</span>)</span><br><span class="line"></span><br><span class="line">evaluator = MulticlassClassificationEvaluator(predictionCol=<span class="string">'prediction'</span>)</span><br><span class="line">print(evaluator.evaluate(predictions))</span><br><span class="line">end_time = time.time()</span><br><span class="line">print(end_time - start_time)</span><br><span class="line"><span class="keyword">from</span> pyspark.ml.tuning <span class="keyword">import</span> ParamGridBuilder, CrossValidator</span><br><span class="line">start_time = time.time()</span><br><span class="line">pipeline = Pipeline(stages=[regexTokenizer, stopwords_remover, count_vectors, label_stringIdx])</span><br><span class="line">pipeline_fit = pipeline.fit(data)</span><br><span class="line">(trainingData, testData) = dataset.randomSplit([<span class="number">0.7</span>, <span class="number">0.3</span>], seed=<span class="number">100</span>)</span><br><span class="line">lr = LogisticRegression(maxIter=<span class="number">20</span>, regParam=<span class="number">0.3</span>, elasticNetParam=<span class="number">0</span>)</span><br><span class="line"><span class="comment"># 为交叉验证创建参数</span></span><br><span class="line"><span class="comment"># ParamGridBuilder：用于基于网格搜索的模型选择的参数网格的生成器</span></span><br><span class="line"><span class="comment"># addGrid：将网格中给定参数设置为固定值</span></span><br><span class="line"><span class="comment"># parameter：正则化参数</span></span><br><span class="line"><span class="comment"># maxIter：迭代次数</span></span><br><span class="line"><span class="comment"># numFeatures：特征值</span></span><br><span class="line">paramGrid = (ParamGridBuilder()</span><br><span class="line">             .addGrid(lr.regParam, [<span class="number">0.1</span>, <span class="number">0.3</span>, <span class="number">0.5</span>])</span><br><span class="line">             .addGrid(lr.elasticNetParam, [<span class="number">0.0</span>, <span class="number">0.1</span>, <span class="number">0.2</span>])</span><br><span class="line">             .addGrid(lr.maxIter, [<span class="number">10</span>, <span class="number">20</span>, <span class="number">50</span>])</span><br><span class="line"><span class="comment">#              .addGrid(idf.numFeatures, [10, 100, 1000])</span></span><br><span class="line">             .build())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建五折交叉验证</span></span><br><span class="line"><span class="comment"># estimator：要交叉验证的估计器</span></span><br><span class="line"><span class="comment"># estimatorParamMaps：网格搜索的最优参数</span></span><br><span class="line"><span class="comment"># evaluator：评估器</span></span><br><span class="line"><span class="comment"># numFolds：交叉次数</span></span><br><span class="line">cv = CrossValidator(estimator=lr,\</span><br><span class="line">                   estimatorParamMaps=paramGrid,\</span><br><span class="line">                   evaluator=evaluator,\</span><br><span class="line">                   numFolds=<span class="number">5</span>)</span><br><span class="line">cv_model = cv.fit(trainingData)</span><br><span class="line">predictions = cv_model.transform(testData)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型评估</span></span><br><span class="line">evaluator = MulticlassClassificationEvaluator(predictionCol=<span class="string">'prediction'</span>)</span><br><span class="line">print(evaluator.evaluate(predictions))</span><br><span class="line">end_time = time.time()</span><br><span class="line">print(end_time - start_time)</span><br><span class="line"></span><br><span class="line"><span class="comment">#朴素贝叶斯</span></span><br><span class="line"><span class="keyword">from</span> pyspark.ml.classification <span class="keyword">import</span> NaiveBayes</span><br><span class="line">start_time = time.time()</span><br><span class="line"><span class="comment"># smoothing：平滑参数</span></span><br><span class="line">nb = NaiveBayes(smoothing=<span class="number">1</span>)</span><br><span class="line">model = nb.fit(trainingData)</span><br><span class="line">predictions = model.transform(testData)</span><br><span class="line">predictions.filter(predictions[<span class="string">'prediction'</span>] == <span class="number">0</span>) \</span><br><span class="line">    .select(<span class="string">'Descript'</span>, <span class="string">'Category'</span>, <span class="string">'probability'</span>, <span class="string">'label'</span>, <span class="string">'prediction'</span>) \</span><br><span class="line">    .orderBy(<span class="string">'probability'</span>, ascending=<span class="literal">False</span>) \</span><br><span class="line">    .show(n=<span class="number">10</span>, truncate=<span class="number">30</span>)</span><br><span class="line">    </span><br><span class="line"><span class="comment">#随机森林</span></span><br><span class="line"><span class="keyword">from</span> pyspark.ml.classification <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line">start_time = time.time()</span><br><span class="line"><span class="comment"># numTree：训练树的个数</span></span><br><span class="line"><span class="comment"># maxDepth：最大深度</span></span><br><span class="line"><span class="comment"># maxBins：连续特征离散化的最大分类数</span></span><br><span class="line">rf = RandomForestClassifier(labelCol=<span class="string">'label'</span>, \</span><br><span class="line">                            featuresCol=<span class="string">'features'</span>, \</span><br><span class="line">                            numTrees=<span class="number">100</span>, \</span><br><span class="line">                            maxDepth=<span class="number">4</span>, \</span><br><span class="line">                            maxBins=<span class="number">32</span>)</span><br><span class="line"><span class="comment"># Train model with Training Data</span></span><br><span class="line">rfModel = rf.fit(trainingData)</span><br><span class="line">predictions = rfModel.transform(testData)</span><br><span class="line">predictions.filter(predictions[<span class="string">'prediction'</span>] == <span class="number">0</span>) \</span><br><span class="line">    .select(<span class="string">'Descript'</span>,<span class="string">'Category'</span>,<span class="string">'probability'</span>,<span class="string">'label'</span>,<span class="string">'prediction'</span>) \</span><br><span class="line">    .orderBy(<span class="string">'probability'</span>, ascending=<span class="literal">False</span>) \</span><br><span class="line">    .show(n = <span class="number">10</span>, truncate = <span class="number">30</span>)</span><br><span class="line">    </span><br><span class="line">evaluator = MulticlassClassificationEvaluator(predictionCol=<span class="string">'prediction'</span>)</span><br><span class="line">print(evaluator.evaluate(predictions))</span><br><span class="line">end_time = time.time()</span><br><span class="line">print(end_time - start_time)</span><br></pre></td></tr></tbody></table></figure>

<ul>
<li>评语分类</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#coding:utf-8</span></span><br><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkContext</span><br><span class="line"><span class="keyword">from</span> pyspark.mllib.regression <span class="keyword">import</span> LabeledPoint</span><br><span class="line"><span class="keyword">from</span> pyspark.mllib.feature <span class="keyword">import</span> HashingTF,IDF,StandardScaler</span><br><span class="line"><span class="keyword">from</span> pyspark.mllib.classification <span class="keyword">import</span> LogisticRegressionWithSGD,SVMWithSGD,NaiveBayes</span><br><span class="line"><span class="keyword">from</span> pyspark.mllib.tree <span class="keyword">import</span> DecisionTree</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">split2</span><span class="params">(line)</span>:</span></span><br><span class="line">    <span class="string">'''自定义的字符串分割函数，本例已步长3分割字符串'''</span></span><br><span class="line">    step = <span class="number">3</span></span><br><span class="line">    result = []</span><br><span class="line">    length = len(line)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> xrange(<span class="number">0</span>,length,step):</span><br><span class="line">        result.append(line[i:i+step])</span><br><span class="line">    <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">check</span><span class="params">(test,model)</span>:</span></span><br><span class="line">    <span class="string">'''模型检测函数，输出模型的正确率、准确率和召回率，本例省略'''</span></span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">"__main__"</span>:</span><br><span class="line">    sc = SparkContext(appName=<span class="string">"test"</span>)</span><br><span class="line">    <span class="comment">#分别读取正文件和负文件的训练集，然后读取测试集</span></span><br><span class="line">    spam = sc.textFile(<span class="string">"hdfs://ubuntu:9000/xxx/bad.txt"</span>)</span><br><span class="line">    normal = sc.textFile(<span class="string">"hdfs://ubuntu:9000/xxx/good.txt"</span>)</span><br><span class="line">    test = sc.textFile(<span class="string">"hdfs://ubuntu:9000/xxx/test.txt"</span>)</span><br><span class="line">    <span class="comment"># 创建一个HashingTF实例来把文本映射为包含10000个特征的向量</span></span><br><span class="line">    tf = HashingTF(numFeatures = <span class="number">10000</span>)</span><br><span class="line">    <span class="comment"># 各http请求都被切分为单词，每个单词被映射为一个特征</span></span><br><span class="line">    spamFeatures = spam.map(<span class="keyword">lambda</span> line: tf.transform(split2(line)))</span><br><span class="line">    normalFeatures = normal.map(<span class="keyword">lambda</span> line: tf.transform(split2(line)))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># =========使用词频统计构建向量=========</span></span><br><span class="line">    <span class="comment"># positiveExamples = spamFeatures.map(lambda features: LabeledPoint(0, features))</span></span><br><span class="line">    <span class="comment"># negativeExamples = normalFeatures.map(lambda features: LabeledPoint(1, features))</span></span><br><span class="line">    <span class="comment"># trainingData = positiveExamples.union(negativeExamples)</span></span><br><span class="line">    <span class="comment"># trainingData.cache() # 因为逻辑回归是迭代算法，所以缓存训练数据RDD</span></span><br><span class="line">    <span class="comment">#print trainingData.take(1)</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># =========使用TF-IDF构建向量=========</span></span><br><span class="line">    spamFeatures.cache() <span class="comment"># 因为逻辑回归是迭代算法，所以缓存训练数据RDD</span></span><br><span class="line">    idf = IDF()</span><br><span class="line">    idfModel = idf.fit(spamFeatures)</span><br><span class="line">    spamVectors = idfModel.transform(spamFeatures)</span><br><span class="line">    normalFeatures.cache() <span class="comment"># 因为逻辑回归是迭代算法，所以缓存训练数据RDD</span></span><br><span class="line">    idfModel = idf.fit(normalFeatures)</span><br><span class="line">    normalVectors = idfModel.transform(normalFeatures)</span><br><span class="line">    positiveExamples = normalVectors.map(<span class="keyword">lambda</span> features: LabeledPoint(<span class="number">1</span>, features))</span><br><span class="line">    negativeExamples = spamVectors.map(<span class="keyword">lambda</span> features: LabeledPoint(<span class="number">0</span>, features))</span><br><span class="line">    dataAll = positiveExamples.union(negativeExamples)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># =========特征向量压缩=========</span></span><br><span class="line">    <span class="comment"># scaler = StandardScaler(withMean=True, withStd=True)</span></span><br><span class="line">    <span class="comment"># normalScaler = scaler.fit(normalVectors)</span></span><br><span class="line">    <span class="comment"># normalResult = normalScaler.transform(normalVectors)</span></span><br><span class="line">    <span class="comment"># spamScaler = scaler.fit(spamVectors)</span></span><br><span class="line">    <span class="comment"># spamResult = spamScaler.transform(spamVectors)</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 使用分类算法进行训练,iterations位迭代次数,step为迭代步长</span></span><br><span class="line">    <span class="comment"># LogisticRegressionWithSGD可以替换为SVMWithSGD和NaiveBayes</span></span><br><span class="line">    <span class="comment"># 其中train函数的参数可以根据模型效果自定义</span></span><br><span class="line">    model = LogisticRegressionWithSGD.train(data=dataAll,iterations=<span class="number">10000</span>,step=<span class="number">1</span>) </span><br><span class="line">    <span class="comment"># 决策树的分类类别为2，映射表为空，不纯净度测量为gini，树的深度为5，数据箱子为32</span></span><br><span class="line">    <span class="comment"># model = DecisionTree.trainClassifier(dataAll, numClasses=2, categoricalFeaturesInfo={},</span></span><br><span class="line"> <span class="comment">#                                     impurity='gini', maxDepth=5, maxBins=32) </span></span><br><span class="line">    check(test,model)</span><br><span class="line">    sc.stop()</span><br></pre></td></tr></tbody></table></figure>

<h5 id="2-2-蘑菇分类"><a href="#2-2-蘑菇分类" class="headerlink" title="2.2. 蘑菇分类"></a>2.2. <a href="https://www.kaggle.com/uciml/mushroom-classification" target="_blank" rel="noopener">蘑菇分类</a></h5><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#分类毒蘑菇和可食用蘑菇，共22个特征值，其中特征描述都是字符，用于机器学习的话，要将特征转换成数值。</span></span><br><span class="line"><span class="comment">#https://blog.csdn.net/m0_37442062/article/details/91357264</span></span><br><span class="line"><span class="keyword">import</span> findspark <span class="comment">#pip install findspark</span></span><br><span class="line">findspark.init()</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf, SparkContext</span><br><span class="line">spark = SparkSession.builder.master(<span class="string">'local[1]'</span>).appName(<span class="string">'classification'</span>).getOrCreate()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 载入数据</span></span><br><span class="line">df0 = spark.read.csv(<span class="string">'mushrooms.csv'</span>, header=<span class="literal">True</span>, inferSchema=<span class="literal">True</span>, encoding=<span class="string">'utf-8'</span>)</span><br><span class="line"><span class="comment"># 查看是否有缺失值</span></span><br><span class="line"><span class="comment"># df0.toPandas().isna().sum()</span></span><br><span class="line">df0.toPandas().isna().values.any()</span><br><span class="line"></span><br><span class="line"><span class="comment">#先使用StringIndexer将字符转化为数值，然后将特征整合到一起</span></span><br><span class="line"><span class="keyword">from</span> pyspark.ml.feature <span class="keyword">import</span> StringIndexer, VectorAssembler</span><br><span class="line">old_columns_names = df0.columns</span><br><span class="line">new_columns_names = [name+<span class="string">'-new'</span> <span class="keyword">for</span> name <span class="keyword">in</span> old_columns_names]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(old_columns_names)):</span><br><span class="line">    indexer = StringIndexer(inputCol=old_columns_names[i], outputCol=new_columns_names[i])</span><br><span class="line">    df0 = indexer.fit(df0).transform(df0)</span><br><span class="line">vecAss = VectorAssembler(inputCols=new_columns_names[<span class="number">1</span>:], outputCol=<span class="string">'features'</span>)</span><br><span class="line">df0 = vecAss.transform(df0)</span><br><span class="line"><span class="comment"># 更换label列名</span></span><br><span class="line">df0 = df0.withColumnRenamed(new_columns_names[<span class="number">0</span>], <span class="string">'label'</span>)</span><br><span class="line"><span class="comment"># df0.show()</span></span><br><span class="line"><span class="comment"># 创建新的只有label和features的表</span></span><br><span class="line">dfi = df0.select([<span class="string">'label'</span>, <span class="string">'features'</span>])</span><br><span class="line"><span class="comment"># 数据概观</span></span><br><span class="line">dfi.show(<span class="number">5</span>, truncate=<span class="number">0</span>)</span><br><span class="line"><span class="comment">#构建训练数据</span></span><br><span class="line">train_data, test_data = dfi.randomSplit([<span class="number">4.0</span>, <span class="number">1.0</span>], <span class="number">100</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> pyspark.ml.classification <span class="keyword">import</span> LogisticRegression</span><br><span class="line">blor = LogisticRegression(regParam=<span class="number">0.01</span>)<span class="comment">#设置regParam为0.01</span></span><br><span class="line">blorModel = blor.fit(train_data)</span><br><span class="line">result = blorModel.transform(test_data)</span><br><span class="line"><span class="comment"># 计算准确率</span></span><br><span class="line">a = result.filter(result.label == result.prediction).count()/result.count()</span><br><span class="line">print(<span class="string">"逻辑回归： "</span>+str(a))</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> pyspark.ml.classification <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line">dt = DecisionTreeClassifier(maxDepth=<span class="number">5</span>) <span class="comment">#树的最大深度</span></span><br><span class="line">dtModel = dt.fit(train_data)</span><br><span class="line">result = dtModel.transform(test_data)</span><br><span class="line"><span class="comment"># accuracy</span></span><br><span class="line">b = result.filter(result.label == result.prediction).count()/result.count()</span><br><span class="line">print(<span class="string">"决策树： "</span>+str(b))</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> pyspark.ml.classification <span class="keyword">import</span> GBTClassifier</span><br><span class="line">gbt = GBTClassifier(maxDepth=<span class="number">5</span>)</span><br><span class="line">gbtModel = gbt.fit(train_data)</span><br><span class="line">result = gbtModel.transform(test_data)</span><br><span class="line"><span class="comment"># accuracy</span></span><br><span class="line">c = result.filter(result.label == result.prediction).count()/result.count()</span><br><span class="line">print(<span class="string">"梯度增强树： "</span>+str(c))</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> pyspark.ml.classification <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line">rf = RandomForestClassifier(numTrees=<span class="number">10</span>, maxDepth=<span class="number">5</span>)</span><br><span class="line">rfModel = rf.fit(train_data)</span><br><span class="line">result = rfModel.transform(test_data)</span><br><span class="line"><span class="comment"># accuracy</span></span><br><span class="line">d = result.filter(result.label == result.prediction).count()/result.count()</span><br><span class="line"><span class="comment"># 1.0</span></span><br><span class="line">print(<span class="string">"随机森林： "</span>+str(d))</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> pyspark.ml.classification <span class="keyword">import</span> NaiveBayes</span><br><span class="line">nb = NaiveBayes()</span><br><span class="line">nbModel = nb.fit(train_data)</span><br><span class="line">result = nbModel.transform(test_data)</span><br><span class="line"><span class="comment">#accuracy</span></span><br><span class="line">e = result.filter(result.label == result.prediction).count()/result.count()</span><br><span class="line"><span class="comment">#0.9231714812538414</span></span><br><span class="line">print(<span class="string">"朴素贝叶斯： "</span>+str(e))</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> pyspark.ml.classification <span class="keyword">import</span> LinearSVC</span><br><span class="line">svm = LinearSVC(maxIter=<span class="number">10</span>, regParam=<span class="number">0.01</span>)</span><br><span class="line">svmModel = svm.fit(train_data)</span><br><span class="line">result = svmModel.transform(test_data)</span><br><span class="line"><span class="comment"># accuracy</span></span><br><span class="line">f = result.filter(result.label == result.prediction).count()/result.count()</span><br><span class="line"><span class="comment"># 0.9797172710510141</span></span><br><span class="line">print(<span class="string">"支持向量机： "</span>+str(f))</span><br></pre></td></tr></tbody></table></figure>

<h5 id="2-3-商品推荐"><a href="#2-3-商品推荐" class="headerlink" title="2.3. 商品推荐"></a>2.3. 商品推荐</h5><ul>
<li>ItemSimilarity</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Item-Item Similarity computation on pySpark with cosine similarity</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">from</span> itertools <span class="keyword">import</span> combinations</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkContext</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parseVector</span><span class="params">(line)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Parse each line of the specified data file, assuming a "|" delimiter.</span></span><br><span class="line"><span class="string">    Converts each rating to a float</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    line = line.split(<span class="string">"|"</span>)</span><br><span class="line">    <span class="keyword">return</span> line[<span class="number">0</span>],(line[<span class="number">1</span>],float(line[<span class="number">2</span>]))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">findItemPairs</span><span class="params">(user_id,items_with_rating)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    For each user, find all item-item pairs combos. (i.e. items with the same user) </span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="keyword">for</span> item1,item2 <span class="keyword">in</span> combinations(items_with_rating,<span class="number">2</span>):</span><br><span class="line">        <span class="keyword">return</span> (item1[<span class="number">0</span>],item2[<span class="number">0</span>]),(item1[<span class="number">1</span>],item2[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calcSim</span><span class="params">(item_pair,rating_pairs)</span>:</span></span><br><span class="line">    <span class="string">''' </span></span><br><span class="line"><span class="string">    For each item-item pair, return the specified similarity measure,</span></span><br><span class="line"><span class="string">    along with co_raters_count</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    sum_xx, sum_xy, sum_yy, sum_x, sum_y, n = (<span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">0</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> rating_pair <span class="keyword">in</span> rating_pairs:</span><br><span class="line">        sum_xx += np.float(rating_pair[<span class="number">0</span>]) * np.float(rating_pair[<span class="number">0</span>])</span><br><span class="line">        sum_yy += np.float(rating_pair[<span class="number">1</span>]) * np.float(rating_pair[<span class="number">1</span>])</span><br><span class="line">        sum_xy += np.float(rating_pair[<span class="number">0</span>]) * np.float(rating_pair[<span class="number">1</span>])</span><br><span class="line">        <span class="comment"># sum_y += rt[1]</span></span><br><span class="line">        <span class="comment"># sum_x += rt[0]</span></span><br><span class="line">        n += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    cos_sim = cosine(sum_xy,np.sqrt(sum_xx),np.sqrt(sum_yy))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> item_pair, (cos_sim,n)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cosine</span><span class="params">(dot_product,rating_norm_squared,rating2_norm_squared)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    The cosine between two vectors A, B</span></span><br><span class="line"><span class="string">       dotProduct(A, B) / (norm(A) * norm(B))</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    numerator = dot_product</span><br><span class="line">    denominator = rating_norm_squared * rating2_norm_squared</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> (numerator / (float(denominator))) <span class="keyword">if</span> denominator <span class="keyword">else</span> <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    <span class="keyword">if</span> len(sys.argv) &lt; <span class="number">3</span>:</span><br><span class="line">        <span class="keyword">print</span> &gt;&gt; sys.stderr, \</span><br><span class="line">            <span class="string">"Usage: PythonUserCF &lt;master&gt; &lt;file&gt;"</span></span><br><span class="line">        exit(<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">    sc = SparkContext(sys.argv[<span class="number">1</span>], <span class="string">"PythonUserCF"</span>)</span><br><span class="line">    lines = sc.textFile(sys.argv[<span class="number">2</span>])</span><br><span class="line"></span><br><span class="line">    <span class="string">''' </span></span><br><span class="line"><span class="string">    Obtain the sparse user-item matrix</span></span><br><span class="line"><span class="string">        user_id -&gt; [(item_id_1, rating_1),</span></span><br><span class="line"><span class="string">                   [(item_id_2, rating_2),</span></span><br><span class="line"><span class="string">                    ...]</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    user_item_pairs = lines.map(parseVector).groupByKey().cache()</span><br><span class="line"></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Get all item-item pair combos</span></span><br><span class="line"><span class="string">        (item1,item2) -&gt;    [(item1_rating,item2_rating),</span></span><br><span class="line"><span class="string">                             (item1_rating,item2_rating),</span></span><br><span class="line"><span class="string">                             ...]</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line"></span><br><span class="line">    pairwise_items = user_item_pairs.filter(</span><br><span class="line">        <span class="keyword">lambda</span> p: len(p[<span class="number">1</span>]) &gt; <span class="number">1</span>).map(</span><br><span class="line">        <span class="keyword">lambda</span> p: findItemPairs(p[<span class="number">0</span>],p[<span class="number">1</span>])).groupByKey()</span><br><span class="line"></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Calculate the cosine similarity for each item pair</span></span><br><span class="line"><span class="string">        (item1,item2) -&gt;    (similarity,co_raters_count)</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line"></span><br><span class="line">    item_sims = pairwise_items.map(</span><br><span class="line">        <span class="keyword">lambda</span> p: calcSim(p[<span class="number">0</span>],p[<span class="number">1</span>])).collect()</span><br></pre></td></tr></tbody></table></figure>

<ul>
<li>UserSimilarity</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># User-User Similarity computation on pySpark</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">from</span> itertools <span class="keyword">import</span> combinations</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pdb</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkContext</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parseVector</span><span class="params">(line)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Parse each line of the specified data file, assuming a "|" delimiter.</span></span><br><span class="line"><span class="string">    Converts each rating to a float</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    line = line.split(<span class="string">"|"</span>)</span><br><span class="line">    <span class="keyword">return</span> line[<span class="number">1</span>],(line[<span class="number">0</span>],float(line[<span class="number">2</span>]))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">keyOnUserPair</span><span class="params">(item_id,user_and_rating_pair)</span>:</span></span><br><span class="line">    <span class="string">''' </span></span><br><span class="line"><span class="string">    Convert each item and co_rating user pairs to a new vector</span></span><br><span class="line"><span class="string">    keyed on the user pair ids, with the co_ratings as their value. </span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    (user1_with_rating,user2_with_rating) = user_and_rating_pair</span><br><span class="line">    user1_id,user2_id = user1_with_rating[<span class="number">0</span>],user2_with_rating[<span class="number">0</span>]</span><br><span class="line">    user1_rating,user2_rating = user1_with_rating[<span class="number">1</span>],user2_with_rating[<span class="number">1</span>]</span><br><span class="line">    <span class="keyword">return</span> (user1_id,user2_id),(user1_rating,user2_rating)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calcSim</span><span class="params">(user_pair,rating_pairs)</span>:</span></span><br><span class="line">    <span class="string">''' </span></span><br><span class="line"><span class="string">    For each user-user pair, return the specified similarity measure,</span></span><br><span class="line"><span class="string">    along with co_raters_count.</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    sum_xx, sum_xy, sum_yy, sum_x, sum_y, n = (<span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">0</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> rating_pair <span class="keyword">in</span> rating_pairs:</span><br><span class="line">        sum_xx += np.float(rating_pair[<span class="number">0</span>]) * np.float(rating_pair[<span class="number">0</span>])</span><br><span class="line">        sum_yy += np.float(rating_pair[<span class="number">1</span>]) * np.float(rating_pair[<span class="number">1</span>])</span><br><span class="line">        sum_xy += np.float(rating_pair[<span class="number">0</span>]) * np.float(rating_pair[<span class="number">1</span>])</span><br><span class="line">        <span class="comment"># sum_y += rt[1]</span></span><br><span class="line">        <span class="comment"># sum_x += rt[0]</span></span><br><span class="line">        n += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    cos_sim = cosine(sum_xy,np.sqrt(sum_xx),np.sqrt(sum_yy))</span><br><span class="line">    <span class="keyword">return</span> user_pair, (cos_sim,n)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cosine</span><span class="params">(dot_product,rating_norm_squared,rating2_norm_squared)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    The cosine between two vectors A, B</span></span><br><span class="line"><span class="string">       dotProduct(A, B) / (norm(A) * norm(B))</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    numerator = dot_product</span><br><span class="line">    denominator = rating_norm_squared * rating2_norm_squared</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> (numerator / (float(denominator))) <span class="keyword">if</span> denominator <span class="keyword">else</span> <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    <span class="keyword">if</span> len(sys.argv) &lt; <span class="number">3</span>:</span><br><span class="line">        <span class="keyword">print</span> &gt;&gt; sys.stderr, \</span><br><span class="line">            <span class="string">"Usage: PythonUserCF &lt;master&gt; &lt;file&gt;"</span></span><br><span class="line">        exit(<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">    sc = SparkContext(sys.argv[<span class="number">1</span>], <span class="string">"PythonUserCF"</span>)</span><br><span class="line">    lines = sc.textFile(sys.argv[<span class="number">2</span>])</span><br><span class="line"></span><br><span class="line">    <span class="string">''' </span></span><br><span class="line"><span class="string">    Parse the vector with item_id as the key:</span></span><br><span class="line"><span class="string">        item_id -&gt; (user_id,rating)</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    item_user = lines.map(parseVector).cache()</span><br><span class="line"></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Get co_rating users by joining on item_id:</span></span><br><span class="line"><span class="string">        item_id -&gt; ((user_1,rating),(user2,rating))</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    item_user_pairs = item_user.join(item_user)</span><br><span class="line"></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Key each item_user_pair on the user_pair and get rid of non-unique </span></span><br><span class="line"><span class="string">    user pairs, then aggregate all co-rating pairs:</span></span><br><span class="line"><span class="string">        (user1_id,user2_id) -&gt; [(rating1,rating2),</span></span><br><span class="line"><span class="string">                                (rating1,rating2),</span></span><br><span class="line"><span class="string">                                (rating1,rating2),</span></span><br><span class="line"><span class="string">                                ...]</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    user_item_rating_pairs = item_user_pairs.map(</span><br><span class="line">        <span class="keyword">lambda</span> p: keyOnUserPair(p[<span class="number">0</span>],p[<span class="number">1</span>])).filter(</span><br><span class="line">        <span class="keyword">lambda</span> p: p[<span class="number">0</span>][<span class="number">0</span>] != p[<span class="number">0</span>][<span class="number">1</span>]).groupByKey()</span><br><span class="line"></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Calculate the cosine similarity for each user pair:</span></span><br><span class="line"><span class="string">        (user1,user2) -&gt;    (similarity,co_raters_count)</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    user_pair_sims = user_item_rating_pairs.map(</span><br><span class="line">        <span class="keyword">lambda</span> p: calcSim(p[<span class="number">0</span>],p[<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> p <span class="keyword">in</span> user_pair_sims.collect():</span><br><span class="line">        <span class="keyword">print</span> p</span><br></pre></td></tr></tbody></table></figure>

<ul>
<li>ItemBasedRecommender</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Item-based Collaborative Filtering on pySpark with cosine similarity and weighted sums</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line"><span class="keyword">from</span> itertools <span class="keyword">import</span> combinations</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> csv</span><br><span class="line"><span class="keyword">import</span> pdb</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkContext</span><br><span class="line"><span class="keyword">from</span> recsys.evaluation.prediction <span class="keyword">import</span> MAE</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parseVector</span><span class="params">(line)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Parse each line of the specified data file, assuming a "|" delimiter.</span></span><br><span class="line"><span class="string">    Converts each rating to a float</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    line = line.split(<span class="string">"|"</span>)</span><br><span class="line">    <span class="keyword">return</span> line[<span class="number">0</span>],(line[<span class="number">1</span>],float(line[<span class="number">2</span>]))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sampleInteractions</span><span class="params">(user_id,items_with_rating,n)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    For users with # interactions &gt; n, replace their interaction history</span></span><br><span class="line"><span class="string">    with a sample of n items_with_rating</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="keyword">if</span> len(items_with_rating) &gt; n:</span><br><span class="line">        <span class="keyword">return</span> user_id, random.sample(items_with_rating,n)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> user_id, items_with_rating</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">findItemPairs</span><span class="params">(user_id,items_with_rating)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    For each user, find all item-item pairs combos. (i.e. items with the same user) </span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="keyword">for</span> item1,item2 <span class="keyword">in</span> combinations(items_with_rating,<span class="number">2</span>):</span><br><span class="line">        <span class="keyword">return</span> (item1[<span class="number">0</span>],item2[<span class="number">0</span>]),(item1[<span class="number">1</span>],item2[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calcSim</span><span class="params">(item_pair,rating_pairs)</span>:</span></span><br><span class="line">    <span class="string">''' </span></span><br><span class="line"><span class="string">    For each item-item pair, return the specified similarity measure,</span></span><br><span class="line"><span class="string">    along with co_raters_count</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    sum_xx, sum_xy, sum_yy, sum_x, sum_y, n = (<span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">0</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> rating_pair <span class="keyword">in</span> rating_pairs:</span><br><span class="line">        sum_xx += np.float(rating_pair[<span class="number">0</span>]) * np.float(rating_pair[<span class="number">0</span>])</span><br><span class="line">        sum_yy += np.float(rating_pair[<span class="number">1</span>]) * np.float(rating_pair[<span class="number">1</span>])</span><br><span class="line">        sum_xy += np.float(rating_pair[<span class="number">0</span>]) * np.float(rating_pair[<span class="number">1</span>])</span><br><span class="line">        <span class="comment"># sum_y += rt[1]</span></span><br><span class="line">        <span class="comment"># sum_x += rt[0]</span></span><br><span class="line">        n += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    cos_sim = cosine(sum_xy,np.sqrt(sum_xx),np.sqrt(sum_yy))</span><br><span class="line">    <span class="keyword">return</span> item_pair, (cos_sim,n)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cosine</span><span class="params">(dot_product,rating_norm_squared,rating2_norm_squared)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    The cosine between two vectors A, B</span></span><br><span class="line"><span class="string">       dotProduct(A, B) / (norm(A) * norm(B))</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    numerator = dot_product</span><br><span class="line">    denominator = rating_norm_squared * rating2_norm_squared</span><br><span class="line">    <span class="keyword">return</span> (numerator / (float(denominator))) <span class="keyword">if</span> denominator <span class="keyword">else</span> <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">correlation</span><span class="params">(size, dot_product, rating_sum, \</span></span></span><br><span class="line"><span class="function"><span class="params">            rating2sum, rating_norm_squared, rating2_norm_squared)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    The correlation between two vectors A, B is</span></span><br><span class="line"><span class="string">      [n * dotProduct(A, B) - sum(A) * sum(B)] /</span></span><br><span class="line"><span class="string">        sqrt{ [n * norm(A)^2 - sum(A)^2] [n * norm(B)^2 - sum(B)^2] }</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    numerator = size * dot_product - rating_sum * rating2sum</span><br><span class="line">    denominator = sqrt(size * rating_norm_squared - rating_sum * rating_sum) * \</span><br><span class="line">                    sqrt(size * rating2_norm_squared - rating2sum * rating2sum)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> (numerator / (float(denominator))) <span class="keyword">if</span> denominator <span class="keyword">else</span> <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">keyOnFirstItem</span><span class="params">(item_pair,item_sim_data)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    For each item-item pair, make the first item's id the key</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    (item1_id,item2_id) = item_pair</span><br><span class="line">    <span class="keyword">return</span> item1_id,(item2_id,item_sim_data)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">nearestNeighbors</span><span class="params">(item_id,items_and_sims,n)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Sort the predictions list by similarity and select the top-N neighbors</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    items_and_sims.sort(key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>][<span class="number">0</span>],reverse=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> item_id, items_and_sims[:n]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">topNRecommendations</span><span class="params">(user_id,items_with_rating,item_sims,n)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Calculate the top-N item recommendations for each user using the </span></span><br><span class="line"><span class="string">    weighted sums method</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># initialize dicts to store the score of each individual item,</span></span><br><span class="line">    <span class="comment"># since an item can exist in more than one item neighborhood</span></span><br><span class="line">    totals = defaultdict(int)</span><br><span class="line">    sim_sums = defaultdict(int)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (item,rating) <span class="keyword">in</span> items_with_rating:</span><br><span class="line"></span><br><span class="line">        <span class="comment"># lookup the nearest neighbors for this item</span></span><br><span class="line">        nearest_neighbors = item_sims.get(item,<span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> nearest_neighbors:</span><br><span class="line">            <span class="keyword">for</span> (neighbor,(sim,count)) <span class="keyword">in</span> nearest_neighbors:</span><br><span class="line">                <span class="keyword">if</span> neighbor != item:</span><br><span class="line"></span><br><span class="line">                    <span class="comment"># update totals and sim_sums with the rating data</span></span><br><span class="line">                    totals[neighbor] += sim * rating</span><br><span class="line">                    sim_sums[neighbor] += sim</span><br><span class="line"></span><br><span class="line">    <span class="comment"># create the normalized list of scored items </span></span><br><span class="line">    scored_items = [(total/sim_sums[item],item) <span class="keyword">for</span> item,total <span class="keyword">in</span> totals.items()]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># sort the scored items in ascending order</span></span><br><span class="line">    scored_items.sort(reverse=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># take out the item score</span></span><br><span class="line">    <span class="comment"># ranked_items = [x[1] for x in scored_items]</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> user_id,scored_items[:n]</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    <span class="keyword">if</span> len(sys.argv) &lt; <span class="number">3</span>:</span><br><span class="line">        <span class="keyword">print</span> &gt;&gt; sys.stderr, \</span><br><span class="line">            <span class="string">"Usage: PythonUserCF &lt;master&gt; &lt;file&gt;"</span></span><br><span class="line">        exit(<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">    sc = SparkContext(sys.argv[<span class="number">1</span>], <span class="string">"PythonUserCF"</span>)</span><br><span class="line">    lines = sc.textFile(sys.argv[<span class="number">2</span>])</span><br><span class="line"></span><br><span class="line">    <span class="string">''' </span></span><br><span class="line"><span class="string">    Obtain the sparse user-item matrix:</span></span><br><span class="line"><span class="string">        user_id -&gt; [(item_id_1, rating_1),</span></span><br><span class="line"><span class="string">                   [(item_id_2, rating_2),</span></span><br><span class="line"><span class="string">                    ...]</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    user_item_pairs = lines.map(parseVector).groupByKey().map(</span><br><span class="line">        <span class="keyword">lambda</span> p: sampleInteractions(p[<span class="number">0</span>],p[<span class="number">1</span>],<span class="number">500</span>)).cache()</span><br><span class="line"></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Get all item-item pair combos:</span></span><br><span class="line"><span class="string">        (item1,item2) -&gt;    [(item1_rating,item2_rating),</span></span><br><span class="line"><span class="string">                             (item1_rating,item2_rating),</span></span><br><span class="line"><span class="string">                             ...]</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line"></span><br><span class="line">    pairwise_items = user_item_pairs.filter(</span><br><span class="line">        <span class="keyword">lambda</span> p: len(p[<span class="number">1</span>]) &gt; <span class="number">1</span>).map(</span><br><span class="line">        <span class="keyword">lambda</span> p: findItemPairs(p[<span class="number">0</span>],p[<span class="number">1</span>])).groupByKey()</span><br><span class="line"></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Calculate the cosine similarity for each item pair and select the top-N nearest neighbors:</span></span><br><span class="line"><span class="string">        (item1,item2) -&gt;    (similarity,co_raters_count)</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line"></span><br><span class="line">    item_sims = pairwise_items.map(</span><br><span class="line">        <span class="keyword">lambda</span> p: calcSim(p[<span class="number">0</span>],p[<span class="number">1</span>])).map(</span><br><span class="line">        <span class="keyword">lambda</span> p: keyOnFirstItem(p[<span class="number">0</span>],p[<span class="number">1</span>])).groupByKey().map(</span><br><span class="line">        <span class="keyword">lambda</span> p : (p[<span class="number">0</span>], list(p[<span class="number">1</span>]))).map(</span><br><span class="line">        <span class="keyword">lambda</span> p: nearestNeighbors(p[<span class="number">0</span>],p[<span class="number">1</span>],<span class="number">50</span>)).collect()</span><br><span class="line"></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Preprocess the item similarity matrix into a dictionary and store it as a broadcast variable:</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line"></span><br><span class="line">    item_sim_dict = {}</span><br><span class="line">    <span class="keyword">for</span> (item,data) <span class="keyword">in</span> item_sims: </span><br><span class="line">        item_sim_dict[item] = data</span><br><span class="line"></span><br><span class="line">    isb = sc.broadcast(item_sim_dict)</span><br><span class="line"></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Calculate the top-N item recommendations for each user</span></span><br><span class="line"><span class="string">        user_id -&gt; [item1,item2,item3,...]</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    user_item_recs = user_item_pairs.map(</span><br><span class="line">        <span class="keyword">lambda</span> p: topNRecommendations(p[<span class="number">0</span>],p[<span class="number">1</span>],isb.value,<span class="number">500</span>)).collect()</span><br><span class="line"></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Read in test data and calculate MAE</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line"></span><br><span class="line">    test_ratings = defaultdict(list)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># read in the test data</span></span><br><span class="line">    f = open(<span class="string">"tests/data/cftest.txt"</span>, <span class="string">'rt'</span>)</span><br><span class="line">    reader = csv.reader(f, delimiter=<span class="string">'|'</span>)</span><br><span class="line">    <span class="keyword">for</span> row <span class="keyword">in</span> reader:</span><br><span class="line">        user = row[<span class="number">0</span>]</span><br><span class="line">        item = row[<span class="number">1</span>]</span><br><span class="line">        rating = row[<span class="number">2</span>]</span><br><span class="line">        test_ratings[user] += [(item,rating)]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># create train-test rating tuples</span></span><br><span class="line">    preds = []</span><br><span class="line">    <span class="keyword">for</span> (user,items_with_rating) <span class="keyword">in</span> user_item_recs:</span><br><span class="line">        <span class="keyword">for</span> (rating,item) <span class="keyword">in</span> items_with_rating:</span><br><span class="line">            <span class="keyword">for</span> (test_item,test_rating) <span class="keyword">in</span> test_ratings[user]:                </span><br><span class="line">                <span class="keyword">if</span> str(test_item) == str(item):</span><br><span class="line">                    preds.append((rating,float(test_rating)))</span><br><span class="line"></span><br><span class="line">    mae = MAE(preds)</span><br><span class="line">    result = mae.compute()</span><br><span class="line">    <span class="keyword">print</span> <span class="string">"Mean Absolute Error: "</span>,result</span><br></pre></td></tr></tbody></table></figure>

<ul>
<li>userbasedRemcommender</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># User-based Collaborative Filtering on pySpark with cosine similarity and weighted sums</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line"><span class="keyword">from</span> itertools <span class="keyword">import</span> combinations</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pdb</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkContext</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parseVectorOnUser</span><span class="params">(line)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Parse each line of the specified data file, assuming a "|" delimiter.</span></span><br><span class="line"><span class="string">    Key is user_id, converts each rating to a float.</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    line = line.split(<span class="string">"|"</span>)</span><br><span class="line">    <span class="keyword">return</span> line[<span class="number">0</span>],(line[<span class="number">1</span>],float(line[<span class="number">2</span>]))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parseVectorOnItem</span><span class="params">(line)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Parse each line of the specified data file, assuming a "|" delimiter.</span></span><br><span class="line"><span class="string">    Key is item_id, converts each rating to a float.</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    line = line.split(<span class="string">"|"</span>)</span><br><span class="line">    <span class="keyword">return</span> line[<span class="number">1</span>],(line[<span class="number">0</span>],float(line[<span class="number">2</span>]))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sampleInteractions</span><span class="params">(item_id,users_with_rating,n)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    For items with # interactions &gt; n, replace their interaction history</span></span><br><span class="line"><span class="string">    with a sample of n users_with_rating</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="keyword">if</span> len(users_with_rating) &gt; n:</span><br><span class="line">        <span class="keyword">return</span> item_id, random.sample(users_with_rating,n)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> item_id, users_with_rating</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">findUserPairs</span><span class="params">(item_id,users_with_rating)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    For each item, find all user-user pairs combos. (i.e. users with the same item) </span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="keyword">for</span> user1,user2 <span class="keyword">in</span> combinations(users_with_rating,<span class="number">2</span>):</span><br><span class="line">        <span class="keyword">return</span> (user1[<span class="number">0</span>],user2[<span class="number">0</span>]),(user1[<span class="number">1</span>],user2[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calcSim</span><span class="params">(user_pair,rating_pairs)</span>:</span></span><br><span class="line">    <span class="string">''' </span></span><br><span class="line"><span class="string">    For each user-user pair, return the specified similarity measure,</span></span><br><span class="line"><span class="string">    along with co_raters_count.</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    sum_xx, sum_xy, sum_yy, sum_x, sum_y, n = (<span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">0</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> rating_pair <span class="keyword">in</span> rating_pairs:</span><br><span class="line">        sum_xx += np.float(rating_pair[<span class="number">0</span>]) * np.float(rating_pair[<span class="number">0</span>])</span><br><span class="line">        sum_yy += np.float(rating_pair[<span class="number">1</span>]) * np.float(rating_pair[<span class="number">1</span>])</span><br><span class="line">        sum_xy += np.float(rating_pair[<span class="number">0</span>]) * np.float(rating_pair[<span class="number">1</span>])</span><br><span class="line">        <span class="comment"># sum_y += rt[1]</span></span><br><span class="line">        <span class="comment"># sum_x += rt[0]</span></span><br><span class="line">        n += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    cos_sim = cosine(sum_xy,np.sqrt(sum_xx),np.sqrt(sum_yy))</span><br><span class="line">    <span class="keyword">return</span> user_pair, (cos_sim,n)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cosine</span><span class="params">(dot_product,rating_norm_squared,rating2_norm_squared)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    The cosine between two vectors A, B</span></span><br><span class="line"><span class="string">       dotProduct(A, B) / (norm(A) * norm(B))</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    numerator = dot_product</span><br><span class="line">    denominator = rating_norm_squared * rating2_norm_squared</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> (numerator / (float(denominator))) <span class="keyword">if</span> denominator <span class="keyword">else</span> <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">keyOnFirstUser</span><span class="params">(user_pair,item_sim_data)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    For each user-user pair, make the first user's id the key</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    (user1_id,user2_id) = user_pair</span><br><span class="line">    <span class="keyword">return</span> user1_id,(user2_id,item_sim_data)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">nearestNeighbors</span><span class="params">(user,users_and_sims,n)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Sort the predictions list by similarity and select the top-N neighbors</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    users_and_sims.sort(key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>][<span class="number">0</span>],reverse=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> user, users_and_sims[:n]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">topNRecommendations</span><span class="params">(user_id,user_sims,users_with_rating,n)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Calculate the top-N item recommendations for each user using the </span></span><br><span class="line"><span class="string">    weighted sums method</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># initialize dicts to store the score of each individual item,</span></span><br><span class="line">    <span class="comment"># since an item can exist in more than one item neighborhood</span></span><br><span class="line">    totals = defaultdict(int)</span><br><span class="line">    sim_sums = defaultdict(int)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (neighbor,(sim,count)) <span class="keyword">in</span> user_sims:</span><br><span class="line"></span><br><span class="line">        <span class="comment"># lookup the item predictions for this neighbor</span></span><br><span class="line">        unscored_items = users_with_rating.get(neighbor,<span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> unscored_items:</span><br><span class="line">            <span class="keyword">for</span> (item,rating) <span class="keyword">in</span> unscored_items:</span><br><span class="line">                <span class="keyword">if</span> neighbor != item:</span><br><span class="line"></span><br><span class="line">                    <span class="comment"># update totals and sim_sums with the rating data</span></span><br><span class="line">                    totals[neighbor] += sim * rating</span><br><span class="line">                    sim_sums[neighbor] += sim</span><br><span class="line"></span><br><span class="line">    <span class="comment"># create the normalized list of scored items </span></span><br><span class="line">    scored_items = [(total/sim_sums[item],item) <span class="keyword">for</span> item,total <span class="keyword">in</span> totals.items()]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># sort the scored items in ascending order</span></span><br><span class="line">    scored_items.sort(reverse=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># take out the item score</span></span><br><span class="line">    ranked_items = [x[<span class="number">1</span>] <span class="keyword">for</span> x <span class="keyword">in</span> scored_items]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> user_id,ranked_items[:n]</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    <span class="keyword">if</span> len(sys.argv) &lt; <span class="number">3</span>:</span><br><span class="line">        <span class="keyword">print</span> &gt;&gt; sys.stderr, \</span><br><span class="line">            <span class="string">"Usage: PythonUserCF &lt;master&gt; &lt;file&gt;"</span></span><br><span class="line">        exit(<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">    sc = SparkContext(sys.argv[<span class="number">1</span>],<span class="string">"PythonUserItemCF"</span>)</span><br><span class="line">    lines = sc.textFile(sys.argv[<span class="number">2</span>])</span><br><span class="line"></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Obtain the sparse item-user matrix:</span></span><br><span class="line"><span class="string">        item_id -&gt; ((user_1,rating),(user2,rating))</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    item_user_pairs = lines.map(parseVectorOnItem).groupByKey().map(</span><br><span class="line">        <span class="keyword">lambda</span> p: sampleInteractions(p[<span class="number">0</span>],p[<span class="number">1</span>],<span class="number">500</span>)).cache()</span><br><span class="line"></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Get all item-item pair combos:</span></span><br><span class="line"><span class="string">        (user1_id,user2_id) -&gt; [(rating1,rating2),</span></span><br><span class="line"><span class="string">                                (rating1,rating2),</span></span><br><span class="line"><span class="string">                                (rating1,rating2),</span></span><br><span class="line"><span class="string">                                ...]</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    pairwise_users = item_user_pairs.filter(</span><br><span class="line">        <span class="keyword">lambda</span> p: len(p[<span class="number">1</span>]) &gt; <span class="number">1</span>).map(</span><br><span class="line">        <span class="keyword">lambda</span> p: findUserPairs(p[<span class="number">0</span>],p[<span class="number">1</span>])).groupByKey()</span><br><span class="line"></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Calculate the cosine similarity for each user pair and select the top-N nearest neighbors:</span></span><br><span class="line"><span class="string">        (user1,user2) -&gt;    (similarity,co_raters_count)</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    user_sims = pairwise_users.map(</span><br><span class="line">        <span class="keyword">lambda</span> p: calcSim(p[<span class="number">0</span>],p[<span class="number">1</span>])).map(</span><br><span class="line">        <span class="keyword">lambda</span> p: keyOnFirstUser(p[<span class="number">0</span>],p[<span class="number">1</span>])).groupByKey().map(</span><br><span class="line">        <span class="keyword">lambda</span> p: nearestNeighbors(p[<span class="number">0</span>],p[<span class="number">1</span>],<span class="number">50</span>))</span><br><span class="line"></span><br><span class="line">    <span class="string">''' </span></span><br><span class="line"><span class="string">    Obtain the the item history for each user and store it as a broadcast variable</span></span><br><span class="line"><span class="string">        user_id -&gt; [(item_id_1, rating_1),</span></span><br><span class="line"><span class="string">                   [(item_id_2, rating_2),</span></span><br><span class="line"><span class="string">                    ...]</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line"></span><br><span class="line">    user_item_hist = lines.map(parseVectorOnUser).groupByKey().collect()</span><br><span class="line"></span><br><span class="line">    ui_dict = {}</span><br><span class="line">    <span class="keyword">for</span> (user,items) <span class="keyword">in</span> user_item_hist: </span><br><span class="line">        ui_dict[user] = items</span><br><span class="line"></span><br><span class="line">    uib = sc.broadcast(ui_dict)</span><br><span class="line"></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Calculate the top-N item recommendations for each user</span></span><br><span class="line"><span class="string">        user_id -&gt; [item1,item2,item3,...]</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    user_item_recs = user_sims.map(</span><br><span class="line">        <span class="keyword">lambda</span> p: topNRecommendations(p[<span class="number">0</span>],p[<span class="number">1</span>],uib.value,<span class="number">100</span>)).collect()</span><br></pre></td></tr></tbody></table></figure>

<h5 id="2-4-图像分类"><a href="#2-4-图像分类" class="headerlink" title="2.4. 图像分类"></a>2.4. <a href="http://172.26.85.202:65501/notebooks/Transfer-Learning-PySpark-master/PySpark-Digit-Image-MultiClass.ipynb" target="_blank" rel="noopener">图像分类</a></h5><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#拼接展示图片数据</span></span><br><span class="line"><span class="keyword">import</span> IPython.display <span class="keyword">as</span> dp</span><br><span class="line"><span class="comment"># collect all .png files in ssample dir</span></span><br><span class="line">fs = !ls sample/*.png</span><br><span class="line"><span class="comment"># create list of image objects</span></span><br><span class="line">images = []</span><br><span class="line"><span class="keyword">for</span> ea <span class="keyword">in</span> fs:</span><br><span class="line">    images.append(dp.Image(filename=ea, format=<span class="string">'png'</span>))</span><br><span class="line"><span class="comment"># display all images</span></span><br><span class="line"><span class="keyword">for</span> ea <span class="keyword">in</span> images:</span><br><span class="line">    dp.display_png(ea)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkContext, SparkConf</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SQLContext</span><br><span class="line"></span><br><span class="line">spark = SparkSession.builder.\</span><br><span class="line">            config(<span class="string">"spark.executor.memory"</span>, <span class="string">"1g"</span>).\</span><br><span class="line">            config(<span class="string">"spark.driver.memory"</span>, <span class="string">"4g"</span>).\</span><br><span class="line">            config(<span class="string">"spark.cores.max"</span>, <span class="string">"2"</span>).\</span><br><span class="line">            appName(<span class="string">'SparkImageClassifier'</span>).getOrCreate()</span><br><span class="line"><span class="comment">#加载图片数据</span></span><br><span class="line"><span class="keyword">import</span> sys, glob, os</span><br><span class="line">sys.path.extend(glob.glob(os.path.join(os.path.expanduser(<span class="string">"~"</span>), <span class="string">".ivy2/jars/*.jar"</span>)))</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> pyspark.ml.image <span class="keyword">import</span> ImageSchema</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> lit</span><br><span class="line"></span><br><span class="line"><span class="comment"># loaded image</span></span><br><span class="line">zero_df = ImageSchema.readImages(<span class="string">"images/0"</span>).withColumn(<span class="string">"label"</span>, lit(<span class="number">0</span>))</span><br><span class="line">one_df = ImageSchema.readImages(<span class="string">"images/1"</span>).withColumn(<span class="string">"label"</span>, lit(<span class="number">1</span>))</span><br><span class="line">two_df = ImageSchema.readImages(<span class="string">"images/2"</span>).withColumn(<span class="string">"label"</span>, lit(<span class="number">2</span>))</span><br><span class="line">three_df = ImageSchema.readImages(<span class="string">"images/3"</span>).withColumn(<span class="string">"label"</span>, lit(<span class="number">3</span>))</span><br><span class="line">four_df = ImageSchema.readImages(<span class="string">"images/4"</span>).withColumn(<span class="string">"label"</span>, lit(<span class="number">4</span>))</span><br><span class="line">five_df = ImageSchema.readImages(<span class="string">"images/5"</span>).withColumn(<span class="string">"label"</span>, lit(<span class="number">5</span>))</span><br><span class="line">six_df = ImageSchema.readImages(<span class="string">"images/6"</span>).withColumn(<span class="string">"label"</span>, lit(<span class="number">6</span>))</span><br><span class="line">seven_df = ImageSchema.readImages(<span class="string">"images/7"</span>).withColumn(<span class="string">"label"</span>, lit(<span class="number">7</span>))</span><br><span class="line">eight_df = ImageSchema.readImages(<span class="string">"images/8"</span>).withColumn(<span class="string">"label"</span>, lit(<span class="number">8</span>))</span><br><span class="line">nine_df = ImageSchema.readImages(<span class="string">"images/9"</span>).withColumn(<span class="string">"label"</span>, lit(<span class="number">9</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># merge data frame</span></span><br><span class="line"><span class="keyword">from</span> functools <span class="keyword">import</span> reduce</span><br><span class="line">dataframes = [zero_df, one_df, two_df, three_df, </span><br><span class="line">              four_df,five_df,six_df,seven_df,eight_df,nine_df]</span><br><span class="line"></span><br><span class="line">df = reduce(<span class="keyword">lambda</span> first, second: first.union(second), dataframes)</span><br><span class="line"><span class="comment"># repartition dataframe </span></span><br><span class="line">df = df.repartition(<span class="number">200</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># On hot encoding </span></span><br><span class="line"><span class="keyword">from</span> pyspark.ml.feature <span class="keyword">import</span> OneHotEncoderEstimator</span><br><span class="line">encoder = OneHotEncoderEstimator(inputCols=[<span class="string">"label"</span>],outputCols=[<span class="string">"one_hot_label"</span>])</span><br><span class="line">model = encoder.fit(df)</span><br><span class="line">df = model.transform(df)</span><br><span class="line"></span><br><span class="line"><span class="comment"># split the data-frame</span></span><br><span class="line">train, test = df.randomSplit([<span class="number">0.8</span>, <span class="number">0.2</span>], <span class="number">42</span>)</span><br><span class="line">train.printSchema()  <span class="comment">#image, label</span></span><br><span class="line">test.printSchema()</span><br><span class="line"></span><br><span class="line">%%time</span><br><span class="line"><span class="keyword">from</span> pyspark.ml.evaluation <span class="keyword">import</span> MulticlassClassificationEvaluator</span><br><span class="line"><span class="keyword">from</span> pyspark.ml.classification <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="keyword">from</span> pyspark.ml <span class="keyword">import</span> Pipeline</span><br><span class="line"><span class="keyword">from</span> sparkdl <span class="keyword">import</span> DeepImageFeaturizer </span><br><span class="line"><span class="comment"># model: InceptionV3</span></span><br><span class="line"><span class="comment"># extracting feature from images</span></span><br><span class="line">featurizer = DeepImageFeaturizer(inputCol=<span class="string">"image"</span>, outputCol=<span class="string">"features"</span>,</span><br><span class="line">                                 modelName=<span class="string">"InceptionV3"</span>)</span><br><span class="line"><span class="comment"># used as a multi class classifier</span></span><br><span class="line">lr = LogisticRegression(maxIter=<span class="number">5</span>, regParam=<span class="number">0.03</span>, </span><br><span class="line">                        elasticNetParam=<span class="number">0.5</span>, labelCol=<span class="string">"label"</span>) </span><br><span class="line"><span class="comment"># define a pipeline model</span></span><br><span class="line">sparkdn = Pipeline(stages=[featurizer, lr])</span><br><span class="line">spark_model = sparkdn.fit(train)</span><br><span class="line"></span><br><span class="line"><span class="comment">#Evaluation</span></span><br><span class="line"><span class="keyword">from</span> pyspark.ml.evaluation <span class="keyword">import</span> MulticlassClassificationEvaluator</span><br><span class="line"><span class="comment"># evaluate the model with test set</span></span><br><span class="line">evaluator = MulticlassClassificationEvaluator() </span><br><span class="line">transform_test = spark_model.transform(test)</span><br><span class="line">print(<span class="string">'F1-Score '</span>, evaluator.evaluate(transform_test, </span><br><span class="line">                                      {evaluator.metricName: <span class="string">'f1'</span>}))</span><br><span class="line">print(<span class="string">'Precision '</span>, evaluator.evaluate(transform_test,</span><br><span class="line">                                       {evaluator.metricName: <span class="string">'weightedPrecision'</span>}))</span><br><span class="line">print(<span class="string">'Recall '</span>, evaluator.evaluate(transform_test, </span><br><span class="line">                                    {evaluator.metricName: <span class="string">'weightedRecall'</span>}))</span><br><span class="line">print(<span class="string">'Accuracy '</span>, evaluator.evaluate(transform_test, </span><br><span class="line">                                      {evaluator.metricName: <span class="string">'accuracy'</span>}))</span><br><span class="line"></span><br><span class="line"><span class="comment">#Confusion Matrix</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> itertools</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_confusion_matrix</span><span class="params">(cm, classes,</span></span></span><br><span class="line"><span class="function"><span class="params">                          normalize=False,</span></span></span><br><span class="line"><span class="function"><span class="params">                          title=<span class="string">'Confusion matrix'</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                          cmap=plt.cm.GnBu)</span>:</span></span><br><span class="line"></span><br><span class="line">    plt.imshow(cm, interpolation=<span class="string">'nearest'</span>, cmap=cmap)</span><br><span class="line">    plt.title(title)</span><br><span class="line">    tick_marks = np.arange(len(classes))</span><br><span class="line">    plt.xticks(tick_marks, classes, rotation=<span class="number">45</span>)</span><br><span class="line">    plt.yticks(tick_marks, classes)</span><br><span class="line"></span><br><span class="line">    fmt = <span class="string">'.2f'</span> <span class="keyword">if</span> normalize <span class="keyword">else</span> <span class="string">'d'</span></span><br><span class="line">    thresh = cm.max() / <span class="number">2.</span></span><br><span class="line">    <span class="keyword">for</span> i, j <span class="keyword">in</span> itertools.product(range(cm.shape[<span class="number">0</span>]), range(cm.shape[<span class="number">1</span>])):</span><br><span class="line">        plt.text(j, i, format(cm[i, j], fmt),</span><br><span class="line">                 horizontalalignment=<span class="string">"center"</span>,</span><br><span class="line">                 color=<span class="string">"white"</span> <span class="keyword">if</span> cm[i, j] &gt; thresh <span class="keyword">else</span> <span class="string">"black"</span>)</span><br><span class="line"></span><br><span class="line">    plt.tight_layout()</span><br><span class="line">    plt.ylabel(<span class="string">'True label'</span>)</span><br><span class="line">    plt.xlabel(<span class="string">'Predicted label'</span>)</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">- Convert Spark-DataFrame to Pnadas-DataFrame</span></span><br><span class="line"><span class="string">- Call Confusion Matrix With 'True' and 'Predicted' Label</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> confusion_matrix</span><br><span class="line">y_true = transform_test.select(<span class="string">"label"</span>)</span><br><span class="line">y_true = y_true.toPandas() <span class="comment"># convert to pandas dataframe from spark dataframe</span></span><br><span class="line">y_pred = transform_test.select(<span class="string">"prediction"</span>)</span><br><span class="line">y_pred = y_pred.toPandas() <span class="comment"># convert to pandas dataframe from spark dataframe</span></span><br><span class="line">cnf_matrix = confusion_matrix(y_true, y_pred,labels=range(<span class="number">10</span>))</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">- Visualize the 'Confusion Matrix' </span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line">sns.set_style(<span class="string">"darkgrid"</span>)</span><br><span class="line">plt.figure(figsize=(<span class="number">7</span>,<span class="number">7</span>))</span><br><span class="line">plt.grid(<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># call pre defined function</span></span><br><span class="line">plot_confusion_matrix(cnf_matrix, classes=range(<span class="number">10</span>)) </span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">- Classification Report of each class group</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> classification_report</span><br><span class="line"></span><br><span class="line">target_names = [<span class="string">"Class {}"</span>.format(i) <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>)]</span><br><span class="line">print(classification_report(y_true, y_pred, target_names = target_names))</span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">- A custom ROC AUC score function for multi-class classification problem</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> roc_curve, auc, roc_auc_score</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> LabelBinarizer</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">multiclass_roc_auc_score</span><span class="params">(y_test, y_pred, average=<span class="string">"macro"</span>)</span>:</span></span><br><span class="line">    lb = LabelBinarizer()</span><br><span class="line">    lb.fit(y_test)</span><br><span class="line">    y_test = lb.transform(y_test)</span><br><span class="line">    y_pred = lb.transform(y_pred)</span><br><span class="line">    <span class="keyword">return</span> roc_auc_score(y_test, y_pred, average=average)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(<span class="string">'ROC AUC score:'</span>, multiclass_roc_auc_score(y_true,y_pred))</span><br></pre></td></tr></tbody></table></figure>

<h4 id="3-文件读取"><a href="#3-文件读取" class="headerlink" title="3. 文件读取"></a>3. 文件读取</h4><ul>
<li>图片数据</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#方式一</span></span><br><span class="line"><span class="keyword">from</span> pyspark.ml.image <span class="keyword">import</span> ImageSchema</span><br><span class="line">image_df = ImageSchema.readImages(<span class="string">"/data/myimages"</span>)</span><br><span class="line">image_df.show()</span><br><span class="line"><span class="comment">#方式二</span></span><br><span class="line"><span class="keyword">from</span> keras.preprocessing <span class="keyword">import</span> image</span><br><span class="line">img = image.load_img(<span class="string">"/data/myimages/daisy.jpg"</span>, target_size=(<span class="number">299</span>, <span class="number">299</span>))</span><br></pre></td></tr></tbody></table></figure>

<ul>
<li>TransferLearning</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#方式三</span></span><br><span class="line"><span class="keyword">from</span> pyspark.ml.classification <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="keyword">from</span> pyspark.ml.evaluation <span class="keyword">import</span> MulticlassClassificationEvaluator</span><br><span class="line"><span class="keyword">from</span> pyspark.ml <span class="keyword">import</span> Pipeline</span><br><span class="line"><span class="keyword">from</span> sparkdl <span class="keyword">import</span> DeepImageFeaturizer</span><br><span class="line"></span><br><span class="line">featurizer = DeepImageFeaturizer(inputCol=<span class="string">"image"</span>, outputCol=<span class="string">"features"</span>, modelName=<span class="string">"InceptionV3"</span>)</span><br><span class="line">lr = LogisticRegression(maxIter=<span class="number">20</span>, regParam=<span class="number">0.05</span>, elasticNetParam=<span class="number">0.3</span>, labelCol=<span class="string">"label"</span>)</span><br><span class="line">p = Pipeline(stages=[featurizer, lr])</span><br><span class="line"></span><br><span class="line">model = p.fit(train_images_df)    <span class="comment"># train_images_df is a dataset of images and labels</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Inspect training error</span></span><br><span class="line">df = model.transform(train_images_df.limit(<span class="number">10</span>)).select(<span class="string">"image"</span>, <span class="string">"probability"</span>,  <span class="string">"uri"</span>, <span class="string">"label"</span>)</span><br><span class="line">predictionAndLabels = df.select(<span class="string">"prediction"</span>, <span class="string">"label"</span>)</span><br><span class="line">evaluator = MulticlassClassificationEvaluator(metricName=<span class="string">"accuracy"</span>)</span><br><span class="line">print(<span class="string">"Training set accuracy = "</span> + str(evaluator.evaluate(predictionAndLabels)))</span><br></pre></td></tr></tbody></table></figure>

<ul>
<li>分布式调参</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.applications <span class="keyword">import</span> InceptionV3</span><br><span class="line">model = InceptionV3(weights=<span class="string">"imagenet"</span>)</span><br><span class="line">model.save(<span class="string">'/tmp/model-full.h5'</span>)</span><br><span class="line"><span class="keyword">import</span> PIL.Image</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> keras.applications.imagenet_utils <span class="keyword">import</span> preprocess_input</span><br><span class="line"><span class="keyword">from</span> sparkdl.estimators.keras_image_file_estimator <span class="keyword">import</span> KerasImageFileEstimator</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_image_from_uri</span><span class="params">(local_uri)</span>:</span></span><br><span class="line">  img = (PIL.Image.open(local_uri).convert(<span class="string">'RGB'</span>).resize((<span class="number">299</span>, <span class="number">299</span>), PIL.Image.ANTIALIAS))</span><br><span class="line">  img_arr = np.array(img).astype(np.float32)</span><br><span class="line">  img_tnsr = preprocess_input(img_arr[np.newaxis, :])</span><br><span class="line">  <span class="keyword">return</span> img_tnsr</span><br><span class="line"></span><br><span class="line">estimator = KerasImageFileEstimator( inputCol=<span class="string">"uri"</span>,</span><br><span class="line">                                     outputCol=<span class="string">"prediction"</span>,</span><br><span class="line">                                     labelCol=<span class="string">"one_hot_label"</span>,</span><br><span class="line">                                     imageLoader=load_image_from_uri,</span><br><span class="line">                                     kerasOptimizer=<span class="string">'adam'</span>,</span><br><span class="line">                                     kerasLoss=<span class="string">'categorical_crossentropy'</span>,</span><br><span class="line">                                     modelFile=<span class="string">'/tmp/model-full-tmp.h5'</span> <span class="comment"># local file path for model</span></span><br><span class="line">                                   )</span><br><span class="line"><span class="keyword">from</span> pyspark.ml.evaluation <span class="keyword">import</span> BinaryClassificationEvaluator</span><br><span class="line"><span class="keyword">from</span> pyspark.ml.tuning <span class="keyword">import</span> CrossValidator, ParamGridBuilder</span><br><span class="line"></span><br><span class="line">paramGrid = (</span><br><span class="line">  ParamGridBuilder()</span><br><span class="line">  .addGrid(estimator.kerasFitParams, [{<span class="string">"batch_size"</span>: <span class="number">32</span>, <span class="string">"verbose"</span>: <span class="number">0</span>},</span><br><span class="line">                                      {<span class="string">"batch_size"</span>: <span class="number">64</span>, <span class="string">"verbose"</span>: <span class="number">0</span>}])</span><br><span class="line">  .build()</span><br><span class="line">)</span><br><span class="line">bc = BinaryClassificationEvaluator(rawPredictionCol=<span class="string">"prediction"</span>, labelCol=<span class="string">"label"</span> )</span><br><span class="line">cv = CrossValidator(estimator=estimator, estimatorParamMaps=paramGrid, evaluator=bc, numFolds=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">cvModel = cv.fit(train_df)</span><br></pre></td></tr></tbody></table></figure>

<ul>
<li>deep learning models</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.ml.image <span class="keyword">import</span> ImageSchema</span><br><span class="line"><span class="keyword">from</span> sparkdl <span class="keyword">import</span> DeepImagePredictor</span><br><span class="line"></span><br><span class="line">image_df = ImageSchema.readImages(sample_img_dir)</span><br><span class="line"></span><br><span class="line">predictor = DeepImagePredictor(inputCol=<span class="string">"image"</span>, outputCol=<span class="string">"predicted_labels"</span>, modelName=<span class="string">"InceptionV3"</span>, decodePredictions=<span class="literal">True</span>, topK=<span class="number">10</span>)</span><br><span class="line">predictions_df = predictor.transform(image_df)</span><br></pre></td></tr></tbody></table></figure>

<ul>
<li>KerasImageFileTransformer</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.applications.inception_v3 <span class="keyword">import</span> preprocess_input</span><br><span class="line"><span class="keyword">from</span> keras.preprocessing.image <span class="keyword">import</span> img_to_array, load_img</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> StringType</span><br><span class="line"><span class="keyword">from</span> sparkdl <span class="keyword">import</span> KerasImageFileTransformer</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loadAndPreprocessKerasInceptionV3</span><span class="params">(uri)</span>:</span></span><br><span class="line">  <span class="comment"># this is a typical way to load and prep images in keras</span></span><br><span class="line">  image = img_to_array(load_img(uri, target_size=(<span class="number">299</span>, <span class="number">299</span>)))  <span class="comment"># image dimensions for InceptionV3</span></span><br><span class="line">  image = np.expand_dims(image, axis=<span class="number">0</span>)</span><br><span class="line">  <span class="keyword">return</span> preprocess_input(image)</span><br><span class="line"></span><br><span class="line">transformer = KerasImageFileTransformer(inputCol=<span class="string">"uri"</span>, outputCol=<span class="string">"predictions"</span>,</span><br><span class="line">                                        modelFile=<span class="string">'/tmp/model-full-tmp.h5'</span>,  <span class="comment"># local file path for model</span></span><br><span class="line">                              imageLoader=loadAndPreprocessKerasInceptionV3,</span><br><span class="line">                                        outputMode=<span class="string">"vector"</span>)</span><br><span class="line">files = [os.path.abspath(os.path.join(dirpath, f)) <span class="keyword">for</span> f <span class="keyword">in</span> os.listdir(<span class="string">"/data/myimages"</span>) <span class="keyword">if</span> f.endswith(<span class="string">'.jpg'</span>)]</span><br><span class="line">uri_df = sqlContext.createDataFrame(files, StringType()).toDF(<span class="string">"uri"</span>)</span><br><span class="line">keras_pred_df = transformer.transform(uri_df)</span><br></pre></td></tr></tbody></table></figure>

<ul>
<li>kerasTransferm</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sparkdl <span class="keyword">import</span> KerasTransformer</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Dense</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># Generate random input data</span></span><br><span class="line">num_features = <span class="number">10</span></span><br><span class="line">num_examples = <span class="number">100</span></span><br><span class="line">input_data = [{<span class="string">"features"</span> : np.random.randn(num_features).tolist()} <span class="keyword">for</span> i <span class="keyword">in</span> range(num_examples)]</span><br><span class="line">input_df = sqlContext.createDataFrame(input_data)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create and save a single-hidden-layer Keras model for binary classification</span></span><br><span class="line"><span class="comment"># <span class="doctag">NOTE:</span> In a typical workflow, we'd train the model before exporting it to disk,</span></span><br><span class="line"><span class="comment"># but we skip that step here for brevity</span></span><br><span class="line">model = Sequential()</span><br><span class="line">model.add(Dense(units=<span class="number">20</span>, input_shape=[num_features], activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(Dense(units=<span class="number">1</span>, activation=<span class="string">'sigmoid'</span>))</span><br><span class="line">model_path = <span class="string">"/tmp/simple-binary-classification"</span></span><br><span class="line">model.save(model_path)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create transformer and apply it to our input data</span></span><br><span class="line">transformer = KerasTransformer(inputCol=<span class="string">"features"</span>, outputCol=<span class="string">"predictions"</span>, modelFile=model_path)</span><br><span class="line">final_df = transformer.transform(input_df)</span><br></pre></td></tr></tbody></table></figure>

<ul>
<li>keras model udf</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.applications <span class="keyword">import</span> InceptionV3</span><br><span class="line"><span class="keyword">from</span> sparkdl.udf.keras_image_model <span class="keyword">import</span> registerKerasImageUDF</span><br><span class="line">registerKerasImageUDF(<span class="string">"inceptionV3_udf"</span>, InceptionV3(weights=<span class="string">"imagenet"</span>))</span><br><span class="line">registerKerasImageUDF(<span class="string">"my_custom_keras_model_udf"</span>, <span class="string">"/tmp/model-full-tmp.h5"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> keras.applications <span class="keyword">import</span> InceptionV3</span><br><span class="line"><span class="keyword">from</span> sparkdl.udf.keras_image_model <span class="keyword">import</span> registerKerasImageUDF</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">keras_load_img</span><span class="params">(fpath)</span>:</span></span><br><span class="line">    <span class="keyword">from</span> keras.preprocessing.image <span class="keyword">import</span> load_img, img_to_array</span><br><span class="line">    <span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">    img = load_img(fpath, target_size=(<span class="number">299</span>, <span class="number">299</span>))</span><br><span class="line">    <span class="keyword">return</span> img_to_array(img).astype(np.uint8)</span><br><span class="line"></span><br><span class="line">registerKerasImageUDF(<span class="string">"inceptionV3_udf_with_preprocessing"</span>, InceptionV3(weights=<span class="string">"imagenet"</span>), keras_load_img)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> pyspark.ml.image <span class="keyword">import</span> ImageSchema</span><br><span class="line">image_df = ImageSchema.readImages(sample_img_dir)</span><br><span class="line">image_df.registerTempTable(<span class="string">"sample_images"</span>)</span><br><span class="line">SELECT my_custom_keras_model_udf(image) <span class="keyword">as</span> predictions <span class="keyword">from</span> sample_images</span><br></pre></td></tr></tbody></table></figure>

<h4 id="4-DL-Relative"><a href="#4-DL-Relative" class="headerlink" title="4. DL Relative"></a>4. DL Relative</h4><h5 id="4-1-spark-pytorch"><a href="#4-1-spark-pytorch" class="headerlink" title="4.1. spark+pytorch"></a>4.1. spark+pytorch</h5><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br></pre></td><td class="code"><pre><span class="line">cuda = <span class="literal">False</span></span><br><span class="line"> </span><br><span class="line">spark.conf.set(<span class="string">"spark.sql.execution.arrow.enabled"</span>, <span class="string">"true"</span>)</span><br><span class="line">spark.conf.set(<span class="string">"spark.sql.execution.arrow.maxRecordsPerBatch"</span>, <span class="string">"2048"</span>)</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> shutil</span><br><span class="line"><span class="keyword">import</span> tarfile</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> zipfile</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    <span class="keyword">from</span> urllib.request <span class="keyword">import</span> urlretrieve</span><br><span class="line"><span class="keyword">except</span> ImportError:</span><br><span class="line">    <span class="keyword">from</span> urllib <span class="keyword">import</span> urlretrieve</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets, models, transforms</span><br><span class="line"><span class="keyword">from</span> torchvision.datasets.folder <span class="keyword">import</span> default_loader  <span class="comment"># private API</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> col, pandas_udf, PandasUDFType</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> ArrayType, FloatType</span><br><span class="line">use_cuda = cuda <span class="keyword">and</span> torch.cuda.is_available()</span><br><span class="line">device = torch.device(<span class="string">"cuda"</span> <span class="keyword">if</span> use_cuda <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br><span class="line"></span><br><span class="line">URL = <span class="string">"http://download.tensorflow.org/example_images/flower_photos.tgz"</span></span><br><span class="line">input_local_dir = <span class="string">"/dbfs/ml/tmp/flower/"</span></span><br><span class="line">output_file_path = <span class="string">"/tmp/predictions"</span></span><br><span class="line">bc_model_state = sc.broadcast(models.resnet50(pretrained=<span class="literal">True</span>).state_dict())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_model_for_eval</span><span class="params">()</span>:</span></span><br><span class="line">  <span class="string">"""Gets the broadcasted model."""</span></span><br><span class="line">  model = models.resnet50(pretrained=<span class="literal">True</span>)</span><br><span class="line">  model.load_state_dict(bc_model_state.value)</span><br><span class="line">  model.eval()</span><br><span class="line">  <span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">maybe_download_and_extract</span><span class="params">(url, download_dir)</span>:</span></span><br><span class="line">    filename = url.split(<span class="string">'/'</span>)[<span class="number">-1</span>]</span><br><span class="line">    file_path = os.path.join(download_dir, filename)</span><br><span class="line">    print(file_path)</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(file_path):</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(download_dir):</span><br><span class="line">            os.makedirs(download_dir)</span><br><span class="line">            </span><br><span class="line">        file_path, _ = urlretrieve(url=url, filename=file_path)</span><br><span class="line">        print()</span><br><span class="line">        print(<span class="string">"Download finished. Extracting files."</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> file_path.endswith(<span class="string">".zip"</span>):</span><br><span class="line">            <span class="comment"># Unpack the zip-file.</span></span><br><span class="line">            zipfile.ZipFile(file=file_path, mode=<span class="string">"r"</span>).extractall(download_dir)</span><br><span class="line">        <span class="keyword">elif</span> file_path.endswith((<span class="string">".tar.gz"</span>, <span class="string">".tgz"</span>)):</span><br><span class="line">            <span class="comment"># Unpack the tar-ball.</span></span><br><span class="line">            tarfile.open(name=file_path, mode=<span class="string">"r:gz"</span>).extractall(download_dir)</span><br><span class="line"></span><br><span class="line">        print(<span class="string">"Done."</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        print(<span class="string">"Data has apparently already been downloaded and unpacked."</span>)</span><br><span class="line">maybe_download_and_extract(url=URL, download_dir=input_local_dir)</span><br><span class="line">local_dir = input_local_dir + <span class="string">'flower_photos/'</span></span><br><span class="line">files = [os.path.join(dp, f) <span class="keyword">for</span> dp, dn, filenames <span class="keyword">in</span> os.walk(local_dir) <span class="keyword">for</span> f <span class="keyword">in</span> filenames <span class="keyword">if</span> os.path.splitext(f)[<span class="number">1</span>] == <span class="string">'.jpg'</span>]</span><br><span class="line">len(files)</span><br><span class="line"></span><br><span class="line">files_df = spark.createDataFrame(</span><br><span class="line">  map(<span class="keyword">lambda</span> path: (path,), files), [<span class="string">"path"</span>]</span><br><span class="line">).repartition(<span class="number">10</span>)  <span class="comment"># number of partitions should be a small multiple of total number of nodes</span></span><br><span class="line">display(files_df.limit(<span class="number">10</span>))</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ImageDataset</span><span class="params">(Dataset)</span>:</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, paths, transform=None)</span>:</span></span><br><span class="line">    self.paths = paths</span><br><span class="line">    self.transform = transform</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> len(self.paths)</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, index)</span>:</span></span><br><span class="line">    image = default_loader(self.paths[index])</span><br><span class="line">    <span class="keyword">if</span> self.transform <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">      image = self.transform(image)</span><br><span class="line">    <span class="keyword">return</span> image</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict_batch</span><span class="params">(paths)</span>:</span></span><br><span class="line">  transform = transforms.Compose([</span><br><span class="line">    transforms.Resize(<span class="number">224</span>),</span><br><span class="line">    transforms.CenterCrop(<span class="number">224</span>),</span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">    transforms.Normalize(mean=[<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>],</span><br><span class="line">                       std=[<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])</span><br><span class="line">  ])</span><br><span class="line">  images = ImageDataset(paths, transform=transform)</span><br><span class="line">  loader = torch.utils.data.DataLoader(images, batch_size=<span class="number">500</span>, num_workers=<span class="number">8</span>)</span><br><span class="line">  model = get_model_for_eval()</span><br><span class="line">  model.to(device)</span><br><span class="line">  all_predictions = []</span><br><span class="line">  <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> loader:</span><br><span class="line">      predictions = list(model(batch.to(device)).cpu().numpy())</span><br><span class="line">      <span class="keyword">for</span> prediction <span class="keyword">in</span> predictions:</span><br><span class="line">        all_predictions.append(prediction)</span><br><span class="line">  <span class="keyword">return</span> pd.Series(all_predictions)</span><br><span class="line"></span><br><span class="line">predictions = predict_batch(pd.Series(files[:<span class="number">200</span>]))  <span class="comment">#本地测试</span></span><br><span class="line"></span><br><span class="line">predict_udf = pandas_udf(ArrayType(FloatType()), PandasUDFType.SCALAR)(predict_batch)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">predictions_df = files_df.select(col(<span class="string">'path'</span>), predict_udf(col(<span class="string">'path'</span>)).alias(<span class="string">"prediction"</span>))</span><br><span class="line">predictions_df.write.mode(<span class="string">"overwrite"</span>).parquet(output_file_path)</span><br><span class="line">result_df = spark.read.load(output_file_path)</span><br><span class="line">display(result_df)</span><br></pre></td></tr></tbody></table></figure>

<h5 id="4-2-Submarine"><a href="#4-2-Submarine" class="headerlink" title="4.2. Submarine"></a>4.2. Submarine</h5><blockquote>
<p>Submarine计算引擎通过命令行向YARN提交定制的深度学习应用程序（如 Tensorflow，Pytorch 等）。这些应用程序与YARN上的其他应用程序并行运行，例如Apache Spark，Hadoop Map / Reduce 等。</p>
<ul>
<li>Submarine-Zeppelin integration：允许数据科学家在 Zeppelin 的notebook中编写算法和调参进行可视化输出，并直接从notebook提交和管理机器学习的训练工作。</li>
<li>Submarine-Azkaban integration：允许数据科学家从Zeppelin 的notebook中直接向Azkaban提交一组具有依赖关系的任务，组成工作流进行周期性调度。</li>
<li>Submarine-installer：在你的服务器环境中安装Submarine和 YARN，轻松解决Docker、Parallel network和nvidia驱动的安装部署难题，以便你更轻松地尝试强大的工具集。</li>
<li>Apache Hadoop 3.1 的 YARN 可以完全无误的支持 Hadoop 2.7 + 以上的 HDFS 系统。</li>
</ul>
</blockquote>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210227083551241.png" alt=""></p>
<h5 id="4-3-SparkDL"><a href="#4-3-SparkDL" class="headerlink" title="4.3.  SparkDL"></a>4.3.  SparkDL</h5><ul>
<li><a href="https://github.com/databricks/spark-deep-learning" target="_blank" rel="noopener">https://github.com/databricks/spark-deep-learning</a>  sparkdl 支持spark版本如下：</li>
</ul>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210226212016988.png" alt=""></p>
<p>4.4. <strong><a href="https://github.com/tensorflow/ecosystem/tree/master/spark/spark-tensorflow-distributor." target="_blank" rel="noopener">Distributed TensorFlow on Apache Spark 3.0</a></strong></p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> spark_tensorflow_distributor <span class="keyword">import</span> MirroredStrategyRunner</span><br><span class="line"></span><br><span class="line"><span class="comment"># Taken from https://github.com/tensorflow/ecosystem/tree/master/spark/spark-tensorflow-distributor#examples</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">    <span class="keyword">import</span> uuid</span><br><span class="line"></span><br><span class="line">    BUFFER_SIZE = <span class="number">10000</span></span><br><span class="line">    BATCH_SIZE = <span class="number">64</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">make_datasets</span><span class="params">()</span>:</span></span><br><span class="line">        (mnist_images, mnist_labels), _ = \</span><br><span class="line">            tf.keras.datasets.mnist.load_data(path=str(uuid.uuid4())+<span class="string">'mnist.npz'</span>)</span><br><span class="line"></span><br><span class="line">        dataset = tf.data.Dataset.from_tensor_slices((</span><br><span class="line">            tf.cast(mnist_images[..., tf.newaxis] / <span class="number">255.0</span>, tf.float32),</span><br><span class="line">            tf.cast(mnist_labels, tf.int64))</span><br><span class="line">        )</span><br><span class="line">        dataset = dataset.repeat().shuffle(BUFFER_SIZE).batch(BATCH_SIZE)</span><br><span class="line">        <span class="keyword">return</span> dataset</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">build_and_compile_cnn_model</span><span class="params">()</span>:</span></span><br><span class="line">        model = tf.keras.Sequential([</span><br><span class="line">            tf.keras.layers.Conv2D(<span class="number">32</span>, <span class="number">3</span>, activation=<span class="string">'relu'</span>, input_shape=(<span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>)),</span><br><span class="line">            tf.keras.layers.MaxPooling2D(),</span><br><span class="line">            tf.keras.layers.Flatten(),</span><br><span class="line">            tf.keras.layers.Dense(<span class="number">64</span>, activation=<span class="string">'relu'</span>),</span><br><span class="line">            tf.keras.layers.Dense(<span class="number">10</span>, activation=<span class="string">'softmax'</span>),</span><br><span class="line">        ])</span><br><span class="line">        model.compile(</span><br><span class="line">            loss=tf.keras.losses.sparse_categorical_crossentropy,</span><br><span class="line">            optimizer=tf.keras.optimizers.SGD(learning_rate=<span class="number">0.001</span>),</span><br><span class="line">            metrics=[<span class="string">'accuracy'</span>],</span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line">    train_datasets = make_datasets()</span><br><span class="line">    options = tf.data.Options()</span><br><span class="line">    options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.DATA</span><br><span class="line">    train_datasets = train_datasets.with_options(options)</span><br><span class="line">    multi_worker_model = build_and_compile_cnn_model()</span><br><span class="line">    multi_worker_model.fit(x=train_datasets, epochs=<span class="number">3</span>, steps_per_epoch=<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">MirroredStrategyRunner(num_slots=<span class="number">8</span>).run(train)</span><br></pre></td></tr></tbody></table></figure>

<h5 id="4-4"><a href="#4-4" class="headerlink" title="4.4."></a>4.4.</h5><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> absolute_import</span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> keras.datasets <span class="keyword">import</span> mnist</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</span><br><span class="line"><span class="keyword">from</span> keras.layers.core <span class="keyword">import</span> Dense, Dropout, Activation, Flatten</span><br><span class="line"><span class="keyword">from</span> keras.optimizers <span class="keyword">import</span> SGD, Adam, RMSprop</span><br><span class="line"><span class="keyword">from</span> keras.layers.convolutional <span class="keyword">import</span> Convolution2D, MaxPooling2D</span><br><span class="line"><span class="keyword">from</span> keras.utils <span class="keyword">import</span> np_utils</span><br><span class="line"><span class="keyword">from</span> elephas.spark_model <span class="keyword">import</span> SparkModel</span><br><span class="line"><span class="keyword">from</span> elephas.utils.rdd_utils <span class="keyword">import</span> to_simple_rdd</span><br><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkContext, SparkConf</span><br><span class="line"><span class="keyword">import</span> gzip</span><br><span class="line"><span class="keyword">import</span> cPickle</span><br><span class="line">APP_NAME = <span class="string">"mnist"</span></span><br><span class="line">MASTER_IP = <span class="string">'local[24]'</span></span><br><span class="line"><span class="comment"># Define basic parameters</span></span><br><span class="line">batch_size = <span class="number">128</span></span><br><span class="line">nb_classes = <span class="number">10</span></span><br><span class="line">nb_epoch = <span class="number">5</span></span><br><span class="line"><span class="comment"># input image dimensions</span></span><br><span class="line">img_rows, img_cols = <span class="number">28</span>, <span class="number">28</span></span><br><span class="line"><span class="comment"># number of convolutional filters to use</span></span><br><span class="line">nb_filters = <span class="number">32</span></span><br><span class="line"><span class="comment"># size of pooling area for max pooling</span></span><br><span class="line">nb_pool = <span class="number">2</span></span><br><span class="line"><span class="comment"># convolution kernel size</span></span><br><span class="line">nb_conv = <span class="number">3</span></span><br><span class="line"><span class="comment"># Load data</span></span><br><span class="line">f = gzip.open(<span class="string">"./mnist.pkl.gz"</span>, <span class="string">"rb"</span>)</span><br><span class="line">dd = cPickle.load(f)</span><br><span class="line">(X_train, y_train), (X_test, y_test) = dd</span><br><span class="line">X_train = X_train.reshape(X_train.shape[<span class="number">0</span>], <span class="number">1</span>, img_rows, img_cols)</span><br><span class="line">X_test = X_test.reshape(X_test.shape[<span class="number">0</span>], <span class="number">1</span>, img_rows, img_cols)</span><br><span class="line">X_train = X_train.astype(<span class="string">"float32"</span>)</span><br><span class="line">X_test = X_test.astype(<span class="string">"float32"</span>)</span><br><span class="line">X_train /= <span class="number">255</span></span><br><span class="line">X_test /= <span class="number">255</span></span><br><span class="line">print(X_train.shape[<span class="number">0</span>], <span class="string">'train samples'</span>)</span><br><span class="line">print(X_test.shape[<span class="number">0</span>], <span class="string">'test samples'</span>)</span><br><span class="line"><span class="comment"># Convert class vectors to binary class matrices</span></span><br><span class="line">Y_train = np_utils.to_categorical(y_train, nb_classes)</span><br><span class="line">Y_test = np_utils.to_categorical(y_test, nb_classes)</span><br><span class="line">model = Sequential()</span><br><span class="line">model.add(Convolution2D(nb_filters, nb_conv, nb_conv,</span><br><span class="line">                        border_mode=<span class="string">'full'</span>,</span><br><span class="line">                        input_shape=(<span class="number">1</span>, img_rows, img_cols)))</span><br><span class="line">model.add(Activation(<span class="string">'relu'</span>))</span><br><span class="line">model.add(Convolution2D(nb_filters, nb_conv, nb_conv))</span><br><span class="line">model.add(Activation(<span class="string">'relu'</span>))</span><br><span class="line">model.add(MaxPooling2D(pool_size=(nb_pool, nb_pool)))</span><br><span class="line">model.add(Dropout(<span class="number">0.25</span>))</span><br><span class="line">model.add(Flatten())</span><br><span class="line">model.add(Dense(<span class="number">128</span>))</span><br><span class="line">model.add(Activation(<span class="string">'relu'</span>))</span><br><span class="line">model.add(Dropout(<span class="number">0.5</span>))</span><br><span class="line">model.add(Dense(nb_classes))</span><br><span class="line">model.add(Activation(<span class="string">'softmax'</span>))</span><br><span class="line">model.compile(loss=<span class="string">'categorical_crossentropy'</span>, optimizer=<span class="string">'adadelta'</span>)</span><br><span class="line"><span class="comment">## spark</span></span><br><span class="line">conf = SparkConf().setAppName(APP_NAME).setMaster(MASTER_IP)</span><br><span class="line">sc = SparkContext(conf=conf)</span><br><span class="line"><span class="comment"># Build RDD from numpy features and labels</span></span><br><span class="line">rdd = to_simple_rdd(sc, X_train, Y_train)</span><br><span class="line"><span class="comment"># Initialize SparkModel from Keras model and Spark context</span></span><br><span class="line">spark_model = SparkModel(sc,model)</span><br><span class="line"><span class="comment"># Train Spark model</span></span><br><span class="line">spark_model.train(rdd, nb_epoch=nb_epoch, batch_size=batch_size, verbose=<span class="number">0</span>, validation_split=<span class="number">0.1</span>, num_workers=<span class="number">24</span>)</span><br><span class="line"><span class="comment"># Evaluate Spark model by evaluating the underlying model</span></span><br><span class="line">score = spark_model.get_network().evaluate(X_test, Y_test, show_accuracy=<span class="literal">True</span>, verbose=<span class="number">2</span>)</span><br><span class="line">print(<span class="string">'Test accuracy:'</span>, score[<span class="number">1</span>])</span><br></pre></td></tr></tbody></table></figure>



<h4 id="4-相关学习资源"><a href="#4-相关学习资源" class="headerlink" title="4. 相关学习资源"></a>4. 相关学习资源</h4><ul>
<li><a href="https://github.com/jupyter-incubator/sparkmagic" target="_blank" rel="noopener">https://github.com/jupyter-incubator/sparkmagic</a></li>
</ul>
<h5 id="4-1-Infrastructure-Projects"><a href="#4-1-Infrastructure-Projects" class="headerlink" title="4.1. Infrastructure Projects"></a>4.1. Infrastructure Projects</h5><ul>
<li><a href="https://github.com/spark-jobserver/spark-jobserver" target="_blank" rel="noopener">REST Job Server for Apache Spark</a> - REST interface for managing and submitting Spark jobs on the same cluster.</li>
<li><a href="http://mlbase.org/" target="_blank" rel="noopener">MLbase</a> - Machine Learning research project on top of Spark</li>
<li><a href="https://mesos.apache.org/" target="_blank" rel="noopener">Apache Mesos</a> - Cluster management system that supports running Spark</li>
<li><a href="https://www.alluxio.org/" target="_blank" rel="noopener">Alluxio</a> (née Tachyon) - Memory speed virtual distributed storage system that supports running Spark</li>
<li><a href="https://github.com/filodb/FiloDB" target="_blank" rel="noopener">FiloDB</a> - a Spark integrated analytical/columnar database, with in-memory option capable of sub-second concurrent queries</li>
<li><a href="http://zeppelin-project.org/" target="_blank" rel="noopener">Zeppelin</a> - Multi-purpose notebook which supports 20+ language backends, including Apache Spark</li>
<li><a href="https://github.com/EclairJS/eclairjs-node" target="_blank" rel="noopener">EclairJS</a> - enables Node.js developers to code against Spark, and data scientists to use Javascript in Jupyter notebooks.</li>
<li><a href="https://github.com/Hydrospheredata/mist" target="_blank" rel="noopener">Mist</a> - Serverless proxy for Spark cluster (spark middleware)</li>
<li><a href="https://github.com/GoogleCloudPlatform/spark-on-k8s-operator" target="_blank" rel="noopener">K8S Operator for Apache Spark</a> - Kubernetes operator for specifying and managing the lifecycle of Apache Spark applications on Kubernetes.</li>
<li><a href="https://developer.ibm.com/storage/products/ibm-spectrum-conductor-spark/" target="_blank" rel="noopener">IBM Spectrum Conductor</a> - Cluster management software that integrates with Spark and modern computing frameworks.</li>
<li><a href="https://delta.io/" target="_blank" rel="noopener">Delta Lake</a> - Storage layer that provides ACID transactions and scalable metadata handling for Apache Spark workloads.</li>
<li><a href="https://mlflow.org/" target="_blank" rel="noopener">MLflow</a> - Open source platform to manage the machine learning lifecycle, including deploying models from diverse machine learning libraries on Apache Spark.</li>
<li><a href="https://github.com/databricks/koalas" target="_blank" rel="noopener">Koalas</a> - Data frame API on Apache Spark that more closely follows Python’s pandas.</li>
<li><a href="https://datafu.apache.org/docs/spark/getting-started.html" target="_blank" rel="noopener">Apache DataFu</a> - A collection of utils and user-defined-functions for working with large scale data in Apache Spark, as well as making Scala-Python interoperability easier.</li>
</ul>
<h5 id="4-2-Applications-Using-Spark"><a href="#4-2-Applications-Using-Spark" class="headerlink" title="4.2. Applications Using Spark"></a>4.2. Applications Using Spark</h5><ul>
<li><a href="https://mahout.apache.org/" target="_blank" rel="noopener">Apache Mahout</a> - Previously on Hadoop MapReduce, Mahout has switched to using Spark as the backend</li>
<li><a href="https://wiki.apache.org/mrql/" target="_blank" rel="noopener">Apache MRQL</a> - A query processing and optimization system for large-scale, distributed data analysis, built on top of Apache Hadoop, Hama, and Spark</li>
<li><a href="http://blinkdb.org/" target="_blank" rel="noopener">BlinkDB</a> - a massively parallel, approximate query engine built on top of Shark and Spark</li>
<li><a href="https://github.com/adobe-research/spindle" target="_blank" rel="noopener">Spindle</a> - Spark/Parquet-based web analytics query engine</li>
<li><a href="https://github.com/thunderain-project/thunderain" target="_blank" rel="noopener">Thunderain</a> - a framework for combining stream processing with historical data, think Lambda architecture</li>
<li><a href="https://github.com/AyasdiOpenSource/df" target="_blank" rel="noopener">DF</a> from Ayasdi - a Pandas-like data frame implementation for Spark</li>
<li><a href="https://github.com/OryxProject/oryx" target="_blank" rel="noopener">Oryx</a> - Lambda architecture on Apache Spark, Apache Kafka for real-time large scale machine learning</li>
<li><a href="https://github.com/bigdatagenomics/adam" target="_blank" rel="noopener">ADAM</a> - A framework and CLI for loading, transforming, and analyzing genomic data using Apache Spark</li>
<li><a href="https://github.com/salesforce/TransmogrifAI" target="_blank" rel="noopener">TransmogrifAI</a> - AutoML library for building modular, reusable, strongly typed machine learning workflows on Spark with minimal hand tuning</li>
<li><a href="https://github.com/JohnSnowLabs/spark-nlp" target="_blank" rel="noopener">Natural Language Processing for Apache Spark</a> - A library to provide simple, performant, and accurate NLP annotations for machine learning pipelines</li>
<li><a href="http://rumbledb.org/" target="_blank" rel="noopener">Rumble for Apache Spark</a> - A JSONiq engine to query, with a functional language, large, nested, and heterogeneous JSON datasets that do not fit in dataframes.</li>
</ul>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script>
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        Author:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://liudongdong1.github.io" rel="external nofollow noreferrer">liudongdong1</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        Link:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://liudongdong1.github.io/2020/01/13/yu-yan-kuang-jia/sparkhadoop/sparkrelative/">https://liudongdong1.github.io/2020/01/13/yu-yan-kuang-jia/sparkhadoop/sparkrelative/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        Reprint policy:
                    </i>
                </span>
                <span class="reprint-info">
                    All articles in this blog are used except for special statements
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    reprint polocy. If reproduced, please indicate source
                    <a href="https://liudongdong1.github.io" target="_blank">liudongdong1</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>Copied successfully, please follow the reprint policy of this article</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">more</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/tags/spark/">
                                    <span class="chip bg-color">spark</span>
                                </a>
                            
                                <a href="/tags/stream/">
                                    <span class="chip bg-color">stream</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="qq,qzone,wechat,weibo,douban" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
                <style>
    #reward {
        margin: 40px 0;
        text-align: center;
    }

    #reward .reward-link {
        font-size: 1.4rem;
        line-height: 38px;
    }

    #reward .btn-floating:hover {
        box-shadow: 0 6px 12px rgba(0, 0, 0, 0.2), 0 5px 15px rgba(0, 0, 0, 0.2);
    }

    #rewardModal {
        width: 320px;
        height: 350px;
    }

    #rewardModal .reward-title {
        margin: 15px auto;
        padding-bottom: 5px;
    }

    #rewardModal .modal-content {
        padding: 10px;
    }

    #rewardModal .close {
        position: absolute;
        right: 15px;
        top: 15px;
        color: rgba(0, 0, 0, 0.5);
        font-size: 1.3rem;
        line-height: 20px;
        cursor: pointer;
    }

    #rewardModal .close:hover {
        color: #ef5350;
        transform: scale(1.3);
        -moz-transform:scale(1.3);
        -webkit-transform:scale(1.3);
        -o-transform:scale(1.3);
    }

    #rewardModal .reward-tabs {
        margin: 0 auto;
        width: 210px;
    }

    .reward-tabs .tabs {
        height: 38px;
        margin: 10px auto;
        padding-left: 0;
    }

    .reward-content ul {
        padding-left: 0 !important;
    }

    .reward-tabs .tabs .tab {
        height: 38px;
        line-height: 38px;
    }

    .reward-tabs .tab a {
        color: #fff;
        background-color: #ccc;
    }

    .reward-tabs .tab a:hover {
        background-color: #ccc;
        color: #fff;
    }

    .reward-tabs .wechat-tab .active {
        color: #fff !important;
        background-color: #22AB38 !important;
    }

    .reward-tabs .alipay-tab .active {
        color: #fff !important;
        background-color: #019FE8 !important;
    }

    .reward-tabs .reward-img {
        width: 210px;
        height: 210px;
    }
</style>

<div id="reward">
    <a href="#rewardModal" class="reward-link modal-trigger btn-floating btn-medium waves-effect waves-light red">赏</a>

    <!-- Modal Structure -->
    <div id="rewardModal" class="modal">
        <div class="modal-content">
            <a class="close modal-close"><i class="fas fa-times"></i></a>
            <h4 class="reward-title">你的赏识是我前进的动力</h4>
            <div class="reward-content">
                <div class="reward-tabs">
                    <ul class="tabs row">
                        <li class="tab col s6 alipay-tab waves-effect waves-light"><a href="#alipay">支付宝</a></li>
                        <li class="tab col s6 wechat-tab waves-effect waves-light"><a href="#wechat">微 信</a></li>
                    </ul>
                    <div id="alipay">
                        <img src="/medias/reward/alipay.jpg" class="reward-img" alt="支付宝打赏二维码">
                    </div>
                    <div id="wechat">
                        <img src="/medias/reward/wechat.png" class="reward-img" alt="微信打赏二维码">
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>

<script>
    $(function () {
        $('.tabs').tabs();
    });
</script>

            
        </div>
    </div>

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;Previous</div>
            <div class="card">
                <a href="/2020/01/18/aiot/laser/virtualkeys/">
                    <div class="card-image">
                        
                        <img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210119174241375.png" class="responsive-img" alt="VirtualKeys">
                        
                        <span class="card-title">VirtualKeys</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            
基于OpenCV，结合图像摄像头、980nm一字红外激光、980nm红外滤光片以及键盘投影激光组成，使用加装了980nm红外滤光片的图像摄像头检测由手指遮挡引起980nm一字红外激光漫反射生成的光点，通过检测和定位这个光点轮廓的中心位置，
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2020-01-18
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/AIOT/" class="post-category">
                                    AIOT
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/Raspberry/">
                        <span class="chip bg-color">Raspberry</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                Next&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/2020/01/11/yu-yan-kuang-jia/java/javatool/zheng-ze-biao-da-shi/">
                    <div class="card-image">
                        
                        <img src="https://cdn.stocksnap.io/img-thumbs/280h/watch-phone_AVXMHULWY9.jpg" class="responsive-img" alt="正则表达式">
                        
                        <span class="card-title">正则表达式</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            
From: https://www.pdai.tech/md/develop/regex/dev-regex-usage.html

1. 校验数字的表达式
数字：^[0-9]*$
n位的数字：^\d{n}$
至少n位的数字：^\d{n,
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2020-01-11
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/%E8%AF%AD%E8%A8%80%E6%A1%86%E6%9E%B6/" class="post-category">
                                    语言框架
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/java/">
                        <span class="chip bg-color">java</span>
                    </a>
                    
                    <a href="/tags/regex/">
                        <span class="chip bg-color">regex</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/libs/codeBlock/codeBlockFuction.js"></script>

<!-- 代码语言 -->

<script type="text/javascript" src="/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/libs/codeBlock/codeShrink.js"></script>


<!-- 代码块折行 -->

<style type="text/css">
code[class*="language-"], pre[class*="language-"] { white-space: pre !important; }
</style>


    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;TOC</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // modify the toc link href to support Chinese.
        let i = 0;
        let tocHeading = 'toc-heading-';
        $('#toc-content a').each(function () {
            $(this).attr('href', '#' + tocHeading + (++i));
        });

        // modify the heading title id to support Chinese.
        i = 0;
        $('#articleContent').children('h2, h3, h4').each(function () {
            $(this).attr('id', tocHeading + (++i));
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>

    

</main>




    <!-- <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="463294659"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/libs/aplayer/APlayer.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/meting@2/dist/Meting.min.js"></script>

    
    <div class="container row center-align" style="margin-bottom: 0px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            <span id="year">2019</span>
            <a href="https://liudongdong1.github.io" target="_blank">liudongdong</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            <br>
            
            &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                class="white-color">1206.4k</span>&nbsp;字
            
            
            
            
            
            
            <span id="busuanzi_container_site_pv">
                |&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;<span id="busuanzi_value_site_pv"
                    class="white-color"></span>&nbsp;次
            </span>
            
            
            <span id="busuanzi_container_site_uv">
                |&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;<span id="busuanzi_value_site_uv"
                    class="white-color"></span>&nbsp;人
            </span>
            
            <br>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/liudongdong1/" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:3463264078@qq.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>














    <a href="https://blog.csdn.net/liudongdong19/" class="tooltipped" target="_blank" data-tooltip="关注我的CSDN: https://blog.csdn.net/liudongdong19/" data-position="top" data-delay="50">
        <i class="fab fa-csdn">C</i>
    </a>





</div>
    </div>
</footer>

<div class="progress-bar"></div>
 -->

    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;Search</span>
            <input type="search" id="searchInput" name="s" placeholder="Please enter a search keyword"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script src="/js/search.js"></script>
<script type="text/javascript">
$(function () {
    searchFunc("/search.xml", 'searchInput', 'searchResult');
});
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script type="text/javascript" src="/js/CFS.Snow.min.js"></script>
    <!-- 点击爆灯效果 -->
    <canvas class="fireworks" style="position: fixed;left: 0;top: 0;z-index: 1; pointer-events: none;" ></canvas> 
    <script type="text/javascript" src="//cdn.bootcss.com/animejs/2.2.0/anime.min.js"></script> 
    <script type="text/javascript" src="/js/fireworks.js"></script>
    <!--动态线条背景-->
    <script type="text/javascript"
        color="122 103 238" opacity='0.7' zIndex="-2" count="200" src="//cdn.bootcss.com/canvas-nest.js/1.0.0/canvas-nest.min.js">
    </script>
    <!-- 天气 -->
    <!-- weather -->
    <!-- weather -->
    <script type="text/javascript">
         WIDGET = {FID: 'knAMQaFanP'}
    </script>
    <script type="text/javascript" src="https://apip.weatherdt.com/float/static/js/r.js?v=1111"></script>
    <script type="text/javascript" src="https://apip.weatherdt.com/float/static/js/r.js?v=1111"></script>
    <script src="/libs/materialize/materialize.min.js"></script>
    <script src="/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/libs/aos/aos.js"></script>
    <script src="/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/js/matery.js"></script>

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    
    <script async src="/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    

    
    
    <script type="text/javascript" size="150" alpha='0.6'
        zIndex="-1" src="/libs/background/ribbon-refresh.min.js" async="async"></script>
    

    

    
    <script src="/libs/instantpage/instantpage.js" type="module"></script>
    
    <!-- {% include '_custom/custom.swig' %} -->

</body>

</html>
