<!DOCTYPE HTML>
<html lang="en">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Hand Analyse Record, AIOT，Space&amp;Temporal Sequence Analysis，SpringBoot，liudongdong1 .etc">
    <meta name="description" content="">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Hand Analyse Record | DaybyDay</title>
    <link rel="icon" type="image/png" href="/favicon.png">

    <link rel="stylesheet" type="text/css" href="/libs/awesome/css/all.css">
    <link rel="stylesheet" type="text/css" href="/libs/materialize/materialize.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/aos/aos.css">
    <link rel="stylesheet" type="text/css" href="/libs/animate/animate.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/lightGallery/css/lightgallery.min.css">
    <link rel="stylesheet" type="text/css" href="/css/matery.css">
    <link rel="stylesheet" type="text/css" href="/css/my.css">

    <script src="/libs/jquery/jquery.min.js"></script>

<meta name="generator" content="Hexo 4.2.0"><link rel="alternate" href="/atom.xml" title="DaybyDay" type="application/atom+xml">
<link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css"></head>


<body>
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/" class="waves-effect waves-light">
                    
                    <img src="/medias/logo.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">DaybyDay</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>Index</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>Tags</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>Categories</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>Archives</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/" class="waves-effect waves-light">

      
      <i class="fas fa-user-circle" style="zoom: 0.6;"></i>
      
      <span>About</span>
      <i class="fas fa-chevron-down" aria-hidden="true" style="zoom: 0.6;"></i>
    </a>
    <ul class="sub-nav menus_item_child ">
      
      <li>
        <a href="/about">
          
          <i class="fas fa-user-circle" style="margin-top: -20px; zoom: 0.6;"></i>
          
          <span>about</span>
        </a>
      </li>
      
      <li>
        <a href="/resume">
          
          <i class="fa fa-user-secret" style="margin-top: -20px; zoom: 0.6;"></i>
          
          <span>resume</span>
        </a>
      </li>
      
    </ul>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/friends" class="waves-effect waves-light">
      
      <i class="fas fa-address-book" style="zoom: 0.6;"></i>
      
      <span>Friends</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="Search" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/medias/logo.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">DaybyDay</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			Index
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			Tags
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			Categories
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			Archives
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="javascript:;">
			
				<i class="fa-fw fas fa-user-circle"></i>
			
			About
			<span class="m-icon"><i class="fas fa-chevron-right"></i></span>
		</a>
            <ul  style="background:  ;" >
              
                <li>

                  <a href="/about " style="margin-left:75px">
				  
				   <i class="fa fas fa-user-circle" style="position: absolute;left:50px" ></i>
			      
		          <span>about</span>
                  </a>
                </li>
              
                <li>

                  <a href="/resume " style="margin-left:75px">
				  
				   <i class="fa fa fa-user-secret" style="position: absolute;left:50px" ></i>
			      
		          <span>resume</span>
                  </a>
                </li>
              
            </ul>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/friends" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-address-book"></i>
			
			Friends
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/liudongdong1" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/liudongdong1" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://gitee.com/github-25970295/blogImage/raw/master/img/dataglove.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Hand Analyse Record</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <link rel="stylesheet" href="/libs/tocbot/tocbot.css">
<style>
    #articleContent h1::before,
    #articleContent h2::before,
    #articleContent h3::before,
    #articleContent h4::before,
    #articleContent h5::before,
    #articleContent h6::before {
        display: block;
        content: " ";
        height: 100px;
        margin-top: -100px;
        visibility: hidden;
    }

    #articleContent :focus {
        outline: none;
    }

    .toc-fixed {
        position: fixed;
        top: 64px;
    }

    .toc-widget {
        width: 345px;
        padding-left: 20px;
    }

    .toc-widget .toc-title {
        margin: 35px 0 15px 0;
        padding-left: 17px;
        font-size: 1.5rem;
        font-weight: bold;
        line-height: 1.5rem;
    }

    .toc-widget ol {
        padding: 0;
        list-style: none;
    }

    #toc-content {
        height: calc(100vh - 250px);
        overflow: auto;
    }

    #toc-content ol {
        padding-left: 10px;
    }

    #toc-content ol li {
        padding-left: 10px;
    }

    #toc-content .toc-link:hover {
        color: #42b983;
        font-weight: 700;
        text-decoration: underline;
    }

    #toc-content .toc-link::before {
        background-color: transparent;
        max-height: 25px;

        position: absolute;
        right: 23.5vw;
        display: block;
    }

    .toc-fixed .toc-link::before{
        position: fixed!important;/*当toc的位置改为fixed时，.toc-link::before也要改为fixed*/
    }

    #toc-content .is-active-link {
        color: #42b983;
    }

    #toc-content .is-active-link::before {
        background-color: #42b983;
    }

    #floating-toc-btn {
        position: fixed;
        right: 15px;
        bottom: 76px;
        padding-top: 15px;
        margin-bottom: 0;
        z-index: 998;
    }

    #floating-toc-btn .btn-floating {
        width: 48px;
        height: 48px;
    }

    #floating-toc-btn .btn-floating i {
        line-height: 48px;
        font-size: 1.4rem;
    }
</style>
<div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/tags/HandPose/">
                                <span class="chip bg-color">HandPose</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/categories/AIOT/" class="post-category">
                                AIOT
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>Publish Date:&nbsp;&nbsp;
                    2020-06-20
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>Update Date:&nbsp;&nbsp;
                    2021-06-21
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>Word Count:&nbsp;&nbsp;
                    4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>Read Times:&nbsp;&nbsp;
                    24 Min
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>Read Count:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">
        <div class="card-content article-card-content">
            <div id="articleContent">
                <p><strong>level</strong>:  CVPR  CCF_A<br><strong>author</strong>: Tomas Simon   Carnegie Mellon University<br><strong>date</strong>: 2017<br><strong>keyword</strong>:</p>
<ul>
<li>hand pose </li>
</ul>
<hr>
<h2 id="Paper-OpenPose-HandKeypoint"><a href="#Paper-OpenPose-HandKeypoint" class="headerlink" title="Paper: OpenPose HandKeypoint"></a>Paper: OpenPose HandKeypoint</h2><div align="center">
<br>
<b>Hand Keypoint Detection in Single Images using Multiview Bootstrapping</b>
</div>

<h4 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h4><ol>
<li>present an approach that uses a multi-camera system to train fine-grained detectors for keypoints.</li>
</ol>
<h4 id="Research-Objective"><a href="#Research-Objective" class="headerlink" title="Research Objective"></a>Research Objective</h4><ul>
<li><strong>Application Area</strong>: hand based HCI and robotics</li>
<li><strong>Purpose</strong>:  to extract hand point coordinate from single RGB images.</li>
</ul>
<h4 id="Proble-Statement"><a href="#Proble-Statement" class="headerlink" title="Proble Statement"></a>Proble Statement</h4><ul>
<li>self-occlusion due to articulation, view-point, grasped object.<img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200313092350789.png" alt=""></li>
</ul>
<p>previous work:</p>
<ul>
<li>many approaches to image-based face and body keypoint localization exist, there are no markerless hand keypoint detectors that work on RGB images in the wild.</li>
</ul>
<h4 id="Methods"><a href="#Methods" class="headerlink" title="Methods"></a>Methods</h4><ul>
<li><strong>Problem Formulation</strong>:</li>
</ul>
<p>input:   a crop image patch $I\epsilon R^{w<em>h</em>3}$</p>
<p>output:  P keypoint location, $X_p\epsilon R^2$,with associated confidence $C_p$.<br>$$<br>Keypoint;detector:  ;;;;d(I)-&gt;[ (X_p,c_p) ;for ; p \epsilon[1….P]]<br>$$</p>
<ul>
<li><strong>system overview</strong>:</li>
</ul>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200307172658147.png" alt=""></p>
<p><strong>【Multiview Bootstrapped Training】</strong></p>
<ul>
<li>$Initial;trainingset: ;;;;T_0:=[ (I_f,y_p^f) ;for;f\epsilon[1…N_0]]$, $f$ denote the particular image frame,set$[y_p^f\epsilon R^2]$ include all labeled keypoints for image $I^f$ .</li>
</ul>
<p>Multiview Bootrstrap:</p>
<ul>
<li>Inputs:<ul>
<li>calibrated cameras configuration</li>
<li>unlabled images: $[ I_v^f ; for ; v\epsilon views,; f\epsilon frames]$</li>
<li>keypoint detector: $d_0(I)-&gt;[(x_p,c_p);for;p\epsilon points]$</li>
<li>labeled training data: $T_0$</li>
</ul>
</li>
<li>Output: improved detector $d_K(.)$ and training set $T_k$</li>
<li>for iteration $i$ in 0  to K:<ol>
<li><font color="red">Triangulate keypoint from weak detections</font><ul>
<li>for every frame $f$:<ul>
<li>run detector $d_i(I_v^f)$ on all views $v$ , $D&lt;-{d_i(I_v^f) ; for ; v\epsilon [1…V] }$                 (1)</li>
<li>robustly triangulate keypoints,   $X_p^f=argmin_X \sum_{v\epsilon I_p^f}{||P_v(X)-x_p^v||_2^2}$      (2)</li>
</ul>
</li>
</ul>
</li>
<li><font color="red">score and sort triangulated frames</font>  ,         $score({X_p^f})=\sum_{p\epsilon [1…P]}\sum_{v\epsilon I_p^f}C_p^v$           (3)</li>
<li><font color="red">retrain with N-best reprojections</font>. $d_{i+1}&lt;-train(T_0;U;T_{i+1})$       (4)</li>
</ol>
</li>
</ul>
<p><strong>supplement for the mathmatic:</strong></p>
<ul>
<li><p>for (1):for one frame,  for each keypoint p, we have V detections $(x_p^v,c_p^v)$ , robustly triangulate each point p into a 3D location, use RANSAC on point D with confidence above a detection threshold $\lambda$.</p>
</li>
<li><p>for (2): $I_p^f$ is the inlier set, $X_p^f \epsilon R^3$   is the 3D triangulated keypoint p in frame f,  $P_v(X) \epsilon R^2$   denotes projection of 3D point $X$ into view $v$.  triangulate all landmarks of each finger(4 points ) at a time.</p>
</li>
<li><p>for(3): pick the best frame for every window of $W$ frames. Sort the frame in descending order according to their score, to obtain an ordered sequence of frames,$[s_1,s_2,…s_F^‘]$, $F^‘$ is the number of subsampled frames, $s_i$ is the ordered frame index. </p>
<ul>
<li>while verigy the good labled frame, using some strategies to automatically removing bad frame:<ul>
<li>average number of inliews</li>
<li>average detection detection confidence</li>
<li>difference of per-point velocity with medium velocity between two video frames</li>
<li>anthropomorphic limits on joint lengths</li>
<li>complete occlusion as determined by camera ray intersection with body joints</li>
</ul>
</li>
</ul>
</li>
<li><p>for(4):           $T_{i+1}={(I_v^{s_n},{P_v(X_p^{s_n}):;v\epsilon [1…V],; p\epsilon [1…P]}); for; n\epsilon[1…N]}$</p>
</li>
</ul>
<p><strong>【Detection Architecture】</strong></p>
<ul>
<li><strong>Hand Bounding Box Detection:</strong> directly use the body pose estimation models from [29], and [4] and use wrist and elbow position to approximate the hand location, assuming the hand extends 0.15 times the length of the forearm(前臂) in the same direction.</li>
<li>using architecture of CPMs with some modification.  <font color="red">CPMs predict a confidence map for each keypoint, representing the keypoint’s location as a Gaussian centered at the true position</font></li>
<li>using pre-trained VGG-19 network</li>
</ul>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200307174424118.png" alt=""></p>
<h4 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a>Evaluation</h4><ul>
<li><strong>Environment</strong>:   <ul>
<li>Dataset:  <ul>
<li>the MPII human pose dataset[2] <font color="red">reflect every-day human activities</font> </li>
<li>Images from the New Zealand Sign Language Exercised os the Victoria University of Wellington <font color="red">contains a variety of hand poses found in conversation</font></li>
</ul>
</li>
</ul>
</li>
</ul>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200307171832277.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200307175621323.png" alt=""></p>
<h4 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h4><ul>
<li>the first real-time hand keypoint detector showing practical applicability to in-the-wild RGB videos</li>
<li>the first markerless 3D hand motion capture system capable of reconstructing challenging hand-object interactions and musical performances without manual intervention</li>
<li>using multi-view bootstrapping, improving both the quality and quantity of the annotations</li>
</ul>
<h4 id="Notes"><a href="#Notes" class="headerlink" title="Notes"></a>Notes</h4><ul>
<li><p><strong>Bootstrap步骤：</strong></p>
<ul>
<li><p>在原有的样本中通过重抽样抽取一定数量（比如100）的新样本。</p>
</li>
<li><p>基于产生的新样本，计算我们需要估计的统计量$\alpha_i$。</p>
</li>
<li><p>重复上述步骤n次（一般是n&gt;1000次）。计算被估计量的均值和方差。</p>
</li>
<li><p>$$<br>\vec{\alpha}=Mean(\alpha_i…)<br>$$</p>
</li>
<li><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200607110758736.png" alt=""></p>
</li>
</ul>
</li>
<li><p><strong><a href="http://www.cse.yorku.ca/~kosta/CompVis_Notes/ransac.pdf" target="_blank" rel="noopener">RANSAC:</a></strong> robust estimation techniques such as M-estimators and least-median squares that have been adopted by the computer vision community from the statistics literature, RANSAC was developed from within the computer vision community</p>
</li>
</ul>
<p>  <img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200307183024378.png" alt=""></p>
<p>  <img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200307183041127.png" alt=""></p>
<p><strong>level</strong>: CVPR<br><strong>author</strong>: Kuo Du1<br><strong>date</strong>: 2019<br><strong>keyword</strong>:</p>
<ul>
<li>hand skeleton</li>
</ul>
<hr>
<h2 id="Paper-CrossInfoNet"><a href="#Paper-CrossInfoNet" class="headerlink" title="Paper: CrossInfoNet"></a>Paper: CrossInfoNet</h2><div align="center">
<br>
<b>CrossInfoNet: Multi-Task Information Sharing Based Hand Pose Estimation
</b>
</div>



<h4 id="Summary-1"><a href="#Summary-1" class="headerlink" title="Summary"></a>Summary</h4><ol>
<li>proposed CrossInfoNet decomposes hand pose estimation task into palm pose estimation sub-task and finger pose estimation sub-task, and adopts two-branch cross-connection structure to share the beneficial complementary information between the sub-tasks.</li>
<li>propose a heat-map guided feature extraction structure to get better feature maps, and train the complete network end-to-end.</li>
</ol>
<h4 id="Proble-Statement-1"><a href="#Proble-Statement-1" class="headerlink" title="Proble Statement"></a>Proble Statement</h4><p>previous work:</p>
<ul>
<li>treating depth maps as 2D images and regressing 3D joint coordinates directly is a commonly used hand pose estimation pipeline.</li>
<li>designing effective networks receives the most attentions. Learning multiple tasks simultaneously will be helpful to enforce a model with better generalizing ability.</li>
<li>the output representations can be classified into the probability density map or the 3D coordinates for each joint.  <font color="red">heat-map based method outperforms direct coordinate regression method, and the final joint coordinates have usually to be inferred by maximum operation on the heat-maps</font></li>
</ul>
<h4 id="Methods-1"><a href="#Methods-1" class="headerlink" title="Methods"></a>Methods</h4><ul>
<li><strong>system overview</strong>:</li>
</ul>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200309194734899.png" alt=""></p>
<p><strong>【Heat-map guided feature extraction】</strong></p>
<ul>
<li>ResNet-50 [15] backbone network with four residual modules</li>
<li>apply the feature pyramid structure to merge different feature layers.</li>
<li>the heat maps are only used as the constraints to guide the feature extraction and will not be passed to the subsequent module.</li>
</ul>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200309194817022.png" alt=""></p>
<p><strong>【Baseline feature refinement architecture】</strong></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200309195036333.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200309195052036.png" alt=""></p>
<p><strong>【New Feature refinement architecture】</strong></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200309195142885.png" alt=""></p>
<p><strong>【Loss Functions Defines】</strong></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200309195243947.png" alt=""></p>
<h4 id="Evaluation-1"><a href="#Evaluation-1" class="headerlink" title="Evaluation"></a>Evaluation</h4><ul>
<li><strong>Environment</strong>:   <ul>
<li>Dataset:  ICVL datasets, NYU datasets, MSRA datasets, Hands 2017 Challenge Frame-based Dataset.</li>
</ul>
</li>
</ul>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200309195412451.png" alt=""></p>
<h4 id="Conclusion-1"><a href="#Conclusion-1" class="headerlink" title="Conclusion"></a>Conclusion</h4><ul>
<li>use hierarchical model to decompose the final task into palm joint regression sub-task and finger joint regression sub-task.</li>
<li>a heat-map guided feature extraction structure is proposed.</li>
</ul>
<h4 id="Notes-去加强了解"><a href="#Notes-去加强了解" class="headerlink" title="Notes 去加强了解"></a>Notes <font color="orange">去加强了解</font></h4><ul>
<li><a href="https://github.com/dumyy/handpose" target="_blank" rel="noopener">https://github.com/dumyy/handpose</a></li>
</ul>
<h2 id="Paper-Emotion-Identification"><a href="#Paper-Emotion-Identification" class="headerlink" title="Paper: Emotion Identification"></a>Paper: Emotion Identification</h2><div align="center">
<br>
<b>Hand Gestures Based Emotion Identification Using Flex Sensors</b>
</div>




<h4 id="Summary-2"><a href="#Summary-2" class="headerlink" title="Summary"></a>Summary</h4><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200314105007698.png" alt=""></p>
<h2 id="Paper-Gesture-To-Speech"><a href="#Paper-Gesture-To-Speech" class="headerlink" title="Paper: Gesture To Speech"></a>Paper: Gesture To Speech</h2><div align="center">
<br>
<b>Gesture To Speech Conversion using Flex sensors,MPY6050 and Python</b>
</div>




<h4 id="Summary-3"><a href="#Summary-3" class="headerlink" title="Summary"></a>Summary</h4><ul>
<li>Arduino Uno, Flex Sensors, MPU6050 an accelerometer gyroscope sensor which is used to detect the alignment of an object.</li>
<li>To recognise the ALS Sign Language <img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200314105349781.png" alt=""></li>
<li><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200314105446162.png" alt=""></li>
</ul>
<h2 id="Paper-Flex"><a href="#Paper-Flex" class="headerlink" title="Paper: Flex"></a>Paper: Flex</h2><div align="center">
<br>
<b>Flex: Hand Gesture Recognition using Muscle Flexing Sensors</b>
</div>
#### Summary

<ul>
<li><p>Flex Sensors from Spectra-Symbol for angle displacement measuremetns.</p>
</li>
<li><p>apply a linear response delay filter to the raw sensors output for noise reduction and signal smoothing.</p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200314110009115.png" alt=""></p>
</li>
</ul>
<h2 id="Paper-Survey-on-Hand-Pose-Estimation"><a href="#Paper-Survey-on-Hand-Pose-Estimation" class="headerlink" title="Paper: Survey on Hand Pose Estimation"></a>Paper: Survey on Hand Pose Estimation</h2><div align="center">
<br>
<b>A Survey on Hand Pose Estimation with Wearable Sensors and Computer-Vision- Based Methods
</b>
</div>
#### Summary

<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200314110451195.png" alt=""></p>
<ul>
<li><p>详细介绍了基于视觉基于传感器方法</p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200314110640684.png" alt=""></p>
</li>
</ul>
<h2 id="Paper-Flex-amp-Gyroscopes"><a href="#Paper-Flex-amp-Gyroscopes" class="headerlink" title="Paper: Flex&amp;Gyroscopes"></a>Paper: Flex&amp;Gyroscopes</h2><div align="center">
<br>
<b>Recognizing words in Thai Sign Language using ﬂex sensors and gyroscopes
</b>
</div>
#### Summary

<ul>
<li><p>some sensors</p>
<ul>
<li>contact sensors for detecting fingers touching each other</li>
<li>accelerometers for measuring the acceleration of the hand in different direction</li>
<li>gyro-scopes for measuring the hand orientation and angular movement</li>
<li>magnetoresistive sensors for measuring the magnetic field for deriving the hand orientation</li>
</ul>
</li>
<li><p>presents a Thai sign language recognition framework using  a glove-based device with flex sensors and gyro-scops.</p>
</li>
<li><p>the measurements from the sensors are processed using finite Legendre and Linear Discriminant Analysis, then classified using k-nearest neighbors. </p>
</li>
<li><p>Handware design:<img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200314111857761.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200314112748401.png" alt=""></p>
</li>
<li><p>the gyroscopes can return values in three different types of measurement</p>
<ul>
<li>the quaternions are the raw data returned from the sensor. This measurement yields a four-dimensional output.</li>
<li>Euler angles are data converted from the four quaternion values. The Euler angles consist of three values, matching x, y, and z axis.</li>
<li>YPR measures the angle but with respect to the direction of the ground. It has three elements like the Euler angles. However it also requires gravity values from the accelerometer in order to calibrate.<font color="red"> to calculate YPR, four quaternion elements and three gravity values are needed</font></li>
</ul>
</li>
<li><p>Date processing </p>
<ul>
<li><p>segment and normalize the data  ???how to segment data unclear??</p>
</li>
<li><p>the value from flex sensors differ greatly depend on person, by requiring a calibration phase which the user clenches and releases his hands at least 3 times to determine th e maximum and minimum values of each flex sensor, and quantize the data to 3 possible values(0,1,2)</p>
</li>
<li><p>这部分不理解：<img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200314140333583.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200314140348967.png" alt=""></p>
</li>
</ul>
</li>
</ul>
<h1 id="Human-Machine-Interaction"><a href="#Human-Machine-Interaction" class="headerlink" title="Human-Machine-Interaction"></a>Human-Machine-Interaction</h1><blockquote>
<p>Taheri, Omid, et al. “GRAB: A dataset of whole-body human grasping of objects.” <em>European Conference on Computer Vision</em>. Springer, Cham, 2020.</p>
</blockquote>
<hr>
<h1 id="Paper-GRAB"><a href="#Paper-GRAB" class="headerlink" title="Paper: GRAB"></a>Paper: GRAB</h1><div align="center">
<br>
<b>GRAB: A dataset of whole-body human grasping of objects
</b>
</div>



<h4 id="Summary-4"><a href="#Summary-4" class="headerlink" title="Summary"></a>Summary</h4><ol>
<li>collect a new dataset, GRAB of whole-body grasps, containing full 3D shape and pose sequences of 10 subjects interacting with 51 every day objects of varying shape and size.</li>
<li>using MoCap markers to fit the full 3D body shape and pose, including the articulated face and hands, as well as the 3D object pose.</li>
<li>adapt MoSh++ to solve for the body, face, and hands of SMPL-X to obtain detailed moving 3D meshes, and according to the meshes and tracked 3D objects, we compute plausible contact on the object and the human and provide an analysis of observed patterns.</li>
</ol>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210224170220204.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210224170341797.png" alt="Contact Annotation"></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210224170455822.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210224170556522.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210224170611285.png" alt=""></p>
<h4 id="Relative"><a href="#Relative" class="headerlink" title="Relative"></a>Relative</h4><ul>
<li>require complex 3D object shapes, detailed contact information, hand pose and shape, and the 3D body motion over time;</li>
<li>MoCap: <a href="https://mocap.reallusion.com/iClone-motion-live-mocap/" target="_blank" rel="noopener">https://mocap.reallusion.com/iClone-motion-live-mocap/</a></li>
</ul>
<hr>
<h2 id="Paper-A-Mobile-Robot-Hand-arm"><a href="#Paper-A-Mobile-Robot-Hand-arm" class="headerlink" title="Paper:  A Mobile Robot Hand-arm"></a>Paper:  A Mobile Robot Hand-arm</h2><div align="center">
<br>
<b> A Mobile Robot Hand-Arm Teleoperation System by Vision and IMU</b>
</div>

<h4 id="Summary-5"><a href="#Summary-5" class="headerlink" title="Summary"></a>Summary</h4><blockquote>
<ol>
<li>present a multi-modal mobile teleoperation system that consists of a novel vision-based hand pose regression network and IMU-based arm tracking methods.</li>
<li>observe the human hand through a  depth camera and generates joint angles and depth images of paired robot hand poses through an image-to-image translation process.</li>
<li>Transteleop takes the depth image of the human hand as input, then estimates the joint angles of the robot hand, and also generates the reconstructed image of the robot hand.</li>
<li>design a keypoint-based reconstruction loss to focus on the local reconstruction quality around the keypoints of the hand. </li>
</ol>
</blockquote>
<h4 id="Research-Objective-1"><a href="#Research-Objective-1" class="headerlink" title="Research Objective"></a>Research Objective</h4><ul>
<li><strong>Application Area</strong>: space, rescue, medical, surgery, imitation learning.<ul>
<li><strong>Purpose</strong>:  implement different manipulation tasks such as pick and place, cup insertion, object pushing, and dual-arm handover tasks</li>
</ul>
</li>
</ul>
<h4 id="Proble-Statement-2"><a href="#Proble-Statement-2" class="headerlink" title="Proble Statement"></a>Proble Statement</h4><ul>
<li>the robot hand and human hand occupy two different domains, how to compensate for kinematic differences between them plays an essential role in markerless vision-based teleoperation</li>
</ul>
<p>previous work:</p>
<ul>
<li><strong>Image-to-Image translation:</strong>  aims to map representation of a scene into another, used in collection of style transfer, object transfiguration, and imitation learning.</li>
</ul>
<h4 id="Methodsj"><a href="#Methodsj" class="headerlink" title="Methodsj"></a>Methodsj</h4><ul>
<li><strong>system overview</strong>:</li>
</ul>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200612092238443.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200612092928609.png" alt=""></p>
<p>【Question 1】how to discover the latent feature embedding the Zpose between the human hand and robot hand?</p>
<blockquote>
<p>using Encoder-decoder module</p>
</blockquote>
<p>【Question 2】how to get more accuracy of local features such as the position of fingertips instead of global features such as image style?</p>
<blockquote>
<p>design a keypoint-based reconstruction loss to capture the overall structure of the hand and concentrate on the pixels around the 15 keypoints of the hand.</p>
<p>using mean squared error(MSE) loss to  calculate the joint from $Z_R$ (robot feature)</p>
</blockquote>
<p>【Question 3】the poses of the human hand vary considerably in their global orientations?</p>
<blockquote>
<p>applied spatial transformation network(STN) provides spatial transformation capabilities of input images before the encoder module.</p>
</blockquote>
<p>【Question 4】the hand easily disappears from the field of view of the camera, and the camera position is uncertain ?</p>
<blockquote>
<p>using a cheap 3D-printed camera holder</p>
<p>using Perception Neuron device to control the arm of the robot.</p>
</blockquote>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200612095958385.png" alt=""></p>
<h4 id="Evaluation-2"><a href="#Evaluation-2" class="headerlink" title="Evaluation"></a>Evaluation</h4><ul>
<li><p><strong>Environment</strong>:   </p>
<ul>
<li>Dataset:  dataset of paired human-robot images, contains 400k pairs of simulated robot depth images and human hand depth images, the ground trush are 19 joint angles of the robot hand, record the 9 depth images of the robot hand from different viewpoints simultaneously corresponding to one human pose.</li>
</ul>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200612100328537.png" alt=""></p>
</li>
</ul>
<h4 id="Notes-去加强了解-1"><a href="#Notes-去加强了解-1" class="headerlink" title="Notes 去加强了解"></a>Notes <font color="orange">去加强了解</font></h4><ul>
<li><a href="https://Smilels.github.io/multimodal-translation-teleop" target="_blank" rel="noopener">https://Smilels.github.io/multimodal-translation-teleop</a></li>
<li>可能有什么问题，</li>
</ul>
<p><strong>level</strong>: PerDial’19<br><strong>author</strong>:<br><strong>date</strong>:  2019<br><strong>keyword</strong>:</p>
<ul>
<li>robot, ASL, </li>
</ul>
<hr>
<h2 id="Paper-Human-Robot"><a href="#Paper-Human-Robot" class="headerlink" title="Paper: Human-Robot"></a>Paper: Human-Robot</h2><div align="center">
<br>
<b>Human-Robot Interaction with Smart Shopping Trolley using Sign Language: Data Collection</b>
</div>
#### Summary

<ol>
<li>presents a concept of smart robotic trolley for supermarkets with multi-modal user interface, including sign language and acoustic speech recognition, and equipped with a touch screen.</li>
</ol>
<h4 id="Proble-Statement-3"><a href="#Proble-Statement-3" class="headerlink" title="Proble Statement"></a>Proble Statement</h4><ul>
<li>continuous or dynamic sign language recognition remains an unresolved challenge.</li>
<li>sensitivity to size and speed variations, poor performance under varying lighting conditions and complex background have limited the use of SLR in modern dialogue systems.</li>
</ul>
<p>previous work:</p>
<ul>
<li>the level of voiced speech and isolated/static hand gesture automatic recognition quality is quite high.</li>
<li>EffiBot[1] takes goods and automatically goes with them to the point of discharge, and follow the user when the corresponding mode is activated.</li>
<li>The Dash Robotic Shopping Cart[2] :<ul>
<li>a supermarket trolley that facilitates shopping and navigation in the store, the car is equipped with a touchscreen for entering a list of products of interest to the client.</li>
</ul>
</li>
<li>Gita by Piaggio[3]: a robotic trolley that follows the owner.</li>
<li><font color="red">none of the interfaces of the aforementioned robotic carts are multimodal.</font></li>
</ul>
<h4 id="Methods-2"><a href="#Methods-2" class="headerlink" title="Methods"></a>Methods</h4><ul>
<li><p><strong>system overview</strong>:</p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200407165820138.png" alt=""></p>
</li>
</ul>
<ol>
<li>speaker-independent system of automatic continuous Russian speech recognition</li>
<li>speaker-independent system of Russian sign language recognition with video processing using Kinect2.0 device</li>
<li>interactive graphical user interface with touchscreen</li>
<li>dialogue and data manager that access an application database, generates multi modal output and synchronizes input modalities fusion and output modalities fission</li>
<li>modules for audio-visual speech synthesis to be applied for a talking avatar</li>
</ol>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200407171743516.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200407171910822.png" alt=""></p>
<h4 id="Conclusion-2"><a href="#Conclusion-2" class="headerlink" title="Conclusion"></a>Conclusion</h4><ul>
<li>understanding voice commands</li>
<li>understanding Russian sign language commands</li>
<li>escort the user to a certain place in the store</li>
<li>speech synthesis, synthesis of answers in Russian sign language using a 3D avatar.</li>
</ul>
<h4 id="Notes-1"><a href="#Notes-1" class="headerlink" title="Notes"></a>Notes</h4><ul>
<li>介绍了一些手语 数据集</li>
<li>【30】，32 ，7 ， 34</li>
<li>机器人：<a href="https://www.effidence.com/effibot" target="_blank" rel="noopener">https://www.effidence.com/effibot</a><ul>
<li>​        <a href="https://mygita.com/#/how-does-gita-work" target="_blank" rel="noopener">https://mygita.com/#/how-does-gita-work</a></li>
</ul>
</li>
</ul>
<hr>
<p><strong>level</strong>:   IJCAI<br><strong>author</strong>: YangYi (MediaLab,Tencent)    FengNi(PekingUniversity)<br><strong>date</strong>: 2019<br><strong>keyword</strong>:</p>
<ul>
<li>hand gesture understand</li>
</ul>
<hr>
<h2 id="Paper-MKTB-amp-GRB"><a href="#Paper-MKTB-amp-GRB" class="headerlink" title="Paper: MKTB&amp;GRB"></a>Paper: MKTB&amp;GRB</h2><div align="center">
<br>
<b>High Performance Gesture Recognition via Effective and Efficient Temporal Modeling</b>
</div>

<h4 id="Research-Objective-2"><a href="#Research-Objective-2" class="headerlink" title="Research Objective"></a>Research Objective</h4><ul>
<li><strong>Purpose</strong>:  hand gesture recognition instead of human-human or human-object relationships.</li>
</ul>
<h4 id="Proble-Statement-4"><a href="#Proble-Statement-4" class="headerlink" title="Proble Statement"></a>Proble Statement</h4><ul>
<li>hand gesture recognition methods based on spatio-temporal features using 3DCNNs or ConvLSTM suffer from the inefficiency due to high computational complexity of their structure.</li>
</ul>
<p>previous work:</p>
<ul>
<li><p>Temporal Modeling for Action Recognition</p>
<ul>
<li>2DCNN by Narayana et al., 2018  </li>
<li>3DCNNs by  Miao et al., 2017</li>
<li>ConvLSTM by Zhang et al., 2017</li>
<li>TSN by Wang et al.2016 models long-range temporal structures with segment-based sampling and aggregation module.</li>
<li>C3D by Li et al.2016 designs a 3DCNN with small 3<em>3</em>3 convolution kernels to learn spatiotemporal features.</li>
<li>I3D by Carreira 2017 inflates convolutional filters and pooling kernels into 3D structures.</li>
<li>R(2+1)D by Wang et al. 2018 present non-local operations to capture long-reange dependencies</li>
</ul>
</li>
<li><p>Gesture Recognition:</p>
<ul>
<li>2DCNN by Narayana et al.2018  (学习下，多模态的,只了解多模态部分) fuses multi-channels(global/left-hand/right-hand/for RGB/depth/RGB-flow/Depth-flow modalities)</li>
<li>combines 3DCNN， bidirectional ConvLSTM and 2DCNN into a unified framework. ( 学习下如何整合到一个框架中)</li>
</ul>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200313162234686.png" alt=""></p>
</li>
</ul>
<h4 id="Methods-3"><a href="#Methods-3" class="headerlink" title="Methods"></a>Methods</h4><ul>
<li><p><strong>system overview</strong>:</p>
<ul>
<li>the model builds upon TSN, for TSN lacks of capability of modeling the temporal information from feature-space, the proposed MKTB and GRB are effective temporal modeling modules in feature-space.</li>
</ul>
</li>
</ul>
<p>【Multi-Kernel Temporal Block】</p>
<ul>
<li>unlike 3DCNNs, performing convolutional operation for both spatial and temporal dimension jointly, the MTKB decouples the joint spatial-temporal modeling process and focuses on learning the temporal information.</li>
<li>the design of multi-kernel works well on shaping the pyramidal and discriminative temporal features.</li>
</ul>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200313162921564.png" alt=""></p>
<ul>
<li>define feature maps from layer $l$ of 2DCNN(ResNet-50) as $F_s\epsilon R^{(B<em>T)</em>C<em>H</em>W}$</li>
<li>reduce the channels of $F_s$  via convolution layer with kernel size of 1*1, denoted as  $F_s^‘ \epsilon R^{ ( B * T) * C^‘ * H * W}$</li>
<li><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200313163941544.png" alt="image-20200313163941544"> using depthwise temporal conv [Chollet,2017]</li>
</ul>
<p>【Global Refinement Block】</p>
<ul>
<li><p>MKTB mainly focuses on the local neighborhoods,but the global temporal features across channels are not sufficiently attended.</p>
</li>
<li><p>GRB is designed to perform the weighted temporal aggregation, in which it allows distant temporal features to contribute to the filtered temporal features according to the cross-similarity.  <font color="red">遗留问题，如何计算similarity， MKTB 中如何sum</font></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200313164405173.png" alt=""></p>
</li>
</ul>
<h4 id="Evaluation-3"><a href="#Evaluation-3" class="headerlink" title="Evaluation"></a>Evaluation</h4><ul>
<li><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200313164608764.png" alt=""></li>
</ul>
<h4 id="Conclusion-3"><a href="#Conclusion-3" class="headerlink" title="Conclusion"></a>Conclusion</h4><ul>
<li>MKTB captures both short-term and long-term temporal information by using the multiple 1D depthwise convolutions.</li>
<li>MKTB and GRB maintain the same size between input and output, and can be easily deployed everywhere.</li>
</ul>
<h4 id="Notes-去加强了解-2"><a href="#Notes-去加强了解-2" class="headerlink" title="Notes 去加强了解"></a>Notes <font color="orange">去加强了解</font></h4><ul>
<li><input disabled="" type="checkbox"> <a href="https://github.com/nemonameless/Gesture-Recognition" target="_blank" rel="noopener">https://github.com/nemonameless/Gesture-Recognition</a></li>
</ul>
<p><strong>level</strong>: CCF_A  CVPR<br><strong>author</strong>: Liuhao Ge, Nanyang Technological University<br><strong>date</strong>:  2018<br><strong>keyword</strong>:</p>
<ul>
<li>hand pose</li>
</ul>
<hr>
<h2 id="Paper-Hand-PointNet"><a href="#Paper-Hand-PointNet" class="headerlink" title="Paper: Hand PointNet"></a>Paper: Hand PointNet</h2><div align="center">
<br>
<b>Hand PointNet: 3D Hand Pose Estimation using Point Sets
</b>
</div>



<h4 id="Summary-6"><a href="#Summary-6" class="headerlink" title="Summary"></a>Summary</h4><ol>
<li>propose HandPointNet model, that directly processes the 3D point cloud that models the visible surface of the hand for pose regression,Taking the normalized point cloud as the input, the regression network capture complex hand structures and accurately regress a low dimensional representation of the 3D hand pose.</li>
<li>design a fingertip refinement network that directly takes the neighboring points of the estimated fingertip location as input to refine the fingertip location.</li>
</ol>
<h4 id="Research-Objective-3"><a href="#Research-Objective-3" class="headerlink" title="Research Objective"></a>Research Objective</h4><ul>
<li><strong>Application Area</strong>: hand based interaction</li>
<li><strong>Purpose</strong>:   exact hand skeleton</li>
</ul>
<h4 id="Proble-Statement-5"><a href="#Proble-Statement-5" class="headerlink" title="Proble Statement"></a>Proble Statement</h4><ul>
<li>high dimensionality of 3D hand pose, large variations in hand orientations, high self-similarity of fingers and servere self-occlusion</li>
</ul>
<p>previous work:</p>
<ul>
<li>large hand pose datasets[38, 34, 33, 49, 48]</li>
<li><strong>CNN model:</strong><ul>
<li>the time and space complexities of the 3D CNN grow cubically with the resolution of the input 3D volume, using low resolution may lose useful details of the hand </li>
<li><font color="red">PointNet</font>: perform 3D object classification and segmentation on point sets directly </li>
<li>using multi-view CNNs-based method and 3D CNN-based method</li>
</ul>
</li>
<li><strong>Hand Pose Estimation:</strong> Discriminative approaches,  generative approaches, hybrid approaches<ul>
<li>feedback loop model[21]</li>
<li>spatial attention network[47]</li>
<li>deep generative models[41]</li>
</ul>
</li>
<li><strong>3D Deep Learning</strong>: <ul>
<li>Multi-view CNNs-based approaches[32, 24, 7, 2] project 3D points into 2D images and use 2D CNNs to process them.</li>
<li>3D CNNs based on octrees[27, 43] are proposed for efficient computation on high resolution volumes.</li>
</ul>
</li>
</ul>
<h4 id="Methods-4"><a href="#Methods-4" class="headerlink" title="Methods"></a>Methods</h4><ul>
<li><strong>Problem Formulation</strong>:</li>
</ul>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200306114552762.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200306121505920.png" alt=""></p>
<ul>
<li><strong>system overview</strong>:</li>
</ul>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200301140035643.png" alt=""></p>
<p>【Basic PointNet】 [23] directly takes a set of points as the input and is able to extract discriminative features of the point cloud.   <font color="red">cannot capture local structures of the point cloud in a hierarchical way</font>.</p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200301141808150.png" alt=""></p>
<p><strong>[Hierarchical PointNet]</strong>[25]: </p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200301141646517.png" alt=""></p>
<p>【<strong>OBB-based Point Cloud Normalization</strong>】to deal with large variation in global orientation of the hand. normalization the hand point cloud into a canonical coordinate system in which the global orientations of the transformed hand point clouds are as consistent as possible. <font color="red">normalization step ensures that our method is robust to variations in hand global orientations</font></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200301141855139.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200301142923208.png" alt=""></p>
<p>【<strong>Refine the Fingertip</strong>】</p>
<p>Based on the obervation: the fingertip location of straightened finger is usually easy to be fined, since K nearest neighboring points of the fingertip will not change a lot even if the estimated location deviates from the ground truth location to some extent when <font color="red">K is relatively large</font> </p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200301143230831.png" alt=""></p>
<h4 id="Conclusion-4"><a href="#Conclusion-4" class="headerlink" title="Conclusion"></a>Conclusion</h4><ul>
<li>estimate 3D hand joint locations directly from 3D point cloud base on the netword architecture of PointNet. better expoit the 3D spatial information in the depth image</li>
<li>robust to variations in hand global orientations, normalize the sampled 3D points in an oriented bounding box without applying any additional network to transform the hand piont cloud.     </li>
<li>refine the fingertip locations with a basic PointNet that takes the Neighboring points of the estimation fingertip location as input to regress the refined fingertip location.</li>
</ul>
<h4 id="Notes-去加强了解-3"><a href="#Notes-去加强了解-3" class="headerlink" title="Notes 去加强了解"></a>Notes <font color="orange">去加强了解</font></h4><ul>
<li>Learning hand articulations by hallucinating heat distribution. In ICCV, 2017.</li>
<li>PointNet: Deep learning on point sets for 3D classiﬁcation and segmentation. In CVPR, 2017.</li>
<li>PointNet++: Deep hierarchical feature learning on point sets in a metric space. In NIPS, 2017</li>
<li>Onlinedetectionandclassiﬁcationofdynamichand gestures with recurrent 3D convolutional neural network</li>
<li>Voxnet: A 3D convolutional neural network for real-time object recognition</li>
<li>Learning to estimate 3D hand pose from single RGB image</li>
<li>Bighand2.2m benchmark: Hand pose dataset and state of the art analysis. In CVPR, 2017. </li>
<li>Depth-based 3D hand pose estimation: From current achievements to future goals. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018. </li>
<li>DataSets:   NYU [38], MSRA [33] and ICVL [34]. </li>
<li>学习代码：  <a href="https://github.com/erikwijmans/Pointnet2_PyTorch.git" target="_blank" rel="noopener">https://github.com/erikwijmans/Pointnet2_PyTorch.git</a></li>
</ul>
<p><strong>level</strong>: CVPR, CCF_A<br><strong>author</strong>:Pavlo Molchanov, Xiaodong Yang (NVIDIA)<br><strong>date</strong>: 2016<br><strong>keyword</strong>:</p>
<ul>
<li>Hand Gesture,</li>
</ul>
<hr>
<h2 id="Paper-R3DCNN-Dynamic-Hand"><a href="#Paper-R3DCNN-Dynamic-Hand" class="headerlink" title="Paper: R3DCNN Dynamic Hand"></a>Paper: R3DCNN Dynamic Hand</h2><div align="center">
<br>
<b>Online Detection and Classiﬁcation of Dynamic Hand Gestures with Recurrent 3D Convolutional Neural Networks</b>
</div>

<h4 id="Proble-Statement-6"><a href="#Proble-Statement-6" class="headerlink" title="Proble Statement"></a>Proble Statement</h4><ul>
<li>Large diversity in how people perform gestures.</li>
<li>Work online to classify before competing a gesture.</li>
<li>Three overlapping phases: preparation, nucleus, and retraction.</li>
</ul>
<p>previous work:</p>
<ul>
<li>Hand-crafted spatio-temporal features.<ul>
<li>Shape, appearance, motion cues( image gradients, optical flow).</li>
</ul>
</li>
<li>Feature representations by DNN.<ul>
<li>uNeverova et al. combine color and depth data from hand regions and upper-body skeletons to recognize SL.</li>
</ul>
</li>
<li>Employ pre-segmented video sequences.</li>
<li>Treate detect and classify separately</li>
</ul>
<h4 id="Methods-5"><a href="#Methods-5" class="headerlink" title="Methods"></a>Methods</h4><ul>
<li><strong>Problem Formulation</strong>:</li>
</ul>
<p>Input: a video clip as volume $C_t$: $C_t\epsilon R^{k<em>l</em>c<em>m}$;  $m$: sequential frames; $C$: channels of size $k</em>l$ pixels.</p>
<p>$h_t\epsilon R_d$:  a hidden state vector;</p>
<p>$W_{in}\epsilon R^{d<em>q}$, $W_h\epsilon R^{d</em>d}$,$W_s\epsilon R^{w*d}$: weight matrices;</p>
<p>$b\epsilon R^w$: bias;</p>
<p>$S$: softmax functions, $R^w-&gt;R^w_{[0,1]},where [S(x)]<em>i=e^{x_i}/ \sum_ke^{xk}$<br>$$<br>F: R^{k<em>l</em>c*m}-&gt;R_q,where f_t=F(C_t)\<br>h_t=R(W</em>{in}f_t+W_hh_{t-1});\<br>s_t=S(W_sh_t+b);\<br>$$<br>For a video $V$ of $T$ clips, get the probabilities set $S$:<br>$$<br>S={s_0,s_1,…,s_{T-1}}\<br>S^{avg}=1/T\sum_{s\epsilon S}s\<br>predicted_label:y=argmax_i([s^{avg}]_i)<br>$$</p>
<ul>
<li><strong>system overview</strong>:</li>
</ul>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200711170646863.png" alt=""></p>
<p>【Pre-training the 3D-CNN】</p>
<ul>
<li>initialize the 3D-CNN with the C3D network [37] trained on the large-scale Sport1M [13] human action recognition dataset. </li>
<li>append a softmax prediction layer to the last fully-connected layer and ﬁne-tune by back-propagation with negative log-likelihood to predict gestures classes from individual clips $C_i$.</li>
</ul>
<p>【Cost Function】</p>
<ul>
<li>For Log-likelihood cost function:</li>
</ul>
<p>$$<br>L_v=-1/P \sum_{i=0}^{P-1}log(p(y_i|V_i))\<br>p(y_i|V_i)=[s^{avg}]_{y_i}<br>$$</p>
<p>【Learning Rule】</p>
<ul>
<li>To optimize the network parameters $W$ with respect to either of the loss functions we use stochastic gradient descent (SGD) with a momentum term$µ = 0.9$. We update each parameter of the network θ ∈ W at every back-propagation step i by:</li>
</ul>
<p>$$<br>\theta_i=\theta_{i-1}+v_i-yj\theta_{i-1}\<br>v_i=uv_{i-1}-jJ(&lt;\sigma E/\sigma \theta&gt;_{batch})<br>$$</p>
<h4 id="Evaluation-4"><a href="#Evaluation-4" class="headerlink" title="Evaluation"></a>Evaluation</h4><ul>
<li><strong>Environment</strong>:   <ul>
<li>Dataset: used the SoftKinetic DS325 sensor to acquire frontview color and depth videos and a top-mounted DUO 3D sensor to record a pair of stereo-IR streams.</li>
<li>randomly split the data by subject into training (70%) and test (30%) sets, resulting in 1050 training and 482 test videos.</li>
<li><strong>SKIG</strong> contains 1080 RGBD hand gesture sequences by 6 subjects collected with a Kinect sensor </li>
<li><strong>ChaLearn 2014 dataset</strong> contains more than 13K RGBD videos of 20 upper-body Italian sign language gestures performed by 20 subjects</li>
<li><strong>Results</strong>:</li>
</ul>
</li>
</ul>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200711173946207.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200711174007019.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200711174023573.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200711174040803.png" alt="predictions with various modalities"></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200711174056380.png" alt="Comparison of 2D-CNN and 3D-CNN trained with different architectures"></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200711174220754.png" alt="Gesture Detection"></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200711174344337.png" alt="SKIG RGBD gesture dataset"></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200711174326620.png" alt="Chalearn 2014 dataset"></p>
<h4 id="Conclusion-5"><a href="#Conclusion-5" class="headerlink" title="Conclusion"></a>Conclusion</h4><ul>
<li>Design R3DCNN to performs simultaneous detection and classification.</li>
<li>Using CTC model to predict label from in-progress gesture in unsegmented input streams.</li>
<li>Achieves high accuracy of 88.4%.</li>
</ul>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script>
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        Author:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://liudongdong1.github.io" rel="external nofollow noreferrer">liudongdong1</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        Link:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://liudongdong1.github.io/2020/06/20/shi-jue-ai/dataglove/hand-analyse-record/">https://liudongdong1.github.io/2020/06/20/shi-jue-ai/dataglove/hand-analyse-record/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        Reprint policy:
                    </i>
                </span>
                <span class="reprint-info">
                    All articles in this blog are used except for special statements
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    reprint polocy. If reproduced, please indicate source
                    <a href="https://liudongdong1.github.io" target="_blank">liudongdong1</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>Copied successfully, please follow the reprint policy of this article</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">more</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/tags/HandPose/">
                                    <span class="chip bg-color">HandPose</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="qq,qzone,wechat,weibo,douban" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
                <style>
    #reward {
        margin: 40px 0;
        text-align: center;
    }

    #reward .reward-link {
        font-size: 1.4rem;
        line-height: 38px;
    }

    #reward .btn-floating:hover {
        box-shadow: 0 6px 12px rgba(0, 0, 0, 0.2), 0 5px 15px rgba(0, 0, 0, 0.2);
    }

    #rewardModal {
        width: 320px;
        height: 350px;
    }

    #rewardModal .reward-title {
        margin: 15px auto;
        padding-bottom: 5px;
    }

    #rewardModal .modal-content {
        padding: 10px;
    }

    #rewardModal .close {
        position: absolute;
        right: 15px;
        top: 15px;
        color: rgba(0, 0, 0, 0.5);
        font-size: 1.3rem;
        line-height: 20px;
        cursor: pointer;
    }

    #rewardModal .close:hover {
        color: #ef5350;
        transform: scale(1.3);
        -moz-transform:scale(1.3);
        -webkit-transform:scale(1.3);
        -o-transform:scale(1.3);
    }

    #rewardModal .reward-tabs {
        margin: 0 auto;
        width: 210px;
    }

    .reward-tabs .tabs {
        height: 38px;
        margin: 10px auto;
        padding-left: 0;
    }

    .reward-content ul {
        padding-left: 0 !important;
    }

    .reward-tabs .tabs .tab {
        height: 38px;
        line-height: 38px;
    }

    .reward-tabs .tab a {
        color: #fff;
        background-color: #ccc;
    }

    .reward-tabs .tab a:hover {
        background-color: #ccc;
        color: #fff;
    }

    .reward-tabs .wechat-tab .active {
        color: #fff !important;
        background-color: #22AB38 !important;
    }

    .reward-tabs .alipay-tab .active {
        color: #fff !important;
        background-color: #019FE8 !important;
    }

    .reward-tabs .reward-img {
        width: 210px;
        height: 210px;
    }
</style>

<div id="reward">
    <a href="#rewardModal" class="reward-link modal-trigger btn-floating btn-medium waves-effect waves-light red">赏</a>

    <!-- Modal Structure -->
    <div id="rewardModal" class="modal">
        <div class="modal-content">
            <a class="close modal-close"><i class="fas fa-times"></i></a>
            <h4 class="reward-title">你的赏识是我前进的动力</h4>
            <div class="reward-content">
                <div class="reward-tabs">
                    <ul class="tabs row">
                        <li class="tab col s6 alipay-tab waves-effect waves-light"><a href="#alipay">支付宝</a></li>
                        <li class="tab col s6 wechat-tab waves-effect waves-light"><a href="#wechat">微 信</a></li>
                    </ul>
                    <div id="alipay">
                        <img src="/" class="reward-img" alt="支付宝打赏二维码">
                    </div>
                    <div id="wechat">
                        <img src="/medias/reward/wechat.png" class="reward-img" alt="微信打赏二维码">
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>

<script>
    $(function () {
        $('.tabs').tabs();
    });
</script>

            
        </div>
    </div>

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;Previous</div>
            <div class="card">
                <a href="/2020/06/22/sheng-huo/photography/camerachoose/">
                    <div class="card-image">
                        
                        <img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200703121347892.png" class="responsive-img" alt="Camera Choose">
                        
                        <span class="card-title">Camera Choose</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            
我是谁，我扮演这怎样的角色，我观察到了什么，我想到了什么，我听到了什么，留神周围的一切，用心感知细微差别，用新的语言定义这个世界，欣赏优美的风景，聆听美妙的音乐，做个快乐的键盘侠。

1、单反和微单1.1. 单反单反的英文是Single 
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2020-06-22
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/%E7%94%9F%E6%B4%BB/" class="post-category">
                                    生活
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/photography/">
                        <span class="chip bg-color">photography</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                Next&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/2020/06/16/aiot/rfid/rfid-actionrecognition/">
                    <div class="card-image">
                        
                        <img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200428195606397.png" class="responsive-img" alt="RFID ActionRecognition">
                        
                        <span class="card-title">RFID ActionRecognition</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            level:  ACM数据库  Embedded Networked Sensor Systems  CCF_Bauthor: Yinggang Yu ,Dong Wang, Run Zhao, Qian Zhang       Shang
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2020-06-16
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/AIOT/" class="post-category">
                                    AIOT
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/RFID/">
                        <span class="chip bg-color">RFID</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/libs/codeBlock/codeBlockFuction.js"></script>

<!-- 代码语言 -->

<script type="text/javascript" src="/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/libs/codeBlock/codeShrink.js"></script>


<!-- 代码块折行 -->

<style type="text/css">
code[class*="language-"], pre[class*="language-"] { white-space: pre !important; }
</style>


    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;TOC</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // modify the toc link href to support Chinese.
        let i = 0;
        let tocHeading = 'toc-heading-';
        $('#toc-content a').each(function () {
            $(this).attr('href', '#' + tocHeading + (++i));
        });

        // modify the heading title id to support Chinese.
        i = 0;
        $('#articleContent').children('h2, h3, h4').each(function () {
            $(this).attr('id', tocHeading + (++i));
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>

    

</main>




    <!-- <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="463294659"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/libs/aplayer/APlayer.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/meting@2/dist/Meting.min.js"></script>

    
    <div class="container row center-align" style="margin-bottom: 0px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            <span id="year">2019</span>
            <a href="https://liudongdong1.github.io" target="_blank">liudongdong</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            <br>
            
            &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                class="white-color">708.1k</span>&nbsp;字
            
            
            
            
            
            
            <span id="busuanzi_container_site_pv">
                |&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;<span id="busuanzi_value_site_pv"
                    class="white-color"></span>&nbsp;次
            </span>
            
            
            <span id="busuanzi_container_site_uv">
                |&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;<span id="busuanzi_value_site_uv"
                    class="white-color"></span>&nbsp;人
            </span>
            
            <br>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/liudongdong1" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:3463264078@qq.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>













    <a href="/atom.xml" class="tooltipped" target="_blank" data-tooltip="RSS 订阅" data-position="top" data-delay="50">
        <i class="fas fa-rss"></i>
    </a>


    <a href="https://blog.csdn.net/liudongdong19/" class="tooltipped" target="_blank" data-tooltip="关注我的CSDN: https://blog.csdn.net/liudongdong19/" data-position="top" data-delay="50">
        <i class="fab fa-csdn">C</i>
    </a>





</div>
    </div>
</footer>

<div class="progress-bar"></div>
 -->

    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;Search</span>
            <input type="search" id="searchInput" name="s" placeholder="Please enter a search keyword"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script src="/js/search.js"></script>
<script type="text/javascript">
$(function () {
    searchFunc("/search.xml", 'searchInput', 'searchResult');
});
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script type="text/javascript" src="/js/CFS.Snow.min.js"></script>
    <!-- 点击爆灯效果 -->
    <canvas class="fireworks" style="position: fixed;left: 0;top: 0;z-index: 1; pointer-events: none;" ></canvas> 
    <script type="text/javascript" src="//cdn.bootcss.com/animejs/2.2.0/anime.min.js"></script> 
    <script type="text/javascript" src="/js/fireworks.js"></script>
    <!--动态线条背景-->
    <script type="text/javascript"
        color="122 103 238" opacity='0.7' zIndex="-2" count="200" src="//cdn.bootcss.com/canvas-nest.js/1.0.0/canvas-nest.min.js">
    </script>
    <!-- 天气 -->
    <!-- weather -->
    <!-- weather -->
    <script type="text/javascript">
         WIDGET = {FID: 'knAMQaFanP'}
    </script>
    <script type="text/javascript" src="https://apip.weatherdt.com/float/static/js/r.js?v=1111"></script>
    <script type="text/javascript" src="https://apip.weatherdt.com/float/static/js/r.js?v=1111"></script>
    <script src="/libs/materialize/materialize.min.js"></script>
    <script src="/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/libs/aos/aos.js"></script>
    <script src="/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/js/matery.js"></script>

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    
    <script async src="/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    

    
    
    <script type="text/javascript" size="150" alpha='0.6'
        zIndex="-1" src="/libs/background/ribbon-refresh.min.js" async="async"></script>
    

    

    
    <script src="/libs/instantpage/instantpage.js" type="module"></script>
    

</body>

</html>
