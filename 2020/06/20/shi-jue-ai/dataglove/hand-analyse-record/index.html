<!DOCTYPE HTML>
<html lang="en">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Hand Analyse Record, AIOT,Space&amp;Temporal Sequence Analysis,SpringBoot,liudongdong1,cloud">
    <meta name="description" content="">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Hand Analyse Record | DaybyDay</title>
    <link rel="icon" type="image/png" href="/favicon.png">

    <link rel="stylesheet" type="text/css" href="/libs/awesome/css/all.css">
    <link rel="stylesheet" type="text/css" href="/libs/materialize/materialize.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/aos/aos.css">
    <link rel="stylesheet" type="text/css" href="/libs/animate/animate.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/lightGallery/css/lightgallery.min.css">
    <link rel="stylesheet" type="text/css" href="/css/matery.css">
    <link rel="stylesheet" type="text/css" href="/css/my.css">

    <script src="/libs/jquery/jquery.min.js"></script>

<meta name="generator" content="Hexo 4.2.0"><link rel="alternate" href="/atom.xml" title="DaybyDay" type="application/atom+xml">
<link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css"></head>


<body>
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/" class="waves-effect waves-light">
                    
                    <img src="/medias/logo.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">DaybyDay</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>Index</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>Tags</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>Categories</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>Archives</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/" class="waves-effect waves-light">

      
      <i class="fas fa-user-circle" style="zoom: 0.6;"></i>
      
      <span>About</span>
      <i class="fas fa-chevron-down" aria-hidden="true" style="zoom: 0.6;"></i>
    </a>
    <ul class="sub-nav menus_item_child ">
      
      <li>
        <a href="/about">
          
          <i class="fas fa-user-circle" style="margin-top: -20px; zoom: 0.6;"></i>
          
          <span>about</span>
        </a>
      </li>
      
      <li>
        <a href="/resume">
          
          <i class="fa fa-user-secret" style="margin-top: -20px; zoom: 0.6;"></i>
          
          <span>resume</span>
        </a>
      </li>
      
    </ul>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/friends" class="waves-effect waves-light">
      
      <i class="fas fa-address-book" style="zoom: 0.6;"></i>
      
      <span>Friends</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/gallery" class="waves-effect waves-light">
      
      <i class="fas fa-camera" style="zoom: 0.6;"></i>
      
      <span>Galleries</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="Search" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/medias/logo.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">DaybyDay</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			Index
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			Tags
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			Categories
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			Archives
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="javascript:;">
			
				<i class="fa-fw fas fa-user-circle"></i>
			
			About
			<span class="m-icon"><i class="fas fa-chevron-right"></i></span>
		</a>
            <ul  style="background:  ;" >
              
                <li>

                  <a href="/about " style="margin-left:75px">
				  
				   <i class="fa fas fa-user-circle" style="position: absolute;left:50px" ></i>
			      
		          <span>about</span>
                  </a>
                </li>
              
                <li>

                  <a href="/resume " style="margin-left:75px">
				  
				   <i class="fa fa fa-user-secret" style="position: absolute;left:50px" ></i>
			      
		          <span>resume</span>
                  </a>
                </li>
              
            </ul>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/friends" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-address-book"></i>
			
			Friends
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/gallery" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-camera"></i>
			
			Galleries
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/liudongdong1" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/liudongdong1" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://gitee.com/github-25970295/blogImage/raw/master/img/dataglove.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Hand Analyse Record</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <link rel="stylesheet" href="/libs/tocbot/tocbot.css">
<style>
    #articleContent h1::before,
    #articleContent h2::before,
    #articleContent h3::before,
    #articleContent h4::before,
    #articleContent h5::before,
    #articleContent h6::before {
        display: block;
        content: " ";
        height: 100px;
        margin-top: -100px;
        visibility: hidden;
    }

    #articleContent :focus {
        outline: none;
    }

    .toc-fixed {
        position: fixed;
        top: 64px;
    }

    .toc-widget {
        width: 345px;
        padding-left: 20px;
    }

    .toc-widget .toc-title {
        margin: 35px 0 15px 0;
        padding-left: 17px;
        font-size: 1.5rem;
        font-weight: bold;
        line-height: 1.5rem;
    }

    .toc-widget ol {
        padding: 0;
        list-style: none;
    }

    #toc-content {
        height: calc(100vh - 250px);
        overflow: auto;
    }

    #toc-content ol {
        padding-left: 10px;
    }

    #toc-content ol li {
        padding-left: 10px;
    }

    #toc-content .toc-link:hover {
        color: #42b983;
        font-weight: 700;
        text-decoration: underline;
    }

    #toc-content .toc-link::before {
        background-color: transparent;
        max-height: 25px;

        position: absolute;
        right: 23.5vw;
        display: block;
    }

    .toc-fixed .toc-link::before{
        position: fixed!important;/*当toc的位置改为fixed时，.toc-link::before也要改为fixed*/
    }

    #toc-content .is-active-link {
        color: #42b983;
    }

    #toc-content .is-active-link::before {
        background-color: #42b983;
    }

    #floating-toc-btn {
        position: fixed;
        right: 15px;
        bottom: 76px;
        padding-top: 15px;
        margin-bottom: 0;
        z-index: 998;
    }

    #floating-toc-btn .btn-floating {
        width: 48px;
        height: 48px;
    }

    #floating-toc-btn .btn-floating i {
        line-height: 48px;
        font-size: 1.4rem;
    }
</style>
<div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/tags/HandPose/">
                                <span class="chip bg-color">HandPose</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/categories/AIOT/" class="post-category">
                                AIOT
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>Publish Date:&nbsp;&nbsp;
                    2020-06-20
                </div>
                

                <!-- 
                    <i class="fa fa-pencil"></i> Author: liudongdong1
                  -->

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>Update Date:&nbsp;&nbsp;
                    2021-10-04
                </div>
                

                <!-- 
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>Word Count:&nbsp;&nbsp;
                    8.1k
                </div>
                 -->

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>Read Times:&nbsp;&nbsp;
                    45 Min
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>Read Count:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">
        <div class="card-content article-card-content">
            <div id="articleContent">
                <p><strong>level</strong>:  CVPR  CCF_A<br><strong>author</strong>: Tomas Simon   Carnegie Mellon University<br><strong>date</strong>: 2017<br><strong>keyword</strong>:</p>
<ul>
<li>hand pose </li>
</ul>
<hr>
<h2 id="Paper-OpenPose-HandKeypoint"><a href="#Paper-OpenPose-HandKeypoint" class="headerlink" title="Paper: OpenPose HandKeypoint"></a>Paper: OpenPose HandKeypoint</h2><div align="center">
<br>
<b>Hand Keypoint Detection in Single Images using Multiview Bootstrapping</b>
</div>

<h4 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h4><ol>
<li>present an approach that uses a multi-camera system to train fine-grained detectors for keypoints.</li>
</ol>
<h4 id="Research-Objective"><a href="#Research-Objective" class="headerlink" title="Research Objective"></a>Research Objective</h4><ul>
<li><strong>Application Area</strong>: hand based HCI and robotics</li>
<li><strong>Purpose</strong>:  to extract hand point coordinate from single RGB images.</li>
</ul>
<h4 id="Proble-Statement"><a href="#Proble-Statement" class="headerlink" title="Proble Statement"></a>Proble Statement</h4><ul>
<li>self-occlusion due to articulation, view-point, grasped object.<img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200313092350789.png" alt=""></li>
</ul>
<p>previous work:</p>
<ul>
<li>many approaches to image-based face and body keypoint localization exist, there are no markerless hand keypoint detectors that work on RGB images in the wild.</li>
</ul>
<h4 id="Methods"><a href="#Methods" class="headerlink" title="Methods"></a>Methods</h4><ul>
<li><strong>Problem Formulation</strong>:</li>
</ul>
<p>input:   a crop image patch $I\epsilon R^{w<em>h</em>3}$</p>
<p>output:  P keypoint location, $X_p\epsilon R^2$,with associated confidence $C_p$.<br>$$<br>Keypoint;detector:  ;;;;d(I)-&gt;[ (X_p,c_p) ;for ; p \epsilon[1….P]]<br>$$</p>
<ul>
<li><strong>system overview</strong>:</li>
</ul>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200307172658147.png" alt=""></p>
<p><strong>【Multiview Bootstrapped Training】</strong></p>
<ul>
<li>$Initial;trainingset: ;;;;T_0:=[ (I_f,y_p^f) ;for;f\epsilon[1…N_0]]$, $f$ denote the particular image frame,set$[y_p^f\epsilon R^2]$ include all labeled keypoints for image $I^f$ .</li>
</ul>
<p>Multiview Bootrstrap:</p>
<ul>
<li>Inputs:<ul>
<li>calibrated cameras configuration</li>
<li>unlabled images: $[ I_v^f ; for ; v\epsilon views,; f\epsilon frames]$</li>
<li>keypoint detector: $d_0(I)-&gt;[(x_p,c_p);for;p\epsilon points]$</li>
<li>labeled training data: $T_0$</li>
</ul>
</li>
<li>Output: improved detector $d_K(.)$ and training set $T_k$</li>
<li>for iteration $i$ in 0  to K:<ol>
<li><font color="red">Triangulate keypoint from weak detections</font><ul>
<li>for every frame $f$:<ul>
<li>run detector $d_i(I_v^f)$ on all views $v$ , $D&lt;-{d_i(I_v^f) ; for ; v\epsilon [1…V] }$                 (1)</li>
<li>robustly triangulate keypoints,   $X_p^f=argmin_X \sum_{v\epsilon I_p^f}{||P_v(X)-x_p^v||_2^2}$      (2)</li>
</ul>
</li>
</ul>
</li>
<li><font color="red">score and sort triangulated frames</font>  ,         $score({X_p^f})=\sum_{p\epsilon [1…P]}\sum_{v\epsilon I_p^f}C_p^v$           (3)</li>
<li><font color="red">retrain with N-best reprojections</font>. $d_{i+1}&lt;-train(T_0;U;T_{i+1})$       (4)</li>
</ol>
</li>
</ul>
<p><strong>supplement for the mathmatic:</strong></p>
<ul>
<li><p>for (1):for one frame,  for each keypoint p, we have V detections $(x_p^v,c_p^v)$ , robustly triangulate each point p into a 3D location, use RANSAC on point D with confidence above a detection threshold $\lambda$.</p>
</li>
<li><p>for (2): $I_p^f$ is the inlier set, $X_p^f \epsilon R^3$   is the 3D triangulated keypoint p in frame f,  $P_v(X) \epsilon R^2$   denotes projection of 3D point $X$ into view $v$.  triangulate all landmarks of each finger(4 points ) at a time.</p>
</li>
<li><p>for(3): pick the best frame for every window of $W$ frames. Sort the frame in descending order according to their score, to obtain an ordered sequence of frames,$[s_1,s_2,…s_F^‘]$, $F^‘$ is the number of subsampled frames, $s_i$ is the ordered frame index. </p>
<ul>
<li>while verigy the good labled frame, using some strategies to automatically removing bad frame:<ul>
<li>average number of inliews</li>
<li>average detection detection confidence</li>
<li>difference of per-point velocity with medium velocity between two video frames</li>
<li>anthropomorphic limits on joint lengths</li>
<li>complete occlusion as determined by camera ray intersection with body joints</li>
</ul>
</li>
</ul>
</li>
<li><p>for(4):           $T_{i+1}={(I_v^{s_n},{P_v(X_p^{s_n}):;v\epsilon [1…V],; p\epsilon [1…P]}); for; n\epsilon[1…N]}$</p>
</li>
</ul>
<p><strong>【Detection Architecture】</strong></p>
<ul>
<li><strong>Hand Bounding Box Detection:</strong> directly use the body pose estimation models from [29], and [4] and use wrist and elbow position to approximate the hand location, assuming the hand extends 0.15 times the length of the forearm(前臂) in the same direction.</li>
<li>using architecture of CPMs with some modification.  <font color="red">CPMs predict a confidence map for each keypoint, representing the keypoint’s location as a Gaussian centered at the true position</font></li>
<li>using pre-trained VGG-19 network</li>
</ul>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200307174424118.png" alt=""></p>
<h4 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a>Evaluation</h4><ul>
<li><strong>Environment</strong>:   <ul>
<li>Dataset:  <ul>
<li>the MPII human pose dataset[2] <font color="red">reflect every-day human activities</font> </li>
<li>Images from the New Zealand Sign Language Exercised os the Victoria University of Wellington <font color="red">contains a variety of hand poses found in conversation</font></li>
</ul>
</li>
</ul>
</li>
</ul>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200307171832277.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200307175621323.png" alt=""></p>
<h4 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h4><ul>
<li>the first real-time hand keypoint detector showing practical applicability to in-the-wild RGB videos</li>
<li>the first markerless 3D hand motion capture system capable of reconstructing challenging hand-object interactions and musical performances without manual intervention</li>
<li>using multi-view bootstrapping, improving both the quality and quantity of the annotations</li>
</ul>
<h4 id="Notes"><a href="#Notes" class="headerlink" title="Notes"></a>Notes</h4><ul>
<li><p><strong>Bootstrap步骤：</strong></p>
<ul>
<li><p>在原有的样本中通过重抽样抽取一定数量（比如100）的新样本。</p>
</li>
<li><p>基于产生的新样本，计算我们需要估计的统计量$\alpha_i$。</p>
</li>
<li><p>重复上述步骤n次（一般是n&gt;1000次）。计算被估计量的均值和方差。</p>
</li>
<li><p>$$<br>\vec{\alpha}=Mean(\alpha_i…)<br>$$</p>
</li>
<li><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200607110758736.png" alt=""></p>
</li>
</ul>
</li>
<li><p><strong><a href="http://www.cse.yorku.ca/~kosta/CompVis_Notes/ransac.pdf" target="_blank" rel="noopener">RANSAC:</a></strong> robust estimation techniques such as M-estimators and least-median squares that have been adopted by the computer vision community from the statistics literature, RANSAC was developed from within the computer vision community</p>
</li>
</ul>
<p>  <img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200307183024378.png" alt=""></p>
<p>  <img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200307183041127.png" alt=""></p>
<p><strong>level</strong>: CVPR<br><strong>author</strong>: Kuo Du1<br><strong>date</strong>: 2019<br><strong>keyword</strong>:</p>
<ul>
<li>hand skeleton</li>
</ul>
<hr>
<h2 id="Paper-CrossInfoNet"><a href="#Paper-CrossInfoNet" class="headerlink" title="Paper: CrossInfoNet"></a>Paper: CrossInfoNet</h2><div align="center">
<br>
<b>CrossInfoNet: Multi-Task Information Sharing Based Hand Pose Estimation
</b>
</div>



<h4 id="Summary-1"><a href="#Summary-1" class="headerlink" title="Summary"></a>Summary</h4><ol>
<li>proposed CrossInfoNet decomposes hand pose estimation task into palm pose estimation sub-task and finger pose estimation sub-task, and adopts two-branch cross-connection structure to share the beneficial complementary information between the sub-tasks.</li>
<li>propose a heat-map guided feature extraction structure to get better feature maps, and train the complete network end-to-end.</li>
</ol>
<h4 id="Proble-Statement-1"><a href="#Proble-Statement-1" class="headerlink" title="Proble Statement"></a>Proble Statement</h4><p>previous work:</p>
<ul>
<li>treating depth maps as 2D images and regressing 3D joint coordinates directly is a commonly used hand pose estimation pipeline.</li>
<li>designing effective networks receives the most attentions. Learning multiple tasks simultaneously will be helpful to enforce a model with better generalizing ability.</li>
<li>the output representations can be classified into the probability density map or the 3D coordinates for each joint.  <font color="red">heat-map based method outperforms direct coordinate regression method, and the final joint coordinates have usually to be inferred by maximum operation on the heat-maps</font></li>
</ul>
<h4 id="Methods-1"><a href="#Methods-1" class="headerlink" title="Methods"></a>Methods</h4><ul>
<li><strong>system overview</strong>:</li>
</ul>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200309194734899.png" alt=""></p>
<p><strong>【Heat-map guided feature extraction】</strong></p>
<ul>
<li>ResNet-50 [15] backbone network with four residual modules</li>
<li>apply the feature pyramid structure to merge different feature layers.</li>
<li>the heat maps are only used as the constraints to guide the feature extraction and will not be passed to the subsequent module.</li>
</ul>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200309194817022.png" alt=""></p>
<p><strong>【Baseline feature refinement architecture】</strong></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200309195036333.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200309195052036.png" alt=""></p>
<p><strong>【New Feature refinement architecture】</strong></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200309195142885.png" alt=""></p>
<p><strong>【Loss Functions Defines】</strong></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200309195243947.png" alt=""></p>
<h4 id="Evaluation-1"><a href="#Evaluation-1" class="headerlink" title="Evaluation"></a>Evaluation</h4><ul>
<li><strong>Environment</strong>:   <ul>
<li>Dataset:  ICVL datasets, NYU datasets, MSRA datasets, Hands 2017 Challenge Frame-based Dataset.</li>
</ul>
</li>
</ul>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200309195412451.png" alt=""></p>
<h4 id="Conclusion-1"><a href="#Conclusion-1" class="headerlink" title="Conclusion"></a>Conclusion</h4><ul>
<li>use hierarchical model to decompose the final task into palm joint regression sub-task and finger joint regression sub-task.</li>
<li>a heat-map guided feature extraction structure is proposed.</li>
</ul>
<h4 id="Notes-去加强了解"><a href="#Notes-去加强了解" class="headerlink" title="Notes 去加强了解"></a>Notes <font color="orange">去加强了解</font></h4><ul>
<li><a href="https://github.com/dumyy/handpose" target="_blank" rel="noopener">https://github.com/dumyy/handpose</a></li>
</ul>
<h2 id="Paper-Emotion-Identification"><a href="#Paper-Emotion-Identification" class="headerlink" title="Paper: Emotion Identification"></a>Paper: Emotion Identification</h2><div align="center">
<br>
<b>Hand Gestures Based Emotion Identification Using Flex Sensors</b>
</div>




<h4 id="Summary-2"><a href="#Summary-2" class="headerlink" title="Summary"></a>Summary</h4><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200314105007698.png" alt=""></p>
<h2 id="Paper-Gesture-To-Speech"><a href="#Paper-Gesture-To-Speech" class="headerlink" title="Paper: Gesture To Speech"></a>Paper: Gesture To Speech</h2><div align="center">
<br>
<b>Gesture To Speech Conversion using Flex sensors,MPY6050 and Python</b>
</div>




<h4 id="Summary-3"><a href="#Summary-3" class="headerlink" title="Summary"></a>Summary</h4><ul>
<li>Arduino Uno, Flex Sensors, MPU6050 an accelerometer gyroscope sensor which is used to detect the alignment of an object.</li>
<li>To recognise the ALS Sign Language <img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200314105349781.png" alt=""></li>
<li><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200314105446162.png" alt=""></li>
</ul>
<h2 id="Paper-Flex"><a href="#Paper-Flex" class="headerlink" title="Paper: Flex"></a>Paper: Flex</h2><div align="center">
<br>
<b>Flex: Hand Gesture Recognition using Muscle Flexing Sensors</b>
</div>
#### Summary

<ul>
<li><p>Flex Sensors from Spectra-Symbol for angle displacement measuremetns.</p>
</li>
<li><p>apply a linear response delay filter to the raw sensors output for noise reduction and signal smoothing.</p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200314110009115.png" alt=""></p>
</li>
</ul>
<h2 id="Paper-Survey-on-Hand-Pose-Estimation"><a href="#Paper-Survey-on-Hand-Pose-Estimation" class="headerlink" title="Paper: Survey on Hand Pose Estimation"></a>Paper: Survey on Hand Pose Estimation</h2><div align="center">
<br>
<b>A Survey on Hand Pose Estimation with Wearable Sensors and Computer-Vision- Based Methods
</b>
</div>
#### Summary

<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200314110451195.png" alt=""></p>
<ul>
<li><p>详细介绍了基于视觉基于传感器方法</p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200314110640684.png" alt=""></p>
</li>
</ul>
<h2 id="Paper-Flex-amp-Gyroscopes"><a href="#Paper-Flex-amp-Gyroscopes" class="headerlink" title="Paper: Flex&amp;Gyroscopes"></a>Paper: Flex&amp;Gyroscopes</h2><div align="center">
<br>
<b>Recognizing words in Thai Sign Language using ﬂex sensors and gyroscopes
</b>
</div>
#### Summary

<ul>
<li><p>some sensors</p>
<ul>
<li>contact sensors for detecting fingers touching each other</li>
<li>accelerometers for measuring the acceleration of the hand in different direction</li>
<li>gyro-scopes for measuring the hand orientation and angular movement</li>
<li>magnetoresistive sensors for measuring the magnetic field for deriving the hand orientation</li>
</ul>
</li>
<li><p>presents a Thai sign language recognition framework using  a glove-based device with flex sensors and gyro-scops.</p>
</li>
<li><p>the measurements from the sensors are processed using finite Legendre and Linear Discriminant Analysis, then classified using k-nearest neighbors. </p>
</li>
<li><p>Handware design:<img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200314111857761.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200314112748401.png" alt=""></p>
</li>
<li><p>the gyroscopes can return values in three different types of measurement</p>
<ul>
<li>the quaternions are the raw data returned from the sensor. This measurement yields a four-dimensional output.</li>
<li>Euler angles are data converted from the four quaternion values. The Euler angles consist of three values, matching x, y, and z axis.</li>
<li>YPR measures the angle but with respect to the direction of the ground. It has three elements like the Euler angles. However it also requires gravity values from the accelerometer in order to calibrate.<font color="red"> to calculate YPR, four quaternion elements and three gravity values are needed</font></li>
</ul>
</li>
<li><p>Date processing </p>
<ul>
<li><p>segment and normalize the data  ???how to segment data unclear??</p>
</li>
<li><p>the value from flex sensors differ greatly depend on person, by requiring a calibration phase which the user clenches and releases his hands at least 3 times to determine th e maximum and minimum values of each flex sensor, and quantize the data to 3 possible values(0,1,2)</p>
</li>
<li><p>这部分不理解：<img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200314140333583.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200314140348967.png" alt=""></p>
</li>
</ul>
</li>
</ul>
<h1 id="Human-Machine-Interaction"><a href="#Human-Machine-Interaction" class="headerlink" title="Human-Machine-Interaction"></a>Human-Machine-Interaction</h1><blockquote>
<p>Taheri, Omid, et al. “GRAB: A dataset of whole-body human grasping of objects.” <em>European Conference on Computer Vision</em>. Springer, Cham, 2020.</p>
</blockquote>
<hr>
<h1 id="Paper-GRAB"><a href="#Paper-GRAB" class="headerlink" title="Paper: GRAB"></a>Paper: GRAB</h1><div align="center">
<br>
<b>GRAB: A dataset of whole-body human grasping of objects
</b>
</div>



<h4 id="Summary-4"><a href="#Summary-4" class="headerlink" title="Summary"></a>Summary</h4><ol>
<li>collect a new dataset, GRAB of whole-body grasps, containing full 3D shape and pose sequences of 10 subjects interacting with 51 every day objects of varying shape and size.</li>
<li>using MoCap markers to fit the full 3D body shape and pose, including the articulated face and hands, as well as the 3D object pose.</li>
<li>adapt MoSh++ to solve for the body, face, and hands of SMPL-X to obtain detailed moving 3D meshes, and according to the meshes and tracked 3D objects, we compute plausible contact on the object and the human and provide an analysis of observed patterns.</li>
</ol>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210224170220204.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210224170341797.png" alt="Contact Annotation"></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210224170455822.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210224170556522.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210224170611285.png" alt=""></p>
<h4 id="Relative"><a href="#Relative" class="headerlink" title="Relative"></a>Relative</h4><ul>
<li>require complex 3D object shapes, detailed contact information, hand pose and shape, and the 3D body motion over time;</li>
<li>MoCap: <a href="https://mocap.reallusion.com/iClone-motion-live-mocap/" target="_blank" rel="noopener">https://mocap.reallusion.com/iClone-motion-live-mocap/</a></li>
</ul>
<hr>
<h2 id="Paper-A-Mobile-Robot-Hand-arm"><a href="#Paper-A-Mobile-Robot-Hand-arm" class="headerlink" title="Paper:  A Mobile Robot Hand-arm"></a>Paper:  A Mobile Robot Hand-arm</h2><div align="center">
<br>
<b> A Mobile Robot Hand-Arm Teleoperation System by Vision and IMU</b>
</div>

<h4 id="Summary-5"><a href="#Summary-5" class="headerlink" title="Summary"></a>Summary</h4><blockquote>
<ol>
<li>present a multi-modal mobile teleoperation system that consists of a novel vision-based hand pose regression network and IMU-based arm tracking methods.</li>
<li>observe the human hand through a  depth camera and generates joint angles and depth images of paired robot hand poses through an image-to-image translation process.</li>
<li>Transteleop takes the depth image of the human hand as input, then estimates the joint angles of the robot hand, and also generates the reconstructed image of the robot hand.</li>
<li>design a keypoint-based reconstruction loss to focus on the local reconstruction quality around the keypoints of the hand. </li>
</ol>
</blockquote>
<h4 id="Research-Objective-1"><a href="#Research-Objective-1" class="headerlink" title="Research Objective"></a>Research Objective</h4><ul>
<li><strong>Application Area</strong>: space, rescue, medical, surgery, imitation learning.<ul>
<li><strong>Purpose</strong>:  implement different manipulation tasks such as pick and place, cup insertion, object pushing, and dual-arm handover tasks</li>
</ul>
</li>
</ul>
<h4 id="Proble-Statement-2"><a href="#Proble-Statement-2" class="headerlink" title="Proble Statement"></a>Proble Statement</h4><ul>
<li>the robot hand and human hand occupy two different domains, how to compensate for kinematic differences between them plays an essential role in markerless vision-based teleoperation</li>
</ul>
<p>previous work:</p>
<ul>
<li><strong>Image-to-Image translation:</strong>  aims to map representation of a scene into another, used in collection of style transfer, object transfiguration, and imitation learning.</li>
</ul>
<h4 id="Methodsj"><a href="#Methodsj" class="headerlink" title="Methodsj"></a>Methodsj</h4><ul>
<li><strong>system overview</strong>:</li>
</ul>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200612092238443.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200612092928609.png" alt=""></p>
<p>【Question 1】how to discover the latent feature embedding the Zpose between the human hand and robot hand?</p>
<blockquote>
<p>using Encoder-decoder module</p>
</blockquote>
<p>【Question 2】how to get more accuracy of local features such as the position of fingertips instead of global features such as image style?</p>
<blockquote>
<p>design a keypoint-based reconstruction loss to capture the overall structure of the hand and concentrate on the pixels around the 15 keypoints of the hand.</p>
<p>using mean squared error(MSE) loss to  calculate the joint from $Z_R$ (robot feature)</p>
</blockquote>
<p>【Question 3】the poses of the human hand vary considerably in their global orientations?</p>
<blockquote>
<p>applied spatial transformation network(STN) provides spatial transformation capabilities of input images before the encoder module.</p>
</blockquote>
<p>【Question 4】the hand easily disappears from the field of view of the camera, and the camera position is uncertain ?</p>
<blockquote>
<p>using a cheap 3D-printed camera holder</p>
<p>using Perception Neuron device to control the arm of the robot.</p>
</blockquote>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200612095958385.png" alt=""></p>
<h4 id="Evaluation-2"><a href="#Evaluation-2" class="headerlink" title="Evaluation"></a>Evaluation</h4><ul>
<li><p><strong>Environment</strong>:   </p>
<ul>
<li>Dataset:  dataset of paired human-robot images, contains 400k pairs of simulated robot depth images and human hand depth images, the ground trush are 19 joint angles of the robot hand, record the 9 depth images of the robot hand from different viewpoints simultaneously corresponding to one human pose.</li>
</ul>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200612100328537.png" alt=""></p>
</li>
</ul>
<h4 id="Notes-去加强了解-1"><a href="#Notes-去加强了解-1" class="headerlink" title="Notes 去加强了解"></a>Notes <font color="orange">去加强了解</font></h4><ul>
<li><a href="https://Smilels.github.io/multimodal-translation-teleop" target="_blank" rel="noopener">https://Smilels.github.io/multimodal-translation-teleop</a></li>
<li>可能有什么问题，</li>
</ul>
<p><strong>level</strong>: PerDial’19<br><strong>author</strong>:<br><strong>date</strong>:  2019<br><strong>keyword</strong>:</p>
<ul>
<li>robot, ASL, </li>
</ul>
<hr>
<h2 id="Paper-Human-Robot"><a href="#Paper-Human-Robot" class="headerlink" title="Paper: Human-Robot"></a>Paper: Human-Robot</h2><div align="center">
<br>
<b>Human-Robot Interaction with Smart Shopping Trolley using Sign Language: Data Collection</b>
</div>
#### Summary

<ol>
<li>presents a concept of smart robotic trolley for supermarkets with multi-modal user interface, including sign language and acoustic speech recognition, and equipped with a touch screen.</li>
</ol>
<h4 id="Proble-Statement-3"><a href="#Proble-Statement-3" class="headerlink" title="Proble Statement"></a>Proble Statement</h4><ul>
<li>continuous or dynamic sign language recognition remains an unresolved challenge.</li>
<li>sensitivity to size and speed variations, poor performance under varying lighting conditions and complex background have limited the use of SLR in modern dialogue systems.</li>
</ul>
<p>previous work:</p>
<ul>
<li>the level of voiced speech and isolated/static hand gesture automatic recognition quality is quite high.</li>
<li>EffiBot[1] takes goods and automatically goes with them to the point of discharge, and follow the user when the corresponding mode is activated.</li>
<li>The Dash Robotic Shopping Cart[2] :<ul>
<li>a supermarket trolley that facilitates shopping and navigation in the store, the car is equipped with a touchscreen for entering a list of products of interest to the client.</li>
</ul>
</li>
<li>Gita by Piaggio[3]: a robotic trolley that follows the owner.</li>
<li><font color="red">none of the interfaces of the aforementioned robotic carts are multimodal.</font></li>
</ul>
<h4 id="Methods-2"><a href="#Methods-2" class="headerlink" title="Methods"></a>Methods</h4><ul>
<li><p><strong>system overview</strong>:</p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200407165820138.png" alt=""></p>
</li>
</ul>
<ol>
<li>speaker-independent system of automatic continuous Russian speech recognition</li>
<li>speaker-independent system of Russian sign language recognition with video processing using Kinect2.0 device</li>
<li>interactive graphical user interface with touchscreen</li>
<li>dialogue and data manager that access an application database, generates multi modal output and synchronizes input modalities fusion and output modalities fission</li>
<li>modules for audio-visual speech synthesis to be applied for a talking avatar</li>
</ol>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200407171743516.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200407171910822.png" alt=""></p>
<h4 id="Conclusion-2"><a href="#Conclusion-2" class="headerlink" title="Conclusion"></a>Conclusion</h4><ul>
<li>understanding voice commands</li>
<li>understanding Russian sign language commands</li>
<li>escort the user to a certain place in the store</li>
<li>speech synthesis, synthesis of answers in Russian sign language using a 3D avatar.</li>
</ul>
<h4 id="Notes-1"><a href="#Notes-1" class="headerlink" title="Notes"></a>Notes</h4><ul>
<li>介绍了一些手语 数据集</li>
<li>【30】，32 ，7 ， 34</li>
<li>机器人：<a href="https://www.effidence.com/effibot" target="_blank" rel="noopener">https://www.effidence.com/effibot</a><ul>
<li>​        <a href="https://mygita.com/#/how-does-gita-work" target="_blank" rel="noopener">https://mygita.com/#/how-does-gita-work</a></li>
</ul>
</li>
</ul>
<hr>
<p><strong>level</strong>:   IJCAI<br><strong>author</strong>: YangYi (MediaLab,Tencent)    FengNi(PekingUniversity)<br><strong>date</strong>: 2019<br><strong>keyword</strong>:</p>
<ul>
<li>hand gesture understand</li>
</ul>
<hr>
<h2 id="Paper-MKTB-amp-GRB"><a href="#Paper-MKTB-amp-GRB" class="headerlink" title="Paper: MKTB&amp;GRB"></a>Paper: MKTB&amp;GRB</h2><div align="center">
<br>
<b>High Performance Gesture Recognition via Effective and Efficient Temporal Modeling</b>
</div>

<h4 id="Research-Objective-2"><a href="#Research-Objective-2" class="headerlink" title="Research Objective"></a>Research Objective</h4><ul>
<li><strong>Purpose</strong>:  hand gesture recognition instead of human-human or human-object relationships.</li>
</ul>
<h4 id="Proble-Statement-4"><a href="#Proble-Statement-4" class="headerlink" title="Proble Statement"></a>Proble Statement</h4><ul>
<li>hand gesture recognition methods based on spatio-temporal features using 3DCNNs or ConvLSTM suffer from the inefficiency due to high computational complexity of their structure.</li>
</ul>
<p>previous work:</p>
<ul>
<li><p>Temporal Modeling for Action Recognition</p>
<ul>
<li>2DCNN by Narayana et al., 2018  </li>
<li>3DCNNs by  Miao et al., 2017</li>
<li>ConvLSTM by Zhang et al., 2017</li>
<li>TSN by Wang et al.2016 models long-range temporal structures with segment-based sampling and aggregation module.</li>
<li>C3D by Li et al.2016 designs a 3DCNN with small 3<em>3</em>3 convolution kernels to learn spatiotemporal features.</li>
<li>I3D by Carreira 2017 inflates convolutional filters and pooling kernels into 3D structures.</li>
<li>R(2+1)D by Wang et al. 2018 present non-local operations to capture long-reange dependencies</li>
</ul>
</li>
<li><p>Gesture Recognition:</p>
<ul>
<li>2DCNN by Narayana et al.2018  (学习下，多模态的,只了解多模态部分) fuses multi-channels(global/left-hand/right-hand/for RGB/depth/RGB-flow/Depth-flow modalities)</li>
<li>combines 3DCNN， bidirectional ConvLSTM and 2DCNN into a unified framework. ( 学习下如何整合到一个框架中)</li>
</ul>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200313162234686.png" alt=""></p>
</li>
</ul>
<h4 id="Methods-3"><a href="#Methods-3" class="headerlink" title="Methods"></a>Methods</h4><ul>
<li><p><strong>system overview</strong>:</p>
<ul>
<li>the model builds upon TSN, for TSN lacks of capability of modeling the temporal information from feature-space, the proposed MKTB and GRB are effective temporal modeling modules in feature-space.</li>
</ul>
</li>
</ul>
<p>【Multi-Kernel Temporal Block】</p>
<ul>
<li>unlike 3DCNNs, performing convolutional operation for both spatial and temporal dimension jointly, the MTKB decouples the joint spatial-temporal modeling process and focuses on learning the temporal information.</li>
<li>the design of multi-kernel works well on shaping the pyramidal and discriminative temporal features.</li>
</ul>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200313162921564.png" alt=""></p>
<ul>
<li>define feature maps from layer $l$ of 2DCNN(ResNet-50) as $F_s\epsilon R^{(B<em>T)</em>C<em>H</em>W}$</li>
<li>reduce the channels of $F_s$  via convolution layer with kernel size of 1*1, denoted as  $F_s^‘ \epsilon R^{ ( B * T) * C^‘ * H * W}$</li>
<li><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200313163941544.png" alt="image-20200313163941544"> using depthwise temporal conv [Chollet,2017]</li>
</ul>
<p>【Global Refinement Block】</p>
<ul>
<li><p>MKTB mainly focuses on the local neighborhoods,but the global temporal features across channels are not sufficiently attended.</p>
</li>
<li><p>GRB is designed to perform the weighted temporal aggregation, in which it allows distant temporal features to contribute to the filtered temporal features according to the cross-similarity.  <font color="red">遗留问题，如何计算similarity， MKTB 中如何sum</font></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200313164405173.png" alt=""></p>
</li>
</ul>
<h4 id="Evaluation-3"><a href="#Evaluation-3" class="headerlink" title="Evaluation"></a>Evaluation</h4><ul>
<li><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200313164608764.png" alt=""></li>
</ul>
<h4 id="Conclusion-3"><a href="#Conclusion-3" class="headerlink" title="Conclusion"></a>Conclusion</h4><ul>
<li>MKTB captures both short-term and long-term temporal information by using the multiple 1D depthwise convolutions.</li>
<li>MKTB and GRB maintain the same size between input and output, and can be easily deployed everywhere.</li>
</ul>
<h4 id="Notes-去加强了解-2"><a href="#Notes-去加强了解-2" class="headerlink" title="Notes 去加强了解"></a>Notes <font color="orange">去加强了解</font></h4><ul>
<li><input disabled="" type="checkbox"> <a href="https://github.com/nemonameless/Gesture-Recognition" target="_blank" rel="noopener">https://github.com/nemonameless/Gesture-Recognition</a></li>
</ul>
<p><strong>level</strong>: CCF_A  CVPR<br><strong>author</strong>: Liuhao Ge, Nanyang Technological University<br><strong>date</strong>:  2018<br><strong>keyword</strong>:</p>
<ul>
<li>hand pose</li>
</ul>
<hr>
<h2 id="Paper-Hand-PointNet"><a href="#Paper-Hand-PointNet" class="headerlink" title="Paper: Hand PointNet"></a>Paper: Hand PointNet</h2><div align="center">
<br>
<b>Hand PointNet: 3D Hand Pose Estimation using Point Sets
</b>
</div>

<h4 id="Summary-6"><a href="#Summary-6" class="headerlink" title="Summary"></a>Summary</h4><ol>
<li>propose HandPointNet model, that directly processes the 3D point cloud that models the visible surface of the hand for pose regression,Taking the normalized point cloud as the input, the regression network capture complex hand structures and accurately regress a low dimensional representation of the 3D hand pose.</li>
<li>design a fingertip refinement network that directly takes the neighboring points of the estimated fingertip location as input to refine the fingertip location.</li>
</ol>
<h4 id="Research-Objective-3"><a href="#Research-Objective-3" class="headerlink" title="Research Objective"></a>Research Objective</h4><ul>
<li><strong>Application Area</strong>: hand based interaction</li>
<li><strong>Purpose</strong>:   exact hand skeleton</li>
</ul>
<h4 id="Proble-Statement-5"><a href="#Proble-Statement-5" class="headerlink" title="Proble Statement"></a>Proble Statement</h4><ul>
<li>high dimensionality of 3D hand pose, large variations in hand orientations, high self-similarity of fingers and servere self-occlusion</li>
</ul>
<p>previous work:</p>
<ul>
<li>large hand pose datasets[38, 34, 33, 49, 48]</li>
<li><strong>CNN model:</strong><ul>
<li>the time and space complexities of the 3D CNN grow cubically with the resolution of the input 3D volume, using low resolution may lose useful details of the hand </li>
<li><font color="red">PointNet</font>: perform 3D object classification and segmentation on point sets directly </li>
<li>using multi-view CNNs-based method and 3D CNN-based method</li>
</ul>
</li>
<li><strong>Hand Pose Estimation:</strong> Discriminative approaches,  generative approaches, hybrid approaches<ul>
<li>feedback loop model[21]</li>
<li>spatial attention network[47]</li>
<li>deep generative models[41]</li>
</ul>
</li>
<li><strong>3D Deep Learning</strong>: <ul>
<li>Multi-view CNNs-based approaches[32, 24, 7, 2] project 3D points into 2D images and use 2D CNNs to process them.</li>
<li>3D CNNs based on octrees[27, 43] are proposed for efficient computation on high resolution volumes.</li>
</ul>
</li>
</ul>
<h4 id="Methods-4"><a href="#Methods-4" class="headerlink" title="Methods"></a>Methods</h4><ul>
<li><strong>Problem Formulation</strong>:</li>
</ul>
<blockquote>
<ul>
<li><p>Input: depth image containing a hand;</p>
</li>
<li><p>outputs: a set of 3D hand joint locations in the amera coordinate system.</p>
</li>
</ul>
</blockquote>
<ul>
<li><strong>system overview</strong>:</li>
</ul>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200301140035643.png" alt=""></p>
<p>【Basic PointNet】 [23] directly takes a set of points as the input and is able to extract discriminative features of the point cloud.   <font color="red">cannot capture local structures of the point cloud in a hierarchical way</font>.</p>
<blockquote>
<p>basic architecture of PointNet takes N points as the input, Each D-dim input point is mapped into a C-dim feature through MLP. Per-point features are aggregated into a global feature by max-pooling, and mapped into F-dim output vector.</p>
</blockquote>
<p><img src="https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211002200807400.png" alt="pointnet classification"></p>
<p><strong>【Hierarchical PointNet】</strong>[25]: </p>
<blockquote>
<p>The hierarchical structure is composed by a number of set abstraction levels, at each level, a  set of points is processed and abstracted to produce a new set with fewer elements. The set abstraction level is made of three key layers: 点云采样+成组+提取局部特征（S+G+P）的方式，包含这三部分的机构称为 Set Abstraction</p>
<ul>
<li><strong>sampling layer</strong>: selects a set of points from input points, which defines the <code>centroids of lcoal regions</code>. use interative farthest point sampling(FPS) to choose the subset of points.</li>
<li><strong>grouping layer</strong>: constructs local region sets by fining “neighboring” points around the centroids. N`<em>K</em>(d+C): d-dim coordinates, and C-dim point feature, K is the number of points in the neighborhood of centroid points. Ball query finds all points that are within a radius to the query point.</li>
<li><strong>PointNet alyer:</strong>  uses a mini-PointNet to encode local region patterns into feaature vectors.</li>
</ul>
</blockquote>
<p><img src="https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211002105943638.png" alt="PointNet++"></p>
<blockquote>
<ul>
<li>分类网络是逐层提取特征，最后总结出全局特征。</li>
<li>分割网络先将点云提取一个全局特征，在通过这个全局特征逐步上采样。每层新的中心点都是从上一层抽取的特征子集，中心点的个数就是成组的点集数，随着层数增加，中心点的个数也会逐渐降低，抽取到点云的局部结构特征。<code>当点云不均匀时</code>，每个子区域中如果在分区的时候使用相同的球半径，会导致部分稀疏区域采样点过小。多尺度成组 (MSG)<strong>和</strong>多分辨率成组 (MRG)<ul>
<li><strong>多尺度成组（MSG）：</strong>对于选取的一个中心点设置多个半径进行成组，并将经过PointNet对每个区域抽取后的特征进行拼接（concat）来当做该中心点的特征.</li>
<li><strong>多分辨率成组（MRG）：</strong>对不同特征层上（分辨率）提取的特征再进行concat，以上图右图为例，最后的concat包含左右两个部分特征，分别来自底层和高层的特征抽取，对于low level点云成组后经过一个pointnet和high level的进行concat，思想是特征的抽取中的跳层连接。当局部点云区域较稀疏时，上层提取到的特征可靠性可能比底层更差，因此考虑对底层特征提升权重。当然，点云密度较高时能够提取到的特征也会更多。这种方法优化了直接在稀疏点云上进行特征抽取产生的问题，且相对于MSG的效率也较高。</li>
</ul>
</li>
</ul>
</blockquote>
<p><img src="https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211002201356244.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211002201115350.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211002200643224.png" alt=""></p>
<p>【<strong>OBB-based Point Cloud Normalization</strong>】to deal with large variation in global orientation of the hand. normalization the hand point cloud into a canonical coordinate system in which the global orientations of the transformed hand point clouds are as consistent as possible. <font color="red">normalization step ensures that our method is robust to variations in hand global orientations</font></p>
<p><img src="https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211002170627689.png" alt=""></p>
<blockquote>
<p><code>each column corresponds to the same local region</code>, and <code>each row correspnd to the same filter.</code>Following pictures show the sensitivity of points in three loacl regions to two fitlers at each of the first two levels.</p>
</blockquote>
<p><img src="https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211002170941689.png" alt="image-20211002170941689"></p>
<p>【<strong>Refine the Fingertip</strong>】</p>
<p>Based on the obervation: the fingertip location of straightened finger is usually easy to be fined, since K nearest neighboring points of the fingertip will not change a lot even if the estimated location deviates from the ground truth location to some extent when <font color="red">K is relatively large</font> </p>
<p><img src="https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211002171012560.png" alt=""></p>
<h4 id="Conclusion-4"><a href="#Conclusion-4" class="headerlink" title="Conclusion"></a>Conclusion</h4><ul>
<li>estimate 3D hand joint locations directly from 3D point cloud base on the netword architecture of PointNet. better expoit the 3D spatial information in the depth image</li>
<li>robust to variations in hand global orientations, normalize the sampled 3D points in an oriented bounding box without applying any additional network to transform the hand piont cloud.     </li>
<li>refine the fingertip locations with a basic PointNet that takes the Neighboring points of the estimation fingertip location as input to regress the refined fingertip location.</li>
</ul>
<h4 id="Notes-去加强了解-3"><a href="#Notes-去加强了解-3" class="headerlink" title="Notes 去加强了解"></a>Notes <font color="orange">去加强了解</font></h4><h5 id="1-最远点采样"><a href="#1-最远点采样" class="headerlink" title="1. 最远点采样"></a>1. 最远点采样</h5><blockquote>
<p>最远点采样(Farthest Point Sampling)是一种非常常用的采样算法，由于能够保证对样本的均匀采样，被广泛使用，像3D点云深度学习框架中的PointNet++对样本点进行FPS采样再聚类作为感受野，3D目标检测网络VoteNet对投票得到的散乱点进行FPS采样再进行聚类，6D位姿估计算法PVN3D中用于选择物体的8个特征点进行投票并计算位姿。</p>
<ol>
<li>输入点云有N个点，从点云中选取一个点P0作为起始点，得到采样点集合S={P0}；</li>
<li>计算所有点到P0的距离，构成N维数组L，从中选择最大值对应的点作为P1，更新采样点集合S={P0，P1}；</li>
<li>计算所有点到P1的距离，对于每一个点Pi，其距离P1的距离如果小于L[i]，则更新L[i] = d(Pi, P1)，因此，数组L中存储的一直是每一个点到采样点集合S的最近距离；</li>
<li>选取L中最大值对应的点作为P2，更新采样点集合S={P0，P1，P2}；</li>
<li>重复2-4步，一直采样到N’个目标采样点为止。</li>
</ol>
</blockquote>
<blockquote>
<ul>
<li>初始点选择：<ul>
<li>随机选择一个点，每次结果不同；</li>
<li>选择距离点云重心的最远点，每次结果相同，一般位于局部极值点，具有刻画能力；</li>
</ul>
</li>
<li>距离度量<ul>
<li>欧氏距离：主要对于点云，在3D体空间均匀采样；</li>
<li>测地距离：主要对于三角网格，在三角网格面上进行均匀采样；</li>
</ul>
</li>
</ul>
</blockquote>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">farthest_point_sample</span><span class="params">(xyz, npoint)</span>:</span> </span><br><span class="line"></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Input:</span></span><br><span class="line"><span class="string">        xyz: pointcloud data, [B, N, 3]</span></span><br><span class="line"><span class="string">        npoint: number of samples</span></span><br><span class="line"><span class="string">    Return:</span></span><br><span class="line"><span class="string">        centroids: sampled pointcloud index, [B, npoint]</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    xyz = xyz.transpose(<span class="number">2</span>,<span class="number">1</span>)</span><br><span class="line">    device = xyz.device</span><br><span class="line">    B, N, C = xyz.shape</span><br><span class="line">    </span><br><span class="line">    centroids = torch.zeros(B, npoint, dtype=torch.long).to(device)     <span class="comment"># 采样点矩阵（B, npoint）</span></span><br><span class="line">    distance = torch.ones(B, N).to(device) * <span class="number">1e10</span>                       <span class="comment"># 采样点到所有点距离（B, N）</span></span><br><span class="line"></span><br><span class="line">    batch_indices = torch.arange(B, dtype=torch.long).to(device)        <span class="comment"># batch_size 数组</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#farthest = torch.randint(0, N, (B,), dtype=torch.long).to(device)  # 初始时随机选择一点</span></span><br><span class="line">    </span><br><span class="line">    barycenter = torch.sum((xyz), <span class="number">1</span>)                                    <span class="comment">#计算重心坐标 及 距离重心最远的点</span></span><br><span class="line">    barycenter = barycenter/xyz.shape[<span class="number">1</span>]</span><br><span class="line">    barycenter = barycenter.view(B, <span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">    dist = torch.sum((xyz - barycenter) ** <span class="number">2</span>, <span class="number">-1</span>)</span><br><span class="line">    farthest = torch.max(dist,<span class="number">1</span>)[<span class="number">1</span>]                                     <span class="comment">#将距离重心最远的点作为第一个点</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(npoint):</span><br><span class="line">        print(<span class="string">"-------------------------------------------------------"</span>)</span><br><span class="line">        print(<span class="string">"The %d farthest pts %s "</span> % (i, farthest))</span><br><span class="line">        centroids[:, i] = farthest                                      <span class="comment"># 更新第i个最远点</span></span><br><span class="line">        centroid = xyz[batch_indices, farthest, :].view(B, <span class="number">1</span>, <span class="number">3</span>)        <span class="comment"># 取出这个最远点的xyz坐标</span></span><br><span class="line">        dist = torch.sum((xyz - centroid) ** <span class="number">2</span>, <span class="number">-1</span>)                     <span class="comment"># 计算点集中的所有点到这个最远点的欧式距离</span></span><br><span class="line">        print(<span class="string">"dist    : "</span>, dist)</span><br><span class="line">        mask = dist &lt; distance</span><br><span class="line">        print(<span class="string">"mask %i : %s"</span> % (i,mask))</span><br><span class="line">        distance[mask] = dist[mask]                                     <span class="comment"># 更新distance，记录样本中每个点距离所有已出现的采样点的最小距离</span></span><br><span class="line">        print(<span class="string">"distance: "</span>, distance)</span><br><span class="line"></span><br><span class="line">        farthest = torch.max(distance, <span class="number">-1</span>)[<span class="number">1</span>]                           <span class="comment"># 返回最远点索引</span></span><br><span class="line"> </span><br><span class="line">    <span class="keyword">return</span> centroids</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line"></span><br><span class="line">    sim_data = Variable(torch.rand(<span class="number">1</span>,<span class="number">3</span>,<span class="number">8</span>))</span><br><span class="line">    print(sim_data)</span><br><span class="line"></span><br><span class="line">    centroids = farthest_point_sample(sim_data, <span class="number">4</span>)</span><br><span class="line">    </span><br><span class="line">    print(<span class="string">"Sampled pts: "</span>, centroids)</span><br></pre></td></tr></tbody></table></figure>

<h5 id="2-PointNet网络结构"><a href="#2-PointNet网络结构" class="headerlink" title="2. PointNet网络结构"></a>2. <a href="https://github.com/yanx27/Pointnet_Pointnet2_pytorch" target="_blank" rel="noopener">PointNet网络结构</a></h5><blockquote>
<p>数据集中每一行是六个点，及每一点有六个特征（3d坐标，法向量）normal意思是法向量，可以自己设置，要不要使用法向量，使用的话初始输入的点云数据除了3个位置信息x，y，z以外还有三个法向量Nx，Ny，Nz，每个点一共是6个特征。</p>
</blockquote>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PointNetEncoder</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, global_feat=True, feature_transform=False, channel=<span class="number">3</span>)</span>:</span></span><br><span class="line">        super(PointNetEncoder, self).__init__()</span><br><span class="line">        self.stn = STN3d(channel)</span><br><span class="line">        self.conv1 = torch.nn.Conv1d(channel, <span class="number">64</span>, <span class="number">1</span>)</span><br><span class="line">        self.conv2 = torch.nn.Conv1d(<span class="number">64</span>, <span class="number">128</span>, <span class="number">1</span>)</span><br><span class="line">        self.conv3 = torch.nn.Conv1d(<span class="number">128</span>, <span class="number">1024</span>, <span class="number">1</span>)</span><br><span class="line">        self.bn1 = nn.BatchNorm1d(<span class="number">64</span>)</span><br><span class="line">        self.bn2 = nn.BatchNorm1d(<span class="number">128</span>)</span><br><span class="line">        self.bn3 = nn.BatchNorm1d(<span class="number">1024</span>)</span><br><span class="line">        self.global_feat = global_feat</span><br><span class="line">        self.feature_transform = feature_transform</span><br><span class="line">        <span class="keyword">if</span> self.feature_transform:</span><br><span class="line">            self.fstn = STNkd(k=<span class="number">64</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        B, D, N = x.size()</span><br><span class="line">        trans = self.stn(x)</span><br><span class="line">        x = x.transpose(<span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">if</span> D &gt; <span class="number">3</span>:</span><br><span class="line">            feature = x[:, :, <span class="number">3</span>:]</span><br><span class="line">            x = x[:, :, :<span class="number">3</span>]</span><br><span class="line">        x = torch.bmm(x, trans)</span><br><span class="line">        <span class="keyword">if</span> D &gt; <span class="number">3</span>:</span><br><span class="line">            x = torch.cat([x, feature], dim=<span class="number">2</span>)</span><br><span class="line">        x = x.transpose(<span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">        x = F.relu(self.bn1(self.conv1(x)))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.feature_transform:</span><br><span class="line">            trans_feat = self.fstn(x)</span><br><span class="line">            x = x.transpose(<span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">            x = torch.bmm(x, trans_feat)</span><br><span class="line">            x = x.transpose(<span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            trans_feat = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        pointfeat = x</span><br><span class="line">        x = F.relu(self.bn2(self.conv2(x)))</span><br><span class="line">        x = self.bn3(self.conv3(x))</span><br><span class="line">        x = torch.max(x, <span class="number">2</span>, keepdim=<span class="literal">True</span>)[<span class="number">0</span>]</span><br><span class="line">        x = x.view(<span class="number">-1</span>, <span class="number">1024</span>)</span><br><span class="line">        <span class="keyword">if</span> self.global_feat:</span><br><span class="line">            <span class="keyword">return</span> x, trans, trans_feat</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            x = x.view(<span class="number">-1</span>, <span class="number">1024</span>, <span class="number">1</span>).repeat(<span class="number">1</span>, <span class="number">1</span>, N)</span><br><span class="line">            <span class="keyword">return</span> torch.cat([x, pointfeat], <span class="number">1</span>), trans, trans_feat</span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PointNetCls</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, k = <span class="number">2</span>)</span>:</span></span><br><span class="line">        super(PointNetCls, self).__init__()</span><br><span class="line">        self.k = k</span><br><span class="line">        self.feat = PointNetEncoder(global_feat=<span class="literal">False</span>)</span><br><span class="line">        self.conv1 = torch.nn.Conv1d(<span class="number">1088</span>, <span class="number">512</span>, <span class="number">1</span>)</span><br><span class="line">        self.conv2 = torch.nn.Conv1d(<span class="number">512</span>, <span class="number">256</span>, <span class="number">1</span>)</span><br><span class="line">        self.conv3 = torch.nn.Conv1d(<span class="number">256</span>, <span class="number">128</span>, <span class="number">1</span>)</span><br><span class="line">        self.conv4 = torch.nn.Conv1d(<span class="number">128</span>, self.k, <span class="number">1</span>)</span><br><span class="line">        self.bn1 = nn.BatchNorm1d(<span class="number">512</span>)</span><br><span class="line">        self.bn2 = nn.BatchNorm1d(<span class="number">256</span>)</span><br><span class="line">        self.bn3 = nn.BatchNorm1d(<span class="number">128</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">    	<span class="string">'''分类网络'''</span></span><br><span class="line">        batchsize = x.size()[<span class="number">0</span>]</span><br><span class="line">        n_pts = x.size()[<span class="number">2</span>]</span><br><span class="line">        x, trans = self.feat(x)</span><br><span class="line">        x = F.relu(self.bn1(self.conv1(x)))</span><br><span class="line">        x = F.relu(self.bn2(self.conv2(x)))</span><br><span class="line">        x = F.relu(self.bn3(self.conv3(x)))</span><br><span class="line">        x = self.conv4(x)</span><br><span class="line">        x = x.transpose(<span class="number">2</span>,<span class="number">1</span>).contiguous()</span><br><span class="line">        x = F.log_softmax(x.view(<span class="number">-1</span>,self.k), dim=<span class="number">-1</span>)</span><br><span class="line">        x = x.view(batchsize, n_pts, self.k)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PointNetPartSeg</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,num_class)</span>:</span></span><br><span class="line">        super(PointNetPartSeg, self).__init__()</span><br><span class="line">        self.k = num_class</span><br><span class="line">        self.feat = PointNetEncoder(global_feat=<span class="literal">False</span>)</span><br><span class="line">        self.conv1 = torch.nn.Conv1d(<span class="number">1088</span>, <span class="number">512</span>, <span class="number">1</span>)</span><br><span class="line">        self.conv2 = torch.nn.Conv1d(<span class="number">512</span>, <span class="number">256</span>, <span class="number">1</span>)</span><br><span class="line">        self.conv3 = torch.nn.Conv1d(<span class="number">256</span>, <span class="number">128</span>, <span class="number">1</span>)</span><br><span class="line">        self.conv4 = torch.nn.Conv1d(<span class="number">128</span>, self.k, <span class="number">1</span>)</span><br><span class="line">        self.bn1 = nn.BatchNorm1d(<span class="number">512</span>)</span><br><span class="line">        self.bn1_1 = nn.BatchNorm1d(<span class="number">1024</span>)</span><br><span class="line">        self.bn2 = nn.BatchNorm1d(<span class="number">256</span>)</span><br><span class="line">        self.bn3 = nn.BatchNorm1d(<span class="number">128</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="string">'''分割网络'''</span></span><br><span class="line">        batchsize = x.size()[<span class="number">0</span>]</span><br><span class="line">        n_pts = x.size()[<span class="number">2</span>]</span><br><span class="line">        x, trans = self.feat(x)</span><br><span class="line">        x = F.relu(self.bn1(self.conv1(x)))</span><br><span class="line">        x = F.relu(self.bn2(self.conv2(x)))</span><br><span class="line">        x = F.relu(self.bn3(self.conv3(x)))</span><br><span class="line">        x = self.conv4(x)</span><br><span class="line">        x = x.transpose(<span class="number">2</span>,<span class="number">1</span>).contiguous()</span><br><span class="line">        x = F.log_softmax(x.view(<span class="number">-1</span>,self.k), dim=<span class="number">-1</span>)</span><br><span class="line">        x = x.view(batchsize, n_pts, self.k)</span><br><span class="line">        <span class="keyword">return</span> x, trans</span><br></pre></td></tr></tbody></table></figure>

<blockquote>
<p>通过引入了<strong>不同分辨率/尺度的Grouping</strong>去对局部做PointNet求局部的全局特征，最后再将不同尺度的特征拼接起来；同时也通过<strong>在训练的时候随机删除一部分的点</strong>来增加模型的缺失鲁棒性。 –&gt;解决点稀疏问题</p>
</blockquote>
<h5 id="3-PointNet-网络结构"><a href="#3-PointNet-网络结构" class="headerlink" title="3. PointNet++ 网络结构"></a>3. <a href="https://github.com/yanx27/Pointnet_Pointnet2_pytorch" target="_blank" rel="noopener">PointNet++ 网络结构</a></h5><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> pointnet2_utils <span class="keyword">import</span> PointNetSetAbstraction</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">get_model</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,num_class,normal_channel=True)</span>:</span></span><br><span class="line">        super(get_model, self).__init__()</span><br><span class="line">        in_channel = <span class="number">6</span> <span class="keyword">if</span> normal_channel <span class="keyword">else</span> <span class="number">3</span></span><br><span class="line">        self.normal_channel = normal_channel</span><br><span class="line">        self.sa1 = PointNetSetAbstraction(npoint=<span class="number">512</span>, radius=<span class="number">0.2</span>, nsample=<span class="number">32</span>, in_channel=in_channel, mlp=[<span class="number">64</span>, <span class="number">64</span>, <span class="number">128</span>], group_all=<span class="literal">False</span>)</span><br><span class="line">        self.sa2 = PointNetSetAbstraction(npoint=<span class="number">128</span>, radius=<span class="number">0.4</span>, nsample=<span class="number">64</span>, in_channel=<span class="number">128</span> + <span class="number">3</span>, mlp=[<span class="number">128</span>, <span class="number">128</span>, <span class="number">256</span>], group_all=<span class="literal">False</span>)</span><br><span class="line">        self.sa3 = PointNetSetAbstraction(npoint=<span class="literal">None</span>, radius=<span class="literal">None</span>, nsample=<span class="literal">None</span>, in_channel=<span class="number">256</span> + <span class="number">3</span>, mlp=[<span class="number">256</span>, <span class="number">512</span>, <span class="number">1024</span>], group_all=<span class="literal">True</span>)</span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">1024</span>, <span class="number">512</span>)</span><br><span class="line">        self.bn1 = nn.BatchNorm1d(<span class="number">512</span>)</span><br><span class="line">        self.drop1 = nn.Dropout(<span class="number">0.4</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">512</span>, <span class="number">256</span>)</span><br><span class="line">        self.bn2 = nn.BatchNorm1d(<span class="number">256</span>)</span><br><span class="line">        self.drop2 = nn.Dropout(<span class="number">0.4</span>)</span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">256</span>, num_class)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, xyz)</span>:</span></span><br><span class="line">        B, _, _ = xyz.shape</span><br><span class="line">        print(<span class="string">"xyz.shape"</span>,xyz.shape)</span><br><span class="line">        <span class="keyword">if</span> self.normal_channel:</span><br><span class="line">            norm = xyz[:, <span class="number">3</span>:, :]</span><br><span class="line">            xyz = xyz[:, :<span class="number">3</span>, :]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            norm = <span class="literal">None</span></span><br><span class="line">        l1_xyz, l1_points = self.sa1(xyz, norm)</span><br><span class="line">        l2_xyz, l2_points = self.sa2(l1_xyz, l1_points)</span><br><span class="line">        l3_xyz, l3_points = self.sa3(l2_xyz, l2_points)</span><br><span class="line">        x = l3_points.view(B, <span class="number">1024</span>)</span><br><span class="line">        x = self.drop1(F.relu(self.bn1(self.fc1(x))))</span><br><span class="line">        x = self.drop2(F.relu(self.bn2(self.fc2(x))))</span><br><span class="line">        x = self.fc3(x)</span><br><span class="line">        x = F.log_softmax(x, <span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x, l3_points</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">get_loss</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(get_loss, self).__init__()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, pred, target, trans_feat)</span>:</span></span><br><span class="line">        total_loss = F.nll_loss(pred, target)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> total_loss</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    data = torch.ones([<span class="number">24</span>,<span class="number">3</span>,<span class="number">1024</span>])</span><br><span class="line">    print(data.shape)</span><br><span class="line">    model = get_model(num_class=<span class="number">40</span>,normal_channel=<span class="literal">False</span>)</span><br><span class="line">    print(model)</span><br><span class="line">    parameters = filter(<span class="keyword">lambda</span> p: p.requires_grad, model.parameters())</span><br><span class="line">    parameters = sum([np.prod(p.size()) <span class="keyword">for</span> p <span class="keyword">in</span> parameters]) / <span class="number">1</span>_000_000</span><br><span class="line">    print(<span class="string">'Trainable Parameters: %.3fM'</span> % parameters)</span><br><span class="line">    pred, trans_feat  = model(data)</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"Shape of out :"</span>, pred.shape)  <span class="comment"># [10,30,10]</span></span><br></pre></td></tr></tbody></table></figure>



<figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">get_model(</span><br><span class="line">  (sa1): PointNetSetAbstraction(</span><br><span class="line">    (mlp_convs): ModuleList(</span><br><span class="line">      (0): Conv2d(3, 64, kernel_size=(1, 1), stride=(1, 1))</span><br><span class="line">      (1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))</span><br><span class="line">      (2): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))</span><br><span class="line">    )</span><br><span class="line">    (mlp_bns): ModuleList(</span><br><span class="line">      (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">  (sa2): PointNetSetAbstraction(</span><br><span class="line">    (mlp_convs): ModuleList(</span><br><span class="line">      (0): Conv2d(131, 128, kernel_size=(1, 1), stride=(1, 1))</span><br><span class="line">      (1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))</span><br><span class="line">      (2): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))</span><br><span class="line">    )</span><br><span class="line">    (mlp_bns): ModuleList(</span><br><span class="line">      (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">  (sa3): PointNetSetAbstraction(</span><br><span class="line">    (mlp_convs): ModuleList(</span><br><span class="line">      (0): Conv2d(259, 256, kernel_size=(1, 1), stride=(1, 1))</span><br><span class="line">      (1): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))</span><br><span class="line">      (2): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1))</span><br><span class="line">    )</span><br><span class="line">    (mlp_bns): ModuleList(</span><br><span class="line">      (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">      (2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">  (fc1): Linear(in_features=1024, out_features=512, bias=True)</span><br><span class="line">  (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">  (drop1): Dropout(p=0.4, inplace=False)</span><br><span class="line">  (fc2): Linear(in_features=512, out_features=256, bias=True)</span><br><span class="line">  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">  (drop2): Dropout(p=0.4, inplace=False)</span><br><span class="line">  (fc3): Linear(in_features=256, out_features=40, bias=True)</span><br><span class="line">)</span><br></pre></td></tr></tbody></table></figure>

<h5 id="4-HandPointNet-代码阅读"><a href="#4-HandPointNet-代码阅读" class="headerlink" title="4. HandPointNet 代码阅读"></a>4. HandPointNet 代码阅读</h5><ul>
<li><strong>数据处理部分</strong></li>
</ul>
<figure class="highlight matlab"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">% create point cloud from depth image</span></span><br><span class="line"><span class="comment">% author: Liuhao Ge</span></span><br><span class="line"></span><br><span class="line">clc;clear;close all;</span><br><span class="line"><span class="comment">%使用 fread，文件标识符无效。使用 fopen 生成有效的文件标识符。 这个错误是文件路径不对。</span></span><br><span class="line">dataset_dir=<span class="string">'C:\Users\liudongdong\OneDrive - tju.edu.cn\桌面\HandPointNet\data\cvpr15_MSRAHandGestureDB\';%'</span>../data/cvpr15_MSRAHandGestureDB/<span class="string">'</span></span><br><span class="line"><span class="string">save_dir='</span>./<span class="string">';</span></span><br><span class="line"><span class="string">subject_names={'</span>P0',<span class="string">'P1'</span>,<span class="string">'P2'</span>,<span class="string">'P3'</span>,<span class="string">'P4'</span>,<span class="string">'P5'</span>,<span class="string">'P6'</span>,<span class="string">'P7'</span>,<span class="string">'P8'</span>};</span><br><span class="line"><span class="comment">%subject_names={'P0'};</span></span><br><span class="line"><span class="comment">%gesture_names={'1'};</span></span><br><span class="line">gesture_names={<span class="string">'1'</span>,<span class="string">'2'</span>,<span class="string">'3'</span>,<span class="string">'4'</span>,<span class="string">'5'</span>,<span class="string">'6'</span>,<span class="string">'7'</span>,<span class="string">'8'</span>,<span class="string">'9'</span>,<span class="string">'I'</span>,<span class="string">'IP'</span>,<span class="string">'L'</span>,<span class="string">'MP'</span>,<span class="string">'RP'</span>,<span class="string">'T'</span>,<span class="string">'TIP'</span>,<span class="string">'Y'</span>};</span><br><span class="line"></span><br><span class="line">JOINT_NUM = <span class="number">21</span>;</span><br><span class="line">SAMPLE_NUM = <span class="number">1024</span>;</span><br><span class="line">sample_num_level1 = <span class="number">512</span>;</span><br><span class="line">sample_num_level2 = <span class="number">128</span>;</span><br><span class="line"></span><br><span class="line">load(<span class="string">'msra_valid.mat'</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> sub_idx = <span class="number">1</span>:<span class="built_in">length</span>(subject_names)</span><br><span class="line">    mkdir([save_dir subject_names{sub_idx}]);</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> ges_idx = <span class="number">1</span>:<span class="built_in">length</span>(gesture_names)</span><br><span class="line">        gesture_dir = [dataset_dir subject_names{sub_idx} <span class="string">'/'</span> gesture_names{ges_idx}];</span><br><span class="line">        depth_files = dir([gesture_dir, <span class="string">'/*.bin'</span>]);</span><br><span class="line">        </span><br><span class="line">        <span class="comment">% 1. read ground truth</span></span><br><span class="line">        fileID = fopen([gesture_dir <span class="string">'/joint.txt'</span>]);</span><br><span class="line">        </span><br><span class="line">        frame_num = fscanf(fileID,<span class="string">'%d'</span>,<span class="number">1</span>);    <span class="comment">% 读取帧的个数</span></span><br><span class="line">        A = fscanf(fileID,<span class="string">'%f'</span>, frame_num*<span class="number">21</span>*<span class="number">3</span>);   <span class="comment">% 读取所有帧的关键点数据</span></span><br><span class="line">        gt_wld=<span class="built_in">reshape</span>(A,[<span class="number">3</span>,<span class="number">21</span>,frame_num]);     <span class="comment">% 数据reshape操作</span></span><br><span class="line">        gt_wld(<span class="number">3</span>,:,:) = -gt_wld(<span class="number">3</span>,:,:);</span><br><span class="line">        gt_wld=<span class="built_in">permute</span>(gt_wld, [<span class="number">3</span> <span class="number">2</span> <span class="number">1</span>]);</span><br><span class="line">        </span><br><span class="line">        fclose(fileID);</span><br><span class="line">        </span><br><span class="line">        <span class="comment">% 2. get point cloud and surface normal</span></span><br><span class="line">        save_gesture_dir = [save_dir subject_names{sub_idx} <span class="string">'/'</span> gesture_names{ges_idx}];  <span class="comment">%matlab 文件拼接</span></span><br><span class="line">        mkdir(save_gesture_dir);    <span class="comment">%创建存储的路径文件</span></span><br><span class="line">        </span><br><span class="line">        display(save_gesture_dir);    <span class="comment">%显示变量的信息</span></span><br><span class="line">        </span><br><span class="line">        Point_Cloud_FPS = <span class="built_in">zeros</span>(frame_num,SAMPLE_NUM,<span class="number">6</span>);</span><br><span class="line">        Volume_rotate = <span class="built_in">zeros</span>(frame_num,<span class="number">3</span>,<span class="number">3</span>);</span><br><span class="line">        Volume_length = <span class="built_in">zeros</span>(frame_num,<span class="number">1</span>);</span><br><span class="line">        Volume_offset = <span class="built_in">zeros</span>(frame_num,<span class="number">3</span>);</span><br><span class="line">        Volume_GT_XYZ = <span class="built_in">zeros</span>(frame_num,JOINT_NUM,<span class="number">3</span>);</span><br><span class="line">        valid = msra_valid{sub_idx, ges_idx};</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> frm_idx = <span class="number">1</span>:<span class="built_in">length</span>(depth_files)</span><br><span class="line">            <span class="keyword">if</span> ~valid(frm_idx)                 <span class="comment">%valid 数组主要用于判断这个数据帧是不是有效的</span></span><br><span class="line">                <span class="keyword">continue</span>;</span><br><span class="line">            <span class="keyword">end</span></span><br><span class="line">            <span class="comment">%% 2.1 read binary file</span></span><br><span class="line">            fileID = fopen([gesture_dir <span class="string">'/'</span> num2str(frm_idx<span class="number">-1</span>,<span class="string">'%06d'</span>), <span class="string">'_depth.bin'</span>]);   <span class="comment">%num2str(id,'%06d')  文件数据格式</span></span><br><span class="line">            img_width = fread(fileID,<span class="number">1</span>,<span class="string">'int32'</span>);</span><br><span class="line">            img_height = fread(fileID,<span class="number">1</span>,<span class="string">'int32'</span>);</span><br><span class="line"></span><br><span class="line">            bb_left = fread(fileID,<span class="number">1</span>,<span class="string">'int32'</span>);</span><br><span class="line">            bb_top = fread(fileID,<span class="number">1</span>,<span class="string">'int32'</span>);</span><br><span class="line">            bb_right = fread(fileID,<span class="number">1</span>,<span class="string">'int32'</span>);</span><br><span class="line">            bb_bottom = fread(fileID,<span class="number">1</span>,<span class="string">'int32'</span>);</span><br><span class="line">            bb_width = bb_right - bb_left;</span><br><span class="line">            bb_height = bb_bottom - bb_top;</span><br><span class="line"></span><br><span class="line">            valid_pixel_num = bb_width*bb_height;</span><br><span class="line"></span><br><span class="line">            hand_depth = fread(fileID,[bb_width, bb_height],<span class="string">'float32'</span>);     <span class="comment">%读取手部区域有效的深度信息</span></span><br><span class="line">            hand_depth = hand_depth';</span><br><span class="line">            </span><br><span class="line">            fclose(fileID);</span><br><span class="line">            </span><br><span class="line">            <span class="comment">%% 2.2 convert depth to xyz</span></span><br><span class="line">            fFocal_MSRA_ = <span class="number">241.42</span>;	<span class="comment">% mm</span></span><br><span class="line">            hand_3d = <span class="built_in">zeros</span>(valid_pixel_num,<span class="number">3</span>);</span><br><span class="line">            <span class="keyword">for</span> ii=<span class="number">1</span>:bb_height</span><br><span class="line">                <span class="keyword">for</span> jj=<span class="number">1</span>:bb_width</span><br><span class="line">                    idx = (jj<span class="number">-1</span>)*bb_height+ii;      <span class="comment">% 手部区域深度图中每一个像素索引，按列优先</span></span><br><span class="line">                    hand_3d(idx, <span class="number">1</span>) = -(img_width/<span class="number">2</span> - (jj+bb_left<span class="number">-1</span>))*hand_depth(ii,jj)/fFocal_MSRA_;</span><br><span class="line">                    hand_3d(idx, <span class="number">2</span>) = (img_height/<span class="number">2</span> - (ii+bb_top<span class="number">-1</span>))*hand_depth(ii,jj)/fFocal_MSRA_;</span><br><span class="line">                    hand_3d(idx, <span class="number">3</span>) = hand_depth(ii,jj);     <span class="comment">% 深度距离值，   这个真实的z应该  是x*x+y*y+z*z=d*d  ??</span></span><br><span class="line">                <span class="keyword">end</span></span><br><span class="line">            <span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">            valid_idx = <span class="number">1</span>:valid_pixel_num;</span><br><span class="line">            valid_idx = valid_idx(hand_3d(:,<span class="number">1</span>)~=<span class="number">0</span> | hand_3d(:,<span class="number">2</span>)~=<span class="number">0</span> | hand_3d(:,<span class="number">3</span>)~=<span class="number">0</span>);</span><br><span class="line">            hand_points = hand_3d(valid_idx,:);             <span class="comment">%过滤无效的数据</span></span><br><span class="line"></span><br><span class="line">            jnt_xyz = <span class="built_in">squeeze</span>(gt_wld(frm_idx,:,:));</span><br><span class="line">            </span><br><span class="line">            <span class="comment">%% 2.3 create OBB</span></span><br><span class="line">            [coeff,score,latent] = pca(hand_points);   <span class="comment">%coeff = pca(X) 返回 n×p 数据矩阵 X 的主成分系数，也称为载荷。X 的行对应于观测值，列对应于变量。</span></span><br><span class="line">                                              <span class="comment">%系数矩阵是 p×p 矩阵。coeff 的每列包含一个主成分的系数，并且这些列按成分方差的降序排列。默认情况下，pca 将数据中心化，并使用奇异值分解 (SVD) 算法。</span></span><br><span class="line">            <span class="keyword">if</span> coeff(<span class="number">2</span>,<span class="number">1</span>)&lt;<span class="number">0</span></span><br><span class="line">                coeff(:,<span class="number">1</span>) = -coeff(:,<span class="number">1</span>);</span><br><span class="line">            <span class="keyword">end</span></span><br><span class="line">            <span class="keyword">if</span> coeff(<span class="number">3</span>,<span class="number">3</span>)&lt;<span class="number">0</span></span><br><span class="line">                coeff(:,<span class="number">3</span>) = -coeff(:,<span class="number">3</span>);</span><br><span class="line">            <span class="keyword">end</span></span><br><span class="line">            coeff(:,<span class="number">2</span>)=<span class="built_in">cross</span>(coeff(:,<span class="number">3</span>),coeff(:,<span class="number">1</span>));   <span class="comment">% 这里几步不太明白作用？</span></span><br><span class="line"></span><br><span class="line">            ptCloud = pointCloud(hand_points);</span><br><span class="line"></span><br><span class="line">            hand_points_rotate = hand_points*coeff;    <span class="comment">%类似归一化处理，是的bounding box 的朝向基本一致</span></span><br><span class="line"></span><br><span class="line">            <span class="comment">%% 2.4 sampling                        %数据少的时候只是在原有的点基础上重复使用了一些点，  这里不知道可不可以直接使用</span></span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">size</span>(hand_points,<span class="number">1</span>)&lt;SAMPLE_NUM</span><br><span class="line">                tmp = <span class="built_in">floor</span>(SAMPLE_NUM/<span class="built_in">size</span>(hand_points,<span class="number">1</span>));</span><br><span class="line">                rand_ind = [];</span><br><span class="line">                <span class="keyword">for</span> tmp_i = <span class="number">1</span>:tmp</span><br><span class="line">                    rand_ind = [rand_ind <span class="number">1</span>:<span class="built_in">size</span>(hand_points,<span class="number">1</span>)];</span><br><span class="line">                <span class="keyword">end</span></span><br><span class="line">                rand_ind = [rand_ind randperm(<span class="built_in">size</span>(hand_points,<span class="number">1</span>), <span class="built_in">mod</span>(SAMPLE_NUM, <span class="built_in">size</span>(hand_points,<span class="number">1</span>)))];  <span class="comment">%返回行向量，其中包含在 1 到 size(hand_points,1) 之间随机选择的 k 个唯一整数。  </span></span><br><span class="line">            <span class="keyword">else</span></span><br><span class="line">                rand_ind = randperm(<span class="built_in">size</span>(hand_points,<span class="number">1</span>),SAMPLE_NUM);</span><br><span class="line">            <span class="keyword">end</span></span><br><span class="line">            hand_points_sampled = hand_points(rand_ind,:);</span><br><span class="line">            hand_points_rotate_sampled = hand_points_rotate(rand_ind,:);</span><br><span class="line">            </span><br><span class="line">            <span class="comment">%% 2.5 compute surface normal</span></span><br><span class="line">            normal_k = <span class="number">30</span>;</span><br><span class="line">            normals = pcnormals(ptCloud, normal_k);</span><br><span class="line">            normals_sampled = normals(rand_ind,:);</span><br><span class="line"></span><br><span class="line">            sensorCenter = [<span class="number">0</span> <span class="number">0</span> <span class="number">0</span>];</span><br><span class="line">            <span class="keyword">for</span> k = <span class="number">1</span> : SAMPLE_NUM</span><br><span class="line">               p1 = sensorCenter - hand_points_sampled(k,:);</span><br><span class="line">               <span class="comment">% Flip the normal vector if it is not pointing towards the sensor.</span></span><br><span class="line">               <span class="built_in">angle</span> = <span class="built_in">atan2</span>(norm(<span class="built_in">cross</span>(p1,normals_sampled(k,:))),p1*normals_sampled(k,:)');</span><br><span class="line">               <span class="keyword">if</span> <span class="built_in">angle</span> &gt; <span class="built_in">pi</span>/<span class="number">2</span> || <span class="built_in">angle</span> &lt; -<span class="built_in">pi</span>/<span class="number">2</span></span><br><span class="line">                   normals_sampled(k,:) = -normals_sampled(k,:);</span><br><span class="line">               <span class="keyword">end</span></span><br><span class="line">            <span class="keyword">end</span></span><br><span class="line">            normals_sampled_rotate = normals_sampled*coeff;</span><br><span class="line"></span><br><span class="line">            <span class="comment">%% 2.6 Normalize Point Cloud    %通过每一轴的最值*scale进行 缩放处理</span></span><br><span class="line">            x_min_max = [<span class="built_in">min</span>(hand_points_rotate(:,<span class="number">1</span>)), <span class="built_in">max</span>(hand_points_rotate(:,<span class="number">1</span>))];</span><br><span class="line">            y_min_max = [<span class="built_in">min</span>(hand_points_rotate(:,<span class="number">2</span>)), <span class="built_in">max</span>(hand_points_rotate(:,<span class="number">2</span>))];</span><br><span class="line">            z_min_max = [<span class="built_in">min</span>(hand_points_rotate(:,<span class="number">3</span>)), <span class="built_in">max</span>(hand_points_rotate(:,<span class="number">3</span>))];</span><br><span class="line"></span><br><span class="line">            scale = <span class="number">1.2</span>;</span><br><span class="line">            bb3d_x_len = scale*(x_min_max(<span class="number">2</span>)-x_min_max(<span class="number">1</span>));</span><br><span class="line">            bb3d_y_len = scale*(y_min_max(<span class="number">2</span>)-y_min_max(<span class="number">1</span>));</span><br><span class="line">            bb3d_z_len = scale*(z_min_max(<span class="number">2</span>)-z_min_max(<span class="number">1</span>));</span><br><span class="line">            max_bb3d_len = bb3d_x_len;</span><br><span class="line"></span><br><span class="line">            hand_points_normalized_sampled = hand_points_rotate_sampled/max_bb3d_len;</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">size</span>(hand_points,<span class="number">1</span>)&lt;SAMPLE_NUM</span><br><span class="line">                offset = <span class="built_in">mean</span>(hand_points_rotate)/max_bb3d_len;</span><br><span class="line">            <span class="keyword">else</span></span><br><span class="line">                offset = <span class="built_in">mean</span>(hand_points_normalized_sampled);</span><br><span class="line">            <span class="keyword">end</span></span><br><span class="line">            hand_points_normalized_sampled = hand_points_normalized_sampled - <span class="built_in">repmat</span>(offset,SAMPLE_NUM,<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">            <span class="comment">%% 2.7 FPS Sampling</span></span><br><span class="line">            pc = [hand_points_normalized_sampled normals_sampled_rotate];</span><br><span class="line">            <span class="comment">% 1st level</span></span><br><span class="line">            sampled_idx_l1 = farthest_point_sampling_fast(hand_points_normalized_sampled, sample_num_level1)';</span><br><span class="line">            other_idx = setdiff(<span class="number">1</span>:SAMPLE_NUM, sampled_idx_l1);</span><br><span class="line">            new_idx = [sampled_idx_l1 other_idx];</span><br><span class="line">            pc = pc(new_idx,:);</span><br><span class="line">            <span class="comment">% 2nd level</span></span><br><span class="line">            sampled_idx_l2 = farthest_point_sampling_fast(pc(<span class="number">1</span>:sample_num_level1,<span class="number">1</span>:<span class="number">3</span>), sample_num_level2)';</span><br><span class="line">            other_idx = setdiff(<span class="number">1</span>:sample_num_level1, sampled_idx_l2);</span><br><span class="line">            new_idx = [sampled_idx_l2 other_idx];</span><br><span class="line">            pc(<span class="number">1</span>:sample_num_level1,:) = pc(new_idx,:);</span><br><span class="line">            </span><br><span class="line">            <span class="comment">%% 2.8 ground truth</span></span><br><span class="line">            jnt_xyz_normalized = (jnt_xyz*coeff)/max_bb3d_len;</span><br><span class="line">            jnt_xyz_normalized = jnt_xyz_normalized - <span class="built_in">repmat</span>(offset,JOINT_NUM,<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">            Point_Cloud_FPS(frm_idx,:,:) = pc;</span><br><span class="line">            Volume_rotate(frm_idx,:,:) = coeff;</span><br><span class="line">            Volume_length(frm_idx) = max_bb3d_len;</span><br><span class="line">            Volume_offset(frm_idx,:) = offset;</span><br><span class="line">            Volume_GT_XYZ(frm_idx,:,:) = jnt_xyz_normalized;</span><br><span class="line">        <span class="keyword">end</span></span><br><span class="line">        <span class="comment">% 3. save files</span></span><br><span class="line">        save([save_gesture_dir <span class="string">'/Point_Cloud_FPS.mat'</span>],<span class="string">'Point_Cloud_FPS'</span>);</span><br><span class="line">        save([save_gesture_dir <span class="string">'/Volume_rotate.mat'</span>],<span class="string">'Volume_rotate'</span>);</span><br><span class="line">        save([save_gesture_dir <span class="string">'/Volume_length.mat'</span>],<span class="string">'Volume_length'</span>);</span><br><span class="line">        save([save_gesture_dir <span class="string">'/Volume_offset.mat'</span>],<span class="string">'Volume_offset'</span>);</span><br><span class="line">        save([save_gesture_dir <span class="string">'/Volume_GT_XYZ.mat'</span>],<span class="string">'Volume_GT_XYZ'</span>);</span><br><span class="line">        save([save_gesture_dir <span class="string">'/valid.mat'</span>],<span class="string">'valid'</span>);</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></tbody></table></figure>

<ul>
<li><strong>网络代码部分</strong></li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">nstates_plus_1 = [<span class="number">64</span>,<span class="number">64</span>,<span class="number">128</span>]</span><br><span class="line">nstates_plus_2 = [<span class="number">128</span>,<span class="number">128</span>,<span class="number">256</span>]</span><br><span class="line">nstates_plus_3 = [<span class="number">256</span>,<span class="number">512</span>,<span class="number">1024</span>,<span class="number">1024</span>,<span class="number">512</span>]</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PointNet_Plus</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, opt)</span>:</span></span><br><span class="line">        super(PointNet_Plus, self).__init__()</span><br><span class="line">        self.num_outputs = opt.PCA_SZ</span><br><span class="line">        self.knn_K = opt.knn_K</span><br><span class="line">        self.ball_radius2 = opt.ball_radius2</span><br><span class="line">        self.sample_num_level1 = opt.sample_num_level1</span><br><span class="line">        self.sample_num_level2 = opt.sample_num_level2</span><br><span class="line">        self.INPUT_FEATURE_NUM = opt.INPUT_FEATURE_NUM</span><br><span class="line">        </span><br><span class="line">        self.netR_1 = nn.Sequential(</span><br><span class="line">            <span class="comment"># B*INPUT_FEATURE_NUM*sample_num_level1*knn_K</span></span><br><span class="line">            nn.Conv2d(self.INPUT_FEATURE_NUM, nstates_plus_1[<span class="number">0</span>], kernel_size=(<span class="number">1</span>, <span class="number">1</span>)),</span><br><span class="line">            nn.BatchNorm2d(nstates_plus_1[<span class="number">0</span>]),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            <span class="comment"># B*64*sample_num_level1*knn_K</span></span><br><span class="line">            nn.Conv2d(nstates_plus_1[<span class="number">0</span>], nstates_plus_1[<span class="number">1</span>], kernel_size=(<span class="number">1</span>, <span class="number">1</span>)),</span><br><span class="line">            nn.BatchNorm2d(nstates_plus_1[<span class="number">1</span>]),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            <span class="comment"># B*64*sample_num_level1*knn_K</span></span><br><span class="line">            nn.Conv2d(nstates_plus_1[<span class="number">1</span>], nstates_plus_1[<span class="number">2</span>], kernel_size=(<span class="number">1</span>, <span class="number">1</span>)),</span><br><span class="line">            nn.BatchNorm2d(nstates_plus_1[<span class="number">2</span>]),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            <span class="comment"># B*128*sample_num_level1*knn_K</span></span><br><span class="line">            nn.MaxPool2d((<span class="number">1</span>,self.knn_K),stride=<span class="number">1</span>)</span><br><span class="line">            <span class="comment"># B*128*sample_num_level1*1</span></span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">        self.netR_2 = nn.Sequential(</span><br><span class="line">            <span class="comment"># B*131*sample_num_level2*knn_K</span></span><br><span class="line">            nn.Conv2d(<span class="number">3</span>+nstates_plus_1[<span class="number">2</span>], nstates_plus_2[<span class="number">0</span>], kernel_size=(<span class="number">1</span>, <span class="number">1</span>)),</span><br><span class="line">            nn.BatchNorm2d(nstates_plus_2[<span class="number">0</span>]),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            <span class="comment"># B*128*sample_num_level2*knn_K</span></span><br><span class="line">            nn.Conv2d(nstates_plus_2[<span class="number">0</span>], nstates_plus_2[<span class="number">1</span>], kernel_size=(<span class="number">1</span>, <span class="number">1</span>)),</span><br><span class="line">            nn.BatchNorm2d(nstates_plus_2[<span class="number">1</span>]),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            <span class="comment"># B*128*sample_num_level2*knn_K</span></span><br><span class="line">            nn.Conv2d(nstates_plus_2[<span class="number">1</span>], nstates_plus_2[<span class="number">2</span>], kernel_size=(<span class="number">1</span>, <span class="number">1</span>)),</span><br><span class="line">            nn.BatchNorm2d(nstates_plus_2[<span class="number">2</span>]),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            <span class="comment"># B*256*sample_num_level2*knn_K</span></span><br><span class="line">            nn.MaxPool2d((<span class="number">1</span>,self.knn_K),stride=<span class="number">1</span>)</span><br><span class="line">            <span class="comment"># B*256*sample_num_level2*1</span></span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">        self.netR_3 = nn.Sequential(</span><br><span class="line">            <span class="comment"># B*259*sample_num_level2*1</span></span><br><span class="line">            nn.Conv2d(<span class="number">3</span>+nstates_plus_2[<span class="number">2</span>], nstates_plus_3[<span class="number">0</span>], kernel_size=(<span class="number">1</span>, <span class="number">1</span>)),</span><br><span class="line">            nn.BatchNorm2d(nstates_plus_3[<span class="number">0</span>]),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            <span class="comment"># B*256*sample_num_level2*1</span></span><br><span class="line">            nn.Conv2d(nstates_plus_3[<span class="number">0</span>], nstates_plus_3[<span class="number">1</span>], kernel_size=(<span class="number">1</span>, <span class="number">1</span>)),</span><br><span class="line">            nn.BatchNorm2d(nstates_plus_3[<span class="number">1</span>]),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            <span class="comment"># B*512*sample_num_level2*1</span></span><br><span class="line">            nn.Conv2d(nstates_plus_3[<span class="number">1</span>], nstates_plus_3[<span class="number">2</span>], kernel_size=(<span class="number">1</span>, <span class="number">1</span>)),</span><br><span class="line">            nn.BatchNorm2d(nstates_plus_3[<span class="number">2</span>]),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            <span class="comment"># B*1024*sample_num_level2*1</span></span><br><span class="line">            nn.MaxPool2d((self.sample_num_level2,<span class="number">1</span>),stride=<span class="number">1</span>),</span><br><span class="line">            <span class="comment"># B*1024*1*1</span></span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">        self.netR_FC = nn.Sequential(</span><br><span class="line">            <span class="comment"># B*1024</span></span><br><span class="line">            nn.Linear(nstates_plus_3[<span class="number">2</span>], nstates_plus_3[<span class="number">3</span>]),</span><br><span class="line">            nn.BatchNorm1d(nstates_plus_3[<span class="number">3</span>]),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            <span class="comment"># B*1024</span></span><br><span class="line">            nn.Linear(nstates_plus_3[<span class="number">3</span>], nstates_plus_3[<span class="number">4</span>]),</span><br><span class="line">            nn.BatchNorm1d(nstates_plus_3[<span class="number">4</span>]),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            <span class="comment"># B*512</span></span><br><span class="line">            nn.Linear(nstates_plus_3[<span class="number">4</span>], self.num_outputs),</span><br><span class="line">            <span class="comment"># B*num_outputs</span></span><br><span class="line">        )</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, y)</span>:</span></span><br><span class="line">        <span class="comment"># x: B*INPUT_FEATURE_NUM*sample_num_level1*knn_K, y: B*3*sample_num_level1*1</span></span><br><span class="line">        x = self.netR_1(x)</span><br><span class="line">        <span class="comment"># B*128*sample_num_level1*1</span></span><br><span class="line">        x = torch.cat((y, x),<span class="number">1</span>).squeeze(<span class="number">-1</span>)</span><br><span class="line">        <span class="comment"># B*(3+128)*sample_num_level1</span></span><br><span class="line">        </span><br><span class="line">        inputs_level2, inputs_level2_center = group_points_2(x, self.sample_num_level1, self.sample_num_level2, self.knn_K, self.ball_radius2)</span><br><span class="line">        <span class="comment"># B*131*sample_num_level2*knn_K, B*3*sample_num_level2*1</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># B*131*sample_num_level2*knn_K</span></span><br><span class="line">        x = self.netR_2(inputs_level2)</span><br><span class="line">        <span class="comment"># B*256*sample_num_level2*1</span></span><br><span class="line">        x = torch.cat((inputs_level2_center, x),<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># B*259*sample_num_level2*1</span></span><br><span class="line">        </span><br><span class="line">        x = self.netR_3(x)</span><br><span class="line">        <span class="comment"># B*1024*1*1</span></span><br><span class="line">        x = x.view(<span class="number">-1</span>,nstates_plus_3[<span class="number">2</span>])</span><br><span class="line">        <span class="comment"># B*1024</span></span><br><span class="line">        x = self.netR_FC(x)</span><br><span class="line">        <span class="comment"># B*num_outputs</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></tbody></table></figure>

<ul>
<li>学习代码：  <a href="https://github.com/erikwijmans/Pointnet2_PyTorch.git" target="_blank" rel="noopener">https://github.com/erikwijmans/Pointnet2_PyTorch.git</a></li>
</ul>
<p><strong>level</strong>: CVPR, CCF_A<br><strong>author</strong>:Pavlo Molchanov, Xiaodong Yang (NVIDIA)<br><strong>date</strong>: 2016<br><strong>keyword</strong>:</p>
<ul>
<li>Hand Gesture,</li>
</ul>
<hr>
<h2 id="Paper-R3DCNN-Dynamic-Hand"><a href="#Paper-R3DCNN-Dynamic-Hand" class="headerlink" title="Paper: R3DCNN Dynamic Hand"></a>Paper: R3DCNN Dynamic Hand</h2><div align="center">
<br>
<b>Online Detection and Classiﬁcation of Dynamic Hand Gestures with Recurrent 3D Convolutional Neural Networks</b>
</div>

<h4 id="Proble-Statement-6"><a href="#Proble-Statement-6" class="headerlink" title="Proble Statement"></a>Proble Statement</h4><ul>
<li>Large diversity in how people perform gestures.</li>
<li>Work online to classify before competing a gesture.</li>
<li>Three overlapping phases: preparation, nucleus, and retraction.</li>
</ul>
<p>previous work:</p>
<ul>
<li>Hand-crafted spatio-temporal features.<ul>
<li>Shape, appearance, motion cues( image gradients, optical flow).</li>
</ul>
</li>
<li>Feature representations by DNN.<ul>
<li>uNeverova et al. combine color and depth data from hand regions and upper-body skeletons to recognize SL.</li>
</ul>
</li>
<li>Employ pre-segmented video sequences.</li>
<li>Treate detect and classify separately</li>
</ul>
<h4 id="Methods-5"><a href="#Methods-5" class="headerlink" title="Methods"></a>Methods</h4><ul>
<li><strong>Problem Formulation</strong>:</li>
</ul>
<p>Input: a video clip as volume $C_t$: $C_t\epsilon R^{k<em>l</em>c<em>m}$;  $m$: sequential frames; $C$: channels of size $k</em>l$ pixels.</p>
<p>$h_t\epsilon R_d$:  a hidden state vector;</p>
<p>$W_{in}\epsilon R^{d<em>q}$, $W_h\epsilon R^{d</em>d}$,$W_s\epsilon R^{w*d}$: weight matrices;</p>
<p>$b\epsilon R^w$: bias;</p>
<p>$S$: softmax functions, $R^w-&gt;R^w_{[0,1]},where [S(x)]<em>i=e^{x_i}/ \sum_ke^{xk}$<br>$$<br>F: R^{k<em>l</em>c*m}-&gt;R_q,where f_t=F(C_t)\<br>h_t=R(W</em>{in}f_t+W_hh_{t-1});\<br>s_t=S(W_sh_t+b);\<br>$$<br>For a video $V$ of $T$ clips, get the probabilities set $S$:<br>$$<br>S={s_0,s_1,…,s_{T-1}}\<br>S^{avg}=1/T\sum_{s\epsilon S}s\<br>predicted_label:y=argmax_i([s^{avg}]_i)<br>$$</p>
<ul>
<li><strong>system overview</strong>:</li>
</ul>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200711170646863.png" alt=""></p>
<p>【Pre-training the 3D-CNN】</p>
<ul>
<li>initialize the 3D-CNN with the C3D network [37] trained on the large-scale Sport1M [13] human action recognition dataset. </li>
<li>append a softmax prediction layer to the last fully-connected layer and ﬁne-tune by back-propagation with negative log-likelihood to predict gestures classes from individual clips $C_i$.</li>
</ul>
<p>【Cost Function】</p>
<ul>
<li>For Log-likelihood cost function:</li>
</ul>
<p>$$<br>L_v=-1/P \sum_{i=0}^{P-1}log(p(y_i|V_i))\<br>p(y_i|V_i)=[s^{avg}]_{y_i}<br>$$</p>
<p>【Learning Rule】</p>
<ul>
<li>To optimize the network parameters $W$ with respect to either of the loss functions we use stochastic gradient descent (SGD) with a momentum term$µ = 0.9$. We update each parameter of the network θ ∈ W at every back-propagation step i by:</li>
</ul>
<p>$$<br>\theta_i=\theta_{i-1}+v_i-yj\theta_{i-1}\<br>v_i=uv_{i-1}-jJ(&lt;\sigma E/\sigma \theta&gt;_{batch})<br>$$</p>
<h4 id="Evaluation-4"><a href="#Evaluation-4" class="headerlink" title="Evaluation"></a>Evaluation</h4><ul>
<li><strong>Environment</strong>:   <ul>
<li>Dataset: used the SoftKinetic DS325 sensor to acquire frontview color and depth videos and a top-mounted DUO 3D sensor to record a pair of stereo-IR streams.</li>
<li>randomly split the data by subject into training (70%) and test (30%) sets, resulting in 1050 training and 482 test videos.</li>
<li><strong>SKIG</strong> contains 1080 RGBD hand gesture sequences by 6 subjects collected with a Kinect sensor </li>
<li><strong>ChaLearn 2014 dataset</strong> contains more than 13K RGBD videos of 20 upper-body Italian sign language gestures performed by 20 subjects</li>
<li><strong>Results</strong>:</li>
</ul>
</li>
</ul>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200711173946207.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200711174007019.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200711174023573.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200711174040803.png" alt="predictions with various modalities"></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200711174056380.png" alt="Comparison of 2D-CNN and 3D-CNN trained with different architectures"></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200711174220754.png" alt="Gesture Detection"></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200711174344337.png" alt="SKIG RGBD gesture dataset"></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200711174326620.png" alt="Chalearn 2014 dataset"></p>
<h4 id="Conclusion-5"><a href="#Conclusion-5" class="headerlink" title="Conclusion"></a>Conclusion</h4><ul>
<li>Design R3DCNN to performs simultaneous detection and classification.</li>
<li>Using CTC model to predict label from in-progress gesture in unsegmented input streams.</li>
<li>Achieves high accuracy of 88.4%.</li>
</ul>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script>
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        Author:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://liudongdong1.github.io" rel="external nofollow noreferrer">liudongdong1</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        Link:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://liudongdong1.github.io/2020/06/20/shi-jue-ai/dataglove/hand-analyse-record/">https://liudongdong1.github.io/2020/06/20/shi-jue-ai/dataglove/hand-analyse-record/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        Reprint policy:
                    </i>
                </span>
                <span class="reprint-info">
                    All articles in this blog are used except for special statements
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    reprint polocy. If reproduced, please indicate source
                    <a href="https://liudongdong1.github.io" target="_blank">liudongdong1</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>Copied successfully, please follow the reprint policy of this article</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">more</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/tags/HandPose/">
                                    <span class="chip bg-color">HandPose</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="qq,qzone,wechat,weibo,douban" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
                <style>
    #reward {
        margin: 40px 0;
        text-align: center;
    }

    #reward .reward-link {
        font-size: 1.4rem;
        line-height: 38px;
    }

    #reward .btn-floating:hover {
        box-shadow: 0 6px 12px rgba(0, 0, 0, 0.2), 0 5px 15px rgba(0, 0, 0, 0.2);
    }

    #rewardModal {
        width: 320px;
        height: 350px;
    }

    #rewardModal .reward-title {
        margin: 15px auto;
        padding-bottom: 5px;
    }

    #rewardModal .modal-content {
        padding: 10px;
    }

    #rewardModal .close {
        position: absolute;
        right: 15px;
        top: 15px;
        color: rgba(0, 0, 0, 0.5);
        font-size: 1.3rem;
        line-height: 20px;
        cursor: pointer;
    }

    #rewardModal .close:hover {
        color: #ef5350;
        transform: scale(1.3);
        -moz-transform:scale(1.3);
        -webkit-transform:scale(1.3);
        -o-transform:scale(1.3);
    }

    #rewardModal .reward-tabs {
        margin: 0 auto;
        width: 210px;
    }

    .reward-tabs .tabs {
        height: 38px;
        margin: 10px auto;
        padding-left: 0;
    }

    .reward-content ul {
        padding-left: 0 !important;
    }

    .reward-tabs .tabs .tab {
        height: 38px;
        line-height: 38px;
    }

    .reward-tabs .tab a {
        color: #fff;
        background-color: #ccc;
    }

    .reward-tabs .tab a:hover {
        background-color: #ccc;
        color: #fff;
    }

    .reward-tabs .wechat-tab .active {
        color: #fff !important;
        background-color: #22AB38 !important;
    }

    .reward-tabs .alipay-tab .active {
        color: #fff !important;
        background-color: #019FE8 !important;
    }

    .reward-tabs .reward-img {
        width: 210px;
        height: 210px;
    }
</style>

<div id="reward">
    <a href="#rewardModal" class="reward-link modal-trigger btn-floating btn-medium waves-effect waves-light red">赏</a>

    <!-- Modal Structure -->
    <div id="rewardModal" class="modal">
        <div class="modal-content">
            <a class="close modal-close"><i class="fas fa-times"></i></a>
            <h4 class="reward-title">你的赏识是我前进的动力</h4>
            <div class="reward-content">
                <div class="reward-tabs">
                    <ul class="tabs row">
                        <li class="tab col s6 alipay-tab waves-effect waves-light"><a href="#alipay">支付宝</a></li>
                        <li class="tab col s6 wechat-tab waves-effect waves-light"><a href="#wechat">微 信</a></li>
                    </ul>
                    <div id="alipay">
                        <img src="/medias/reward/alipay.jpg" class="reward-img" alt="支付宝打赏二维码">
                    </div>
                    <div id="wechat">
                        <img src="/medias/reward/wechat.png" class="reward-img" alt="微信打赏二维码">
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>

<script>
    $(function () {
        $('.tabs').tabs();
    });
</script>

            
        </div>
    </div>

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;Previous</div>
            <div class="card">
                <a href="/2020/06/21/aiot/bluetooth/bluepaperrecord/">
                    <div class="card-image">
                        
                        <img src="https://cdn.stocksnap.io/img-thumbs/280h/abstract-flower_NXMSAM0UIR.jpg" class="responsive-img" alt="BlueTooth Paper">
                        
                        <span class="card-title">BlueTooth Paper</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            
Billah, Md Fazlay Rabbi Masum, et al. “BLE Can See: A Reinforcement Learning Approach for RF-based Indoor Occupancy Det
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2020-06-21
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/AIOT/" class="post-category">
                                    AIOT
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/BlueTooth/">
                        <span class="chip bg-color">BlueTooth</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                Next&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/2020/06/16/aiot/rfid/rfid-actionrecognition/">
                    <div class="card-image">
                        
                        <img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200428195606397.png" class="responsive-img" alt="RFID ActionRecognition">
                        
                        <span class="card-title">RFID ActionRecognition</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            level:  ACM数据库  Embedded Networked Sensor Systems  CCF_Bauthor: Yinggang Yu ,Dong Wang, Run Zhao, Qian Zhang       Shang
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2020-06-16
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/AIOT/" class="post-category">
                                    AIOT
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/RFID/">
                        <span class="chip bg-color">RFID</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/libs/codeBlock/codeBlockFuction.js"></script>

<!-- 代码语言 -->

<script type="text/javascript" src="/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/libs/codeBlock/codeShrink.js"></script>


<!-- 代码块折行 -->

<style type="text/css">
code[class*="language-"], pre[class*="language-"] { white-space: pre !important; }
</style>


    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;TOC</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // modify the toc link href to support Chinese.
        let i = 0;
        let tocHeading = 'toc-heading-';
        $('#toc-content a').each(function () {
            $(this).attr('href', '#' + tocHeading + (++i));
        });

        // modify the heading title id to support Chinese.
        i = 0;
        $('#articleContent').children('h2, h3, h4').each(function () {
            $(this).attr('id', tocHeading + (++i));
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>

    

</main>




    <!-- <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="463294659"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/libs/aplayer/APlayer.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/meting@2/dist/Meting.min.js"></script>

    
    <div class="container row center-align" style="margin-bottom: 0px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            <span id="year">2019</span>
            <a href="https://liudongdong1.github.io" target="_blank">liudongdong</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            <br>
            
            &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                class="white-color">1206.4k</span>&nbsp;字
            
            
            
            
            
            
            <span id="busuanzi_container_site_pv">
                |&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;<span id="busuanzi_value_site_pv"
                    class="white-color"></span>&nbsp;次
            </span>
            
            
            <span id="busuanzi_container_site_uv">
                |&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;<span id="busuanzi_value_site_uv"
                    class="white-color"></span>&nbsp;人
            </span>
            
            <br>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/liudongdong1/" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:3463264078@qq.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>














    <a href="https://blog.csdn.net/liudongdong19/" class="tooltipped" target="_blank" data-tooltip="关注我的CSDN: https://blog.csdn.net/liudongdong19/" data-position="top" data-delay="50">
        <i class="fab fa-csdn">C</i>
    </a>





</div>
    </div>
</footer>

<div class="progress-bar"></div>
 -->

    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;Search</span>
            <input type="search" id="searchInput" name="s" placeholder="Please enter a search keyword"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script src="/js/search.js"></script>
<script type="text/javascript">
$(function () {
    searchFunc("/search.xml", 'searchInput', 'searchResult');
});
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script type="text/javascript" src="/js/CFS.Snow.min.js"></script>
    <!-- 点击爆灯效果 -->
    <canvas class="fireworks" style="position: fixed;left: 0;top: 0;z-index: 1; pointer-events: none;" ></canvas> 
    <script type="text/javascript" src="//cdn.bootcss.com/animejs/2.2.0/anime.min.js"></script> 
    <script type="text/javascript" src="/js/fireworks.js"></script>
    <!--动态线条背景-->
    <script type="text/javascript"
        color="122 103 238" opacity='0.7' zIndex="-2" count="200" src="//cdn.bootcss.com/canvas-nest.js/1.0.0/canvas-nest.min.js">
    </script>
    <!-- 天气 -->
    <!-- weather -->
    <!-- weather -->
    <script type="text/javascript">
         WIDGET = {FID: 'knAMQaFanP'}
    </script>
    <script type="text/javascript" src="https://apip.weatherdt.com/float/static/js/r.js?v=1111"></script>
    <script type="text/javascript" src="https://apip.weatherdt.com/float/static/js/r.js?v=1111"></script>
    <script src="/libs/materialize/materialize.min.js"></script>
    <script src="/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/libs/aos/aos.js"></script>
    <script src="/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/js/matery.js"></script>

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    
    <script async src="/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    

    
    
    <script type="text/javascript" size="150" alpha='0.6'
        zIndex="-1" src="/libs/background/ribbon-refresh.min.js" async="async"></script>
    

    

    
    <script src="/libs/instantpage/instantpage.js" type="module"></script>
    
    <!-- {% include '_custom/custom.swig' %} -->

</body>

</html>
