<!DOCTYPE HTML>
<html lang="en">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="RFID ActionRecognition, AIOT,Space&amp;Temporal Sequence Analysis,SpringBoot,liudongdong1,cloud">
    <meta name="description" content="">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>RFID ActionRecognition | DaybyDay</title>
    <link rel="icon" type="image/png" href="/favicon.png">

    <link rel="stylesheet" type="text/css" href="/libs/awesome/css/all.css">
    <link rel="stylesheet" type="text/css" href="/libs/materialize/materialize.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/aos/aos.css">
    <link rel="stylesheet" type="text/css" href="/libs/animate/animate.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/lightGallery/css/lightgallery.min.css">
    <link rel="stylesheet" type="text/css" href="/css/matery.css">
    <link rel="stylesheet" type="text/css" href="/css/my.css">

    <script src="/libs/jquery/jquery.min.js"></script>

<meta name="generator" content="Hexo 4.2.0"><link rel="alternate" href="/atom.xml" title="DaybyDay" type="application/atom+xml">
<link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css"></head>


<body>
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/" class="waves-effect waves-light">
                    
                    <img src="/medias/logo.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">DaybyDay</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>Index</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>Tags</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>Categories</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>Archives</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/" class="waves-effect waves-light">

      
      <i class="fas fa-user-circle" style="zoom: 0.6;"></i>
      
      <span>About</span>
      <i class="fas fa-chevron-down" aria-hidden="true" style="zoom: 0.6;"></i>
    </a>
    <ul class="sub-nav menus_item_child ">
      
      <li>
        <a href="/about">
          
          <i class="fas fa-user-circle" style="margin-top: -20px; zoom: 0.6;"></i>
          
          <span>about</span>
        </a>
      </li>
      
      <li>
        <a href="/resume">
          
          <i class="fa fa-user-secret" style="margin-top: -20px; zoom: 0.6;"></i>
          
          <span>resume</span>
        </a>
      </li>
      
    </ul>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/friends" class="waves-effect waves-light">
      
      <i class="fas fa-address-book" style="zoom: 0.6;"></i>
      
      <span>Friends</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/gallery" class="waves-effect waves-light">
      
      <i class="fas fa-camera" style="zoom: 0.6;"></i>
      
      <span>Galleries</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="Search" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/medias/logo.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">DaybyDay</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			Index
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			Tags
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			Categories
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			Archives
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="javascript:;">
			
				<i class="fa-fw fas fa-user-circle"></i>
			
			About
			<span class="m-icon"><i class="fas fa-chevron-right"></i></span>
		</a>
            <ul  style="background:  ;" >
              
                <li>

                  <a href="/about " style="margin-left:75px">
				  
				   <i class="fa fas fa-user-circle" style="position: absolute;left:50px" ></i>
			      
		          <span>about</span>
                  </a>
                </li>
              
                <li>

                  <a href="/resume " style="margin-left:75px">
				  
				   <i class="fa fa fa-user-secret" style="position: absolute;left:50px" ></i>
			      
		          <span>resume</span>
                  </a>
                </li>
              
            </ul>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/friends" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-address-book"></i>
			
			Friends
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/gallery" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-camera"></i>
			
			Galleries
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/liudongdong1" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/liudongdong1" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200428195606397.png')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">RFID ActionRecognition</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <link rel="stylesheet" href="/libs/tocbot/tocbot.css">
<style>
    #articleContent h1::before,
    #articleContent h2::before,
    #articleContent h3::before,
    #articleContent h4::before,
    #articleContent h5::before,
    #articleContent h6::before {
        display: block;
        content: " ";
        height: 100px;
        margin-top: -100px;
        visibility: hidden;
    }

    #articleContent :focus {
        outline: none;
    }

    .toc-fixed {
        position: fixed;
        top: 64px;
    }

    .toc-widget {
        width: 345px;
        padding-left: 20px;
    }

    .toc-widget .toc-title {
        margin: 35px 0 15px 0;
        padding-left: 17px;
        font-size: 1.5rem;
        font-weight: bold;
        line-height: 1.5rem;
    }

    .toc-widget ol {
        padding: 0;
        list-style: none;
    }

    #toc-content {
        height: calc(100vh - 250px);
        overflow: auto;
    }

    #toc-content ol {
        padding-left: 10px;
    }

    #toc-content ol li {
        padding-left: 10px;
    }

    #toc-content .toc-link:hover {
        color: #42b983;
        font-weight: 700;
        text-decoration: underline;
    }

    #toc-content .toc-link::before {
        background-color: transparent;
        max-height: 25px;

        position: absolute;
        right: 23.5vw;
        display: block;
    }

    .toc-fixed .toc-link::before{
        position: fixed!important;/*当toc的位置改为fixed时，.toc-link::before也要改为fixed*/
    }

    #toc-content .is-active-link {
        color: #42b983;
    }

    #toc-content .is-active-link::before {
        background-color: #42b983;
    }

    #floating-toc-btn {
        position: fixed;
        right: 15px;
        bottom: 76px;
        padding-top: 15px;
        margin-bottom: 0;
        z-index: 998;
    }

    #floating-toc-btn .btn-floating {
        width: 48px;
        height: 48px;
    }

    #floating-toc-btn .btn-floating i {
        line-height: 48px;
        font-size: 1.4rem;
    }
</style>
<div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/tags/RFID/">
                                <span class="chip bg-color">RFID</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/categories/AIOT/" class="post-category">
                                AIOT
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>Publish Date:&nbsp;&nbsp;
                    2020-06-16
                </div>
                

                <!-- 
                    <i class="fa fa-pencil"></i> Author: liudongdong1
                  -->

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>Update Date:&nbsp;&nbsp;
                    2021-12-14
                </div>
                

                <!-- 
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>Word Count:&nbsp;&nbsp;
                    8.3k
                </div>
                 -->

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>Read Times:&nbsp;&nbsp;
                    49 Min
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>Read Count:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">
        <div class="card-content article-card-content">
            <div id="articleContent">
                <p><strong>level</strong>:  ACM数据库  Embedded Networked Sensor Systems  CCF_B<br><strong>author</strong>: Yinggang Yu ,Dong Wang, Run Zhao, Qian Zhang       ShangHaiJiaoTongUniversity<br><strong>date</strong>: 2019 .11<br><strong>keyword</strong>:</p>
<ul>
<li>RFID,wireless sensing ,ongoing gesture recognition,adversarial learning</li>
</ul>
<hr>
<h1 id="Paper-RFID-ongoing-Gesture"><a href="#Paper-RFID-ongoing-Gesture" class="headerlink" title="Paper: RFID ongoing Gesture"></a>Paper: RFID ongoing Gesture</h1><div align="center">
<br>
<b>RFID Based Real-time Recognition of ongoing Gesture with Adversarial Learning</b>
</div>


<h4 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h4><ol>
<li>通过一个阅读器多个标签进行实验，使用CNN来分别提取相位和RSSI特征，并将特征连接，通过LSTM网络得到一个行为概率分布向量，通过SVM支持向量机模型来预测某个行为。</li>
</ol>
<h4 id="Research-Objective"><a href="#Research-Objective" class="headerlink" title="Research Objective"></a>Research Objective</h4><ul>
<li><strong>Application Area</strong>:<ul>
<li><strong>gesture-driven applications:</strong> input for video games have paramount and unavoidable issue about latency between completion of a gesture and its recognition</li>
</ul>
</li>
<li><strong>Purpose</strong>:  <font color="red">fusing multimodal RFID data and extracting space-temporal information to enable a general ,pervasive ,environment independent ,user-invariant and real-time gesture-driven interactive system</font></li>
</ul>
<h4 id="Proble-Statement"><a href="#Proble-Statement" class="headerlink" title="Proble Statement"></a>Proble Statement</h4><ul>
<li>RFID based gesture recognition methods can handle gestures due to dynamic environment and <font color="red">degrade significantly when it comes to user diversity and environment variety</font></li>
<li>recent works on gesture detection are designed for <font color="red">recognition gesture after it is completed</font>,there remain <font color="red">latency</font>.</li>
</ul>
<p>previous work:</p>
<ul>
<li><strong>Gesture Recognition</strong><ul>
<li><strong>wearable sensor based</strong>: wrist-worn devices containing inertial sensors are utilized to recognize the eating gesture[28], identify smoking gesture[27], translate sign language[39, 41] ,identify fine-grained interactive gesture[12]               require users to charge devices  <font color="red">ios phone will deploy UWB ,what if achieve gesture recognize using UWB</font></li>
<li><strong>Camera based</strong>: Leap Motion or Kinect to build gesture recognition systems to enable sign language translation at both word and sentence levels with high accuracy [9], recognize continuous sign language with weakly supervised learning [5]          <font color="red"> privacy ,light sensitive, NLOS</font></li>
<li><strong>Wireless based</strong>: ultra-sound, WIFI, RFID ,mmWave active gestures recognition[19, 20, 36] ,sign language translation[22],keystrokes recognition [1], limb-level gesture detection[3, 6, 29, 44]         <font color="red">  work well in relatively limited and controlled environment and achieve high accuracy on some special users,but performance may degrade in some unstable or uncontrolled environments</font></li>
</ul>
</li>
<li><strong>Ongoing gesture detection</strong>: <ul>
<li>[33] propose a smart watch based early gesture detection technology</li>
<li>[14] use deep learning techniques to predict twenty-five hand gestures online from videos</li>
<li>[15] design computer vision based early event detectors which enable sign language translation ,emotions recognitions</li>
</ul>
</li>
<li><strong>Domain Adversarial learning</strong>: learn robust representations which are discriminative on source domain while invariant between domains[10, 32] <ul>
<li>[42] combine CNN and RNN with adversarial learning to extract <font color="red">sleep-specific and subject-invariant features from RF signals to predict sleep stage </font></li>
<li>[24] utilizes multi-task optimization to <font color="red">reduce variance across speakers</font> and keep representations senone-discriminative</li>
<li>[31] a robust end-to-end speech recognition framework using generative adversarial network in a data-driven way.</li>
<li><font color="red">differs from above , EUIGR takes the ongoing gesture into consideration and the training objectives of the user domain,environment domain, and gesture domain undertake different tasks, namely classification and sequence labeling respectively</font>  ??</li>
</ul>
</li>
</ul>
<h4 id="Methods"><a href="#Methods" class="headerlink" title="Methods"></a>Methods</h4><ul>
<li><strong>RFID Communication Model</strong>: </li>
</ul>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191204194801514.png" alt=""></p>
<ul>
<li><strong>Define parameters notations</strong>:</li>
</ul>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191204194931940.png" alt=""></p>
<ul>
<li><strong>system overview</strong>:   ??判别式网络是如何指导预测的？ 是就用LSTM双向和定义三个loss函数？这部分不理解？</li>
</ul>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191204195151337.png" alt=""></p>
<p>【Qustion 1】<font color="green"><strong>how to fuse multimodal RFID data</strong></font></p>
<ul>
<li>Data Collector Module:    <ul>
<li><strong>Unwrapping Phase</strong>: <img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191213092945361.png" alt=""></li>
<li><strong>Resampling</strong>: tag reply sequence is not uniform in the time domain,due to random RFID tags respond to the reader,the packets loss caused by tag conficts and noise   <font color="red"> sample 的频率是如何设置</font>  employed Hample identifier to filter out abnormal values in the stream and then resample</li>
<li><strong>Constructing RFID Clip</strong>: sliding windows length is 20 ,sliding length is 10 ,(为什么这样设置没说，一个动作大概多长时间，根据采样频率来设置滑动窗口的大小会怎样)</li>
<li><strong>one reader with n tags</strong>  :the sliding window data representing</li>
</ul>
</li>
</ul>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191204200036187.png" alt=""></p>
<ul>
<li><strong>Feature Extractor Module</strong>:  $F_R^{(t)}=FE(C^(t);\theta _{fe})$  ,$\theta _{fe}$ denotes the set of all parameters in the feature extractor FE.  <font color="red">?? how to merge, how to flatten? 2-layer CNN filter (1*d)2D convolutional kernel?? the detail of full connector??  Can i design the network on this description??</font></li>
</ul>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191204200340668.png" alt=""></p>
<p>【Qustion 2】<font color="green"><strong>how to recognize gesture before it’s completed?</strong></font></p>
<ul>
<li><strong>Gesture Sequence Labeler</strong> ：model the temporal dependencies of the RFID input sequences using RNN+LSTM</li>
</ul>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191204202000126.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191204202014250.png" alt=""><br>$$<br>i_t=\theta(W_i<em>[h_{t-1,x_t}]+b_i)\ f_t=\theta(W_f</em>[h_{t-1},x_t]+b_f)\\vec{C_t}=\theta(W_C<em>[h_{t-1},x_t]+b_C)\O_t=\theta(W_o</em>[h_{t-1,x_t}]+b_o)\C_t=f_t<em>C_{t-1}+i_t</em>\vec{C_t} \h_t=O_t*tanh(C_t)\这是LSTM的cell单元中的基本公式<br>$$</p>
<ul>
<li><strong>the probabilities of the $K^{th}$ gesture</strong> : <font color="red">这个概率分布公式含义不懂？ ！上面表示一个动作概率</font></li>
</ul>
<p>$$<br>G_p^{(t,k)}=\frac{exp(h_t<em>W_p+b_p)^k}{\sum_k{exp(h_t</em>W_p+b_p)^k}}<br>$$</p>
<ul>
<li><strong>Highly general</strong>: $\theta_{sl}$ is a set of gesture ; $G_P^{(t)}$ is a k*1 prediction probability vector of all the gesture</li>
</ul>
<p>$$<br>G_p^{t}=LSTM_g(F_R^{(t)};\theta_{sl})<br>$$</p>
<p>​        <strong>Loss Function</strong>:  <font color="red">这个损失函数公式含义不懂</font>？<br>$$<br>L_g(\theta_{fe},\theta_{sl})=-\frac{1}{N}\sum_{n=1}^{N}{\frac{1}{N}}\sum_{t=1}^{T_n}\sum_{k=1}^{K}G_T^{(t,k)}logG_P^{(t,k)}<br>$$</p>
<ul>
<li><p><strong>Using SVM classifier instead of probability threshold</strong> .</p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191205093447023.png" alt=""></p>
</li>
</ul>
<p>【Qustion 3】<font color="green"><strong>how to extract environment &amp; user invariant features</strong></font>    the data streams of the same gesture performed by<font color="red"> diverse users in different positions may differ in both spatial and temporal. </font>the feature generated by feature extractor should be unrelated to the users and environments as possible ,we applies two domain discriminators including user discriminator and environment discriminator which maps the representations $F_R$ to the user predictions and position predictions. Model the user diversity and enviroment discrepancy using BLSTM classifiers </p>
<p>BLSTM ensures the inference about user or position depends on the full sequence with a better capability of temporal modeling.</p>
<ul>
<li><strong>User discriminator</strong>:  $U_p=BLSTM_u(F_R;\theta_{ud})$ <ul>
<li><strong>Loss Function</strong>: $L_u(\theta_{fe},\theta_{ud})=-\frac{1}{N}\sum_{n=1}^{N}\sum_{j=1}^{J}U_T^{(j)}logU_p^{(j)}$</li>
</ul>
</li>
<li><strong>Environment discriminator</strong>:  $E_p=BLSTM_e(F_R;\theta_{ed})$ <ul>
<li><strong>Loss Function</strong>: $L_e(\theta_{fe},\theta_{ed})=-\frac{1}{N}\sum_{n=1}^{N}\sum_{j=1}^{J}E_T^{(j)}logE_p^{(j)}$</li>
</ul>
</li>
</ul>
<p>[]<font color="green">optimizing the parameters $\theta_{fe} ,\theta_{sl}$** </font> </p>
<p>the features extracted from same kind of gestures performed by different users or in different positions are required to follow the same distribution.<font color="red"> the purpose of two domain discriminators is oppsite to our final objective</font>   ??  BLSTM 是发现整个动作序列之间的关系，还是不理解</p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191205095225350.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191205095255812.png" alt=""></p>
<h4 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a>Evaluation</h4><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191205095604407.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191205095623430.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191206100135113.png" alt="image-20191206100135113"></p>
<ul>
<li><strong>General Performance</strong>:</li>
</ul>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191206100720217.png" alt=""></p>
<ul>
<li><strong>ComparatoOtherMethods</strong>:</li>
</ul>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191206100821438.png" alt=""></p>
<ul>
<li><strong>Data Fusion Influence</strong>:</li>
</ul>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191206100841898.png" alt=""></p>
<ul>
<li><strong>UI Model</strong>: training method: y we train the User Invariant (UI) model using the samples from &amp; ! 1 volunteers and test the system using the samples from another volunteer, repeating this process for every distinct volunteer  。<font color="red">有个问题，使用留1交叉验证训练，那些它的测试样本也在里面吗？没有看到文章中训练样本和测试样本是怎么分配的</font></li>
</ul>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191206101406107.png" alt=""></p>
<p>UI训练过程中特征的分布情况：</p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191206101451095.png" alt=""></p>
<ul>
<li><strong>EI Model</strong>：all samples are collected from the gestures performed by several users in 12 positions in three weeks. The samples of two users from a same position are in different domains among users but in the same environment and are regarded as performed by the same participant</li>
</ul>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191206101629270.png" alt=""> </p>
<ul>
<li><strong>Real Time</strong>:</li>
</ul>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191206101549888.png" alt=""></p>
<h4 id="Notes-去加强了解"><a href="#Notes-去加强了解" class="headerlink" title="Notes 去加强了解"></a>Notes <font color="orange">去加强了解</font></h4><ul>
<li><input checked="" disabled="" type="checkbox"> RFID  low-cost ,mini-size and battery-free  that widely employed </li>
<li><input checked="" disabled="" type="checkbox"> T-distributed stochastic neighbor embedding visualization :一种非线性降维算法，非常适用于高维数据降维到2维或者3维，进行可视化.<a href="http://www.datakit.cn/blog/2017/02/05/t_sne_full.html" target="_blank" rel="noopener">http://www.datakit.cn/blog/2017/02/05/t_sne_full.html</a></li>
<li><input checked="" disabled="" type="checkbox"> BLSTM网络有什么优点？？</li>
<li><input checked="" disabled="" type="checkbox"> Hample： 对输入向量x进行hampel滤波，检测和删除异常值。对于x的每个样本，该函数计算由样本及其周围六个样本组成的窗口的<strong>中值</strong>，每边三个。并利用中位数绝对值估计了各样本对中值的标准差。如果某个样本与中值相差超过三个标准差，则用中值替换该样本。如果x是一个矩阵，hampel将x的每一列都看成是独立的通道。 </li>
<li><input disabled="" type="checkbox"> paper 22, 3, 6, 9, 24</li>
<li><input disabled="" type="checkbox"> LLRP [7]、</li>
<li><input disabled="" type="checkbox"> RFID is widely used in activity recognition with its stable low-level physical characters such as phase and RSS ,intuitively delineate its movements</li>
<li><input disabled="" type="checkbox"> gesture recognition builds a friendly and straight forward bridge between human and computer compared with text-based and graphic user interface.</li>
</ul>
<p><strong>level</strong>: CCF_B  IEEE International Conference on Sensing ,Communication and Networking(SECON)<br><strong>author</strong>: Shigeng Zhang,Chengwei Yang<br><strong>date</strong>:  2016<br><strong>keyword</strong>:</p>
<ul>
<li></li>
</ul>
<hr>
<h1 id="Paper-ReActor"><a href="#Paper-ReActor" class="headerlink" title="Paper: ReActor"></a>Paper: ReActor</h1><div align="center">
<br>
<b>ReActor:Real-time and Accurate Contactless Gesture Recognition with RFID</b>
</div>


<h4 id="Summary-1"><a href="#Summary-1" class="headerlink" title="Summary"></a>Summary</h4><ol>
<li>use machine learning to distinguish different gestures ,instead of DTW <font color="red">the statics of the signal profile that characterize coarse-grained features and the wavelet(transformation) coefficients of the signal profile that characterize the fine-grained local features</font></li>
</ol>
<h4 id="Proble-Statement-1"><a href="#Proble-Statement-1" class="headerlink" title="Proble Statement"></a>Proble Statement</h4><p>previous work:</p>
<ul>
<li>RF-Finger[12] uses 35 tags to classify different hand gestures by convolutional neural networks</li>
<li>Contact-based Gesture Recognition<ul>
<li>uWave[20] uses a single three-axis accelerometer sensor to recognize personalized gestures.</li>
<li>FEMD[21] uses the Kinect sensor to classify ten different gestures</li>
<li>The Magic Ring [7] recognizes different gestures by attaching a ring to the user’s finger </li>
<li>Femo[14] recognize the user’s activity during body exercise and assesses the quality of exercise movements</li>
<li>ShopMiner[17] and CBid[22] monitor the customer’ behaviors by attaching RFID tags to goods in the surpermarket and recognizing different behavior patterns by tracign motions of tags</li>
<li><strong>[23]</strong> combine Kinect-based activity recognition and RFID-based user identification to improve the quality of augmented reality application</li>
<li><strong>[16]</strong> propose an approach to detecting the user’s coarse-grained gesture by attaching tags to goods ,which supports online commenting of goods quality</li>
<li><strong>IDSense[24]</strong> enables smart interaction between the user and objects by developing an activity detection systems based on RFID</li>
<li>[15, 25] using deep learning to recognize user’s body activities </li>
</ul>
</li>
<li>Contactless Gesture Recognition<ul>
<li>WiGest[26] detects  basic primitive gesture in a device-free manner</li>
<li>E-eyes detects user’s activity at home based on channel state information(CSI)</li>
<li>WiFinger[18] detects fine-grained hand gestures based on CSI changes</li>
<li>GRfid[11]uses dynamic time warping to match different gestures </li>
</ul>
</li>
</ul>
<h4 id="Methods-1"><a href="#Methods-1" class="headerlink" title="Methods"></a>Methods</h4><ul>
<li><strong>Environment</strong>:</li>
</ul>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191213131436563.png" alt=""></p>
<ul>
<li><strong>system overview</strong>:</li>
</ul>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191213131453197.png" alt=""></p>
<ul>
<li><strong>Signal preprocessing</strong><ul>
<li>phase unwrapping </li>
<li>phase ambiguity processing  </li>
<li>signal smoothing  using Savitzky-Golay (S-G) filter [32]</li>
<li>signal normalization   (<font color="red">这一步作用是什么</font>)</li>
</ul>
</li>
</ul>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191213132046470.png" alt=""></p>
<ul>
<li><strong>Gesture Segmentation:</strong><ul>
<li>Varri method</li>
</ul>
</li>
</ul>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191213132737848.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191213132934088.png" alt=""></p>
<ul>
<li><p><strong>Attribute Extract</strong></p>
<ul>
<li>Static Attributes: <ul>
<li>the mode, median, the first quartile, the third quartile, and the arithmetic mean  reflect the central tendency of the data </li>
<li>the max, min, variance, standard deviation, the third-order central moment that reflect dispersion of the data</li>
<li>skewess, Kurtosis that reflect the distribution shape</li>
</ul>
</li>
<li>Wavelet Decomposition Coefficient Attributes:<ul>
<li>Data interpolation：  handle the sample question</li>
<li>Wavelet coefficient calculation: use the Daubechies wavelet as the wavelet base to decompose the signal profile of each gesture</li>
</ul>
</li>
</ul>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191213133836434.png" alt=""></p>
</li>
</ul>
<h4 id="Evaluation-1"><a href="#Evaluation-1" class="headerlink" title="Evaluation"></a>Evaluation</h4><ul>
<li><strong>Environment</strong>:   <ul>
<li>Dataset: </li>
</ul>
</li>
<li>the Gestures to Recognise</li>
</ul>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191213134253071.png" alt=""></p>
<ul>
<li>the Impact of Tag Number</li>
<li>Recognition Latency</li>
<li>Impact of Gesture Speed</li>
<li>Impact of Operation Distance</li>
</ul>
<h4 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h4><ul>
<li><h4 id="Notes-去加强了解-1"><a href="#Notes-去加强了解-1" class="headerlink" title="Notes 去加强了解"></a>Notes <font color="orange">去加强了解</font></h4></li>
<li><input disabled="" type="checkbox"> <p>Varri method [29]  segment different activity: uses a sliding window that combine amplitude measure and frequency measure of the signal</p>
</li>
<li><input disabled="" type="checkbox"> <p>Savitzky-Golay (S-G) filter : a method based on local polynomial least square fitting in the time domain  ,preserve the shape and width of the raw signal after filtering out noises</p>
</li>
<li><input disabled="" type="checkbox"> <p>Signal normalization: can manify the signal changes caused by gestures and meanwhile suppress the impact of background signals by mapping them to values around zero<img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191213132452914.png" alt="image-20191213132452914"></p>
</li>
<li><input disabled="" type="checkbox"> <p>学习小波变换算法作用</p>
</li>
</ul>
<p><strong>level</strong>: CCF_C  WCNC (IEEE Wireless Communications and Networking Conference)<br><strong>author</strong>: Dong Wang Shanghai jiaotong Ust<br><strong>date</strong>: 2018</p>
<hr>
<h1 id="Paper-SGRS"><a href="#Paper-SGRS" class="headerlink" title="Paper: SGRS"></a>Paper: SGRS</h1><div align="center">
<br>
<b>SGRS:A Sequential Gesture Recognition System using COTS RFID</b>
</div>
- **system overview**:

<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191213120638484.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/1571906933201.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/1571906933201.png" alt=""></p>
<h1 id="Paper1《RF-Based-Fall-Monitoring-Using-Convolutional-Neural-Networks》"><a href="#Paper1《RF-Based-Fall-Monitoring-Using-Convolutional-Neural-Networks》" class="headerlink" title="Paper1《RF-Based Fall Monitoring Using Convolutional Neural Networks》"></a>Paper1《RF-Based Fall Monitoring Using Convolutional Neural Networks》</h1><p><strong>cited:</strong>                                                                                <strong>keyword:</strong> Fall Detection, Device-free, Deep learning</p>
<h4 id="Phenomenon-amp-Challenge"><a href="#Phenomenon-amp-Challenge" class="headerlink" title="Phenomenon&amp;Challenge:"></a>Phenomenon&amp;Challenge:</h4><ol>
<li>These revelations have led to new passive sensors that infer falls by analyzing Radio Frequency (RF) signals in homes.</li>
<li>They typically train and test their classifiers on the same people in the same environments, and cannot generalize to new people or new environments</li>
<li>they cannot separate motions from different people and can easily miss a fall in the presence of other motions.</li>
</ol>
<h4 id="RelatedWork"><a href="#RelatedWork" class="headerlink" title="RelatedWork:"></a>RelatedWork:</h4><ol>
<li><p>proposed systems that transmit a low power RF signal and analyze its reflections off people’s bodies to infer falls [4, 15, 35, 41, 45, 56, 58, 59].</p>
</li>
<li><p>State-of-the-art RF-based fall detection systems can be divided into two categories: The first category is based on Doppler radar [15, 22, 45]. These solutions exploit the relationship between the Doppler frequency and motion velocity. They associate falls with a spike in the Doppler frequency due to a fast fall motion. The second category is based on WiFi channel state information (CSI) [41, 56, 58, 59]. While this category differs in its input signal, it typically relies on the same basic principle.</p>
</li>
<li><p>convolutional neural networks (CNNs) [31], which have demonstrated the ability to extract complex patterns from various types of signals, such as images and videos [16, 20, 30, 51, 52, 57, 60, 62, 63]. </p>
</li>
<li><p>wearable devices include accelerometers [12, 33], smart phones [1, 12], RFID [10], etc.non-wearable technologies, cameras [32, 39] are accurate but they invade people’s privacy, Audio and vibration based sensors [5, 34] have a relatively low accuracy due to interference from the environment [38]. Pressure mats and pulling cords work only when the fall occurs near the installed device [37]. </p>
</li>
<li><p>past papers on RF-based fall monitoring [15, 35, 41, 45, 56, 58, 59]</p>
</li>
<li><p>Convolutional Neural Networks (CNN) have been the main workhorse of recent breakthroughs in </p>
<p>understanding images [30], videos [28, 55] and audio signals [7, 53]. object detection [30], image segmentation [43], speech synthesis [53], machine translation [26], and AlphaGo [47]</p>
</li>
</ol>
<h4 id="Contribution"><a href="#Contribution" class="headerlink" title="Contribution:"></a>Contribution:</h4><ol>
<li><em>Dealing with complex falls and fast non-fall movements</em></li>
<li><em>Generalization to new homes and new people:</em></li>
<li><em>Detect falls in the presence of other motion</em></li>
<li>first convolutional neural network architecture for RF-based fall detection. Our CNN design extracts complex spatio-temporal information about body motion from RF signals. As a result, it can characterize complex falls and fast non-fall motions, separate a fall from other motions in the environment, and generalize to new environments and people. </li>
<li>multi-function design that combines fall detection with the ability to infer stand-up events and fall duration</li>
<li>an extensive empirical evaluation with multiple sources of motion</li>
</ol>
<h4 id="Innovation-amp-consolution"><a href="#Innovation-amp-consolution" class="headerlink" title="Innovation&amp;consolution:"></a>Innovation&amp;consolution:</h4><ol>
<li>an RF-based fall detection system that uses convolutional neural networks governed by a state machine</li>
<li>works with new people and environments unseen in the training set</li>
<li>FMCW can separate RF reflections based on distance from the reflecting body, and the vertical and horizontal arrays separate reflections based on their elevation and azimuthal angles</li>
</ol>
<h4 id="Chart-amp-Analyse"><a href="#Chart-amp-Analyse" class="headerlink" title="Chart&amp;Analyse:"></a>Chart&amp;Analyse:</h4><ol>
<li>combine two CNNs: the first detects a fall event while the second detects a stand-up event. The two networks are coordinated via a state machine that tracks the transition of a person from a normal state to a fall state, and potentially back to a normal state.</li>
</ol>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/1565400278880.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/1565400463270.png" alt="1565400463270"></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/1565400404847.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/1565400547010.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/1565400626118.png" alt=""></p>
<h1 id="Paper6《TagFree-Activity-Identifification-with-RFIDs》"><a href="#Paper6《TagFree-Activity-Identifification-with-RFIDs》" class="headerlink" title="Paper6《TagFree Activity Identifification with RFIDs》"></a>Paper6《TagFree Activity Identifification with RFIDs》</h1><p><strong>cited:</strong>                                                                                <strong>keyword:</strong> Network mobility; Sensor networks</p>
<h4 id="Phenomenon-amp-Challenge-1"><a href="#Phenomenon-amp-Challenge-1" class="headerlink" title="Phenomenon&amp;Challenge:"></a>Phenomenon&amp;Challenge:</h4><ol>
<li>the accuracy of the readings can be noticeably affected by multipath, which unfortunately is inevitable in an indoor environment and is complicated with multiple reference tags. </li>
<li>human activity identification has become a key service in many IoT applications, such as healthcare and smart homes [1]. </li>
<li>the peak amplitudes may dramatically change in a short time, which could be filtered out as noises for activity identification.</li>
</ol>
<h4 id="RelatedWork-1"><a href="#RelatedWork-1" class="headerlink" title="RelatedWork:"></a>RelatedWork:</h4><ol>
<li><p>TagFree can further facilitate various smart home applications, e.g., activity-based temperature adjustment in homes or exercise assistant equipment in gyms.</p>
</li>
<li><p>Unfortunately, the activity information inferred from the raw RSSI can be quite unreliable and inaccurate for small movement.</p>
</li>
<li><p>Radio Frequency Identifification (RFID) is a promising technology due to its low cost, small form size, </p>
<p>and batterylessness, making it widely used in a range of mobile applications, including detection of </p>
<p><font color="red">human-object interaction [25], people/object tracking [31] and more complex activity identifification </font></p>
</li>
<li><p>previous solutions exploited the changing of RSSI (received signal strength </p>
<p>indicator) [35][5][19] incurred by human actions. Yet RSSI is insensitive to small body movement, and </p>
<p>thus difficult to achieve high-precision identifification</p>
</li>
<li><p>LSTM networks have been successfully applied to many tasks such as handwriting [9] and speech recognition [10].</p>
</li>
<li><p>RF-compass [24] presented a WiFi-based approach to classify a predefifined set of nine gestures; E-eyes [28] proposed a location-oriented activity identifification system, which utilized WiFi signals to recognize </p>
<p>in-home human activities; Ding et al. [6] further developed FEMO that uses the frequency shifts of the movements to determine what exercise a user is performing.</p>
</li>
</ol>
<h4 id="Innovation-amp-consolution-1"><a href="#Innovation-amp-consolution-1" class="headerlink" title="Innovation&amp;consolution:"></a>Innovation&amp;consolution:</h4><ol>
<li>TagFree gathers massive angle information as spectrum frames from multiple tags, and preprocesses them to extract key features. It then analyzes their patterns through a deep learning framework（h Convolutional Neural Network (CNN) [15] and Long Short Term Memory (LSTM) network [13]）</li>
<li>Our experiments suggest that both the backscattered signal power and angle are highly related to human activities, impacting multiple paths with different levels.</li>
<li>DataProcessing<ol>
<li><strong>Phase calibration</strong> different frequencies induce different initial phase-offffsets at the reader. We accordingly design a mechanism to calibrate the phase difference between frequencies</li>
<li><strong>Multipath Decoupling</strong> ???? in practice the AoA estimation may not work well because of the multipath effffect.in order to The M higher peaks are of great power [20] and corresponds to the estimated direction of arrival of the signal source with the angles θ1, . . . , θM</li>
</ol>
</li>
</ol>
<h4 id="Chart-amp-Analyse-1"><a href="#Chart-amp-Analyse-1" class="headerlink" title="Chart&amp;Analyse:"></a>Chart&amp;Analyse:</h4><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/1566090279410.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/1566091141662.png" alt=""></p>
<h4 id="Shortcoming"><a href="#Shortcoming" class="headerlink" title="Shortcoming:"></a>Shortcoming:</h4><ol>
<li>Multiple emitter location and signal parameter estimation 待读论文</li>
<li>#A novel connectionist system for unconstrained handwriting recognition</li>
<li>#Speech recognition with deep recurrent neural networks</li>
<li>A platform for free-weight exercise monitoring with rfifids</li>
<li>robot object manipulation using RFIDs</li>
<li>device-free location-oriented activity identifification using fifine-grained WiFi signatures</li>
<li>Beyond short snippets: Deep networks for video classifification.</li>
<li>RF-IDraw: virtual touch screen in the air using RF signals</li>
<li>Relative Localization of RFID Tags using Spatial-Temporal Phase Profifiling</li>
<li>Deep Learning for RFID-Based Activity Recognition</li>
<li>22,25,6</li>
</ol>
<h1 id="Paper7《Through-Wall-Human-Pose-Estimation-Using-Radio-Signals》"><a href="#Paper7《Through-Wall-Human-Pose-Estimation-Using-Radio-Signals》" class="headerlink" title="Paper7《Through-Wall Human Pose Estimation Using Radio Signals》"></a>Paper7《<strong>Through-Wall Human Pose Estimation Using Radio Signals</strong>》</h1><h4 id="Phenomenon-amp-Challenge-2"><a href="#Phenomenon-amp-Challenge-2" class="headerlink" title="Phenomenon&amp;Challenge:"></a>Phenomenon&amp;Challenge:</h4><ol>
<li>infeasible when the person is fully occluded, behind a wall or in a different room. </li>
<li>In particular, there is no labeled data for this task. It is also infeasible for humans to annotate radio signals with keypoints.</li>
<li>some body parts may not reflect much RF signals towards our sensor, and hence may be de-emphasized or missing in some heatmaps, even though they are not occluded</li>
</ol>
<h4 id="RelatedWork-2"><a href="#RelatedWork-2" class="headerlink" title="RelatedWork:"></a>RelatedWork:</h4><ol>
<li><strong>Computer Vision:</strong> Human pose estimation from RGB images generally falls into two main categories: Top-down and bottom-up methods. Top-down methods [16, 14, 29, 15] fifirst detect each people in the image, and then apply a single-person pose estimator to each people to extract keypoints. Bottom-up methods [10, 31, 20], on the other hand, fifirst detect all keypoints in the image, then use post processing to associate the keypoints belonging to the same person</li>
<li><strong>Wireless Systems:</strong> Recent years have witnessed much interest in localizing people and tracking their motion using wireless signals. The literature can be classifified into two categories. The fifirst category operates at very high frequencies (e.g., millimeter wave or terahertz) [3].The second category uses lower frequencies, around a few GHz, and hence can track people through walls and occlusions.The second category uses lower frequencies, around a few GHz, and hence can track people through walls and occlusions.</li>
</ol>
<h4 id="Contribution-1"><a href="#Contribution-1" class="headerlink" title="Contribution:"></a>Contribution:</h4><ol>
<li><em>the system uses synchronized wireless and visual inputs, extracts pose information from the visual stream, and uses it to guide the training</em> process.</li>
<li>We first perform non-maximum suppression on the keypoint confifidence maps to obtain discrete peaks of keypoint candidates. To associate keypoints of different persons, we use the relaxation method proposed by Cao <em>et al</em>. [10] and we use Euclidean distance for the weight of two candidates</li>
</ol>
<h4 id="Innovation-amp-consolution-2"><a href="#Innovation-amp-consolution-2" class="headerlink" title="Innovation&amp;consolution:"></a>Innovation&amp;consolution:</h4><ol>
<li>we make the network learn to aggregate information from multiple snapshots of RF heatmaps so that it can capture different limbs and model the dynamics of body movement. </li>
</ol>
<h4 id="Chart-amp-Analyse-2"><a href="#Chart-amp-Analyse-2" class="headerlink" title="Chart&amp;Analyse:"></a>Chart&amp;Analyse:</h4><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/1566125432899.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/1566125432899.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/1566123900007.png" alt=""></p>
<h4 id="Shortcoming-amp-confusing"><a href="#Shortcoming-amp-confusing" class="headerlink" title="Shortcoming&amp;confusing:"></a>Shortcoming&amp;confusing:</h4><ol>
<li><p>First, the human body is opaque at the frequencies of interest – i.e., frequencies that traverse walls</p>
</li>
<li><p>the operating distance of a radio is dependent on its transmission power</p>
</li>
<li><p>less activity recognize</p>
</li>
<li><p>待读论文如下</p>
</li>
<li><p>Realtime multiperson 2D pose estimation using part affifinity fifields</p>
</li>
<li><p>3D convolutional neural networks for human action recognition</p>
</li>
<li><p>Learning spatiotemporal features with 3D convolutional networks.</p>
</li>
<li><p>Temporal segment networks: Towards good practices for deep action recognition.</p>
</li>
<li><p>Microsoft COCO: Com-mon objects in context.</p>
</li>
<li><p>Realtime multiperson 2D pose estimation using part affifinity fifields</p>
</li>
</ol>
<h1 id="Paper8《Compressive-Representation-for-Device-Free-Activity-Recognition-with-Passive-RFID-Signal-Strength》"><a href="#Paper8《Compressive-Representation-for-Device-Free-Activity-Recognition-with-Passive-RFID-Signal-Strength》" class="headerlink" title="Paper8《Compressive Representation for Device-Free Activity Recognition with Passive RFID Signal Strength》"></a>Paper8《Compressive Representation for Device-Free Activity Recognition with Passive RFID Signal Strength》</h1><h4 id="Chart-amp-Analyse-3"><a href="#Chart-amp-Analyse-3" class="headerlink" title="Chart&amp;Analyse:"></a>Chart&amp;Analyse:</h4><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/1566127402044.png" alt=""></p>
<h1 id="Paper《Sharing-the-Load-Human-Robot-Team-Lifting-Using-Muscle-Activity》"><a href="#Paper《Sharing-the-Load-Human-Robot-Team-Lifting-Using-Muscle-Activity》" class="headerlink" title="Paper《Sharing the Load: Human-Robot Team Lifting Using Muscle Activity》"></a>Paper《<strong>Sharing the Load: Human-Robot Team Lifting Using Muscle Activity</strong>》</h1><h4 id="RelatedWork-3"><a href="#RelatedWork-3" class="headerlink" title="RelatedWork:"></a>RelatedWork:</h4><ol>
<li><em>Human-Robot Interaction</em> using modalities such as vision, speech, force sensors, and gesture tracking datagloves [8], [9], [10], [11], [12].</li>
<li><em>Using Muscle Signals for Robot Control</em> EMG can yield effective human-robot interfaces, but also demonstrate associated challenges such as noise, variance between users, and complex muscle dynamics.</li>
</ol>
<h4 id="Contribution-2"><a href="#Contribution-2" class="headerlink" title="Contribution:"></a>Contribution:</h4><ol>
<li><em>•</em> an algorithm to continuously estimate a lifting setpoint from biceps activity, roughly matching a person’s hand height while also providing a closed-loop control interface for quickly commanding coarse adjustments;</li>
<li><em>•</em> a plug-and-play rolling classififier for detecting up or down gestures from biceps and triceps activity</li>
<li>an end-to-end system integrating these pipelines to collaboratively lift objects with a robot using only muscle activity associated with the task; </li>
</ol>
<h4 id="Innovation-amp-consolution-3"><a href="#Innovation-amp-consolution-3" class="headerlink" title="Innovation&amp;consolution:"></a>Innovation&amp;consolution:</h4><ol>
<li>The setpoint algorithm aims to estimate changes in the person’s hand height while also creating a task-based control interface.</li>
</ol>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/1566196090162.png" alt=""></p>
<h4 id="Chart-amp-Analyse-4"><a href="#Chart-amp-Analyse-4" class="headerlink" title="Chart&amp;Analyse:"></a>Chart&amp;Analyse:</h4><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/1566195602286.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/1566197637640.png" alt=""></p>
<h4 id="Shortcoming-amp-Question"><a href="#Shortcoming-amp-Question" class="headerlink" title="Shortcoming&amp;Question:"></a>Shortcoming&amp;Question:</h4><ol>
<li>don’t understand SetpointAlgorithm? </li>
<li>after feature extraction,what do i get? how to detect up and down categories according to musal activities????</li>
</ol>
<h1 id="Paper《Emotion-Recognition-using-Wireless-Signals》"><a href="#Paper《Emotion-Recognition-using-Wireless-Signals》" class="headerlink" title="Paper《Emotion Recognition using Wireless Signals》"></a>Paper《<strong>Emotion Recognition using Wireless Signals</strong>》</h1><h4 id="Phenomenon-amp-Challenge-3"><a href="#Phenomenon-amp-Challenge-3" class="headerlink" title="Phenomenon&amp;Challenge:"></a>Phenomenon&amp;Challenge:</h4><ol>
<li><p>Emotion recognition is an emerging field that has attracted much interest from both the industry and the research community [52, 16, 30, 47, 23].</p>
</li>
<li><p>measure inner feelings [14, 48, 21]</p>
</li>
<li><p>Recent research has shown that such RF reflfections can be used to measure a person’s </p>
<p>breathing and average heart rate without body contact [7, 19, 25, 45, 31].</p>
</li>
<li><p>RF signals reflected off a person’s body are modulated by both breathing and heartbeats.</p>
</li>
<li><p>heartbeats in the RF signal lack the sharp peaks which characterize the ECG signal, making it harder to accurately identify beat boundaries</p>
</li>
<li><p>the difffference in inter-beat-intervals (IBI) is only a few tens of milliseconds.</p>
</li>
<li><p>the shape of a heartbeat in RF reflflections is unknown and varies depending on the person’s body and exact posture with respect to the device</p>
</li>
</ol>
<h4 id="RelatedWork-4"><a href="#RelatedWork-4" class="headerlink" title="RelatedWork:"></a>RelatedWork:</h4><ol>
<li>Existing approaches for inferring a person’s emotions either rely on audiovisual cues, such as images and audio clips [64, 30, 54], or require the person to wear physiological sensors like an ECG monitor [28, 48, 34, 8].</li>
<li><strong>Emotion Recognition</strong>:they extract emotionrelated signals (e.g., audio-visual cues or physiological signals); second, they feed these signals into a classififier in order to recognize emotions. Existing approaches for extracting emotion-related signals fall under two categories: audiovisual techniques and physiological techniques.</li>
<li><strong>RF-based Sensing</strong> it transmits an RF signal and analyzes its reflflections to track user locations [5], gestures [6, 50, 56, 10, 61, 3], activities [59, 60], and vital signs [7, 19, 20].</li>
<li>past work that does not require users to hold their breath has an average error of 30-50 milliseconds [13, 40, 27]</li>
</ol>
<h4 id="Contribution-3"><a href="#Contribution-3" class="headerlink" title="Contribution:"></a>Contribution:</h4><ol>
<li>demonstrates the feasibility of emotion recognition using RF re-flections offff one’s body. </li>
<li>introduces a new algorithm for extracting individual heartbeats from RF reflections offff the human body.</li>
<li>Mitigating the Impact of Breathing  &amp;&amp;  Heartbeat Segmentation</li>
<li>EMOTION CLASSIFICATION：Feature Selection and Classifification</li>
</ol>
<h4 id="Innovation-amp-consolution-4"><a href="#Innovation-amp-consolution-4" class="headerlink" title="Innovation&amp;consolution:"></a>Innovation&amp;consolution:</h4><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/1566201185377.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/1566201185377.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/1566201185377.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/1566201185377.png" alt=""></p>
<h4 id="Chart-amp-Analyse-5"><a href="#Chart-amp-Analyse-5" class="headerlink" title="Chart&amp;Analyse:"></a>Chart&amp;Analyse:</h4><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/1566201185377.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/1566201185377.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/1566200846046.png" alt=""></p>
<h4 id="Code"><a href="#Code" class="headerlink" title="Code:"></a>Code:</h4><h4 id="Shortcoming-amp-Question-1"><a href="#Shortcoming-amp-Question-1" class="headerlink" title="Shortcoming&amp;Question:"></a>Shortcoming&amp;Question:</h4><ol>
<li><p>测试环节没有细看跳过。</p>
</li>
<li><p>what’s IBI features???</p>
</li>
<li><p>待读论文：1-norm support vector machines. Advances in neural information processing systems</p>
</li>
<li><p>machine emotional intelligence: Analysis of affffective physiological state</p>
</li>
<li><p>Comparison of detrended flfluctuation analysis and spectral analysis for heart rate variability </p>
<p>in sleep and sleep apnea 2003</p>
</li>
<li><p>Sample entropy analysis of neonatal heart rate variability  2002</p>
</li>
<li><p>Emotion recognition based on physiological changes in music listening 2008</p>
</li>
<li><p>Physiological signals based human emotion  recognition: a review 2011</p>
</li>
<li><p>An introduction to variable and feature selection     2003</p>
</li>
</ol>
<h1 id="Paper《Interacting-with-Soli-Exploring-Fine-Grained-Dynamic-Gesture-Recognition-in-the-Radio-Frequency-Spectrum》"><a href="#Paper《Interacting-with-Soli-Exploring-Fine-Grained-Dynamic-Gesture-Recognition-in-the-Radio-Frequency-Spectrum》" class="headerlink" title="Paper《Interacting with Soli: Exploring Fine-Grained Dynamic Gesture Recognition in the Radio-Frequency Spectrum》"></a>Paper《<strong>Interacting with Soli: Exploring Fine-Grained Dynamic</strong> <strong>Gesture Recognition in the Radio-Frequency Spectrum</strong>》</h1><p><strong>cited:</strong>                                        <strong>keyword:</strong> gesture recognition; wearables; deep learning; radar sensing</p>
<h4 id="Phenomenon-amp-Challenge-4"><a href="#Phenomenon-amp-Challenge-4" class="headerlink" title="Phenomenon&amp;Challenge:"></a>Phenomenon&amp;Challenge:</h4><ol>
<li><p>sensing in the electro-magnetic spectrum eschews spatial information for temporal resolution. Capturing a superposition of reflflected energy from multiple parts of the hand such as the palm or fingertips, the signal is therefore not directly suitable* to reconstruct the spatial structure or the </p>
<p>shape of objects in front of the sensor</p>
</li>
</ol>
<h4 id="RelatedWork-5"><a href="#RelatedWork-5" class="headerlink" title="RelatedWork:"></a>RelatedWork:</h4><ol>
<li>Google Soli resolving motion at a very fifine level and allowing for segmentation in range and velocity spaces rather than image space.</li>
</ol>
<h4 id="Contribution-4"><a href="#Contribution-4" class="headerlink" title="Contribution:"></a>Contribution:</h4><ol>
<li>a novel end-to-end trained stack of convolutional and recurrent neural networks (CNN/RNN) for RF signal based <em>dynamic</em> gesture recognition</li>
<li>an in-depth analysis of sensor signal properties and highlight inherent issues in traditional frame-level approaches</li>
</ol>
<h4 id="Chart-amp-Analyse-6"><a href="#Chart-amp-Analyse-6" class="headerlink" title="Chart&amp;Analyse:"></a>Chart&amp;Analyse:</h4><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/1566205024703.png" alt=""></p>
<h1 id="Paper《Compressive-Representation-for-Device-Free-Activity-Recognition-with-Passive-RFID-Signal-Strength》"><a href="#Paper《Compressive-Representation-for-Device-Free-Activity-Recognition-with-Passive-RFID-Signal-Strength》" class="headerlink" title="Paper《Compressive Representation for Device-Free Activity Recognition with Passive RFID Signal Strength》"></a>Paper《Compressive Representation for Device-Free Activity Recognition with Passive RFID Signal Strength》</h1><p><strong>cited:</strong>                                         <strong>keyword:</strong> —Activity recognition, RFID, compressive sensing</p>
<h4 id="Phenomenon-amp-Challenge-5"><a href="#Phenomenon-amp-Challenge-5" class="headerlink" title="Phenomenon&amp;Challenge:"></a>Phenomenon&amp;Challenge:</h4><ol>
<li>RSSI is quite complicated in real environments due to signal reflection,diffraction,and scattering,especially for the passive RFID tags.</li>
</ol>
<h4 id="RelatedWork-6"><a href="#RelatedWork-6" class="headerlink" title="RelatedWork:"></a>RelatedWork:</h4><ol>
<li>many efforts have been made to learn human activities by mining from a broad range of signal sources, such as videos and images [6], radio frequency of wearable or wireless sensors [7], [8], Wi-Fi [9], and even object vibration flfluctuations [10].</li>
<li>Fall Detection</li>
<li>Sleep Monitoring</li>
<li>Ambulatory Monitoring.Posture recognition and monitoring are critical in the medical care.</li>
</ol>
<h4 id="Contribution-5"><a href="#Contribution-5" class="headerlink" title="Contribution:"></a>Contribution:</h4><ol>
<li><p>The system interprets what a person is doing by deciphering signal flfluctuations using radio-frequency identifification (RFID) technology and machine learning algorithms</p>
</li>
<li><p>compressive sensing, dictionary-based approach that can learn a set of compact and informative dictionaries of activities using an unsupervised subspace decomposition.</p>
</li>
<li><p>propose a dictionary learning approach to uncover the structural information between RSSI </p>
<p>signals of different activities by learning the compact and discriminative dictionaries per activity</p>
</li>
<li><p>model each predefifined human activity by learning discriminative dictionaries and its corresponding sparse coeffificients using features extracted and selected from raw RSSI streams.</p>
</li>
<li><p>develop a compressive sensing dictionary-based learning approach to uncover structural information </p>
<p>among RFID signals of different activities. </p>
</li>
<li><p>propose a lightweight but effective feature selection method to assist the extraction of more discrimi</p>
<p>native signal patterns from noisy RFID streams.</p>
</li>
</ol>
<h4 id="Chart-amp-Analyse-7"><a href="#Chart-amp-Analyse-7" class="headerlink" title="Chart&amp;Analyse:"></a>Chart&amp;Analyse:</h4><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/1566367769398.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/1566368209269.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/1566368345130.png" alt=""></p>
<ol>
<li>the variations of signal strength reflect different patterns,which can be exploited to distinguish different activities.</li>
</ol>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/1566368345130.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/1566368345130.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/1566368345130.png" alt=""></p>
<ol>
<li>但系统框架没看懂具体怎么处理的，individual segments 使用滑动窗口处理，但不明白在不知道起点的情况下如何更具滑动窗口进行切割。</li>
</ol>
<h4 id="Shortcoming-amp-Confusion"><a href="#Shortcoming-amp-Confusion" class="headerlink" title="Shortcoming&amp;Confusion:"></a>Shortcoming&amp;Confusion:</h4><ol>
<li>Sparse coding is a common technique to model data vectors as sparse linear combinations (i.e., sparse representation) of basis elements, and has been widely used in image processing and computer vision applications [23], [24], [25].        Dontkonw the Sparse coding</li>
</ol>
<h1 id="Paper《RF-Based-3D-Skeletons》"><a href="#Paper《RF-Based-3D-Skeletons》" class="headerlink" title="Paper《RF-Based 3D Skeletons》"></a>Paper《<strong>RF-Based 3D Skeletons</strong>》</h1><p><strong>cited:</strong>        <strong>keyword:</strong> RF Sensing, 3D Human Pose Estimation, Machine Learning, Neural Networks, Localization, Smart Homes</p>
<h4 id="Phenomenon-amp-Challenge-6"><a href="#Phenomenon-amp-Challenge-6" class="headerlink" title="Phenomenon&amp;Challenge:"></a>Phenomenon&amp;Challenge:</h4><ol>
<li>images have high spatial resolution whereas RF signals have low spatial resolution, even when using multi-antenna systems</li>
<li>only a few body parts are visible to the radio [1]</li>
<li>Existing datasets for inferring 3D poses from images are limited to one environment or one person (e.g., Human3.6M [7])</li>
</ol>
<h4 id="RelatedWork-7"><a href="#RelatedWork-7" class="headerlink" title="RelatedWork:"></a>RelatedWork:</h4><ol>
<li><p>Novel algorithms have led to accurate localization within tens of centimeters [19, 34]. Advanced sensing technologies have enabled people tracking based on the RF signals that bounce off their bodies, even when they do not carry any wireless transmitters [2, 17, 35]. Various papers have developed clas</p>
<p>sifiers that use RF reflections to detect actions like falling, walking, sitting, etc. [21, 23, 32].</p>
</li>
<li><p><strong>Wireless Systems:</strong>Different papers localize the people in the environment [2, 17], monitor their walking speed [15, 31], track their chest motion to extract breathing and heartbeats [3, 39, 41], or track the arm motion to identify a particular gesture [21, 23]. </p>
</li>
<li><p><strong>Computer Vision:</strong>2D pose estimation has achieved remarkable success recently [6, 8, 11, 13, 16, 22, 33].  advances in 3D human pose estimation remain limited due to the difficulty and ambiguity of recovering 3D information from 2D images. </p>
</li>
</ol>
<h4 id="Innovation-amp-consolution-5"><a href="#Innovation-amp-consolution-5" class="headerlink" title="Innovation&amp;consolution:"></a>Innovation&amp;consolution:</h4><ol>
<li>RF-Pose3D provides a significant leap in RF-based sensing and enables new applications in gaming, healthcare, and smart homes</li>
<li>model the relationship between the observed radio waves and the human body, as well as the constraints on the location and movement of different body parts.</li>
<li><strong>Sensing the 3D Skeleton:</strong> common deep learning platforms (e.g., Pytorch, Tensorflow) do not support 4D CNNs   we leverage the properties of RF signals to decompose 4D convolutions into a combination of 3D convolutions performed on two planes and the time axis. We also decompose CNN training and inference to operate on those two planes.</li>
<li><strong>Scaling to Multiple People:</strong>  run past localization algorithms, locate each person in the scene, and zoom in on signals from that location. The drawbacks of such approach are: 1) localization errors will lead to errors in skeleton estimation, and 2) multipath effects can create fictitious people. instead of zooming in on people in the physical space, the network first transforms the RF signal into an abstract domain that condenses the relevant information, then separates the information pertaining to different individuals in the abstract domain</li>
<li><strong>Testing</strong> given an image of people, identifies the pixels that correspond to their keypoints [6]. develop a coordinated system of 12 cameras. We collect 2D skeletons from each camera, and design an optimization problem based on multi-view geometry to find the 3D location of each keypoint of each person</li>
</ol>
<h4 id="Chart-amp-Analyse-8"><a href="#Chart-amp-Analyse-8" class="headerlink" title="Chart&amp;Analyse:"></a>Chart&amp;Analyse:</h4><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/1566371475791.png" alt=""></p>
<h4 id="Shortcoming-amp-Confusion-1"><a href="#Shortcoming-amp-Confusion-1" class="headerlink" title="Shortcoming&amp;Confusion:"></a>Shortcoming&amp;Confusion:</h4><ol>
<li>openpose[6]</li>
<li>只了解了大概，离具体复现差很多，里面具体的设计细节不清楚</li>
</ol>
<h1 id="Paper《RF-Dial-an-RFID-based-2D-Human-Computer-Interaction-via-Tag-Array-》"><a href="#Paper《RF-Dial-an-RFID-based-2D-Human-Computer-Interaction-via-Tag-Array-》" class="headerlink" title="Paper《RF-Dial: an RFID-based 2D Human-Computer Interaction via Tag Array 》"></a>Paper《RF-Dial: an RFID-based 2D Human-Computer Interaction via Tag Array 》</h1><h4 id="Contribution-6"><a href="#Contribution-6" class="headerlink" title="Contribution:"></a>Contribution:</h4><ol>
<li><p>propose a novel scheme of 2D human-computer interaction, by attaching a tag array on the the surface of an ordinary object, thus turning it into an intelligent HCI device</p>
</li>
<li><p>to track the rigid transformation including translation and rotation, we build a geometric model to depict the relationship between the phase variations of the tag array and the rigid transformation of the tagged object. By referring to the fifixed topology of at least two tags from the tag array, we are able to accurately estimate the 2D rigid body motion of the object</p>
</li>
<li><p>implemented a prototype system of RF-Dial with COTS RFID </p>
<p>and evaluated its performance in the real environment</p>
</li>
</ol>
<h4 id="Innovation-amp-consolution-6"><a href="#Innovation-amp-consolution-6" class="headerlink" title="Innovation&amp;consolution:"></a>Innovation&amp;consolution:</h4><ol>
<li>RF-Pose3D provides a significant leap in RF-based sensing and enables new applications in gaming, healthcare, and smart homes. </li>
</ol>
<h4 id="Chart-amp-Analyse-9"><a href="#Chart-amp-Analyse-9" class="headerlink" title="Chart&amp;Analyse:"></a>Chart&amp;Analyse:</h4><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/1570259323573.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/1570259710101.png" alt=""></p>
<ol>
<li><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/1570259415686.png" alt=""></li>
</ol>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/1570259403205.png" alt=""></p>
<h1 id="Paper《Multi-Target-Intense-Human-Motion-Analysis-and-Detection-Using-Channel-State-Information》"><a href="#Paper《Multi-Target-Intense-Human-Motion-Analysis-and-Detection-Using-Channel-State-Information》" class="headerlink" title="Paper《Multi-Target Intense Human Motion Analysis and Detection Using Channel State Information》"></a>Paper《Multi-Target Intense Human Motion Analysis and Detection Using Channel State Information》</h1><p><strong>lab</strong>:Key Laboratory for Ubiquitous Network and Service Software of Liaoning Province, School of Software,</p>
<p><strong>meeting</strong>:In Proceedings of the 2017 IEEE<br>19th International Conference on e-Health Networking, Applications and Services (Healthcom),=12–15 October 2017.</p>
<p><strong>cited:</strong>           <strong>keyword:</strong> </p>
<h4 id="Phenomenon-amp-Challenge-7"><a href="#Phenomenon-amp-Challenge-7" class="headerlink" title="Phenomenon&amp;Challenge:"></a>Phenomenon&amp;Challenge:</h4><ol>
<li>intense human motion usually has the characteristics of intensity, rapid change, irregularity, large amplitude, and continuity</li>
</ol>
<h4 id="RelatedWork-8"><a href="#RelatedWork-8" class="headerlink" title="RelatedWork:"></a>RelatedWork:</h4><ol>
<li>Camera-Based Human Motion Detection crowd counting gesture recognition [3], target tracking [4], violence detection</li>
<li>Wi-Fi-Based Passive Human Detection many research studies have realized passive human detection by leveraging the variance of<br>Received Signal Strength Indicator (RSSI) on the receiver [8?11].</li>
<li>Wi-Fi-Based Activity Recognition</li>
</ol>
<h4 id="Contribution-7"><a href="#Contribution-7" class="headerlink" title="Contribution:"></a>Contribution:</h4><ol>
<li>finding out the pattern of the relationship between human motion and CSI variation. Then, we extract the feature from CSI to depict different human motion,and use machine learning methods to detect intense human motion from human activities</li>
<li>analyzed the signal variation difference under LOS and NLOS conditions, and then we identify whether the current wireless link status.</li>
<li>human motion detection system which can be deployed on the Wi-Fi APs</li>
</ol>
<h4 id="Innovation-amp-consolution-7"><a href="#Innovation-amp-consolution-7" class="headerlink" title="Innovation&amp;consolution:"></a>Innovation&amp;consolution:</h4><ol>
<li>RF-Pose3D provides a significant leap in RF-based sensing and enables new applications in gaming, healthcare, and smart homes. </li>
</ol>
<h4 id="Chart-amp-Analyse-10"><a href="#Chart-amp-Analyse-10" class="headerlink" title="Chart&amp;Analyse:"></a>Chart&amp;Analyse:</h4><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/1571718080714.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/1571718345782.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/1571718365293.png" alt=""></p>
<h4 id="Code-1"><a href="#Code-1" class="headerlink" title="Code:"></a>Code:</h4><h4 id="Shortcoming-amp-Confusion-2"><a href="#Shortcoming-amp-Confusion-2" class="headerlink" title="Shortcoming&amp;Confusion:"></a>Shortcoming&amp;Confusion:</h4><ol>
<li>Position-independent indicator:</li>
<li>Multiple targets settings:</li>
<li>People counting:</li>
<li>Training-free human motion detection</li>
</ol>
<h1 id="Paper《Enabling-Contactless-Detection-of-Moving-Humans"><a href="#Paper《Enabling-Contactless-Detection-of-Moving-Humans" class="headerlink" title="Paper《Enabling Contactless Detection of Moving Humans"></a>Paper《Enabling Contactless Detection of Moving Humans</h1><p>with Dynamic Speeds Using CSI》</p>
<h4 id="RelatedWork-9"><a href="#RelatedWork-9" class="headerlink" title="RelatedWork:"></a>RelatedWork:</h4><ol>
<li>RSS-based detection</li>
<li>CSI-based detection</li>
<li>Detection as prerequisite.</li>
</ol>
<h4 id="Contribution-8"><a href="#Contribution-8" class="headerlink" title="Contribution:"></a>Contribution:</h4><ol>
<li>passive human detection leveraging full information of CSI.</li>
<li>a novel unified feature using the eigenvalue of the correlation matrix of CSI.<br>The feature holds excellent properties for device-free detection due to its stability for both amplitude and phase and irrelevance to specific power parameters that vary over different links and over time and space.</li>
<li>space diversity provided by multiantennas supported by modern MIMO com<br>municating systems to enable more accurate and robust detection</li>
</ol>
<h4 id="Innovation-amp-consolution-8"><a href="#Innovation-amp-consolution-8" class="headerlink" title="Innovation&amp;consolution:"></a>Innovation&amp;consolution:</h4><ol>
<li>using linear transformation on phase information of CSI, we apply phase differences across antennas as a new<br>feature.</li>
</ol>
<h4 id="Chart-amp-Analyse-11"><a href="#Chart-amp-Analyse-11" class="headerlink" title="Chart&amp;Analyse:"></a>Chart&amp;Analyse:</h4><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/1571726206671.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/1571726307850.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/1571726326851.png" alt=""></p>
<h1 id="Paper《HuAc-Human-Activity-Recognition-Using-Crowdsourced-WiFi-Signals-and-Skeleton-Data》"><a href="#Paper《HuAc-Human-Activity-Recognition-Using-Crowdsourced-WiFi-Signals-and-Skeleton-Data》" class="headerlink" title="Paper《HuAc: Human Activity Recognition Using Crowdsourced WiFi Signals and Skeleton Data》"></a>Paper《HuAc: Human Activity Recognition Using Crowdsourced WiFi Signals and Skeleton Data》</h1><h4 id="RelatedWork-10"><a href="#RelatedWork-10" class="headerlink" title="RelatedWork:"></a>RelatedWork:</h4><ol>
<li>Kinect-Based Activity Recognition  keleton joints overlapping, and position-dependence factors.</li>
<li>WiFi-Based Activity Recognition WiFall system [2] detects a fall behavior<br>by learning the specific CSI pattern.WiFall system [2] E-eyes [9] recognizes<br>walking activity and in-place activity by adopting movingvariance of CSI and fingerprint technique.CARM [10] shows the correlation between CSI<br>value andhumanactivityby constructingCSI-speedandCSI<br>activity model.</li>
</ol>
<h4 id="Contribution-9"><a href="#Contribution-9" class="headerlink" title="Contribution:"></a>Contribution:</h4><ol>
<li>We propose a HuAc system to recognize human<br>activity and also construct a WiFi-based activity<br>recognition dataset named WiAR as a benchmark to<br>evaluate the performance of existing activity recognition<br>systems. We use the kNN, Random Forest, and<br>Decision Tree algorithms to verify the effectiveness of<br>theWiAR dataset</li>
<li>We detect the start and end of the activity using<br>the moving variance of CSI. Moreover, we leverage<br>𝐾-means algorithm to cluster effective subcarriers<br>according to subcarrier’s sensitivity and improve the<br>robustness of activity recognition.</li>
<li>We develop a selection method of skeleton joints<br>based on KARD’s work named SSJ, and it considers<br>the spatial relationship and the angle of adjacent<br>joints as auxiliary information of human activity<br>recognition to improve the accuracy of tracking.</li>
<li>We implement the fusion framework of CSI and<br>skeleton data to sense the activity and solve the<br>limitations of CSI-based and skeleton-based activity<br>recognition, respectively. Experimental results show<br>that HuAc achieves the accuracy of greater than 93%.</li>
</ol>
<h4 id="Innovation-amp-consolution-9"><a href="#Innovation-amp-consolution-9" class="headerlink" title="Innovation&amp;consolution:"></a>Innovation&amp;consolution:</h4><ol>
<li>RF-Pose3D provides a significant leap in RF-based sensing and enables new applications in gaming, healthcare, and smart homes. </li>
</ol>
<h4 id="Chart-amp-Analyse-12"><a href="#Chart-amp-Analyse-12" class="headerlink" title="Chart&amp;Analyse"></a>Chart&amp;Analyse</h4><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/1571798448225.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/1571798402105.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/1571798389448.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/1571798482547.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/1571798514117.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/1571798525373.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/1571798541197.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/1571798562529.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/1571798576758.png" alt=""></p>
<h4 id="Code-2"><a href="#Code-2" class="headerlink" title="Code:"></a>Code:</h4><h4 id="Shortcoming-amp-Confusion-3"><a href="#Shortcoming-amp-Confusion-3" class="headerlink" title="Shortcoming&amp;Confusion:"></a>Shortcoming&amp;Confusion:</h4><ol>
<li>Data Fusion   the balance between CSI and kinect recognition</li>
<li>Extending to Multiple People Activity Recognition</li>
<li>Extending to Shadow Recognition</li>
</ol>
<h1 id="Paper《In-Air-Gesture-Interaction-Real-Time-Hand-Posture-Recognition-Using-Passive-RFID-Tags》"><a href="#Paper《In-Air-Gesture-Interaction-Real-Time-Hand-Posture-Recognition-Using-Passive-RFID-Tags》" class="headerlink" title="Paper《In-Air Gesture Interaction: Real Time Hand Posture Recognition Using Passive RFID Tags》"></a>Paper《In-Air Gesture Interaction: Real Time Hand Posture Recognition Using Passive RFID Tags》</h1><p><strong>keyword:</strong>                                                     <strong>author:</strong>Ning Ye (<a href="mailto:yening@njupt.edu.cn">yening@njupt.edu.cn</a>) and Reza Malekian</p>
<h4 id="Phenomenon-amp-Challenge-8"><a href="#Phenomenon-amp-Challenge-8" class="headerlink" title="Phenomenon&amp;Challenge:"></a>Phenomenon&amp;Challenge:</h4><ol>
<li>smart home to control appliances at home [1], which reduces the<br>dependence on remote controllers and mobile terminals.</li>
<li>sign language recognition, gestures can help deaf people or other inconvenient crowds improve their standard of living[2]. Another common application is the Remote Control Robot [3],</li>
<li>how to eliminate the influence of phase wrapping?</li>
<li>how to extract the feature template of each gesture</li>
<li>how to recognize the predefined gestures</li>
</ol>
<h4 id="RelatedWork-11"><a href="#RelatedWork-11" class="headerlink" title="RelatedWork:"></a>RelatedWork:</h4><ol>
<li>wearable sensors</li>
<li>computer vision-based systems</li>
<li>previous work poor portability , Low robustness , high cost</li>
<li>Warehouse Management System (WMS) [15], the tracking of target object [16][18] and indoor location [19][23].</li>
<li>RFID indoor location :TagOram [16], LANDMARC [19] and RF-IDraw [23]. LANDMARC</li>
<li>activity recognition :GRfid  FEMO</li>
</ol>
<h4 id="Contribution-10"><a href="#Contribution-10" class="headerlink" title="Contribution:"></a>Contribution:</h4><ol>
<li>design a gesture recognition system which utilizes available phase from COTs devices to support both static and dynamic gesture recognition</li>
<li>discover unique features differentiating each gesture type.static gestures tend to appear within the time period of phase data stabilization while dynamic gestures occur during the periods of fluctuation.</li>
<li>carry out different normalization and classification schemes on static and dynamic gestures.</li>
</ol>
<h4 id="Innovation-amp-consolution-10"><a href="#Innovation-amp-consolution-10" class="headerlink" title="Innovation&amp;consolution:"></a>Innovation&amp;consolution:</h4><ol>
<li>Noe-line of sight identification ,  energy-free sensing , low cost and lightly carrying.</li>
</ol>
<h4 id="Chart-amp-Analyse-13"><a href="#Chart-amp-Analyse-13" class="headerlink" title="Chart&amp;Analyse:"></a>Chart&amp;Analyse:</h4><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/1571809407857.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/1571809426183.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/1571809446827.png" alt=""></p>
<p>analyse of the above three pictures:1.the phase is more reliable and well-regulated than other output parameters from the reader,such as RSSI and Doppler shift.</p>
<ol start="2">
<li>once the tag position is fixed the raw phase obeys the Gaussian distribution and is barely influenced by the tag orientation.</li>
<li>The phase has linear relation to the distance within an intra-wave and a stable periodicity at inter-wave</li>
</ol>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/1571810288872.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/1571810301098.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/1571810310922.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/1571810977120.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/1571810350329.png" alt=""></p>
<h1 id="Paper《SmartWall-Novel-RFID-Enabled-Ambient-Human-Activity-Recognition-Using-Machine-Learning-for-Unobtrusive-Health-Monitoring》"><a href="#Paper《SmartWall-Novel-RFID-Enabled-Ambient-Human-Activity-Recognition-Using-Machine-Learning-for-Unobtrusive-Health-Monitoring》" class="headerlink" title="Paper《SmartWall: Novel RFID-Enabled Ambient Human Activity Recognition Using Machine Learning for Unobtrusive Health Monitoring》"></a>Paper《SmartWall: Novel RFID-Enabled Ambient Human Activity Recognition Using Machine Learning for Unobtrusive Health Monitoring》</h1><p><strong>keyword:</strong>      AAL(activity assisted living)                                               <strong>author:</strong>George A. Oguntala (<a href="mailto:g.a.oguntala@bradford.ac.uk">g.a.oguntala@bradford.ac.uk</a>)</p>
<p><strong>level:</strong>Digital Object Identifier 10.1109/ACCESS.2019.2917125</p>
<h4 id="Contribution-11"><a href="#Contribution-11" class="headerlink" title="Contribution:"></a>Contribution:</h4><ol>
<li>a novel RFID-enabled approach that implements the pervasive nature of UHF<br>passive RFID tags to recognize sequential and concurrent activities.</li>
<li>We develop machine learning via multivariate Gaussian algorithm using maximum likelihood estimation to classify and predict the sampled activities.</li>
<li>We conduct comprehensive experiments of various reallife physical activities via ambient sensing for data collection, evaluation and classication.</li>
</ol>
<h1 id="Paper《RF-ECG-Heart-Rate-Variability-Assessment-Based-on-COTS-RFID-Tag-Array》"><a href="#Paper《RF-ECG-Heart-Rate-Variability-Assessment-Based-on-COTS-RFID-Tag-Array》" class="headerlink" title="Paper《RF-ECG: Heart Rate Variability Assessment Based on COTS RFID Tag Array》"></a>Paper《RF-ECG: Heart Rate Variability Assessment Based on COTS RFID Tag Array》</h1><p><strong>keyword:</strong> networks,human centered computing                                    <strong>author:</strong> </p>
<p><strong>level:</strong> Proc.ACM Interact Wearable Ubiquitous </p>
<p><strong>lab:</strong>State Key Laboratory for Novel Software Technology </p>
<h4 id="Phenomenon-amp-Challenge-9"><a href="#Phenomenon-amp-Challenge-9" class="headerlink" title="Phenomenon&amp;Challenge:"></a>Phenomenon&amp;Challenge:</h4><ol>
<li>detect and extract weak heartbeat signals from RFID tags among multiple interferences caused by human respiration and ambient noises.</li>
<li>how to achieve a fine-grained heart beat estimation for the HRV assessment according to the reflection effect.  -&gt; accurate beat segmentation. apply the wavelet-based denoising method to further  filter out the ambient noise signals outside the frequency band of heart rate .PCA-based scheme to derive a template to depict the inter beat signals and use it to iteratively perform the IBI segmentation.</li>
<li>how to understand the sensing mechanism of RFID tag array and  leverage the RFID tag array to perform accurate sensing .</li>
</ol>
<h4 id="RelatedWork-12"><a href="#RelatedWork-12" class="headerlink" title="RelatedWork:"></a>RelatedWork:</h4><ol>
<li>Heart rate variability is widely used for general health evaluation .ECG suffer form low accuracy and limited battery life.FMCW utilize dedicated device for mearsurement</li>
<li>Heart Rate Variability represents the variation of the time interval between adjacent heartbeats.HRV reflects hwo the cardiovascular regulatory system responds to demands stress and illness,quantitatively measure physiological and mental changes during treatment.</li>
<li>ECG requires direct skin contact ,indicating some people need to remove the chest hair to achieve better signal quality.</li>
<li>sensor based heartbeat detection  PPG</li>
<li>RF-based heartbeat detection , Radar based approaches FMCW,doppler radar are accureate at measuring such tiny environmental changes  ,wifi based detection  but they can’t label the subject</li>
</ol>
<h4 id="Contribution-12"><a href="#Contribution-12" class="headerlink" title="Contribution:"></a>Contribution:</h4><ol>
<li>leverage the RFID tag array to perform accurate sensing on HRV</li>
<li>conducted in depth investigation on the sensing mechanism of RFID tag array,to capture the relationship between the RF-signal from the tag array and corresponding movement from the heart beat or respiration.</li>
<li>an algorithms use to extract the HRV from the RF-signals mixed with heartbeat signals ,respiration signals and ambient noises.</li>
</ol>
<h4 id="Chart-amp-Analyse-14"><a href="#Chart-amp-Analyse-14" class="headerlink" title="Chart&amp;Analyse:"></a>Chart&amp;Analyse:</h4><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/1571883033221.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/1571884009157.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/1571884059860.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/1571884243273.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/1571884277895.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/1571884363395.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/1571884392759.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/1571884444028.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/1571884452222.png" alt=""></p>
<h1 id="Paper《Spin-Antenna-3D-Motion-Tracking-for-Tag-Array-Labeled-Objects-via-Spinning-Antenna》"><a href="#Paper《Spin-Antenna-3D-Motion-Tracking-for-Tag-Array-Labeled-Objects-via-Spinning-Antenna》" class="headerlink" title="Paper《Spin-Antenna 3D Motion Tracking for Tag Array Labeled Objects via Spinning Antenna》"></a>Paper《Spin-Antenna 3D Motion Tracking for Tag Array Labeled Objects via Spinning Antenna》</h1><p><strong>keyword:</strong>                                                     <strong>author:</strong></p>
<p><strong>level:</strong></p>
<p><strong>lab:</strong>State Key Laboratory for Novel Software Technology,</p>
<h4 id="Challenge"><a href="#Challenge" class="headerlink" title="Challenge:"></a>Challenge:</h4><ol>
<li>how to accurately estimate the 3D motion of the tag array ,including the translation and thee rotation,</li>
<li>how to tackle the variation of signal features when spinning the antenna and use these features to derive six degrees for the freedom for the tag array.</li>
<li>relationship between the signal feature variations and the matching/mismatching direction fo the antenna-tag pair.</li>
</ol>
<h4 id="RelatedWork-13"><a href="#RelatedWork-13" class="headerlink" title="RelatedWork:"></a>RelatedWork:</h4><ol>
<li>HCI approaches mainly fall into three categories,the computer vision(CV)-based ,sensor-based,sensorless approaches.</li>
<li>previous work tracks the motion of the tagged object only in the 2D space ,Tagyro tracks the orientaion of a tagged object via multiple antennas ,but not able to track the absolute translation of the object simultaneously. Tag-compass estimates the orientation of one tag based on multiple spinning antennas,but only based on the precondition that the tagged object is deployed in a specified 2D plane.</li>
</ol>
<h4 id="Chart-amp-Analyse-15"><a href="#Chart-amp-Analyse-15" class="headerlink" title="Chart&amp;Analyse:"></a>Chart&amp;Analyse:</h4><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/1571887460789.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/1571887497435.png" alt=""></p>
<p>the analysis of the above pictures are as follows:</p>
<p>​    1.    during the spinning precess ,in comparison to the circularly polarized antenna, the phase variation of the linearly polarized antenna is more stable ,and the RSSI variation of the linearly polarized antenna is more distinctive.</p>
<pre><code>2.    for the linearly polarized antenna,the mismatching direction ,corresponding to the minimum RSSI value is more distinctive for the estimation of the tag orientation than the matching direction ,corresponding to the maximun.
3.    For the linearly polarized antenna, the phase value keeps stable when the polarization direction of the antenna matches the tag orientation perfectly; and the phase value fluctuates when the polarization direction mismatches the tag orientation due to the multi-path effect
4.    &lt;font color="red"&gt;the linearly polarized antenna can capture the more stable phase value and distinctive RSSI variance compared to the circular polarized antenna. use linearly polarized antenna to estimate the position with the phase value ,and estimate the orientation with RSSI   2.the mismatching direction based on the  RSSI variance is more distinctive to estimate the tag orientation compared to the matching direction.   3. the phase arround the matching direction is more stable ,which can be used to calibrate the phase value by removing the noisy phase around the mismatching direction.&lt;/font&gt;</code></pre><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/1571887817739.png" alt="">ly </p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/1571887850404.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/1571887929450.png" alt=""></p>
<h4 id="Shortcoming-amp-Confusion-4"><a href="#Shortcoming-amp-Confusion-4" class="headerlink" title="Shortcoming&amp;Confusion:"></a>Shortcoming&amp;Confusion:</h4><ol>
<li>4.8.9.11.15 文章阅读下</li>
</ol>
<p><strong>level</strong>: MobiCom2017<br><strong>author</strong>:<br><strong>date</strong>: ‘2017-09-05’<br><strong>keyword</strong>:</p>
<ul>
<li>RFID,Touch Interface,Mutual Coupling,Impedance Tracking</li>
</ul>
<hr>
<h4 id="Paper-RIO"><a href="#Paper-RIO" class="headerlink" title="Paper:  RIO"></a>Paper:  RIO</h4><div align="center">
<br>
<b>RIO: A Pervasive RFID-based Touch Gesture Interface</b>
</div>


<h4 id="Summary-2"><a href="#Summary-2" class="headerlink" title="Summary"></a>Summary</h4><p>利用手在标签上划过，观察由于阻抗带来的相位变化，使用DWT 时序匹配算法来判断是否有划过事件。   问题:只是检测了手指在一个Tag上划过？如果不是沿标签方向划过会怎么样？</p>
<h4 id="Research-Objective-1"><a href="#Research-Objective-1" class="headerlink" title="Research Objective"></a>Research Objective</h4><ul>
<li><strong>Application Area</strong>:  使用天线耦合产生阻抗检测手指在那个标签上划过。</li>
<li><strong>Purpose</strong>: design and develop RIO,battery-free touch sensing user interface primitive for future IOT and smart spaces.</li>
</ul>
<h4 id="Methods-2"><a href="#Methods-2" class="headerlink" title="Methods"></a>Methods</h4><ul>
<li>how to detect touch event on a single tag ?</li>
</ul>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/1571897686528.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/1571897755368.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/1571898179892.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/1571898179892.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/1571898179892.png" alt=""></p>
<h4 id="Evaluation-2"><a href="#Evaluation-2" class="headerlink" title="Evaluation"></a>Evaluation</h4><ul>
<li><strong>Environment</strong>：Imping R420 reader continuously queries the tags in range(at_200reads/second),recording the RF phase of all RFID responses to get time series of phase readings for each individual tag. the camera is time synchronized with the reader control software.</li>
</ul>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/1571898179892.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/1571897714625.png" alt=""></p>
<h4 id="Conclusion-1"><a href="#Conclusion-1" class="headerlink" title="Conclusion"></a>Conclusion</h4><ul>
<li>as a reliable primitive for Touch Sensing ,the impedance of the RFID antenna will vary in response to physical touch 2. the amount of variantion depends on the location of the physical contact with the antenna  </li>
<li>making Rio resilient in a multi-Tag environment</li>
<li>a touch and gesture UI primitive for smart space  ,robust touching and gesture sensing</li>
</ul>
<h4 id="Notes"><a href="#Notes" class="headerlink" title="Notes"></a>Notes</h4><ul>
<li>标签很近的时候会有<font color="red">coupling effec</font>t。</li>
<li>一个场景下会产生什么事件—》对应的可区分现象是什么——-》现象是否有很好的区分性—–》是回归模型还是分类模型还是时序分析DWT算法联想到相关的模型—-》相似的研究有哪些可能的影响不足—–》</li>
</ul>
<h1 id="Paper《Multi-Touch-in-the-Air-Device-Free-Finger-Tracking-and-Gesture-Recognition-via-COTS-RFID》"><a href="#Paper《Multi-Touch-in-the-Air-Device-Free-Finger-Tracking-and-Gesture-Recognition-via-COTS-RFID》" class="headerlink" title="Paper《Multi-Touch in the Air: Device-Free Finger Tracking and Gesture Recognition via COTS RFID》"></a>Paper《Multi-Touch in the Air: Device-Free Finger Tracking and Gesture Recognition via COTS RFID》</h1><p><strong>keyword:</strong>                                                     <strong>author:</strong></p>
<p><strong>level:</strong>IEEE INFOCOM 2018</p>
<p><strong>lab:</strong>State Key Laboratory for Novel Software Technology</p>
<h4 id="Phenomenon-amp-Challenge-10"><a href="#Phenomenon-amp-Challenge-10" class="headerlink" title="Phenomenon&amp;Challenge:"></a>Phenomenon&amp;Challenge:</h4><ol>
<li>how to track the trajectory of the finger writings, model the impact of the moving finger on the tag array to extract the reflection features.</li>
<li>how to recognize the multi-touch gesture? regard the multiple fingers as a whole for recognition and then extract the reflection feature of the multiple fingers as images. using CNN model to classify.</li>
<li>how to abtain signal quality from the tag array?  utilize a signal model to depict the mutual interference between tags.</li>
</ol>
<h4 id="RelatedWork-14"><a href="#RelatedWork-14" class="headerlink" title="RelatedWork:"></a>RelatedWork:</h4><ol>
<li>RF-finger focuses on tracking the finger trace and recognizing the multi-touch gestures.</li>
</ol>
<h4 id="Contribution-13"><a href="#Contribution-13" class="headerlink" title="Contribution:"></a>Contribution:</h4><ol>
<li>presented RF-finger a device-free system based on COTS RFID,which leverage a tag array on a letter-size paper to sense the finge-grained fingermovements perform in front of paper.</li>
<li>focus on two kinds of sensing modes:finger tracking recovers the moving trace of finger writings.multi-touch gesture recognition identifies the multi-touch gestures involving multiple fingers.</li>
<li>investigate the impact of tag array deployment on the signal quality. We analyze the mutual interference between tags via a signal model and provide recommendations on tag deployment to reduce the interference</li>
</ol>
<h4 id="Chart-amp-Analyse-16"><a href="#Chart-amp-Analyse-16" class="headerlink" title="Chart&amp;Analyse:"></a>Chart&amp;Analyse:</h4><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/1571899445346.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/1571900222707.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/1571899529692.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/1571900365674.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/1571900383616.png" alt="1571900383616"></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/1571900390264.png" alt=""></p>
<h1 id="Paper《ShopMiner-Mining-Customer-Shopping-Behavior-inPhysical-Clothing-Stores-with-COTS-RFID-Devices》"><a href="#Paper《ShopMiner-Mining-Customer-Shopping-Behavior-inPhysical-Clothing-Stores-with-COTS-RFID-Devices》" class="headerlink" title="Paper《ShopMiner: Mining Customer Shopping Behavior inPhysical Clothing Stores with COTS RFID Devices》"></a>Paper《ShopMiner: Mining Customer Shopping Behavior inPhysical Clothing Stores with COTS RFID Devices》</h1><h4 id="Phenomenon-amp-Challenge-11"><a href="#Phenomenon-amp-Challenge-11" class="headerlink" title="Phenomenon&amp;Challenge:"></a>Phenomenon&amp;Challenge:</h4><ol>
<li><font color="red">Popular Item</font> represents the clothes frequently<br>viewed by customers.Since customers pay more views<br>on items that meet their tastes, popular category data<br>reveal customers’ flavor, hence providing valuable<br>information to retailers’ trading strategy.</li>
<li><font color="red">Hot items</font> are the clothes frequently picked up or turned over by customers.hot items reveal whether customers show deeper interest in items after their first glance.</li>
<li><font color="red">Correlated items</font>  are the clothes that are frequently matched with or tried on together which can facilitate retailers to infer customer shopping habits and adopt bundle-selling strategies to boost profit.</li>
</ol>
<h4 id="RelatedWork-15"><a href="#RelatedWork-15" class="headerlink" title="RelatedWork:"></a>RelatedWork:</h4><ol>
<li>camera based  requre densely deployed cameras</li>
<li>video based are susceptible to non-line of sight</li>
<li>mining hot zones and popular products.</li>
</ol>
<h4 id="Contribution-14"><a href="#Contribution-14" class="headerlink" title="Contribution:"></a>Contribution:</h4><ol>
<li>We design Shop Miner, a framework that harnesses these unique spatial-temporal correlations of time-series phase readings to detect comprehensive shopping behaviors (look at,pick up,turn over desired item )</li>
</ol>
<h4 id="Chart-amp-Analyse-17"><a href="#Chart-amp-Analyse-17" class="headerlink" title="Chart&amp;Analyse:"></a>Chart&amp;Analyse:</h4><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/1571903456063.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/1571903478125.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/1571903765935.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/1571903905010.png" alt=""></p>
<p>influenced by noise ,the phase values fluctuate continuously and form a Gaussian-like distribution.</p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/1571904324430.png" alt=""></p>
<h1 id="Paper《Demo-IMU-Kinect-A-Motion-Sensor-based-Gait-Monitoring-System-for-Intelligent-Healthcare》"><a href="#Paper《Demo-IMU-Kinect-A-Motion-Sensor-based-Gait-Monitoring-System-for-Intelligent-Healthcare》" class="headerlink" title="Paper《Demo: IMU-Kinect: A Motion Sensor-based Gait Monitoring System for Intelligent Healthcare》"></a>Paper《Demo: IMU-Kinect: A Motion Sensor-based Gait Monitoring System for Intelligent Healthcare》</h1><p><strong>keyword:</strong>       Gait rehabilitation                                              <strong>author:</strong></p>
<p><strong>level:</strong>2019 ACM International Joint Conference on Pervasive and Ubiquitous Computing<br>and the 2019 International Symposium on Wearable Computers (UbiComp/ISWC ’19 Adjunct),</p>
<p><strong>lab:</strong>State Key Laboratory for Novel Software Technology</p>
<h4 id="Phenomenon-amp-Challenge-12"><a href="#Phenomenon-amp-Challenge-12" class="headerlink" title="Phenomenon&amp;Challenge:"></a>Phenomenon&amp;Challenge:</h4><ol>
<li>Gait rehabilitation is a common method of postoperative recovery,which assists the patients to learn how to walk after sustaining an injury or disability.</li>
<li>wearable sensors reported in[8], such as pressure sensors and shoe sensors in [7]. The gait parameters can be easily extracted from measurements of wearable sensors, but we can’t obtain the changing trace of lower limbs because these sensors only provide patchy measurements.</li>
</ol>
<h4 id="RelatedWork-16"><a href="#RelatedWork-16" class="headerlink" title="RelatedWork:"></a>RelatedWork:</h4><ol>
<li>Computer vision-based solutions in [2] and [6] can directly track the movements of lower limbs, but it is difficult to calculate gait parameters efficiently because such calculations require high performance devices and training data</li>
</ol>
<h4 id="Contribution-15"><a href="#Contribution-15" class="headerlink" title="Contribution:"></a>Contribution:</h4><ol>
<li>IMU-Kinect track the movements of lower limbs  and estimate the gait parameters  ,basic idea is that we estimate the rotation and displacement of thighs and shanks based on the Inertial Measurement Unites</li>
<li><font color="red">Gait Paremeters Estimation : temporal parameters (swing time,stance time ,stride time)  这几个time还不明确， Spatial parameters（step length，stride length，stride width）</font></li>
<li>The gait phases are defined by consecutive occurrences of foot strike (FS), flat foot (FF), heel<br>off (HO) and toe off (TO) [5].</li>
</ol>
<h4 id="Chart-amp-Analyse-18"><a href="#Chart-amp-Analyse-18" class="headerlink" title="Chart&amp;Analyse:"></a>Chart&amp;Analyse:</h4><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/1571905403264.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/1571905417370.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/1571905448885.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/1571906461100.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/1571906507536.png" alt=""></p>
<p><strong>level</strong>:   just know the idea<br><strong>author</strong>: Departmentof Electrical Engineering, City University of Hong Kong, Tat Chee Avenue<br><strong>date</strong>:<br><strong>keyword</strong>:</p>
<ul>
<li>Action Recognition</li>
</ul>
<hr>
<h1 id="Paper-IMU-amp-Acoustic-wrist"><a href="#Paper-IMU-amp-Acoustic-wrist" class="headerlink" title="Paper: IMU&amp;Acoustic wrist"></a>Paper: IMU&amp;Acoustic wrist</h1><div align="center">
<br>
<b>Multimodal hand gesture recognition using single IMU and acoustic measurements at wrist
</b>
</div>



<h4 id="Summary-3"><a href="#Summary-3" class="headerlink" title="Summary"></a>Summary</h4><ol>
<li>investigate the use of acoustic signals with accelerometer and gyroscope at the human wrist.</li>
</ol>
<h4 id="Research-Objective-2"><a href="#Research-Objective-2" class="headerlink" title="Research Objective"></a>Research Objective</h4><ul>
<li><strong>Application Area</strong>: gesture recognition</li>
<li><strong>Purpose</strong>:  recognize 10 daily activity gesture</li>
</ul>
<h4 id="Proble-Statement-2"><a href="#Proble-Statement-2" class="headerlink" title="Proble Statement"></a>Proble Statement</h4><p>previous work:</p>
<ul>
<li>cameras, sensors gloves, muscle-based gadgets[1], surface electromyography(sEMG), optical sensor, accelerometer and gyroscope, </li>
</ul>
<h4 id="Methods-3"><a href="#Methods-3" class="headerlink" title="Methods"></a>Methods</h4><ul>
<li><p><strong>Problem Formulation</strong>:</p>
</li>
<li><p><strong>system overview</strong>:</p>
</li>
</ul>
<p>recorded 10 acoustic channels and 6 channels of IMU data (10 microphones, three-axis accelerometer, and three-axis gyroscope) from the wrists of 10 subjects. These subjects performed 1 trials for each of the 13 daily life gestures: hand lift, hands up, thumbs up/down, single/double tap,hand/finger swipe, okay sign, victory sign, and fist.</p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200304154722699.png" alt=""></p>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script>
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        Author:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://liudongdong1.github.io" rel="external nofollow noreferrer">liudongdong1</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        Link:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://liudongdong1.github.io/2020/06/16/aiot/rfid/rfid-actionrecognition/">https://liudongdong1.github.io/2020/06/16/aiot/rfid/rfid-actionrecognition/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        Reprint policy:
                    </i>
                </span>
                <span class="reprint-info">
                    All articles in this blog are used except for special statements
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    reprint polocy. If reproduced, please indicate source
                    <a href="https://liudongdong1.github.io" target="_blank">liudongdong1</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>Copied successfully, please follow the reprint policy of this article</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">more</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/tags/RFID/">
                                    <span class="chip bg-color">RFID</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="qq,qzone,wechat,weibo,douban" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
                <style>
    #reward {
        margin: 40px 0;
        text-align: center;
    }

    #reward .reward-link {
        font-size: 1.4rem;
        line-height: 38px;
    }

    #reward .btn-floating:hover {
        box-shadow: 0 6px 12px rgba(0, 0, 0, 0.2), 0 5px 15px rgba(0, 0, 0, 0.2);
    }

    #rewardModal {
        width: 320px;
        height: 350px;
    }

    #rewardModal .reward-title {
        margin: 15px auto;
        padding-bottom: 5px;
    }

    #rewardModal .modal-content {
        padding: 10px;
    }

    #rewardModal .close {
        position: absolute;
        right: 15px;
        top: 15px;
        color: rgba(0, 0, 0, 0.5);
        font-size: 1.3rem;
        line-height: 20px;
        cursor: pointer;
    }

    #rewardModal .close:hover {
        color: #ef5350;
        transform: scale(1.3);
        -moz-transform:scale(1.3);
        -webkit-transform:scale(1.3);
        -o-transform:scale(1.3);
    }

    #rewardModal .reward-tabs {
        margin: 0 auto;
        width: 210px;
    }

    .reward-tabs .tabs {
        height: 38px;
        margin: 10px auto;
        padding-left: 0;
    }

    .reward-content ul {
        padding-left: 0 !important;
    }

    .reward-tabs .tabs .tab {
        height: 38px;
        line-height: 38px;
    }

    .reward-tabs .tab a {
        color: #fff;
        background-color: #ccc;
    }

    .reward-tabs .tab a:hover {
        background-color: #ccc;
        color: #fff;
    }

    .reward-tabs .wechat-tab .active {
        color: #fff !important;
        background-color: #22AB38 !important;
    }

    .reward-tabs .alipay-tab .active {
        color: #fff !important;
        background-color: #019FE8 !important;
    }

    .reward-tabs .reward-img {
        width: 210px;
        height: 210px;
    }
</style>

<div id="reward">
    <a href="#rewardModal" class="reward-link modal-trigger btn-floating btn-medium waves-effect waves-light red">赏</a>

    <!-- Modal Structure -->
    <div id="rewardModal" class="modal">
        <div class="modal-content">
            <a class="close modal-close"><i class="fas fa-times"></i></a>
            <h4 class="reward-title">你的赏识是我前进的动力</h4>
            <div class="reward-content">
                <div class="reward-tabs">
                    <ul class="tabs row">
                        <li class="tab col s6 alipay-tab waves-effect waves-light"><a href="#alipay">支付宝</a></li>
                        <li class="tab col s6 wechat-tab waves-effect waves-light"><a href="#wechat">微 信</a></li>
                    </ul>
                    <div id="alipay">
                        <img src="/medias/reward/alipay.jpg" class="reward-img" alt="支付宝打赏二维码">
                    </div>
                    <div id="wechat">
                        <img src="/medias/reward/wechat.png" class="reward-img" alt="微信打赏二维码">
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>

<script>
    $(function () {
        $('.tabs').tabs();
    });
</script>

            
        </div>
    </div>

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;Previous</div>
            <div class="card">
                <a href="/2020/06/20/shi-jue-ai/dataglove/hand-analyse-record/">
                    <div class="card-image">
                        
                        <img src="https://gitee.com/github-25970295/blogImage/raw/master/img/dataglove.jpg" class="responsive-img" alt="Hand Analyse Record">
                        
                        <span class="card-title">Hand Analyse Record</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            level:  CVPR  CCF_Aauthor: Tomas Simon   Carnegie Mellon Universitydate: 2017keyword:

hand pose 


Paper: OpenPose Hand
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2020-06-20
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/AIOT/" class="post-category">
                                    AIOT
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/HandPose/">
                        <span class="chip bg-color">HandPose</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                Next&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/2020/06/13/writing/taskmind/">
                    <div class="card-image">
                        
                        <img src="https://cdn.pixabay.com/photo/2016/10/18/19/40/anatomy-1751201__340.png" class="responsive-img" alt="TaskMind">
                        
                        <span class="card-title">TaskMind</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            
算法刷题，sql刷题；

1. 技术栈1. java 基础
精通io、多线程、集合等基础框架，熟悉设计模式和代码规范，很强的代码编写能力、java虚拟机诊断和调优能力；

![](https://gitee.com/github-2597
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2020-06-13
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/TechStack/" class="post-category">
                                    TechStack
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/TechStack/">
                        <span class="chip bg-color">TechStack</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/libs/codeBlock/codeBlockFuction.js"></script>

<!-- 代码语言 -->

<script type="text/javascript" src="/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/libs/codeBlock/codeShrink.js"></script>


<!-- 代码块折行 -->

<style type="text/css">
code[class*="language-"], pre[class*="language-"] { white-space: pre !important; }
</style>


    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;TOC</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // modify the toc link href to support Chinese.
        let i = 0;
        let tocHeading = 'toc-heading-';
        $('#toc-content a').each(function () {
            $(this).attr('href', '#' + tocHeading + (++i));
        });

        // modify the heading title id to support Chinese.
        i = 0;
        $('#articleContent').children('h2, h3, h4').each(function () {
            $(this).attr('id', tocHeading + (++i));
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>

    

</main>




    <!-- <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="463294659"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/libs/aplayer/APlayer.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/meting@2/dist/Meting.min.js"></script>

    
    <div class="container row center-align" style="margin-bottom: 0px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            <span id="year">2019</span>
            <a href="https://liudongdong1.github.io" target="_blank">liudongdong</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            <br>
            
            &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                class="white-color">1267.2k</span>&nbsp;字
            
            
            
            
            
            
            
            <br>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/liudongdong1/" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:3463264078@qq.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>














    <a href="https://blog.csdn.net/liudongdong19/" class="tooltipped" target="_blank" data-tooltip="关注我的CSDN: https://blog.csdn.net/liudongdong19/" data-position="top" data-delay="50">
        <i class="fab fa-csdn">C</i>
    </a>





</div>
    </div>
</footer>

<div class="progress-bar"></div>
 -->

    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;Search</span>
            <input type="search" id="searchInput" name="s" placeholder="Please enter a search keyword"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script src="/js/search.js"></script>
<script type="text/javascript">
$(function () {
    searchFunc("/search.xml", 'searchInput', 'searchResult');
});
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script type="text/javascript" src="/js/CFS.Snow.min.js"></script>
    <!-- 点击爆灯效果 -->
    <canvas class="fireworks" style="position: fixed;left: 0;top: 0;z-index: 1; pointer-events: none;" ></canvas> 
    <script type="text/javascript" src="//cdn.bootcss.com/animejs/2.2.0/anime.min.js"></script> 
    <script type="text/javascript" src="/js/fireworks.js"></script>
    <!--动态线条背景-->
    <script type="text/javascript"
        color="122 103 238" opacity='0.7' zIndex="-2" count="200" src="//cdn.bootcss.com/canvas-nest.js/1.0.0/canvas-nest.min.js">
    </script>
    <!-- 天气 -->
    <!-- weather -->
    <!-- weather -->
    <script type="text/javascript">
         WIDGET = {FID: 'knAMQaFanP'}
    </script>
    <script type="text/javascript" src="https://apip.weatherdt.com/float/static/js/r.js?v=1111"></script>
    <script type="text/javascript" src="https://apip.weatherdt.com/float/static/js/r.js?v=1111"></script>
    <script src="/libs/materialize/materialize.min.js"></script>
    <script src="/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/libs/aos/aos.js"></script>
    <script src="/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/js/matery.js"></script>

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    
    <script async src="/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    

    
    
    <script type="text/javascript" size="150" alpha='0.6'
        zIndex="-1" src="/libs/background/ribbon-refresh.min.js" async="async"></script>
    

    

    
    <script src="/libs/instantpage/instantpage.js" type="module"></script>
    
    <!-- {% include '_custom/custom.swig' %} -->

</body>

</html>
