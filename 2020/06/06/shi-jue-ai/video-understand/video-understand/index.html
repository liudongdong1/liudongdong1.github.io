<!DOCTYPE HTML>
<html lang="en">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Video_Undertand, AIOT,Space&amp;Temporal Sequence Analysis,SpringBoot,liudongdong1,cloud">
    <meta name="description" content="">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Video_Undertand | DaybyDay</title>
    <link rel="icon" type="image/png" href="/favicon.png">

    <link rel="stylesheet" type="text/css" href="/libs/awesome/css/all.css">
    <link rel="stylesheet" type="text/css" href="/libs/materialize/materialize.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/aos/aos.css">
    <link rel="stylesheet" type="text/css" href="/libs/animate/animate.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/lightGallery/css/lightgallery.min.css">
    <link rel="stylesheet" type="text/css" href="/css/matery.css">
    <link rel="stylesheet" type="text/css" href="/css/my.css">

    <script src="/libs/jquery/jquery.min.js"></script>

<meta name="generator" content="Hexo 4.2.0"><link rel="alternate" href="/atom.xml" title="DaybyDay" type="application/atom+xml">
<link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css"></head>


<body>
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/" class="waves-effect waves-light">
                    
                    <img src="/medias/logo.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">DaybyDay</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>Index</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>Tags</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>Categories</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>Archives</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/" class="waves-effect waves-light">

      
      <i class="fas fa-user-circle" style="zoom: 0.6;"></i>
      
      <span>About</span>
      <i class="fas fa-chevron-down" aria-hidden="true" style="zoom: 0.6;"></i>
    </a>
    <ul class="sub-nav menus_item_child ">
      
      <li>
        <a href="/about">
          
          <i class="fas fa-user-circle" style="margin-top: -20px; zoom: 0.6;"></i>
          
          <span>about</span>
        </a>
      </li>
      
      <li>
        <a href="/resume">
          
          <i class="fa fa-user-secret" style="margin-top: -20px; zoom: 0.6;"></i>
          
          <span>resume</span>
        </a>
      </li>
      
    </ul>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/friends" class="waves-effect waves-light">
      
      <i class="fas fa-address-book" style="zoom: 0.6;"></i>
      
      <span>Friends</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/gallery" class="waves-effect waves-light">
      
      <i class="fas fa-camera" style="zoom: 0.6;"></i>
      
      <span>Galleries</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="Search" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/medias/logo.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">DaybyDay</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			Index
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			Tags
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			Categories
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			Archives
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="javascript:;">
			
				<i class="fa-fw fas fa-user-circle"></i>
			
			About
			<span class="m-icon"><i class="fas fa-chevron-right"></i></span>
		</a>
            <ul  style="background:  ;" >
              
                <li>

                  <a href="/about " style="margin-left:75px">
				  
				   <i class="fa fas fa-user-circle" style="position: absolute;left:50px" ></i>
			      
		          <span>about</span>
                  </a>
                </li>
              
                <li>

                  <a href="/resume " style="margin-left:75px">
				  
				   <i class="fa fa fa-user-secret" style="position: absolute;left:50px" ></i>
			      
		          <span>resume</span>
                  </a>
                </li>
              
            </ul>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/friends" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-address-book"></i>
			
			Friends
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/gallery" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-camera"></i>
			
			Galleries
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/liudongdong1" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/liudongdong1" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://gitee.com/github-25970295/blogImage/raw/master/img/voice-recognition-speech-detect-deep-260nw-694633963.webp')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Video_Undertand</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <link rel="stylesheet" href="/libs/tocbot/tocbot.css">
<style>
    #articleContent h1::before,
    #articleContent h2::before,
    #articleContent h3::before,
    #articleContent h4::before,
    #articleContent h5::before,
    #articleContent h6::before {
        display: block;
        content: " ";
        height: 100px;
        margin-top: -100px;
        visibility: hidden;
    }

    #articleContent :focus {
        outline: none;
    }

    .toc-fixed {
        position: fixed;
        top: 64px;
    }

    .toc-widget {
        width: 345px;
        padding-left: 20px;
    }

    .toc-widget .toc-title {
        margin: 35px 0 15px 0;
        padding-left: 17px;
        font-size: 1.5rem;
        font-weight: bold;
        line-height: 1.5rem;
    }

    .toc-widget ol {
        padding: 0;
        list-style: none;
    }

    #toc-content {
        height: calc(100vh - 250px);
        overflow: auto;
    }

    #toc-content ol {
        padding-left: 10px;
    }

    #toc-content ol li {
        padding-left: 10px;
    }

    #toc-content .toc-link:hover {
        color: #42b983;
        font-weight: 700;
        text-decoration: underline;
    }

    #toc-content .toc-link::before {
        background-color: transparent;
        max-height: 25px;

        position: absolute;
        right: 23.5vw;
        display: block;
    }

    .toc-fixed .toc-link::before{
        position: fixed!important;/*当toc的位置改为fixed时，.toc-link::before也要改为fixed*/
    }

    #toc-content .is-active-link {
        color: #42b983;
    }

    #toc-content .is-active-link::before {
        background-color: #42b983;
    }

    #floating-toc-btn {
        position: fixed;
        right: 15px;
        bottom: 76px;
        padding-top: 15px;
        margin-bottom: 0;
        z-index: 998;
    }

    #floating-toc-btn .btn-floating {
        width: 48px;
        height: 48px;
    }

    #floating-toc-btn .btn-floating i {
        line-height: 48px;
        font-size: 1.4rem;
    }
</style>
<div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/tags/VideoAnalyse/">
                                <span class="chip bg-color">VideoAnalyse</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/categories/%E8%A7%86%E8%A7%89AI/" class="post-category">
                                视觉AI
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>Publish Date:&nbsp;&nbsp;
                    2020-06-06
                </div>
                

                <!-- 
                    <i class="fa fa-pencil"></i> Author: liudongdong1
                  -->

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>Update Date:&nbsp;&nbsp;
                    2021-12-14
                </div>
                

                <!-- 
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>Word Count:&nbsp;&nbsp;
                    13.2k
                </div>
                 -->

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>Read Times:&nbsp;&nbsp;
                    76 Min
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>Read Count:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">
        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>安防监控领域，包括人脸识别、行为识别、运动跟踪、人群分析等等，利用卡口精准位置布控视频监测，实现了监控区域内异常的自动识别，例如动态视频中的人脸与黑名单库实时比对检测，多视点视频协同分析运行轨迹，视频数据结构化后对关键目标的检索等等；</p>
<p>互联网娱乐场景，包括拍照优化、视频优化、实时人像美颜、AR特效、自定义背景等等，丰富了直播、短视频等互联网娱乐应用；</p>
<p>金融身份认证场景，包括各种刷脸的金融应用，如远程开户、支付取款等等；</p>
<p>无人商场与广告营销，包括线下零售、商品识别、广告AR赋能等等；</p>
<p>工业机器的视觉系统，包括物品分拣、缺陷检验等等，通常是自动图像分析与光学成像等其他方法技术相结合；</p>
<p>无人机无人车控制，包括视觉导航、行人分析、障碍物检测等等，通常作为一种传感器和激光雷达、毫米波雷达、红外探头与惯性测量单元融合生成供自主决策的信息；</p>
</blockquote>
<h1 id="0-视频理解方向"><a href="#0-视频理解方向" class="headerlink" title="0. 视频理解方向"></a>0. 视频理解方向</h1><blockquote>
<ul>
<li>Task1：未修剪视频分类(Untrimmed Video Classification)。这个有点类似于图像的分类，未修剪的视频中通常含有多个动作，而且视频很长。有许多动作或许都不是我们所关注的。所以这里提出的Task就是希望通过对输入的长视频进行全局分析，然后软分类到多个类别。</li>
<li>Task2：修剪视频识别(Trimmed Action Recognition)。这个在计算机视觉领域已经研究多年，给出一段只包含一个动作的修剪视频，要求给视频分类。</li>
<li>Task3：时序行为提名(Temporal Action Proposal)。这个同样类似于图像目标检测任务中的候选框提取。在一段长视频中通常含有很多动作，这个任务就是从视频中找出可能含有动作的视频段。</li>
<li>Task4：时序行为定位(Temporal Action Localization)。相比于上面的时序行为提名而言，时序行为定位于我们常说的目标检测一致。要求从视频中找到可能存在行为的视频段，并且给视频段分类。</li>
<li>Task5：密集行为描述(Dense-Captioning Events)。之所以称为密集行为描述，主要是因为该任务要求在时序行为定位(检测)的基础上进行视频行为描述。也就是说，该任务需要将一段未修剪的视频进行时序行为定位得到许多包含行为的视频段后，对该视频段进行行为描述。比如：man playing a piano</li>
</ul>
</blockquote>
<h1 id="1-手语论文"><a href="#1-手语论文" class="headerlink" title="1. 手语论文"></a>1. 手语论文</h1><blockquote>
<h3 id="工业界："><a href="#工业界：" class="headerlink" title="工业界："></a>工业界：</h3><p>腾讯优图实验室AI手语识别 <a href="https://www.jiqizhixin.com/articles/2019-05-16-16" target="_blank" rel="noopener">https://www.jiqizhixin.com/articles/2019-05-16-16</a></p>
<p>中科大和微软推出了基于Kinect的手语翻译系统，加州大学曾经推出过的手语识别手套</p>
<h5 id="潜在需求分析："><a href="#潜在需求分析：" class="headerlink" title="潜在需求分析："></a><strong>潜在需求分析</strong>：</h5><p>​    1. <strong>听障人士数量数量多</strong> 世界卫生组织最新数据显示[1]，目前全球约有4.66亿人患有残疾性听力损失，超过全世界人口的5%，估计到2050年将有9亿多人（约十分之一）出现残疾性听力损失。据北京听力协会2017年公开数据，估计中国残疾性听力障碍人士已达7200万[2]，</p>
<ol start="2">
<li><p><strong>无障碍普及率有待提升，听障人群需求被忽视</strong></p>
</li>
<li><p>提供一套兼容全球手语的双向翻译器/或是简单的识别器</p>
<ul>
<li>立即可以为上千万聋哑人获得更多的电脑控制权</li>
<li>结合 IFTTT 以及 Home 类似智能家庭控制器</li>
<li>完全可以形成一个嵌入专用硬件的产业了</li>
</ul>
</li>
</ol>
<h5 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h5><p>​    1.  自动区分手语表达中的各类手势、动作以及这些手势和动作之间的切换，最后将表达的手语翻译成文字。传统的方法通常会针对特定的数据集设计合理的特征，再利用这些特征进行动作和手势的分类。受限于人工的特征设计和数据量大小，这些方法在适应性、泛化性和鲁棒性上都非常有限。</p>
<p>使用Kinect摄像机的多种传感器来提前获取手语表达者的肢体关节点信息： 传感器手套、或配备EMG、IMU传感器的手环来获取手臂和手掌的活动信息</p>
</blockquote>
<p><strong>level</strong>: CVPR  CCF_A<br><strong>author</strong>:Junfu Pu    CAS Key Laboratory of GIPAS, University of Science and Technology of China<br><strong>date</strong>: 2019<br><strong>keyword</strong>:</p>
<ul>
<li>ASL , CTC</li>
</ul>
<hr>
<h2 id="Paper-Iterative-Alignment-Network"><a href="#Paper-Iterative-Alignment-Network" class="headerlink" title="Paper: Iterative Alignment Network"></a>Paper: Iterative Alignment Network</h2><div align="center">
<br>
<b>Iterative Alignment Network for Continuous Sign Language Recognition</b>
</div>


<h4 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h4><ol>
<li><h4 id="Research-Objective"><a href="#Research-Objective" class="headerlink" title="Research Objective"></a>Research Objective</h4></li>
</ol>
<ul>
<li><strong>Application Area</strong>:<ul>
<li>sign language (SL) is used by millions of people with hearing or spoken damage in their daily life</li>
<li>lack of systematic study for sign language, it becomes very difficult for many people to communicate with the deaf-mute</li>
</ul>
</li>
<li><strong>Purpose</strong>:  propose an alignment network with iterative optimization for weakly supervised continuous signlanguage recognition</li>
</ul>
<h4 id="Proble-Statement"><a href="#Proble-Statement" class="headerlink" title="Proble Statement"></a>Proble Statement</h4><p>previous work:</p>
<ul>
<li>isolated SLR  recognition [16, 22, 42, 43]</li>
<li>video representation： 3D-CNN  ResNet  P3D </li>
<li>sequence modeling:<ul>
<li>attention-based encoder-decoder network<ul>
<li>Bahdanau et al. [1] introduce attention mechanism into encoder-decoder network to learn the correspondence between source sequence and target sequence</li>
</ul>
</li>
<li>connectionist temporal classification(CTC) based network<ul>
<li>CTC is able to deal withunsegmented input data, and learn the correspondence between the input sequence and output sequence.</li>
</ul>
</li>
</ul>
</li>
<li>continuous SLR <ul>
<li>hand-crafted feature based<ul>
<li>Hidden Markov Model (HMM) or Hidden Conditional Random Fields (HCRF)</li>
<li>[35] two real-time HMM-based systems for recognizing<br>sentence-level continuous American Sign Language (ASL).</li>
<li>[40]a discriminative sequence model with Hidden Conditional Random Field (HCRF) for gesture recognition</li>
</ul>
</li>
<li>deep learning based  [9, 23, 25] datasets 了解一下<ul>
<li>video represntations by redidual network ResNet[18], 3D-CNN [33, 37]</li>
<li>[23] with hierarchical attention in latent space</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="Methods"><a href="#Methods" class="headerlink" title="Methods"></a>Methods</h4><ul>
<li><strong>Problem Formulation</strong>:</li>
</ul>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191223092845392.png" alt=""></p>
<ul>
<li><strong>system overview</strong>:</li>
</ul>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191223092906125.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191223093357368.png" alt=""></p>
<p><strong>CTC_Loss</strong>: </p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191223093649877.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191223093518165.png" alt=""></p>
<p><strong>LSTM_Loss</strong>:</p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191223093740963.png" alt=""></p>
<p><img src="../../../../MEGA/MEGAsync/actionPrediction/ActionPrediction.assets/image-20191223093756890.png" alt="image-20191223093756890"></p>
<p><strong>The Whole NetworkLoss</strong>:</p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191223093851361.png" alt="image-20191223093851361"></p>
<p><img src="../../../../MEGA/MEGAsync/actionPrediction/ActionPrediction.assets/image-20191223093859610.png" alt="image-20191223093859610"></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191223093025875.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191223093947114.png" alt=""></p>
<h4 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a>Evaluation</h4><ul>
<li><strong>Environment</strong>:   <ul>
<li>Dataset: <ul>
<li>RWTH-PHOENIX-Weather multi-signer [25] for German SLR</li>
<li>CSL [23] for Chinese SLR</li>
</ul>
</li>
</ul>
</li>
<li><strong>Evaluate Methods</strong>: <img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191223094117184.png" alt="image-20191223094117184"></li>
<li>The window size is set to be 8 with a stride of 4,the 3D-ResNet is pre-trained on an isolated sign language recognition dataset released in [43]</li>
<li><strong>Performance</strong>:</li>
</ul>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191223094423096.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191223094431353.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191223094443353.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191223094507331.png" alt=""></p>
<h4 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h4><ul>
<li>A unified deep learning architecture integrating encoderdecoder network and connectionist temporal classification (CTC) for continuous sign language recognition.</li>
<li>A soft dynamic time warping (soft-DTW) alignment constraint between the LSTM and CTC decoders, which indicates the temporal segmentation in sign videos</li>
<li>Iterative optimization strategy to train feature extractor and encoder-decoder network alternately with alignment proposals by warping path</li>
</ul>
<h4 id="Notes-去加强了解"><a href="#Notes-去加强了解" class="headerlink" title="Notes 去加强了解"></a>Notes <font color="orange">去加强了解</font></h4><ul>
<li><input disabled="" type="checkbox"> 论文23: Video-based sign language recognition without<br>temporal segmentation   China</li>
<li><input disabled="" type="checkbox"> paper25 数据集 German Continuous<br>sign language recognition: Towards large vocabulary statistical recognition systems handling multiple signers</li>
<li><input disabled="" type="checkbox"> SubUNets: End-to-end hand shape and continuous sign language recognition</li>
<li><input disabled="" type="checkbox"> Online early-late fusion based on adaptive HMM for sign language recognition</li>
<li><input disabled="" type="checkbox"> Can spatiotemporal 3D CNNs retrace the history of 2D CNNs and imagenet</li>
<li><input disabled="" type="checkbox"> Attention based 3D-CNNs for large-vocabulary sign language recognition</li>
<li><input disabled="" type="checkbox"> Video-based sign language recognition without temporal segmentation</li>
<li><input disabled="" type="checkbox"> Dilated convolutional network with iterative optimization for continuous<br>sign language recognition</li>
<li><input disabled="" type="checkbox"> Continuous sign language recognition: Towards large vocabulary statistical recognition systems handling multiple signers</li>
<li><input disabled="" type="checkbox"> Online early-late fusion based on adaptive HMM for sign<br>language recognition</li>
<li><input disabled="" type="checkbox"> Joint CTC/attention decoding for end-to-end speech recognition</li>
<li><input disabled="" type="checkbox"> Attention based 3D-CNNs for large-vocabulary sign language recognition</li>
<li><input disabled="" type="checkbox"> Video-based sign language recognition without<br>temporal segmentation</li>
<li><input disabled="" type="checkbox"> Deep sign: hybrid CNN-HMM for continuous sign language recognition</li>
<li><input disabled="" type="checkbox"> Re-sign: Re-aligned end-to-end sequence modelling with deep recurrent CNN-HMMs.  </li>
<li><input disabled="" type="checkbox"> Online detection and classification of dynamic hand gestures with recurrent 3D<br>convolutional neural networks</li>
<li><input disabled="" type="checkbox"> Dilated convolutional network with iterative optimization for continuous sign language recognition</li>
</ul>
<p><strong>level</strong>: Sensys     CCF_B<br><strong>author</strong>:Biyi Fang  Michigan State University<br><strong>date</strong>: 2017<br><strong>keyword</strong>:</p>
<ul>
<li>ASL, Leep Motion(an infrared light-based sensing device)</li>
</ul>
<hr>
<h2 id="Paper-DeepASL"><a href="#Paper-DeepASL" class="headerlink" title="Paper: DeepASL"></a>Paper: DeepASL</h2><div align="center">
<br>
<b>DeepASL: Enabling Ubiquitous and Non-Intrusive Word and
Sentence-Level Sign Language Translation</b>
</div>
#### Summary

<ol>
<li>performance at both word level and sentence level (unseen ASL sentences ,unseen users)</li>
<li>robustness under various real-world settings (various ambient lighting conditions, body postures,and interference sources )</li>
<li>system performance test in terms of runtime , memory usage and energy consumption.</li>
</ol>
<h4 id="Research-Objective-1"><a href="#Research-Objective-1" class="headerlink" title="Research Objective"></a>Research Objective</h4><ul>
<li><strong>Application Area</strong>:seeking help from a sign language interpreter, writing on paper, or typing on a mobile phone,each of these methods has its own key limitations in terms of cost,<br>availability, or convenience</li>
<li><strong>Purpose</strong>:  </li>
</ul>
<h4 id="Proble-Statement-1"><a href="#Proble-Statement-1" class="headerlink" title="Proble Statement"></a>Proble Statement</h4><ul>
<li>ASL : hand shape, hand movement, relative location of two hands, body movement, face emotions</li>
<li>Electromyography (EMG) sensors, RGB cameras, Kinect sensors intrusive where<br>sensors have to be attached to !ngers and palms of users, lack of resolutions to capture the key characteristics of signs, or significantly constrained by ambient lighting conditions or backgrounds<br>in real-world settings</li>
<li>existing sign language translation systems can only translate a single sign at a time, thus<br>requiring users to pause between adjacent signs.</li>
</ul>
<p>previous work:</p>
<ul>
<li>wearable sensor-based :motion sensors(accelerometers, gyroscopes), EMG sensors, bending of fingers to infer the performed fingers. <font color="red">intrusive and impractical for daily usage</font></li>
<li>Radio Frequency-based: <font color="red">wire-less signals have very limited resolutions to see the hands</font></li>
<li>RGB camera-based: <font color="red"> poor lighting conditions or generally uncontrolled backgrounds, privacy </font></li>
<li>Kinect-based: hard to capture the hand shape information</li>
</ul>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191224090129594.png" alt=""></p>
<ul>
<li>Leap Motion is able to extract skeleton joints of the fingers, palms and forearms from the raw infrared images.<img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191224090111350.png" alt=""></li>
</ul>
<h4 id="Methods-1"><a href="#Methods-1" class="headerlink" title="Methods"></a>Methods</h4><ul>
<li><strong>system overview</strong>:</li>
</ul>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191224084552045.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191224090211496.png" alt=""></p>
<ol>
<li>a temporal sequence of 3D coordinates of the skeleton joints of !ngers, palms and forearms</li>
<li>the key characteristics of ASL signs including hand shape, hand movement and relative location of two hands    spatio-temporal trajectories of ASL characteristics</li>
<li>models the spatial structure and temporal dynamics of the spatio-temporal trajectories of ASL characteristics for word-level ASL translation</li>
<li>CTC-based framework that leverages the captured probabilistic dependencies between words in one complete sentence and translates the whole sentence end-to-end without requiring users to pause between adjacent signs.</li>
</ol>
<p><strong>【ASL Characteristics Extraction】</strong></p>
<ul>
<li>Savitzky-Golay flter [37] to improve the signal to noise ratio of the raw skeleton joints data</li>
</ul>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191224090953165.png" alt=""></p>
<ul>
<li>extract hand shape: <img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191224092151069.png" alt=""></li>
<li>hand movement information:<img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191224092217948.png" alt="image-20191224092217948"></li>
</ul>
<p><strong>【Word-Level ASL Translation】</strong>： translation errors when different signs share very similar characteristics at the beginning of the signs</p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191224092513888.png" alt=""></p>
<ul>
<li>Hierarchical Bidirectional RNN for Single-Sign Modeling:<img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191224092932396.png" alt=""></li>
</ul>
<p><strong>【Sentence level Translation】</strong> using CTC network</p>
<h1 id="2-视频理解"><a href="#2-视频理解" class="headerlink" title="2. 视频理解"></a>2. 视频理解</h1><p><strong>level</strong>:  CVPR_CCFA<br><strong>author</strong>:Romero Morais<br><strong>date</strong>:<br><strong>keyword</strong>:</p>
<ul>
<li>video analyse, anomaly detection</li>
</ul>
<hr>
<h2 id="Paper-Anomaly-Detection"><a href="#Paper-Anomaly-Detection" class="headerlink" title="Paper: Anomaly Detection"></a>Paper: Anomaly Detection</h2><div align="center">
<br>
<b>Learning Regularity in Skeleton Trajectories for Anomaly Detection in Videos
</b>
</div>




<h4 id="Summary-1"><a href="#Summary-1" class="headerlink" title="Summary"></a>Summary</h4><ol>
<li>model the normal patterns of human movement in surveillance video for <code>anomaly detection using dynamic skeleton features.</code></li>
<li>decompose the skeletal movements into two sub-components: global body movement and local body posture. The global body movement tracks the dynamics of the whole body in the scene, while the body posture describe the skeleton configuration in the canonical coordinate frame of the body’s bounding box.</li>
<li>model the dynamic and interaction of the coupled features in our novel Message-Passing Encoder-Decoder Recurrent Network.</li>
<li>skeleton features are compact, strongly structured, semantically rich, and highly descriptive about human aciton and movement, which are keys to anomaly detection. </li>
</ol>
<h4 id="Proble-Statement-2"><a href="#Proble-Statement-2" class="headerlink" title="Proble Statement"></a>Proble Statement</h4><ul>
<li>The human behavioral irregularity can be factorized into few factors regarding body motion and posture: location, velocity, direction, pose, and action.</li>
</ul>
<h4 id="Methods-2"><a href="#Methods-2" class="headerlink" title="Methods"></a>Methods</h4><p>【Qustion 1】 the scales of human skelons vary largely depending on their location and actions</p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200422104952789.png" alt=""><br>$$<br>f_t^i=f_t^g+f_t^{l,i};f^g=(x^g,y^g,w,h),f^{l,i}=(x^{l,i},y^{l,i})<br>$$<br><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200422105234027.png" alt=""></p>
<p>【qustion 2】how to fuse local and global features</p>
<ul>
<li>propose MPED-RNN models, consisting two recurrent encoder-decoder network branches, each of them dedicated to one of the components, each branch of them has the single-encoder-dual-decoder architecture with three RNNS: Encoder,Reconstructing Decoder and Predicting Decoder.</li>
<li>use Gated Recurrnet Units in every segment of MPED_RNN for its simplicity and similar performance to LSTM</li>
</ul>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200422105321588.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200422105656331.png" alt=""></p>
<p>【qurestion 3】 how to detect video anomalies?</p>
<ol>
<li><strong>Extract segments</strong>: select the overlapping skeleton segments by using sliding window of size T and stride s on the trajectory</li>
<li><strong>Estimate segment losses</strong>: decompose the segment to two sub-component, feed all segment features to the traind MPED-RNN, output the normality loss</li>
<li><strong>Gather skeleton anomaly score</strong>:  the measure the conformity of a sequence to the model given both the past and future context, using voting scheme to gather the losses of related segments into an anomaly score of each skeleton instance:<img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200422111136464.png" alt=""></li>
<li>Calculate frame anomaly score: <img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200422111221056.png" alt=""></li>
</ol>
<h4 id="Evaluation-1"><a href="#Evaluation-1" class="headerlink" title="Evaluation"></a>Evaluation</h4><ul>
<li><strong>Environment</strong>:   <ul>
<li>Dataset: ShanghaiTech Campus Dataset for video anomaly detection currently available, combines footage of 13 different cameras .</li>
</ul>
</li>
</ul>
<h4 id="Notes-去加强了解-1"><a href="#Notes-去加强了解-1" class="headerlink" title="Notes 去加强了解"></a>Notes <font color="orange">去加强了解</font></h4><ul>
<li>video anomaly detection</li>
<li>human trajectory modeling</li>
<li>sequence 一致性</li>
</ul>
<p><strong>level</strong>:<br><strong>author</strong>: waqas sultani, UCF<br><strong>date</strong>:<br><strong>keyword</strong>:</p>
<ul>
<li>anomaly detection, video analyse</li>
</ul>
<hr>
<h1 id="Paper-Real-world-Detection"><a href="#Paper-Real-world-Detection" class="headerlink" title="Paper: Real-world Detection"></a>Paper: Real-world Detection</h1><div align="center">
<br>
<b>Real-world Anomaly Detection in Surveillance Videos</b>
</div>
#### Summary

<ol>
<li>propose to learn anomaly through the deep multiple instance ranking framework by leveraging weakly labeled training video, the training labels(anomalous or normal) are at video-level instead of clip-level.</li>
<li>introduce a new large-scale dataset of 128 hours of videos with 13 realistic anomalies such as fighting, road accident, burglary robbery.</li>
<li>propose a MIL solution to anomaly detection by leveraging only weakly labeled training videos, propose MIL ranking loss with sparsity and smoothness constraints for a deep learning network to learn anomaly scores for video segments.</li>
</ol>
<h4 id="Research-Objective-2"><a href="#Research-Objective-2" class="headerlink" title="Research Objective"></a>Research Objective</h4><ul>
<li><strong>Application Area</strong>:  traffic accidents, crimes  or illegal activities.</li>
</ul>
<h4 id="Proble-Statement-3"><a href="#Proble-Statement-3" class="headerlink" title="Proble Statement"></a>Proble Statement</h4><ul>
<li><strong>Anomaly detection</strong>:<ul>
<li>considering all anomalies in one group and all normal activities in another group</li>
<li>recognise specific activities.</li>
<li>impossible to define a normal event which takes all possible normal patterns/behaviors into account.</li>
<li>detect human violence by exploiting motion and limbs orientation of people </li>
<li>employed video and audio data to detect aggressive actions in surveillance videos.</li>
<li>violent flow descriptors to detect violence in crowd videos.</li>
<li>using deep learning based autoencoders to learn the model of normal behaviors and employed reconstruction loss to detect anomalies.</li>
</ul>
</li>
<li><strong>Ranking</strong>: focus on improving relative scores of the items instead of individual scores.<ul>
<li>deep rankinng networking: used for feature learning, highlight detection, graphics interchange format generation, face detection and verification, person re-identification, place recognition, metric learning and image retrieval.</li>
</ul>
</li>
</ul>
<h4 id="Methods-3"><a href="#Methods-3" class="headerlink" title="Methods"></a>Methods</h4><ul>
<li><strong>system overview</strong>:</li>
</ul>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200422150129902.png" alt=""></p>
<p>【Qustion 1】less annotation learning</p>
<ul>
<li>only video-level labels indicating the presence of an anomaly in the whole video is needed. A video containing anomalies is labeled as positive and a video without any anomaly is labeled as negative.</li>
</ul>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200422150330960.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200422150529485.png" alt=""></p>
<p>【qustion 2】 how to detect anomaly activities without much precise annotation?</p>
<ul>
<li>Deep MIL Ranking model:  the scores of instances in the anomalous bag should be sparse,    the anomaly score should vary smoothly between video segments.</li>
<li><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200422150817733.png" alt=""></li>
</ul>
<h4 id="Notes-去加强了解-2"><a href="#Notes-去加强了解-2" class="headerlink" title="Notes 去加强了解"></a>Notes <font color="orange">去加强了解</font></h4><ul>
<li>sparse-coding based approaches.</li>
<li>deep rank</li>
<li>项目代码： <a href="https://github.com/hangxu124/MyRes3D_AnoDect" target="_blank" rel="noopener">https://github.com/hangxu124/MyRes3D_AnoDect</a><ul>
<li><a href="https://github.com/dexXxed/abnormal-event-detection" target="_blank" rel="noopener">https://github.com/dexXxed/abnormal-event-detection</a></li>
<li><a href="https://github.com/nevinbaiju/anomaly-detection" target="_blank" rel="noopener">https://github.com/nevinbaiju/anomaly-detection</a></li>
</ul>
</li>
</ul>
<p><strong>level</strong>:  AAAI   CCF_A<br><strong>author</strong>: Yijun Cai , Haoxin Li, Jian-Fang Hu , Wei-Shi Zheng<br><strong>date</strong>: 2019<br><strong>keyword</strong>:</p>
<ul>
<li></li>
</ul>
<hr>
<h2 id="Paper-Action-Knowledge-Transfer"><a href="#Paper-Action-Knowledge-Transfer" class="headerlink" title="Paper: Action Knowledge Transfer"></a>Paper: Action Knowledge Transfer</h2><div align="center">
<br>
<b>Action Knowledge Transfer for Action Prediction with Partial Videos</b>
</div>


<h4 id="Summary-2"><a href="#Summary-2" class="headerlink" title="Summary"></a>Summary</h4><ol>
<li>通过完整的视频动作序列来指导部分视频序列的预测？</li>
</ol>
<h4 id="Research-Objective-3"><a href="#Research-Objective-3" class="headerlink" title="Research Objective"></a>Research Objective</h4><ul>
<li><strong>Application Area</strong>: in reducing computational resource, traffic system. </li>
<li><strong>Purpose</strong>:  Propose to transfer action knowledge learned from fully observed videos for improving the prediction of partially observed videos</li>
</ul>
<h4 id="Proble-Statement-4"><a href="#Proble-Statement-4" class="headerlink" title="Proble Statement"></a>Proble Statement</h4><ul>
<li>action prediction mainly lies in the lack of discriminative action information for the partially observed videos. partially observed videos often contain incomplete action executions thus have less action information than the fully observed ones.</li>
</ul>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191202141323929.png" alt=""></p>
<ul>
<li>the existing action recognition systems can be directly used for action prediction by treating partial videos as full videos.</li>
</ul>
<p>previous work:</p>
<ul>
<li>focus on improving the discriminative power of partial videos by developing max margin learning(Kong and Fu 2015) or soft regression  framework(Hu 2016)</li>
<li>Action prediction: <ul>
<li>Ryoo et al.(Ryoo2012) proposed to use integral and dynamic bag-fo-words for action prediction</li>
<li>Kong and Fu 2015 a max margin learning framework was presented to learn discriminative features for prediction</li>
<li>Vondrick,Pirsiavash and Torralba2016 propose to predict the feature of future frames to learn better representations for action prediction</li>
<li>Lan,Chen developed hierarchical representations at multiple granularities to predict human action </li>
<li><font color="red">they dont seek to make use of the action knowledge learned from full sequences for prediction, we propose to mine rich action knowledge from full videos</font></li>
</ul>
</li>
<li>Knowledge distillation  :(Hinto ,Vinyals and Dean 2015; Huang and Wang 2017; Yim et al.2017) the knowledge contained in a large network was distilled and transferred to a small network,by enforcing the outputs or intermediate activations of the small network to match those the large network . <font color="red">our goal is to improve the discriminative power of partially observed videos </font></li>
</ul>
<h4 id="Methods-4"><a href="#Methods-4" class="headerlink" title="Methods"></a>Methods</h4><ul>
<li><strong>system overview</strong>:</li>
</ul>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191202143008477.png" alt=""></p>
<p>[<strong>Question one</strong>] how to Learn Action Knowledge from Full Videos</p>
<p>Given a set of full videos {x i } with corresponding features {f i } and labels {y i }, we intend to learn an embedding function G to project the original feature onto an embed-ding space, and a discriminative classifier D to project the embedding to the label space:<br>$$<br>e i = G(f i ),\<br>p i = D(e i ).<br>$$</p>
<p>To encourage large distances between embeddings from different classes:</p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191202143440633.png" alt=""></p>
<p>[<strong>Qustion Two</strong>] Transferring Action Knowledge to Partial Videos</p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191202143637577.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191202143706194.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191202143144128.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191202143845522.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191202143753309.png" alt=""></p>
<h4 id="Conclusion-1"><a href="#Conclusion-1" class="headerlink" title="Conclusion"></a>Conclusion</h4><ul>
<li>propose a novel knowledge transfer framework to boost the performance of action prediction with partial videos ,by transferring knowledge from feature embeddings and discriminative classifier of full videos.</li>
<li>the method shows remarkable improvement for action prediction </li>
</ul>
<h4 id="Notes-去加强了解-3"><a href="#Notes-去加强了解-3" class="headerlink" title="Notes 去加强了解"></a>Notes <font color="orange">去加强了解</font></h4><ul>
<li><input disabled="" type="checkbox"> Kong Tao and Fu 2017    Qin et al.2017</li>
<li><input disabled="" type="checkbox"> paper 18</li>
<li><input disabled="" type="checkbox"> Additive Margin (AM) Softmax (Wang et al. 2018)</li>
<li><input disabled="" type="checkbox"> Max-margin action predictionmachine</li>
<li><input disabled="" type="checkbox"> spartiotemporal multiplier networks for video action recognition</li>
<li><input disabled="" type="checkbox"> Distilling the knowlege in a neural network   2015 Hinton</li>
<li><input disabled="" type="checkbox"> early action prediction by soft regression  </li>
<li><input disabled="" type="checkbox"> Like what you like: Knowledge distill via neuron selectivity transfer</li>
<li><input disabled="" type="checkbox"> Deep sequential context networks for action prediction</li>
<li><input disabled="" type="checkbox"> learning activity progression in lstm for activity detection and early detection</li>
<li><input disabled="" type="checkbox"> learning spatialtemporal features with 3d convolutional networks</li>
<li><input disabled="" type="checkbox"> action recognition with improved trajectories</li>
<li><input disabled="" type="checkbox"> action recognition by dense trajectories</li>
<li><input disabled="" type="checkbox"> a gift from knowledge distillation:Fast optimization network minimization and transfer learning</li>
</ul>
<p><strong>level</strong>:  CCF_A<br><strong>author</strong>:  Tian Lan , Tsung-Chuan , Silvio Savarese  Standford University<br><strong>date</strong>: 2014 ECCV<br><strong>keyword</strong>:</p>
<ul>
<li>action prediction</li>
</ul>
<hr>
<h2 id="Paper-Hierarchical-Representation"><a href="#Paper-Hierarchical-Representation" class="headerlink" title="Paper: Hierarchical Representation"></a>Paper: Hierarchical Representation</h2><div align="center">
<br>
<b></b>
</div>


<h4 id="Summary-3"><a href="#Summary-3" class="headerlink" title="Summary"></a>Summary</h4><ol>
<li>adop an hierarchical structure to predict action from different granularity.</li>
</ol>
<h4 id="Research-Objective-4"><a href="#Research-Objective-4" class="headerlink" title="Research Objective"></a>Research Objective</h4><ul>
<li><strong>Application Area</strong>: autonomous robots, surveillance and health care , robotic applications[24], [29]</li>
<li><strong>Purpose</strong>:  predict future action</li>
</ul>
<h4 id="Problem-Statement"><a href="#Problem-Statement" class="headerlink" title="Problem Statement"></a>Problem Statement</h4><ul>
<li>capture the subtle details inherent in human movements that may imply a future action</li>
<li>humans are highly articulated objects</li>
<li>actions can be described at different levels of semantic granularities.</li>
<li>prediction should carried out as quickly as possible</li>
<li>from recognizing simple human actions such as walking and standing in constrained settings[19] to understanding complex actions in realistic video and still images collected from movies,TV show , sport games , Internet (background clutter, occlusions, viewpoint, changes)<ul>
<li>in video: bag-of-features representations of local space-time features [22]</li>
<li>in image : contextural information such as attributes ,objects ,poses are jointly modeled with actions.</li>
</ul>
</li>
</ul>
<p>previous work:</p>
<ul>
<li>Human ability of the visual system to predict future actions based on previous observations of interactions among humans</li>
<li>recent early event detection: expand spectrum of human action recognition to actions in future<ul>
<li>[18] addresses the problem of early recognition of unfinished activities</li>
<li>[6] SVM framework for early event detection</li>
<li>predicting motion from still images[29]</li>
<li>prediction the future trajectories of pedestrians[15, 7]</li>
</ul>
</li>
<li><font color="red">different from previous work</font><ul>
<li><font color="red">predict future actions from any timestamp in a video , don’t constrain the input to the “early stage of an action”</font></li>
<li><font color="red">predict from a short video clip or even a static image</font></li>
<li><font color="red">expand the scope of action prediction from controlled lab settings to unconstrained “in-the-wild” footage</font></li>
<li><font color="red"> predicting future actions from still images  or short video clips in unconstrained data</font></li>
</ul>
</li>
</ul>
<h4 id="Methods-5"><a href="#Methods-5" class="headerlink" title="Methods"></a>Methods</h4><ul>
<li><strong>Problem Formulation</strong>:</li>
</ul>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191202132446815.png" alt=""></p>
<ul>
<li><strong>system overview</strong>:</li>
</ul>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191202134658266.png" alt=""></p>
<p>【Qustion 1】how to construct hierarchy construction?</p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191202132617020.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191202134937117.png" alt=""></p>
<p>【Qustion 2】Model formulation</p>
<p>define X : person example     $Y={y_i}_{i=1}^L$   ,L the total number of levels of the hierarchy and $y_i$ is the index of the corresponding moveme at level i.</p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191202135449997.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191202135543989.png" alt="image-20191202135543989"></p>
<p>【Qustion 3】 optimization problem</p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191202135652819.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191202135725669.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191202135751580.png" alt=""></p>
<h4 id="Conclusion-2"><a href="#Conclusion-2" class="headerlink" title="Conclusion"></a>Conclusion</h4><ul>
<li>predict future actions from a single frame in the challenging real-word scenarios</li>
<li>a hierarchical movemes to capture multiple levels of granularities in human movements</li>
<li>develop a max-margin learning framework that jointly learns the appearance models of different movemes as well as their relations</li>
</ul>
<h4 id="Notes"><a href="#Notes" class="headerlink" title="Notes"></a>Notes</h4><ul>
<li><input disabled="" type="checkbox"> 论文【1】 moveme concept   Learning and recognizing human dynamics in video sequences</li>
<li><input disabled="" type="checkbox"> paper [22] Action recognition with improved trajectories</li>
<li><input disabled="" type="checkbox"> paper[18]  Early recognition of ongoing activities from streaming videos</li>
<li><input disabled="" type="checkbox"> paper[24]  Probabilistic modeling of human movements for intention inference</li>
<li><input disabled="" type="checkbox"> paper[29] A data-driven approach for event prediction</li>
<li><input disabled="" type="checkbox"> paper[9] anticipating future activities from RGB-D data by considering<br>human-object interactions   Anticipating human activities using object a↵ordances<br>for reactive robotic response</li>
<li><input disabled="" type="checkbox"> 有没有实现代码 运行看看预测效果，通过代码进一步加深理解，在优化问题定义那块需要加强理解</li>
</ul>
<p><strong>level</strong>: ECCV  CCF_A<br><strong>author</strong>: George Papandreou ,Tyler Zhu  Google Research</p>
<hr>
<h2 id="Paper-PersonLab"><a href="#Paper-PersonLab" class="headerlink" title="Paper: PersonLab"></a>Paper: PersonLab</h2><div align="center">
<br>
<b>Person Pose Estimation and Instance Segmentation with a Bottom-Up,Part-based,Geometric Embedding Model</b>
</div>


<h4 id="Summary-4"><a href="#Summary-4" class="headerlink" title="Summary"></a>Summary</h4><ol>
<li>present a box-free bottom-up approach for the tasks of pose estimation and instance segmentation of people in multi-person images using an efficient single-shot model</li>
<li>tackles both semantic-level reasoning and object-part associations using part-based modeling. Empoys a convolutional network to learns to detect individual keypoints and predict their relative displacements,then group key-points into person pose instances</li>
<li>propose a part-induced geometric embedding descriptor which allows us to associate semantic person pixels with their corresponding person instance,dilevering instance-level person segmentations</li>
</ol>
<h4 id="Research-Objective-5"><a href="#Research-Objective-5" class="headerlink" title="Research Objective"></a>Research Objective</h4><ul>
<li><strong>Application Area</strong>:</li>
<li><strong>Purpose</strong>:  </li>
</ul>
<h4 id="Methods-6"><a href="#Methods-6" class="headerlink" title="Methods"></a>Methods</h4><ul>
<li><strong>system overview</strong>:</li>
</ul>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191130152232740.png" alt=""></p>
<ul>
<li><strong>Keypoint detection</strong> : detect all visible key-points belonging to any person in the image .<ul>
<li>具体计算heatmap 到时再细看 ， short-range offset vector is to improve the keypoint localization accuracy.  <font color="red">aggregate the heatmap and short-range offsets via Hough voting into 2-D Hough score maps</font></li><font color="red">
</font></ul><font color="red">
</font></li><font color="red">
<li><strong>Grouping keypoints into person detection instances</strong>    Fast greedy decoding algorithm</li>
<li><strong>Instance-level person segmentation</strong> : Given the set of keypoint-level person instance detections, the task of our method’s egmentation stage is to identify pixels that belong to people (recognition) and associate them with the detected person instances (grouping)<img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191130154003763.png" alt=""></li>
<li><strong>Semantic person segmentation  &amp;&amp; Associating segments with instances via geometric embeddings</strong></li>
</font></ul><font color="red">
<h4 id="Notes-去加强了解-4"><a href="#Notes-去加强了解-4" class="headerlink" title="Notes 去加强了解"></a>Notes <font color="orange">去加强了解</font></h4><ul>
<li>not read carefully</li>
</ul>
<h1 id="3-视频行为预测-不同粒度"><a href="#3-视频行为预测-不同粒度" class="headerlink" title="3. 视频行为预测(不同粒度)"></a>3. 视频行为预测(不同粒度)</h1><p><strong>level</strong>: CVPR<br><strong>author</strong>: Agrim Gupta , Li Fei-Fie<br><strong>date</strong>: 2018<br><strong>keyword</strong>:</p>
<ul>
<li>trajectory prediction</li>
</ul>
<hr>
<h2 id="Paper-Social-GAN"><a href="#Paper-Social-GAN" class="headerlink" title="Paper: Social GAN"></a>Paper: Social GAN</h2><div align="center">
<br>
<b>Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks</b>
</div>


<h4 id="Summary-5"><a href="#Summary-5" class="headerlink" title="Summary"></a>Summary</h4><ol>
<li>使用LSTM 来编码用户的行为,使用SocialLSTM 池化层来表示较远距离的行为关系,使用生成模型来产生多种路径,利用判别模型从中选择最佳路径</li>
</ol>
<h4 id="Research-Objective-6"><a href="#Research-Objective-6" class="headerlink" title="Research Objective"></a>Research Objective</h4><ul>
<li><strong>Purpose</strong>:  predict the future trajectory</li>
</ul>
<h4 id="Proble-Statement-5"><a href="#Proble-Statement-5" class="headerlink" title="Proble Statement"></a>Proble Statement</h4><ul>
<li>InterPersonal: human have innate ability to read the behavior of others when navigating crowds</li>
<li>Socially Acceptable: social norms</li>
<li>Multimodal: multiple trajectories</li>
</ul>
<p>previous work:</p>
<ul>
<li>theymodel a local neighborhood around each person </li>
<li>they tend to learn average behavior <font color="red">we aim in learning multiple socially acceptable trajectories</font></li>
</ul>
<h4 id="Methods-7"><a href="#Methods-7" class="headerlink" title="Methods"></a>Methods</h4><ul>
<li><strong>system overview</strong>:</li>
</ul>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191118093649383.png" alt=""></p>
<p>【Qustion 1】Using LSTM to encode the location of each person.And model human-human interaction via a Pooling Module (PM). After tobs we pool hidden states of all the people present inthe scene to get a pooled tensor Pi for each person.</p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191118094457342.png" alt=""></p>
<p>condition the generationof output trajectories by initializing the hidden state of the decoder as to produce future scenarios which are consistent<br>with the past<img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191118094655624.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191118094908418.png" alt=""></p>
<p>Discriminator. The discriminator consists of a separate encoder. Specifically, it takes as input Treal = [Xi, Yi] or fake = [Xi, Yˆi] and classifies them as real/fake</p>
<p>[Question 2] Pooling Module Challenge: 1. Variable and large number of people in a scene,we need a compact representation whichcombines information from all the people. 2. Scattered Human-Human Interaction,the network needs to model global configuration.</p>
<ul>
<li>passing the input coordinates through a MLP followed by symmetric function(Max-Pooling).use relative coordinates for translation invariance  we augment the input to the pooling module with relative position of each person with respect to person i.</li>
</ul>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191118095339973.png" alt=""></p>
<p>[question3 ] diverse sample generation . propose a variety lossfunction that encourages the network to produce diverse sample .generate k possible out put samples and choose the best prediction in L2 sense <img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191118100138050.png" alt=""></p>
<h4 id="Conclusion-3"><a href="#Conclusion-3" class="headerlink" title="Conclusion"></a>Conclusion</h4><ul>
<li>introduce variety loss which encourages the generative network of GAN to spread its distribution and cover the space of possible paths while being consistent with the observed inputs.</li>
<li>a new pooling mechanism that learns a global pooling vector which encodes the subtle cues for all people involved in a scene.</li>
</ul>
<p><strong>level</strong>: CVPR<br><strong>author</strong>:Chih-Yao Ma ,Min-Hung Chen<br><strong>date</strong>: 30 Mar 2017<br><strong>keyword</strong>:</p>
<ul>
<li>LSTM, action prediction</li>
</ul>
<hr>
<h2 id="Paper-TS-LSTM-and-Temporal-Inception"><a href="#Paper-TS-LSTM-and-Temporal-Inception" class="headerlink" title="Paper: TS-LSTM and Temporal-Inception"></a>Paper: TS-LSTM and Temporal-Inception</h2><div align="center">
<br>
<b>Exploiting Spatiotemporal Dynamics for Activity Recognition</b>
</div>


<h4 id="Proble-Statement-6"><a href="#Proble-Statement-6" class="headerlink" title="Proble Statement"></a>Proble Statement</h4><ul>
<li>methods extending the basic-stream ConvNet have not systematically explored possible network architectures to further exploit spatiotemporal dynamics within video sequences.The network often use different baseline two-stream networks.</li>
<li>traditional two-stream ConvNets unable to expoit the most critical component in action recognition<font color="red"> visual appearance across both spatial and temporal streams and their correlations are not considered </font>,</li>
<li>previous work mainly try individual methods with little analysis of whether and how they can successfully use temporal information.</li>
<li>each individual work uses different networks for the baseline two-stream approach with varied performance depending on training and testing procedure as well as the optical flow method used.</li>
</ul>
<p>previous work:</p>
<ul>
<li><font color="red"> hand-craft or learned features for training</font>   3D ConvNets:  [9] stacked consecutive video frames and extended the first convolutional layer to learn the spatiotemporal features while exploring different fusion approaches including early fusion and slow fusion. C3D[20] replacing all the 2D convolutional kernels with 3D kernels at the expense of GPU memory. [16] factorize the original 3D kernels into 2D spatial and 1D temporal kernels and achieve comparable performance.  <font color="red"> multiple layers can extract temporal correlations at different time scales and provide better capability to distinguish different types of actions</font></li>
<li>ConvNets with RNNs: directly take variable length inputs and learn long-term dependencies.</li>
<li>Two-stream ConvNets:spatial features and temporal features from optical flow images.  <font color="red">we only use the feature vector representations instead of features maps</font></li>
</ul>
<h4 id="Methods-8"><a href="#Methods-8" class="headerlink" title="Methods"></a>Methods</h4><ul>
<li><strong>system overview</strong>:</li>
</ul>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191124134305103.png" alt=""></p>
<p>【Qustion 1】Spatial stream  $ Temporal stream</p>
<p>spatial stream: the ResNet-101 spatial-stream ConvNet is pre-trained on ImageNet and fine-tured on RGB images extracted from UCF101 datasets with classification loss for predicting activities.</p>
<p>Temporal stream: stacking 10 optical flow images for temporal stream has been considered as a standard for two-stream ConvNets[13,6,28,25,27] <font color="red">follow the same pre-train procedure shown by [25]</font></p>
<p>[model 1] <strong>Temporal Segment LSTM</strong>:  using 25 to divide the sampled video frames into several segments, a temporal pooling layer is applied to extrct distinguishing features from each of the segments.and LSTM is used ot extract the embedded features from all segments.</p>
<p> <img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191124140942805.png" alt=""></p>
<p>[model 2] Temporal-ConvNet :leveraging the temporal relation across diferent frames.</p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191124141556429.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20191124141419689.png" alt="image-20191124141419689"></p>
<p>different types o faction have different temporal characteristics and different kernels in different layers essentially search for different actions by expoiting different receptive fields to encode the temporal characteristics.</p>
<h4 id="Evaluation-2"><a href="#Evaluation-2" class="headerlink" title="Evaluation"></a>Evaluation</h4><ul>
<li><strong>Environment</strong>:   <ul>
<li>Dataset:  experiment on spatial-stream ,temporal-stream and two-stream on three different splits in the UCF101, and HMDB51 datasets.</li>
</ul>
</li>
<li>comparison evaluation 这部分没有看,如果以后用到再来细看</li>
</ul>
<h4 id="Conclusion-4"><a href="#Conclusion-4" class="headerlink" title="Conclusion"></a>Conclusion</h4><ul>
<li>first demostrate a strong baseline two-stream ConvNet using ResNet-101.</li>
<li>propose and investigate two different networks to further integrate spatiotemporal information: temporal segment RNN  and Inception-style Temporal-ConvNet.  but all need propercare.</li>
</ul>
<h4 id="Notes-去加强了解-5"><a href="#Notes-去加强了解-5" class="headerlink" title="Notes 去加强了解"></a>Notes <font color="orange">去加强了解</font></h4><ul>
<li><input disabled="" type="checkbox"> [13] incorporate spatial and temporal information extracted from RGB and optical flow images.Two-stream convolutional networks for action recognition in videos</li>
<li><input disabled="" type="checkbox"> 了解学习 14,18 8 模型 [25][28 ] 7 </li>
<li><input disabled="" type="checkbox"> code available: </li>
<li><input disabled="" type="checkbox"> [6] fusion stage   Convolutional two-stream network fusion for video action recognition</li>
<li><input disabled="" type="checkbox"> Temporal segment networks: Towards good practices for deep action recognition</li>
<li><input disabled="" type="checkbox"> optical methods Brox[2] or TV-L1[29],and the results <img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191124141955787.png" alt=""></li>
</ul>
<p><strong>level</strong>:   2019 winter conference on application of computer vision<br><strong>author</strong>: Erwin Wu   Tokyo Institute of Technology<br><strong>date</strong>: 2019<br><strong>keyword</strong>:</p>
<ul>
<li>action prediction</li>
</ul>
<hr>
<h2 id="Paper-FutruePose"><a href="#Paper-FutruePose" class="headerlink" title="Paper: FutruePose"></a>Paper: FutruePose</h2><div align="center">
<br>
<b>Mixed Reality Martial Arts Training using Real-time 3D Human Pose Forecasting with a RGB Camera</b>
</div>


<h4 id="Summary-6"><a href="#Summary-6" class="headerlink" title="Summary"></a>Summary</h4><ol>
<li>这篇文章将人体2D坐标和光流信息结合起来通过LSTM网络预测未来0.5s 姿态2D坐标,并使用VNect网络建立3D模型,去3D模型上的若干点通过数值分析模型判断是否碰撞.</li>
<li>shortcoming: <ol>
<li>only experiment on boxing,there are still other activities</li>
<li>foces on inference and accuracies on different algorithms ,if using hyper-parameter like d for lattice point flow and threshhold for noise filter</li>
<li>the forcasting information is limited, if building an orientation-based 3D pose estimation by dividing the human body into different parts and learning the bone rotation ,not only to related to their mother joints but relative to the entire body part</li>
<li>the frame rate of normal high speed movement like keck form a professional martial athlete</li>
<li>this paper focus on single person ,if there are multiperson.</li>
</ol>
</li>
</ol>
<h4 id="Research-Objective-7"><a href="#Research-Objective-7" class="headerlink" title="Research Objective"></a>Research Objective</h4><ul>
<li><strong>Application Area</strong>:analyse a player’s habit ,determinate strengths and predict next movement </li>
<li><strong>Purpose</strong>:  a novel mixed reality martial arts training system using deep learning based real time human pose forecasting.</li>
</ul>
<h4 id="Proble-Statement-7"><a href="#Proble-Statement-7" class="headerlink" title="Proble Statement"></a>Proble Statement</h4><ul>
<li>Recent 3D motion capture systems are based on<font color="red"> fabric technology</font> ,requiring to wear specific suits or sensors.</li>
<li>special cameral <font color="red">RGB-Depth,IR cameras</font></li>
<li><font color="red">normal dense optical flow</font> requires many computations and leads to a heavy inference time in LSTM</li>
</ul>
<p>previous work:</p>
<ul>
<li>Martial Sports in AR/VR: wear VR HMD and take a pair of controllers</li>
<li>Real-time 3D pose estimation:<ul>
<li>VNect :provide a better accuracy for the 3D skeleton recognition with less computation and good real-time ability, can’t be used in multi-person detection.</li>
<li>OpenPose detect multiple people in a single image,but the inference time is greater.</li>
<li>[20] 3D pose Recovery using a simple and deep neural network with only two linear layers and two residual blocks ,demostrate 3D pose could be created by 2D joint positions.</li>
</ul>
</li>
<li>Pose forecasting:<ul>
<li>3D-PFNet :the first to forecasting human dynamics from single RGB images. forcasting 2D skeletal poses and converting them into 3D space   87.6mm error.  <font color="red">offline network require large computation</font></li>
<li>[12] forecast human body motion 0.5s advance using five layered neural network   ,7.9cm  <font color="red">IR sensor is not suitable to use in an outdoor environment or a large area.  not for more complicated athletic movemnet such as boxing</font></li>
</ul>
</li>
</ul>
<h4 id="Methods-9"><a href="#Methods-9" class="headerlink" title="Methods"></a>Methods</h4><ul>
<li><strong>Problem Formulation</strong>:forecasting of 3D pose from a single image and the model fitting and collision detection.</li>
<li><strong>system overview</strong>:<img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191116143629118.png" alt=""></li>
<li><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191116152552658.png" alt=""></li>
</ul>
<p>【Qustion 1】<font color="red">how to estmate 2D pose?</font> cropped using bounding box tracker.</p>
<p>use ResNet50[10] to allow the convolutional layer to regress the 2D joint data </p>
<p>【Qustion 2】 <font color="red">2D pose forecasting?</font></p>
<p>using optical flow and joint positions data to do a regression on the LSTMs.  developed a sparse optical flow called Keypoint Lattice-Optical Flow ,creates several lattice points and only calculates the optical flows of the lattice points which close to keypoint.</p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191116153025285.png" alt=""></p>
<p>【Qustion 3】 <font color="red">3D pose recovery?</font>  [20] an effective 3D pose recovery  using VNect network</p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191116153558928.png" alt=""></p>
<p>【Qustion 4】<font color="red">How to understand person’s position and detect the collision in virtual environment ?</font></p>
<p>using 3D model to represent user and have a surface to collide with one another.</p>
<p>using makehumanAPI to gernerate 3D model<img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191116154051111.png" alt=""></p>
<p>divided model into more than 200 segments,called ‘hulls’,each of these hulls contains a convex(凸) collider .detect a collision between two hulls using basic convex polytopes.</p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191116154239879.png" alt=""></p>
<h4 id="Evaluation-3"><a href="#Evaluation-3" class="headerlink" title="Evaluation"></a>Evaluation</h4><ul>
<li><p><strong>Environment</strong>:   </p>
<ul>
<li>Hardware: using TensorFlow on the TSUBAME3.0( Xeon E5-2680  v4 CPU<em>2,Nvidia SXM2 P100 GPU</em>4),tensorflow1.4.1,cuda 8.0 cudnn 5.1lib    HTC VIve(VR HMD),Sony DSCQX10 camera,LogitechC270 webcamera.</li>
<li>Dataset: MPI-INF-3D  and Human36M datasets for pre-training and validation. ratio of 6:2:2 for<br>training, testing, and validation</li>
</ul>
</li>
<li><p><strong>result</strong></p>
<p><strong>RealTimePerformance</strong>:</p>
</li>
</ul>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191116155736081.png" alt=""></p>
<p><strong>Pose forecasting accuracy:</strong></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191116155725964.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191116155800489.png" alt=""></p>
<p><strong>User case study:</strong></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191116161933715.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191116162001653.png" alt=""></p>
<h4 id="Contribution"><a href="#Contribution" class="headerlink" title="Contribution"></a>Contribution</h4><ul>
<li>the first to realize real-time 3D human pose forecasting based on normal video frames and apply it to mixed reality martial arts use</li>
<li>a customized residual network[10]to obtain 2d human joints ,uses recurrent networks to learn the temporal features of the human motion.</li>
<li>use a lattice optical flow algorithm to calculate the joint movement with less computation</li>
</ul>
<h4 id="Notes-去加强了解-6"><a href="#Notes-去加强了解-6" class="headerlink" title="Notes 去加强了解"></a>Notes <font color="orange">去加强了解</font></h4><ul>
<li><input disabled="" type="checkbox"> paper[20],[12],[22],[6],19],[18]</li>
<li><input checked="" disabled="" type="checkbox"> 3D-PFNet  12</li>
<li><input checked="" disabled="" type="checkbox"> <a href="mailto:PCKh@0.05">PCKh@0.05</a> evaluation [1] measure which calculates the percentage of correct key point that uses a matching threshold of 50% of the head segment length.</li>
<li><input checked="" disabled="" type="checkbox"> RMSE: The root-mean-squared error (RMSE) was also calculated to show the deviation of the predicted data</li>
</ul>
<hr>
<p><strong>level</strong>:  ACM<br><strong>author</strong>: Yuuki Horiuchi , Yasutoshi Makino<br><strong>date</strong>: 2017 .10<br><strong>keyword</strong>:</p>
<ul>
<li>Machine learning , Motion estimation,Human-centered computing ,computing methodologies</li>
</ul>
<hr>
<h2 id="Paper-Computational-Foresight"><a href="#Paper-Computational-Foresight" class="headerlink" title="Paper: Computational Foresight"></a>Paper: Computational Foresight</h2><div align="center">
<br>
<b>Forecasting Human Body Motion in Real-time for Reducing Delays in Interactive System</b>
</div>

<h4 id="Research-Objective-8"><a href="#Research-Objective-8" class="headerlink" title="Research Objective"></a>Research Objective</h4><ul>
<li><strong>Application Area</strong>:<font color="red">instruct sports actions ,prevent elderly form falling to the ground, prevent accident in advance. reducing delay in remote interactive system</font></li>
<li><strong>Purpose</strong>:  forecast human body 0.5s before the actual motion in real-time  with accuracy of 7.9cm</li>
</ul>
<h4 id="Proble-Statement-8"><a href="#Proble-Statement-8" class="headerlink" title="Proble Statement"></a>Proble Statement</h4><ul>
<li>diverse communication with remote areas has become possible, <font color="red">information transmission delay</font></li>
</ul>
<p>previous work:</p>
<ul>
<li>Holoportation system[1] communicate with remote people using HMD.</li>
<li>TELESAR V system[2] feel object through remotely connected robot with haptic sensing and feedback.</li>
<li>pattern categorized or estimation using DNN,  Predicting trajectory of movie in realtime.  <font color="red">there is no research that forecast body motions which is not repetitive and personalized but universal in realtime and visualize it to user.</font></li>
</ul>
<h4 id="Methods-10"><a href="#Methods-10" class="headerlink" title="Methods"></a>Methods</h4><ul>
<li><strong>system overview</strong>:</li>
</ul>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191122164724372.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191122164824683.png" alt=""></p>
<p>【Qustion 1】how to extract 25 body point and COG?    论文[14] 需要学一下</p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191122165225765.png" alt=""></p>
<p>[解决问题2]Neural Network design ?</p>
<p>combine past 10 frames of 26 data as a one learning dataset .(joints+COG position)<em>3 demensions(x,y,z)</em> * 10 frames=780</p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191122165657463.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191122165302517.png" alt=""></p>
<p>[解决问题3] 损失函数 和 优化器选择</p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191122165849130.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20191122170046854.png" alt="image-20191122170046854"></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191122171352648.png" alt=""></p>
<h4 id="Evaluation-4"><a href="#Evaluation-4" class="headerlink" title="Evaluation"></a>Evaluation</h4><ul>
<li><strong>Environment</strong>:   <ul>
<li>Dataset:   Kinect V2  eleven subject to jump as many time as they could ,one duration for one minute. they allow to jump either ways in random order and the distance less than 2.5m. </li>
<li>laptop(CPU: intel core-i7-7820HK 2.9-3.9Ghz,GPU: Nvidia Geforece GTX 1080)  3.32ms for COG NN  matrix operation, 5.75ms for redering bone image less than 33ms for measuring depth map and 3D position of 25 body joints and COG</li>
<li><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191122171323598.png" alt=""></li>
<li><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191122170458248.png" alt=""></li>
</ul>
</li>
<li></li>
</ul>
<h4 id="Conclusion-5"><a href="#Conclusion-5" class="headerlink" title="Conclusion"></a>Conclusion</h4><ul>
<li>均方误差MSE   注:RMSE（即MSE的平方根)<img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191122171538058.png" alt=""></li>
<li>平均绝对误差（MAE）<img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191122171610156.png" alt=""></li>
</ul>
<h4 id="Notes-去加强了解-7"><a href="#Notes-去加强了解-7" class="headerlink" title="Notes 去加强了解"></a>Notes <font color="orange">去加强了解</font></h4><ul>
<li>human gesture recognition by using depth map [10]  Neural network for<br>dynamic human motion prediction</li>
<li>论文14 计算重心</li>
</ul>
<hr>
<p><strong>level</strong>: CVPR   CCF A<br><strong>author</strong>: Junwei Liang ,Li Fei-Fei<br><strong>date</strong>: ‘2019-05-31’<br><strong>keyword</strong>:</p>
<ul>
<li>LSTM, activity prediction</li>
</ul>
<hr>
<h2 id="Paper-Peeking-into-the-future"><a href="#Paper-Peeking-into-the-future" class="headerlink" title="Paper: Peeking into the future"></a>Paper: Peeking into the future</h2><div align="center">
<br>
<b>Predicting future person activities and location in videos</b>
</div>


<h4 id="Summary-7"><a href="#Summary-7" class="headerlink" title="Summary"></a>Summary</h4><p>这篇文章通过 分析 人的位置，行为，与周围事务的距离，周围的环境信息来预测未来轨迹何未来动作，并通过位置预测算法来减少 人位置的累计误差。</p>
<ol>
<li>在编码用户与周围的事务互动时，能不能编码用户的之间的动作联系对应起来，而不是简单的距离</li>
</ol>
<h4 id="Research-Objective-9"><a href="#Research-Objective-9" class="headerlink" title="Research Objective"></a>Research Objective</h4><ul>
<li><strong>Application Area</strong>:Future person path/trajectory activity prediction (accident avoidance , smart personal assistance , self-driving car , socially-aware robots , anticipating pedestrian movement at traffic intersections or a road)</li>
<li><strong>Purpose</strong>:  deciphering human behaviors to predict pedestrian’s future path jointly with future activities.</li>
</ul>
<h4 id="Proble-Statement-9"><a href="#Proble-Statement-9" class="headerlink" title="Proble Statement"></a>Proble Statement</h4><ul>
<li>Humans navigate through public spaces often with specific purposes in mind.</li>
</ul>
<p>previous work:</p>
<ul>
<li>Person-person models for trajectory prediction.<ul>
<li>[32,34] predict person path by considering human social interactions and behaviors in crowded scene.</li>
<li>[36]learned human hehavior in crowds by imitating a decision-making process</li>
<li>Social-LSTM[1] added social pooling to model nearby pedestrian trajectory patterns.</li>
<li>Social-GAN[7] added advertisarial training on social LSTM to improve perfomance.</li>
<li><font color="red">they simply consider a person as points ,we use geometric ralation to explicitly model the person-scene interaction and the person-object relatoinsj</font></li>
</ul>
</li>
<li>Person-scene models for trajectory prediction :learning the effect of the physical scene<ul>
<li>[13] using Inverse Reinforcement learning to forecast human trajectory</li>
<li>Scene-LSTM divided the static scene into Manhattan Grid and predict pedestrian’s location using LSTM</li>
<li>CAR-Net proposed an attention network on top of scene semantic CNN to predict person trajectory</li>
<li>SoPhie vombined deep neural network features form scene semantic segmentation model and generatice adbersarial network using attention to model person trajectory</li>
<li><font color="red">we explicitly pool scene semantic features around each person at each time instant ,the model directly learn from such interactions</font></li>
</ul>
</li>
<li>Person visual features for trajectory prediction  :using individual’s visual features instead of considering them as points in the scene.<ul>
<li>[14] looked at pedestrian ‘s faces to model their awareness to  predict whether they wil corss the road using Dynamic Bayesian Network .</li>
<li>[33] person keypoint features with a convolutional neural network to predict future path .</li>
<li><font color="red">we consider both person behavior and their interactions with soundings</font></li>
</ul>
</li>
<li>Activity prediction /early recognition <ul>
<li>[29] utilized unsupervised learning with LSTM to reconstruct and predict video representations.</li>
</ul>
</li>
<li>Multiple cues for tracking/group activity recognition:  <ul>
<li>previous works take into account multiple cues in video for tracking ,group activity recognition</li>
<li><font color="red">rich vision features focal attention,  location prediction to bridge the two taks</font></li>
</ul>
</li>
<li>most existing work [31,1,7,26,21,31] which oversimplifies a person as a point in space,we encode a person through <font color="red">rich semantic features about visual appearance ,body movement ,and interaction with the surroundings ,</font></li>
</ul>
<h4 id="Methods-11"><a href="#Methods-11" class="headerlink" title="Methods"></a>Methods</h4><ul>
<li><p><strong>Problem Formulation</strong>:<img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191109103802813.png" alt=""></p>
</li>
<li><p><strong>system overview</strong>:</p>
</li>
</ul>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191109103550662.png" alt=""></p>
<p>【Qustion 1】<font color="red">how to model the person’s appearance and body movement about every individual in a scene</font>?</p>
<ul>
<li>utilize a pre-trained object detection model with “<strong>RoIAlign[8</strong>]” to extrace fixed size CNN features for each person bounding box. for every person ,average the feature along the spatial dimentions and feed them into LSTM encoder   -&gt; obtain T*d ,where d is the hidden size of the LSTM.</li>
<li>utilize a person key-point detection model trained on MSCOCO dataset[6] to extract preson keypoint information.we apply <font color="red">the linear transformation to embed the keypoint coordinates ,这里的线性处理不理解</font>before feed into LSTM.-&gt;obtain T*d ,where d is the hidden size of the LSTM<img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191109111014991.png" alt=""></li>
</ul>
<p>【Qustion 2】<font color="red">how to model the interaction between a person and their surroundings,person-scene and person-object</font></p>
<ul>
<li><p>Person-scene: <font color="red">whether the person is near the sidewolk or grass</font> use a <strong>pre-trained scene segmentation model[4]</strong> to extract pixel-level scene semantic classes(10 class eg.roads , sidewalks…) for each frame. the scene   the semantic features are integeres of the size T * h * w ,Given a person’s xy coordinate ,we pool the scene features at the person;s current location from the convolution feature map.  <img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191109111844802.png" alt=""></p>
</li>
<li><p>Person-object: <font color="red">how far away the person is to the other person or object</font>models the <strong>geometric relation</strong> and <strong>the object type</strong> of all objects/persons in the scene.在论文[9]中证明了这个方法的高效性<img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191109112732633.png" alt=""></p>
<ul>
<li>geometric relation:                   </li>
</ul>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191109112036904.png" alt=""></p>
<ul>
<li>object type: </li>
</ul>
</li>
</ul>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191109112647171.png" alt=""></p>
<p>【Qustion 3】<font color="red">how to predict the trajectory ?</font> using effective focal attention[17]  ,原始模型见[7]</p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191109113310088.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20191109113735888.png" alt="image-20191109113735888"></p>
<p>【Qustion 4】<font color="red">how to predict activity ?</font> introduce an auxiliary task :activity location prediction in addition to predicting the future activity label of the person .<img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191109114434310.png" alt=""></p>
<ul>
<li>activity location prediction with Manhattan Grid  (location classification(to predict correct grid block in which the  final location coordinates reside),location regression(to predict the deviation of the grid block center to final location coordinate))   <font color="red">how to accurate localization using multi-scale features in a cost-effective way</font></li>
<li>activity label prediction: <img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191109142619975.png" alt=""></li>
</ul>
<h4 id="Evaluation-5"><a href="#Evaluation-5" class="headerlink" title="Evaluation"></a>Evaluation</h4><ul>
<li><strong>Environment</strong>:   <ul>
<li>Dataset: ActEV/ViRAT </li>
</ul>
</li>
<li>model the intention in terms of a predefined set of 29 activities provided by NIST .</li>
</ul>
<h4 id="Conclusion-6"><a href="#Conclusion-6" class="headerlink" title="Conclusion"></a>Conclusion</h4><ul>
<li>propose an end-to-end multi-task learnig system<font color="blue"> utilizing rich visual features about human behavioral information and interaction with their surroundings</font> .</li>
<li>the first empirical evidence that joint medeling of paths and activities benefits future path prediction.<ul>
<li>learning activity together with the path may benefit the future path prediction</li>
<li>joint model advances the capability of understanding not only the future path but also the future activity by taking into account the rich semantic context in videos.</li>
<li>introduce an auxiliary task for future activity prediction,activity location.</li>
</ul>
</li>
<li>propose multi-task learning framework with new techniques to tackle the challenge of joint future path and activity prediction.</li>
<li>validate the model on two benchmarks: ETH&amp;UCY , and ActEV/VIRAT.</li>
</ul>
<h4 id="Notes-去加强了解-8"><a href="#Notes-去加强了解-8" class="headerlink" title="Notes 去加强了解"></a>Notes <font color="orange">去加强了解</font></h4><ul>
<li><input disabled="" type="checkbox"> Effective focal attention was originally proposed to carry out multimode inference over a sequence of images for visual question answering. key idea is project multiple features into a space of correlation where discriminative features can be easier to capture by attention mechanism.</li>
<li><input disabled="" type="checkbox"> Attention mechanism   ???</li>
<li><input disabled="" type="checkbox"> 论文37 decision-making process 方法是什么？</li>
<li><input disabled="" type="checkbox"> [13] using Inverse Reinforcement  方法是什么</li>
<li><input disabled="" type="checkbox"> [33] person keypoint features to predict trajectory ?</li>
<li><input disabled="" type="checkbox"> <strong>RoIAlign[8</strong>]   学习使用这个网络</li>
<li><input disabled="" type="checkbox"> <strong>pre-trained scene segmentation model[4]</strong>  学习了解下场景分割技术</li>
<li><input disabled="" type="checkbox"> Code 学习使用： <a href="https://github.com/google/next-prediction" target="_blank" rel="noopener">https://github.com/google/next-prediction</a> </li>
</ul>
<hr>
<p><strong>level</strong>: CVPR ccf A<br><strong>author</strong>:  alexandre Alahi , Kratarth Goel     stanford.edu<br><strong>date</strong>:<br><strong>keyword</strong>:</p>
<ul>
<li></li>
</ul>
<hr>
<h2 id="Paper-Social-LSTM"><a href="#Paper-Social-LSTM" class="headerlink" title="Paper: Social LSTM"></a>Paper: Social LSTM</h2><div align="center">
<br>
<b>Human Trajectory Prediction in Crowded Space</b>
</div>


<h4 id="Research-Objective-10"><a href="#Research-Objective-10" class="headerlink" title="Research Objective"></a>Research Objective</h4><ul>
<li><strong>Application Area</strong>: social aware roots[41], intelligent tracking system[43]</li>
<li><strong>Purpose</strong>:  predict the motion dynamics in crowded scenes.</li>
</ul>
<h4 id="Proble-Statement-10"><a href="#Proble-Statement-10" class="headerlink" title="Proble Statement"></a>Proble Statement</h4><p>previous work:</p>
<ul>
<li>they use hand-craft functions(人工特征) to model interactions for specific settings rather than inferring them in data driven fashion.</li>
<li>they focus on modeling interactions among people in close proximity to each other(to avoid immediate collisions), don‘t anticipate interactions that could occur in the more distant future.</li>
<li>RNN model for sequence prediction (speech recognition , caption generation , machine translation , image/vedio classification, human dynamic)<ul>
<li>LSTM  and <font color="red">Gated Recurrent Units[12]</font>  most common methods.</li>
<li>[20] predict isolated handwriting sequence </li>
</ul>
</li>
</ul>
<h4 id="Methods-12"><a href="#Methods-12" class="headerlink" title="Methods"></a>Methods</h4><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191112092925006.png" alt=""></p>
<p>【定义问题1】every person has different motion pattern,they move with different velocities ,acceleration and have different gaits ,how to model person-specific motion properties from a limited set of initial observation corrosponding to the person</p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191112092949875.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20191112093015584.png" alt="image-20191112093015584"></p>
<p>【定义问题2】 every person has a different number of neighbors and in very dese crowds,the number could prohibitively high?</p>
<p> a compact representaion “Social “ pooling layers ,and preserve the spatial information through grid based pooling .</p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191112094414250.png" alt=""></p>
<p>【定义问题3】 how to estimate the Position?</p>
<p><img src="https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20191112100947577.png" alt="image-20191112100947577"></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191112101003868.png" alt=""></p>
<p>【定义问题4】 how to deal with occupancy map pooling?</p>
<p>the Social LSTM model can be used to pool any set of features from neighboring trajectory ,and learn to reposition a trajectory to avoid immediate collision with neighbors.<font color="red">这一部分不太明白</font></p>
<h4 id="Evaluation-6"><a href="#Evaluation-6" class="headerlink" title="Evaluation"></a>Evaluation</h4><ul>
<li><strong>Environment</strong>: ETH  ,UCY</li>
<li><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191112101541086.png" alt=""></li>
</ul>
<h4 id="Conclusion-7"><a href="#Conclusion-7" class="headerlink" title="Conclusion"></a>Conclusion</h4><ul>
<li>introduce the Social pooling layer which allows the LSTMs of partially proximal sequences to share their hidden-states with each other.</li>
<li>analyze the trajectory patterns generated by our model to understant the social constrains learned from the trajectory datasets.</li>
<li>predicting the trajectories of pedestrians much  more accurately than state-of-the-art models on ETH,UCY</li>
</ul>
<h4 id="Notes-去加强了解-9"><a href="#Notes-去加强了解-9" class="headerlink" title="Notes  去加强了解"></a>Notes  <font color="yellow">去加强了解</font></h4><ul>
<li><input disabled="" type="checkbox"> <p>Generating sequences with recurrent neural networks</p>
</li>
<li><input disabled="" type="checkbox"> <p>LSTM speech generation【21】demo  去github上找代码 </p>
</li>
<li><input disabled="" type="checkbox"> <p>[32] 学习和了解 Inverse Reinforcement Learning to predict human paths in static scenes.</p>
</li>
<li><input disabled="" type="checkbox"> <p>Theano: A cpu and gpu math compiler in python</p>
</li>
<li><input checked="" disabled="" type="checkbox"> <p>bivariate Gaussian distribution多元正态分布<img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191121101725026.png" alt=""></p>
</li>
</ul>
<hr>
<p><strong>level</strong>: IEEE Access   CCF B 类<br><strong>author</strong>: 10.25.2019<br><strong>date</strong>: ‘2019-10-25’<br><strong>keyword</strong>:</p>
<ul>
<li>Action recognition,deep learning ,pedestrian detection ,time-to-cross estimation</li>
</ul>
<hr>
<h2 id="Paper-Multi-Task-Pedestrian"><a href="#Paper-Multi-Task-Pedestrian" class="headerlink" title="Paper: Multi-Task Pedestrian"></a>Paper: Multi-Task Pedestrian</h2><div align="center">
<br>
<b>Multi-Task Deep learning for Pedestrian Detection ,Action Recognition and Time to Cross Prediction</b>
</div>


<h4 id="Summary-8"><a href="#Summary-8" class="headerlink" title="Summary"></a>Summary</h4><p>这篇文章解决了如何去检测行人，识别行人的动作（利用JAAD数据库和现有的方法），并且预测了在行人过马路的状态下穿过时间预测。使用了RetinaNet 网络检测，LSTM网络去预测。</p>
<p>问题：</p>
<ol>
<li>如何去检测多个人的，以及多个人的相应的动作  引用文章的没有细说？需要看下</li>
<li>在预测的时候LSTM网络仅仅是BB坐标，每个人的步速步伐大小可能不一样这里应该怎么解决？</li>
<li>现有RF实现骨架检测技术，以及手势识别技术，人体骨架各部分运动检测，能否用LSTM预测下一个动作，这是预测行人过马路，如果在室内可能需要检测或预测哪些动作</li>
</ol>
<h4 id="Research-Objective-11"><a href="#Research-Objective-11" class="headerlink" title="Research Objective"></a>Research Objective</h4><ul>
<li><strong>Application Area</strong>:understand the intention of road users involved to ensure their safety and secure the traffic flow.</li>
<li><strong>Purpose</strong>:  estimate TTC.</li>
<li><strong>System_Design</strong>: </li>
<li><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191106112647294.png" alt=""></li>
<li><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191106111040275.png" alt=""></li>
<li><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191106111058015.png" alt=""></li>
<li><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191106111116174.png" alt=""></li>
<li><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191106111520160.png" alt=""></li>
</ul>
<h4 id="Proble-Statement-11"><a href="#Proble-Statement-11" class="headerlink" title="Proble Statement"></a>Proble Statement</h4><ul>
<li><strong>pedestrian detection problem</strong>: progress in pedestrian detection is hindered by the difficulty of detecting all(partially)occluded pedestrians and the problem of operating efficiently in severe weather conditions.</li>
<li><strong>ADAS need to solve three problems</strong>: 1. a detection model for localizing and recognizing the pedestrians among other road users 2. a prediction model to estimate the pedestrian actions over next frames(short,medium,long-time prediction)</li>
<li><strong>Datashortcoming:</strong> there are no public databases annotated with pedestrian time to cross while there are several interesting huge pedestrian detection databases(Kitti,caltech,among others),and some databases don’t provide any pedestrian action labels</li>
<li>Estimation of the pedestrian intention and especcially of the pedestrian  actions is even more challenging because of the <font color="red">ambiguities</font> in pedestrian motions.</li>
</ul>
<p>previous work:</p>
<ul>
<li>pedestrian movement and pedestrian behaviors[13],[14],interacctions between pedestrians[15] [16] ,pedestrian tracking paths[9] ,[10],  a review of the predicting pedestrian behavior[12],<font color="red">pedestrian intention requires to use pedestrian specific dynamic information and contextual road environment</font> ,in [17]  present a pedestrian action recognition based on <font color="red">AlexNet handling JAAD dataset and use temporal and spatial-temporal contextual information to increase the prediction perfomance</font></li>
<li>[9] A pedestrian position estimation based on the<font color="red"> Extended Kalman Filter and Interacting Multiple Model algorithm using Constant Velocity</font></li>
<li>[18] combination of the Gaussian Precess Dynamic Models ,Probablistic Hierarchical Trajectory Machine with Kalman Filter and interacting Multiple Model-based on the Daimler Data.</li>
<li>[10] A short-term prediction of pedestian behaviors <font color="blue">using Daimler datasets,to predict the pedestrian trajectory and its final destination using CNN base on LSTM and path planning</font>.</li>
<li>[13] mixture of CNN based pedestrian detection tracking and pose estimation to predict the pedestrian crossing actions based on the JAAD dataset </li>
<li><strong>Summary</strong>:previous only discriminates between the pedetrian from the non-pedestrian among other road users and estimates the pedestrian action or its final destination for the next frames（short medium and long term) <font color="red">the Time to cross estimation of pedestrians is more challengin than predicting the pedestrian action since it requires contextual spatial-termprary:a fine analysis  of the pedestrian motion and the whole scene</font></li>
</ul>
<h4 id="Methods-2-19"><a href="#Methods-2-19" class="headerlink" title="Methods         [2],[19]"></a>Methods         [2],[19]</h4><p>【定义问题0】no public databases anntated with pedestrian time to cross,the databases don’t provide any pedestrian action labels?</p>
<p>we select some cues from the JAAD [1] public data set in order to solve this issue and then we made our pedestrian TTC annotation for all videos.  <font color="red">这个cue是指什么</font>    JAAD 数据已经包括了 pedestrian bounding boxes for pedestrian detection and pedestrian attributes.</p>
<p>【定义问题1】how to detect pedestrian ?</p>
<p>Applying a generic object detector based on the public RetineNet[2], the author handled the Resnet50[19]CNN architecture for the classification task with the Keras public open-source implementation described in [2],all the training process is based on the JAAD dataset,which provides an annotation of pedestrians with behavioral tags and pedestrians without behaciors tags.</p>
<p><img src="https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20191106112333433.png" alt="image-20191106112333433"></p>
<p>【定义问题2】how to split the pedestrian Joint Attention for Autonomous Driving into four class?   previous work</p>
<p>【定义问题3】how to estimate Time to cross?</p>
<p><img src="https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20191106112423122.png" alt="image-20191106112423122"></p>
<h4 id="Evaluation-7"><a href="#Evaluation-7" class="headerlink" title="Evaluation"></a>Evaluation</h4><ul>
<li><strong>Environment</strong>: dataset: JAAD dataset[17] provides pedestrian bounding boxes for pedestrian detection,pedestrian attributes for estimating the pedestrian behavior and traffic scene elements.</li>
</ul>
<h4 id="Conclusion-8"><a href="#Conclusion-8" class="headerlink" title="Conclusion"></a>Conclusion</h4><ul>
<li>Train all pedestrian Bounding Boxes samples with the <font color="red">RetinaNet</font> for pedestrian detection purpose</li>
<li>Split the pedestrian Joint Attention for Autonomous Driving(<font color="red">JAAD</font>) data set into four classes for pedestrian action functionality :pedestrian is preparing to cross the street ,pedestrian is crossing the street ,pedestrian is about to cross the street and pedestrian intention is ambiguous</li>
<li>Train <font color="red">LSTM</font> model using only BB coordinates in order to estimate the time to cross of each pedestrian.</li>
</ul>
<h4 id="Notes-去加强了解-10"><a href="#Notes-去加强了解-10" class="headerlink" title="Notes  去加强了解"></a>Notes  <font color="orange">去加强了解</font></h4><ul>
<li><input disabled="" type="checkbox"> 了解 JAAD 数据集什么格式</li>
<li><input disabled="" type="checkbox"> 学习和运行RetinaNet网络</li>
<li><input disabled="" type="checkbox"> 学习何使用AlexNet 网洛</li>
<li><input checked="" disabled="" type="checkbox"> 了解LSTM网络</li>
<li><input disabled="" type="checkbox"> LSTM 网络运行使用</li>
<li><input disabled="" type="checkbox"> 论文 9，18，10中的方法</li>
<li><input disabled="" type="checkbox"> SSD 网络学习使用</li>
<li><input disabled="" type="checkbox"> Faster-RCNN网络学习使用</li>
<li><input disabled="" type="checkbox"> Yolo3网络学习使用</li>
</ul>
<h1 id="4-感知系统"><a href="#4-感知系统" class="headerlink" title="4.感知系统"></a>4.感知系统</h1><p><strong>level</strong>: Pro.ACM Interact.Mob   Wearable Ubiquitous Technol<br><strong>author</strong>:      karan ahuja Carnegie Mello University<br><strong>date</strong>: 2019 9<br><strong>keyword</strong>:</p>
<ul>
<li>Human-centered computing ,Interactive systems and tools, Classroom sensing,Compute Vision,Speech</li>
</ul>
<hr>
<h2 id="Paper-EduSense"><a href="#Paper-EduSense" class="headerlink" title="Paper: EduSense"></a>Paper: EduSense</h2><div align="center">
<br>
<b>Practical Classroom Sensing at Scale</b>
</div>


<h4 id="Proble-Statement-12"><a href="#Proble-Statement-12" class="headerlink" title="Proble Statement"></a>Proble Statement</h4><ul>
<li>need an expert to instructs   expensive</li>
<li>lack of sufficient feedback opportunities on pedagogical skill</li>
</ul>
<p>previous work:</p>
<ul>
<li>Instrumented Classrooms<ul>
<li>instrumented with <font color="red"> pressure sensors</font> to characterize varying levels of interest and engagement,such as slumped back sitting upright.</li>
<li>Affectiva’s wrist-worn Q sensor[62] which senses the wearer’s skin conducance ,temperature and motion to infer engegement level.</li>
<li>EngageMeter[32] used electroencephalography headsets to detect shifts in student engagement,alertness and workload</li>
</ul>
</li>
<li>Non-Invasive Class Sensing<ul>
<li>[19] an omnidirectional room microphone and head-mounted teacher microphone to automatically segment teacher and student speech events, intervals of silence.</li>
<li>Oral presentation practice systems AwareMe[11],Presentation Sensei[46],PoboCOP[75] compute speeck quality metrics (pitch variety,pauses fillers speaking rate)</li>
<li>Equally versatile using cameras ,detect hand rises,skin tone ,edge detection</li>
<li>Robust Face detection find and count student,estimate their  head orientation,coarsely signaling their area of focus,facial landmarks to analyse engagement,frustration,off-task behavior.</li>
</ul>
</li>
</ul>
<h4 id="Methods-13"><a href="#Methods-13" class="headerlink" title="Methods"></a>Methods</h4><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191112104005467.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191112114633868.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191112114906902.png" alt=""></p>
<p>【定义问题1】Featurization Modules</p>
<ul>
<li><p>Sit &amp; Stand detection: using body keypoints hips , knees , feet  and ratio of distances between chest and foot, the chest and knee both legs.</p>
</li>
<li><p>Hand Raise detection: neck ,chest, shoulder elbow wrist and compute the direction unit vectors btween all pairs of these points,and compute the distance between all pairs of points ,normalized by the distance between all pairs of these points.</p>
</li>
<li><p>Upper Body detection: utilize the same eight upper body keypoints  to predict arms at rest, arms at closed and hands on face.<img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191112115847153.png" alt=""></p>
</li>
<li><p>Smile Detection: ten mouth landmarks on the outer lip and ten landmarks on the inner lip. compute direction unit vectors from the left lip corner to all other points  SVM to binary classifaction.</p>
</li>
<li><p>Mouse Open Detection: estimate if a mouse is open,to produce the talking confidence.use a Binary SVM and two highly descriptive features adapted from [71 predict eys open &amp; closed],the height of the mouth to the left and right of center ,divided by the width of the mouth.</p>
</li>
<li><p>Head Oriention &amp; Class Gaze : Using a perspective-n-point algorithm[50] in combination with anthropometric face data[53],produces a coarse 3D orientation of the head for each body.</p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191112121139650.png" alt=""></p>
</li>
<li><p>Body Position &amp; Classroom Topology: perspective-n-point produces the orientation for each body founded in a scene ,and estimate 3D position in real world coordinates. to reveal the classroom topologies and help illuminate spatial patterns in the class.</p>
</li>
<li><p>Synthetic Accelerometer: uuse the 3D head position produced during scene parsing and claculate a delta X/Y/Z .</p>
</li>
<li><p>Student &amp; Instructor Speech: the RMS of the student-facing camera’s microphone,the RMS of the instructor facing camera’s microphone ,the ratio between the latter two values. uisng random forest classifier to predict the current speech is coming from the instructor or students.</p>
</li>
<li><p>Speech Act delimiting: </p>
</li>
</ul>
<h4 id="Evaluation-8"><a href="#Evaluation-8" class="headerlink" title="Evaluation"></a>Evaluation</h4><ul>
<li><strong>Environment</strong>: </li>
<li><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191112121150907.png" alt=""></li>
</ul>
<h4 id="Conclusion-9"><a href="#Conclusion-9" class="headerlink" title="Conclusion"></a>Conclusion</h4><ul>
<li>a comprehensive sensing system that produces a plethora of theoretically-motivated visual and audio features correlated with effective instruction.</li>
<li>the first to unify them into a cohesive real-time,in-the-wild evaluated and practically-deployable system.</li>
</ul>
<h4 id="Notes-1"><a href="#Notes-1" class="headerlink" title="Notes"></a>Notes</h4><ul>
<li><input disabled="" type="checkbox"> Classroom Discourse Analyzer [15] 了解这个系统</li>
<li><input disabled="" type="checkbox"> [19]了解下这篇文章</li>
<li><input disabled="" type="checkbox"> 了解下  Equally versatile 系统</li>
<li><input disabled="" type="checkbox"> 了解 CERT 技术</li>
<li><input disabled="" type="checkbox"> 了解下FFMPEG</li>
<li><input disabled="" type="checkbox"> NVIDIA Visual Profiler 技术是什么</li>
<li><input disabled="" type="checkbox"> 学习使用OpenPose   dlib64-point face landmarks【44】</li>
<li><input disabled="" type="checkbox"> adaptive background noise filter  to remove background noises how??</li>
<li><input disabled="" type="checkbox"> open source system  <a href="http://www.EduSense.io" target="_blank" rel="noopener">http://www.EduSense.io</a></li>
</ul>
<h1 id="5-骨骼提取"><a href="#5-骨骼提取" class="headerlink" title="5. 骨骼提取"></a>5. 骨骼提取</h1><h2 id="5-1-40个骨骼提取开源项目"><a href="#5-1-40个骨骼提取开源项目" class="headerlink" title="5.1. 40个骨骼提取开源项目"></a>5.1. <a href="https://mp.weixin.qq.com/s?__biz=MzI5MDUyMDIxNA==&amp;mid=2247507854&amp;idx=2&amp;sn=e02294c31e6867bf2e4270178c2a75e8&amp;chksm=ec1c3277db6bbb612de6d5b3edaa2a4fb8da19e7b6e62b10feb6c21d8fddc748e570ef05c050&amp;scene=126&amp;sessionid=1600264884&amp;key=3542bed875d644de951ff14ae71a83001ab1e1812ce7aa66a998b68fd92197d07eb6ed465593d3aac71554ab7ccfb47f11533fe51eec433dba65046ba6244a25e8050051445366bb86635b10cd4bcf9f1c9220c9515042c7795e056f147b995b4688cde692c65c611ca97ef9d78c191310ee8ebb1c30e3cd4a12b77aa5d24fe7&amp;ascene=1&amp;uin=MzE0ODMxOTQzMQ%3D%3D&amp;devicetype=Windows+10+x64&amp;version=62090529&amp;lang=zh_CN&amp;exportkey=Ax1vPAqMPtdpFIw823EzgRY%3D&amp;pass_ticket=TfC86Xzy4b6ESRk%2FasnYpQs4p0qrNXFR4RzKdh4co%2FPl3pb2EHboMmNDJmdTviPd&amp;wx_header=0" target="_blank" rel="noopener">40个骨骼提取开源项目</a></h2><p><strong>level</strong>: CVPR  CCF A<br><strong>author</strong>: Zhe Cao<br><strong>date</strong>: ‘2019-5-30’<br><strong>keyword</strong>:</p>
<ul>
<li>2D human pose estimate ，2D foot keypoint estimate，real time，multiple person，part affinity fields</li>
</ul>
<hr>
<h2 id="Paper-OpenPose"><a href="#Paper-OpenPose" class="headerlink" title="Paper: OpenPose"></a>Paper: OpenPose</h2><div align="center">
<br>
<b>Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields</b>
</div>


<h4 id="Summary-9"><a href="#Summary-9" class="headerlink" title="Summary"></a>Summary</h4><ul>
<li>prove PAF refinement is critical and sufficient for high accuracy ,removing the body part confidence map refinement while increasing the network depth.</li>
<li>using body and foot key-point detector.</li>
<li>Open-Pose library</li>
</ul>
<h4 id="Research-Objective-12"><a href="#Research-Objective-12" class="headerlink" title="Research Objective"></a>Research Objective</h4><ul>
<li><strong>Application Area</strong>: key-point detect eg: body skeleton ,hand skeleton ,face skeleton,hand skeleton ,the body part location for futher explored like prediction</li>
<li><strong>Purpose</strong>:  using part affinity fields to real-time calculate multiperson 2D pose.</li>
</ul>
<h4 id="Problem-Statement-1"><a href="#Problem-Statement-1" class="headerlink" title="Problem Statement"></a>Problem Statement</h4><ul>
<li>each image may contain an <font color="red">unknown number</font> of people that can appear at any <font color="red">position or scale</font></li>
<li><font color="red">interactions</font> between people induce <font color="red"> complex spatial interference due to contact occlusion or limb articulations,making association of parts difficult</font></li>
<li>runtime complexity tends to grow with the number of people </li>
</ul>
<p>previous work:</p>
<ul>
<li><p><strong>Single Person Pose Estimation</strong>: perform inference over a combination of local observations on body parts and the spatial dependencies . the spatial model for articulated pose is either based on <font color="red">tree-structured graphical models</font>, which parametrically encode the spatial relationship between adjacent parts following a kinematic chain or not tree models that <font color="red">augment the tree structure with additional edges </font>to capture occlusion symmetry and long range relationship.</p>
<ul>
<li>[34] used a multi-stage architecture based on a sequential prediction framework, incorporating global context to refine part confidence maps and preserving multi-modal uncertainty form previous iterations.</li>
<li><font color="red">all this methods assume a single person ,where the location and scale of the person of interest is given.</font></li>
</ul>
</li>
<li><p><strong>Multi-person Pose estimation</strong>: <font color="red">Top-down</font> strategy that first detects people and then have estimated the pose of each person independently on  detected region.<font color="blue">suffers form early commitment  on person detection,fail to capture the spatial dependencies across different people</font> .<font color="red">bottom up</font> approach that jointly labels part detection candidates and associated them to individual people,with pairwise scores regressed from spatial offsets of detections</p>
<ul>
<li>[47] further simplified their body-part relationship graph for faster inference in single-frame model and formulated articulated human tracking as spatial-temporal grouping of part proposals.</li>
<li>[49] detect individual key-points and predict their relative displacements,using greedy decoding method.</li>
</ul>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191111163031878.png" alt=""></p>
</li>
</ul>
<h4 id="Methods-14"><a href="#Methods-14" class="headerlink" title="Methods"></a>Methods</h4><p><strong>system overview</strong>: given a color image of size w*h ,produces the 2D positions of anatomical key-points for each person in the image.</p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191111163106168.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191111182334970.png" alt=""></p>
<p>【Question1】how to detect limbs and bodypart?</p>
<p>define <font color="red"> body part location S</font>:<br>$$<br>S=(S_1,S_2,…,S_j)  ,S_j\varepsilon R^{w<em>h</em>2}<br>$$<br>define <font color="red">vetor field of PAFS L</font>:<br>$$<br>L=(L_1,L_2,…,L_j) , L_c represent a limb,L_C\varepsilon R^{w<em>h</em>2}<br>$$</p>
<p>$$<br>Image-&gt;\frac{VGG-19}{CNN}-&gt;asetofeature mapsF\frac{stage\quad t,(t&lt;T_p)}{limbs}&gt;\begin{cases} L^1=\phi^1(F),t=1\ L^t=\phi ^t(F,L^{t-1}),\forall 2\leq t\leq T_p \end{cases}\quad \frac{}{bodyPartLocation}&gt;\begin{cases} S^T_p=\rho(F,L^T_p),\forall t\varepsilon T_p\ S^t=\rho(F,L^{T_p},S^{t-1}),\forall T_p\le t\leq T_p+c \end{cases}<br>$$</p>
<p><font color="red">define Loss functions</font>:  w is a binary mask with  W(p)=0 when annotation is missing.<br>$$<br>L_{stage}: f_L^{t_i}=\sum_{c=1}^{C}\sum_Pw(p)<em>||L_c^{ti}(p)-L_c^</em>(p)||^2 \<br>S_{stage}: f_S^{t_i}=\sum_{j=1}^{J}\sum_Pw(p)<em>||S_j^{ti}(p)-S_j^</em>(p)||^2<br>$$<br><font color="red">For vanishing gradient </font>:<br>$$<br>f=\sum_{i=1}^{T_p}f_L^t+\sum_{t=T_p+1}^{T_p+T_c}f_S^t<br>$$<br>confidence map: maximum   ,belief a particularly body part can be located at any given pixel</p>
<p>obtain body part conditions: Non-maximum suppression</p>
<p><font color="red">confidence mpas:</font>  $S_{j,k}^* $   :  individual maps for each person k ,  $X_{j,k}\epsilon R^2$ ground truth,body part j for person k</p>
<p>for calculate the confidence body part: the value at location $p\epsilon R^2 \quad S_{j,k}^<em>(p)=exp(-\frac{||p-x_{j,k}||^2}{\rho^2})$  ,$S_j^</em>(p)=max_kS_{j,k}^*(p)$</p>
<p>【Question2】Given a set of detected body parts ,how do we assemble them to form the full-body poses o fan unknown number of people？<font color="red"> ecodes both the location and orientation, don’t reduce the region of support of a limbs to a single point</font></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191129113237565.png" alt=""></p>
<p>$X_{j_1,k} ,X_{j_2,k}$  : the groundtruth positions of body parts j1,j2 from the limb c.<br>$$<br>L_{c,k}^<em>(p)=\begin{cases} v \quad if \quad p \quad on \quad limb c,k \0 \quad otherwise\end{cases}, \frac {v=(x_{j2,k}-x_{j1,k})/||x_{j_2,k}-x_{j_1,k}||^2}{0\leq v</em>(p-x_{j_1,k})\leq l_{c,k} and |v\bot <em>(p-x_{j_1,k)})|\leq \rho_l}<br>$$<br>ground-truth part affinity field : $L_c^</em>(p)=\frac{1}{n_c(p)}\sum_kL_{c,k}^*(p)\quad n_c(p)$:is the number of non-zero vectors at point p across all k people.</p>
<p>eg: for two candidate part locations $d_{j_1}\quad d_{j2}$,we sample the predicted part affinity field, Lc alone the line segment to measure the confidence in their association:$E=\int_{u=0}^{u=1}{L_c(p(u))*\frac {d_{j2}-d{j1}}{||d_j2-d_j1||^2}du}dx$     where p(u) interpolates the position of two body parts dj1,dj2 ,$p(u)=(1-u)d_{j1}+ud_{j2}$</p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191125091725286-1577068664572.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191129115349521.png" alt=""></p>
<p>a set of body part detection candidates Dj for multiple people,where $D_j={d_j^m :for\quad j\epsilon {1…J},m\epsilon {1,…,N_j}} $ where Nj is the number of candidates of part j, and $d_j^m\epsilon R^2$ is the location of m-th detection candidate of body part </p>
<p>define $z_{j_1,j_2}^{mn}\epsilon (0,1)$ to indicate wether two detection candidates $d_{j_1}^m ,d_{j_2}^n $ are connected ,the goal is to find optimal assignment for the rest set of all possible connections ,<br>$$<br>Z={z_{j_1j_2}^{mn}:for \quad j_1,j_2\epsilon (1,…,J),m\epsilon(1,…,N_{j_1}),n\epsilon(1,…,N_{j_2})}​<br>$$<br><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191129121106421.png" alt=""></p>
<ol>
<li>choose the minimal number of edges to obtain a spanning tree skeleton of human pose rather than using the complete graph</li>
<li>decompose the matching problem into a set of bipartite matching subproblems and determine the matching in adjacent tree nodes independently.</li>
</ol>
<h4 id="Evaluation-9"><a href="#Evaluation-9" class="headerlink" title="Evaluation"></a>Evaluation</h4><ul>
<li><strong>Environment</strong>: <ul>
<li>datasets : MPII human multi-person dataset[66] consisting of 3844 training and 1758 testing groups of multiple interacting individuals in highly articulated pose with 14 body parts, COCO keypoint challenge dataset  requires simultaneously detecting people and localizing 17 key-points</li>
</ul>
</li>
</ul>
<h4 id="Conclusion-10"><a href="#Conclusion-10" class="headerlink" title="Conclusion"></a>Conclusion</h4><ul>
<li>present an explicit nonparametric representation of the key-point association that encodes <font color="red">both position and orientation of human limbs</font></li>
<li>design an architecture that jointly <font color="red">learns part detection and association </font></li>
<li>demonstrate that <font color="red"> a greedy parsing algorithm is sufficient to produce high-quality parses of body poses and preserves efficiency regardless of the number of people </font></li>
<li>prove that PAF and body part location refinement is far more important that combined PAF and body part location refinement </li>
<li>combining body and foot estimation into a single model boosts thee accuracy of each component individually and reduces the inference time of running them sequentially</li>
<li>open-sourced OpenPose system and included in OpenCV library.</li>
</ul>
<h4 id="Notes-去加强了解-记录下以下论文"><a href="#Notes-去加强了解-记录下以下论文" class="headerlink" title="Notes   去加强了解  记录下以下论文"></a>Notes   <font color="orange">去加强了解</font>  记录下以下论文</h4><ul>
<li><input disabled="" type="checkbox"> 学习何使用OpenPose system  ,进行到一般,模型文件需要下载</li>
<li><input checked="" disabled="" type="checkbox"> [20] Convolutional pose machines,   DenseNet[52]Densely connected convolutional networks, [3] Realtime multi-person 2d pose estimation using part affinity fields,  <font color="orange"> 大致了解了网络,需要几个代码了解下运行效果</font></li>
<li><input checked="" disabled="" type="checkbox"> 学习和使用 Mask R-CNN[5]  <font color="orange">need to practice with code</font></li>
<li><input checked="" disabled="" type="checkbox"> 学习和使用Alpha-Pose[6]</li>
<li><input checked="" disabled="" type="checkbox"> ResNet[46] 学习使用    <font color="orange">need to practice with code</font></li>
<li><input disabled="" type="checkbox"> [2] pairwise representations   了解下是什么</li>
<li><input disabled="" type="checkbox"> 论文34 ,47,49 ,[50]需要学习了解</li>
<li><input disabled="" type="checkbox"> [49] detect individual key-points and predict their relative displacement allowing a greedy decoding process to group keypoints.</li>
<li><input disabled="" type="checkbox"> 学习和了解VGG-19【53】模型并学会使用fine-tuned方法</li>
<li><input checked="" disabled="" type="checkbox"> Hungrian ALgorithm 用于解决二分图匹配问题</li>
</ul>
<p>level**: CCF_A   CVPR</p>
<p><strong>author</strong>: ChaoLi,QiaoyongZhong,DiXie,ShiliangPu HikvisionResearchInstitute </p>
<p><strong>date</strong>: 2018 4.17</p>
<p><strong>keyword</strong>:</p>
<ul>
<li>skeleton based action recognition</li>
</ul>
<hr>
<h2 id="Paper-Co-occurence-Feature"><a href="#Paper-Co-occurence-Feature" class="headerlink" title="Paper: Co-occurence Feature"></a>Paper: Co-occurence Feature</h2><div align="center">
<br>
<b>Co-occurrence Feature Learning fromS keletonData for ActionRecognition and  Detection with Hierarchical Aggregation
</b>
</div>
#### Summary

<ol>
<li>point-level information of each joint is encoded independently,and then assembled into semantic representation  in both spatial and temporal domains</li>
<li>independent point-level features learning  and cross joint co-occurrence feature learning</li>
</ol>
<h4 id="Research-Objective-13"><a href="#Research-Objective-13" class="headerlink" title="Research Objective"></a>Research Objective</h4><ul>
<li><strong>Application Area</strong>:  intelligent surveillance system, human-computer interaction, game-control and robotics.<ul>
<li>skeleton provides good representation for describing human actions</li>
<li>skeleton data are inherently robust against background noise and provide abstract information and high-level features of human action</li>
<li>compared with RGB data, skeleton data are extremely small in size</li>
</ul>
</li>
<li><strong>Purpose</strong>:  </li>
</ul>
<h4 id="Proble-Statement-13"><a href="#Proble-Statement-13" class="headerlink" title="Proble Statement"></a>Proble Statement</h4><p>previous work:</p>
<ul>
<li>design and extract co-occurrence features form skeleton sequences<ul>
<li>pairwise relative position of each joint</li>
<li>spatial orientation of pair wise joints</li>
<li>statistics-based feature like Conv3Dj   HOJ3D</li>
</ul>
</li>
<li>RNN with LSTM to model the time series of skeleton prevalently</li>
<li>CNN models to learn spatial-temporal features from skeletons <ul>
<li>cast the frame, joint, and coordinate dimensions of skeleton sequence into width, height, and channel of an image respectively  [Du et al., 2016]</li>
<li>3D coordinates are separated into three gray-scale images [Ke et al.,2017]</li>
<li>a new skeleton transformer module to incorporate skeleton motion features [Li et al., 2017b]</li>
<li>Shortcoming:   the co-occurrence feature are aggregated locally, which may not be able to capture the long-range joint interactions involved in actions like wearing ..</li>
</ul>
</li>
</ul>
<h4 id="Methods-15"><a href="#Methods-15" class="headerlink" title="Methods"></a>Methods</h4><ul>
<li><strong>system overview</strong>:</li>
</ul>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200229112040352.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200229115749562.png" alt=""></p>
<p><strong>[Input Define]</strong></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200229114736296.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200229114716246.png" alt=""></p>
<p><strong>[Multiple Persons]</strong></p>
<ul>
<li>early fusion: all joints from multiple persons are stacked as input of the network, zero padding if less than pre-defined maximal number</li>
<li>Late fusion:  input of multiple persons go through the same subnetwork and their conv6 features maps are merged with either concatenation along channels or element-wise maximum/mean operation</li>
</ul>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200229114756753.png" alt=""></p>
<p><strong>[Loss Function Define]</strong></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200229115411034.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200229115422855.png" alt=""></p>
<h4 id="Evaluation-10"><a href="#Evaluation-10" class="headerlink" title="Evaluation"></a>Evaluation</h4><ul>
<li><strong>Environment</strong>:   <ul>
<li>Dataset: </li>
</ul>
</li>
<li><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200229115851023.png" alt=""></li>
<li><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200229115907932.png" alt=""></li>
</ul>
<h4 id="Conclusion-11"><a href="#Conclusion-11" class="headerlink" title="Conclusion"></a>Conclusion</h4><ul>
<li>CNN model for learning global co-occurrences from skeleton data</li>
<li>end-to-end hierarchical feature learning network, where features are aggregated gradually from point level features to global co-occurrence features</li>
<li>exploit multi-person feature fusion strategies </li>
</ul>
<h4 id="Notes-去加强了解-11"><a href="#Notes-去加强了解-11" class="headerlink" title="Notes 去加强了解"></a>Notes <font color="orange">去加强了解</font></h4><ul>
<li><input disabled="" type="checkbox"> recognition and detection benchmarks <font color="red">NTU RGB+D,SBU kinect Interaction and PKU-MMD</font></li>
<li><input disabled="" type="checkbox"> Learning actionlet ensemble for 3d human actionrecognition  2014</li>
<li><input disabled="" type="checkbox"> Essential body-joint and atomic action detection for human activity recognition using longest common subsequence algorithm   2012</li>
<li><input disabled="" type="checkbox"> A NewRepresentationofSkeletonSequencesfor3DAction Recognition   2017</li>
<li><input disabled="" type="checkbox"> Co-occurrence feature learning for skeleton based action recognition using regularized deep lstm networks   2016</li>
<li><input disabled="" type="checkbox"> PKU-MMD: A large scale benchmark for continuous multi-modal human action understanding.  2017</li>
<li><input disabled="" type="checkbox"> Two-stream convolutional networks for action recognition in videos   2014</li>
<li><input disabled="" type="checkbox"> window regression   Girshick et al., 2014 </li>
<li><input disabled="" type="checkbox"> Cascade region proposal and global context for deep object detection  2016 </li>
<li><input disabled="" type="checkbox"> An end-to-end spatiotemporal attention model for human action recognition fromskeletondata   2017 AAAi</li>
</ul>
<p><strong>level</strong>:<br><strong>author</strong>:<br><strong>date</strong>:  2016<br><strong>keyword</strong>:</p>
<ul>
<li>point detection; heatmap</li>
</ul>
<hr>
<h1 id="Paper-Stacked-Hourglass"><a href="#Paper-Stacked-Hourglass" class="headerlink" title="Paper: Stacked Hourglass"></a>Paper: Stacked Hourglass</h1><div align="center">
<br>
<b>Stacked Hourglass Networks for
Human Pose Estimation</b>
</div>


<h4 id="Summary-10"><a href="#Summary-10" class="headerlink" title="Summary"></a>Summary</h4><ol>
<li>On MPII there is over a 2% average accuracy improvement across all joints, with as much as a 4-5% improvement on more difficult joints like the knees and ankles.</li>
<li>propose  a stacked hourglass networks for human pose estimation;</li>
</ol>
<h4 id="Methods-16"><a href="#Methods-16" class="headerlink" title="Methods"></a>Methods</h4><ul>
<li><strong>system overview</strong>:</li>
</ul>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201019210140895.png" alt=""></p>
<p>【*<em>Single Hourglass module *</em>】</p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201019210700629.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201019210520022.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201020134658441.png" alt=""></p>
<pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true">#coding=utf-8</span>
<span class="token keyword">import</span> torch
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn
<span class="token keyword">from</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">import</span> Upsample
<span class="token keyword">from</span> torch<span class="token punctuation">.</span>autograd <span class="token keyword">import</span> Variable
<span class="token comment" spellcheck="true">#https://sourcegraph.com/github.com/raymon-tian/hourglass-facekeypoints-detection/-/blob/models.py</span>
<span class="token keyword">class</span> <span class="token class-name">HourGlass</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""不改变特征图的高宽"""</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>n<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">,</span>f<span class="token operator">=</span><span class="token number">128</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""
        :param n: hourglass模块的层级数目
        :param f: hourglass模块中的特征图数量
        :return:
        """</span>
        super<span class="token punctuation">(</span>HourGlass<span class="token punctuation">,</span>self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>_n <span class="token operator">=</span> n
        self<span class="token punctuation">.</span>_f <span class="token operator">=</span> f
        self<span class="token punctuation">.</span>_init_layers<span class="token punctuation">(</span>self<span class="token punctuation">.</span>_n<span class="token punctuation">,</span>self<span class="token punctuation">.</span>_f<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">_init_layers</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>n<span class="token punctuation">,</span>f<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment" spellcheck="true"># 上分支</span>
        setattr<span class="token punctuation">(</span>self<span class="token punctuation">,</span><span class="token string">'res'</span><span class="token operator">+</span>str<span class="token punctuation">(</span>n<span class="token punctuation">)</span><span class="token operator">+</span><span class="token string">'_1'</span><span class="token punctuation">,</span>Residual<span class="token punctuation">(</span>f<span class="token punctuation">,</span>f<span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token comment" spellcheck="true"># 下分支</span>
        setattr<span class="token punctuation">(</span>self<span class="token punctuation">,</span><span class="token string">'pool'</span><span class="token operator">+</span>str<span class="token punctuation">(</span>n<span class="token punctuation">)</span><span class="token operator">+</span><span class="token string">'_1'</span><span class="token punctuation">,</span>nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        setattr<span class="token punctuation">(</span>self<span class="token punctuation">,</span><span class="token string">'res'</span><span class="token operator">+</span>str<span class="token punctuation">(</span>n<span class="token punctuation">)</span><span class="token operator">+</span><span class="token string">'_2'</span><span class="token punctuation">,</span>Residual<span class="token punctuation">(</span>f<span class="token punctuation">,</span>f<span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">if</span> n <span class="token operator">></span> <span class="token number">1</span><span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>_init_layers<span class="token punctuation">(</span>n<span class="token number">-1</span><span class="token punctuation">,</span>f<span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>res_center <span class="token operator">=</span> Residual<span class="token punctuation">(</span>f<span class="token punctuation">,</span>f<span class="token punctuation">)</span>
        setattr<span class="token punctuation">(</span>self<span class="token punctuation">,</span><span class="token string">'res'</span><span class="token operator">+</span>str<span class="token punctuation">(</span>n<span class="token punctuation">)</span><span class="token operator">+</span><span class="token string">'_3'</span><span class="token punctuation">,</span>Residual<span class="token punctuation">(</span>f<span class="token punctuation">,</span>f<span class="token punctuation">)</span><span class="token punctuation">)</span>
        setattr<span class="token punctuation">(</span>self<span class="token punctuation">,</span><span class="token string">'unsample'</span><span class="token operator">+</span>str<span class="token punctuation">(</span>n<span class="token punctuation">)</span><span class="token punctuation">,</span>Upsample<span class="token punctuation">(</span>scale_factor<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>


    <span class="token keyword">def</span> <span class="token function">_forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>x<span class="token punctuation">,</span>n<span class="token punctuation">,</span>f<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment" spellcheck="true"># 上分支</span>
        up1 <span class="token operator">=</span> x
        up1 <span class="token operator">=</span> eval<span class="token punctuation">(</span><span class="token string">'self.res'</span><span class="token operator">+</span>str<span class="token punctuation">(</span>n<span class="token punctuation">)</span><span class="token operator">+</span><span class="token string">'_1'</span><span class="token punctuation">)</span><span class="token punctuation">(</span>up1<span class="token punctuation">)</span>
        <span class="token comment" spellcheck="true"># 下分支</span>
        low1 <span class="token operator">=</span> eval<span class="token punctuation">(</span><span class="token string">'self.pool'</span><span class="token operator">+</span>str<span class="token punctuation">(</span>n<span class="token punctuation">)</span><span class="token operator">+</span><span class="token string">'_1'</span><span class="token punctuation">)</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        low1 <span class="token operator">=</span> eval<span class="token punctuation">(</span><span class="token string">'self.res'</span><span class="token operator">+</span>str<span class="token punctuation">(</span>n<span class="token punctuation">)</span><span class="token operator">+</span><span class="token string">'_2'</span><span class="token punctuation">)</span><span class="token punctuation">(</span>low1<span class="token punctuation">)</span>
        <span class="token keyword">if</span> n <span class="token operator">></span> <span class="token number">1</span><span class="token punctuation">:</span>
            low2 <span class="token operator">=</span> self<span class="token punctuation">.</span>_forward<span class="token punctuation">(</span>low1<span class="token punctuation">,</span>n<span class="token number">-1</span><span class="token punctuation">,</span>f<span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            low2 <span class="token operator">=</span> self<span class="token punctuation">.</span>res_center<span class="token punctuation">(</span>low1<span class="token punctuation">)</span>
        low3 <span class="token operator">=</span> low2
        low3 <span class="token operator">=</span> eval<span class="token punctuation">(</span><span class="token string">'self.'</span><span class="token operator">+</span><span class="token string">'res'</span><span class="token operator">+</span>str<span class="token punctuation">(</span>n<span class="token punctuation">)</span><span class="token operator">+</span><span class="token string">'_3'</span><span class="token punctuation">)</span><span class="token punctuation">(</span>low3<span class="token punctuation">)</span>
        up2 <span class="token operator">=</span> eval<span class="token punctuation">(</span><span class="token string">'self.'</span><span class="token operator">+</span><span class="token string">'unsample'</span><span class="token operator">+</span>str<span class="token punctuation">(</span>n<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>forward<span class="token punctuation">(</span>low3<span class="token punctuation">)</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span>up1<span class="token punctuation">.</span>shape<span class="token punctuation">,</span>up2<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
        <span class="token keyword">return</span> up1<span class="token operator">+</span>up2

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>_forward<span class="token punctuation">(</span>x<span class="token punctuation">,</span>self<span class="token punctuation">.</span>_n<span class="token punctuation">,</span>self<span class="token punctuation">.</span>_f<span class="token punctuation">)</span>

<span class="token keyword">class</span> <span class="token class-name">Residual</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""
    残差模块，并不改变特征图的宽高
    """</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>ins<span class="token punctuation">,</span>outs<span class="token punctuation">)</span><span class="token punctuation">:</span>
        super<span class="token punctuation">(</span>Residual<span class="token punctuation">,</span>self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token comment" spellcheck="true"># 卷积模块</span>
        self<span class="token punctuation">.</span>convBlock <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>
            nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span>ins<span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span>inplace<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>ins<span class="token punctuation">,</span>outs<span class="token operator">//</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span>outs<span class="token operator">//</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span>inplace<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>outs<span class="token operator">//</span><span class="token number">2</span><span class="token punctuation">,</span>outs<span class="token operator">//</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span>outs<span class="token operator">//</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span>inplace<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>outs<span class="token operator">//</span><span class="token number">2</span><span class="token punctuation">,</span>outs<span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span>
        <span class="token punctuation">)</span>
        <span class="token comment" spellcheck="true"># 跳层</span>
        <span class="token keyword">if</span> ins <span class="token operator">!=</span> outs<span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>skipConv <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>ins<span class="token punctuation">,</span>outs<span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>ins <span class="token operator">=</span> ins
        self<span class="token punctuation">.</span>outs <span class="token operator">=</span> outs
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        residual <span class="token operator">=</span> x
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>convBlock<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>ins <span class="token operator">!=</span> self<span class="token punctuation">.</span>outs<span class="token punctuation">:</span>
            residual <span class="token operator">=</span> self<span class="token punctuation">.</span>skipConv<span class="token punctuation">(</span>residual<span class="token punctuation">)</span>
        x <span class="token operator">+=</span> residual
        <span class="token keyword">return</span> x

<span class="token keyword">class</span> <span class="token class-name">Lin</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>numIn<span class="token operator">=</span><span class="token number">128</span><span class="token punctuation">,</span>numout<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        super<span class="token punctuation">(</span>Lin<span class="token punctuation">,</span>self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>conv <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>numIn<span class="token punctuation">,</span>numout<span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>bn <span class="token operator">=</span> nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span>numout<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>relu <span class="token operator">=</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span>inplace<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>bn<span class="token punctuation">(</span>self<span class="token punctuation">.</span>conv<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>


<span class="token keyword">class</span> <span class="token class-name">KFSGNet</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>

    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        super<span class="token punctuation">(</span>KFSGNet<span class="token punctuation">,</span>self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>__conv1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">64</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>__relu1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span>inplace<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>__conv2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">,</span><span class="token number">128</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>__relu2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span>inplace<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>__hg <span class="token operator">=</span> HourGlass<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>__lin <span class="token operator">=</span> Lin<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>__relu1<span class="token punctuation">(</span>self<span class="token punctuation">.</span>__conv1<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>__relu2<span class="token punctuation">(</span>self<span class="token punctuation">.</span>__conv2<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>__hg<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>__lin<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        <span class="token keyword">return</span> x


<span class="token keyword">from</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data <span class="token keyword">import</span> Dataset<span class="token punctuation">,</span>DataLoader
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>optim <span class="token keyword">as</span> optim

<span class="token keyword">class</span> <span class="token class-name">tempDataset</span><span class="token punctuation">(</span>Dataset<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>X <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">16</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">512</span><span class="token punctuation">,</span> <span class="token number">512</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>Y <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">16</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">,</span><span class="token number">512</span><span class="token punctuation">,</span> <span class="token number">512</span><span class="token punctuation">)</span>
    <span class="token keyword">def</span> <span class="token function">__len__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> len<span class="token punctuation">(</span>self<span class="token punctuation">.</span>X<span class="token punctuation">)</span>
    <span class="token keyword">def</span> <span class="token function">__getitem__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> item<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment" spellcheck="true"># 这里返回的时候不要设置batch_size</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>X<span class="token punctuation">[</span>item<span class="token punctuation">]</span><span class="token punctuation">,</span>self<span class="token punctuation">.</span>Y<span class="token punctuation">[</span>item<span class="token punctuation">]</span>

<span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">'__main__'</span><span class="token punctuation">:</span>
    <span class="token keyword">from</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">import</span> MSELoss
    critical <span class="token operator">=</span> MSELoss<span class="token punctuation">(</span><span class="token punctuation">)</span>

    dataset <span class="token operator">=</span> tempDataset<span class="token punctuation">(</span><span class="token punctuation">)</span>
    dataLoader <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span>dataset<span class="token operator">=</span>dataset<span class="token punctuation">,</span>batch_size<span class="token operator">=</span><span class="token number">64</span><span class="token punctuation">)</span>
    shg <span class="token operator">=</span> KFSGNet<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span>
    optimizer <span class="token operator">=</span> optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>shg<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.001</span><span class="token punctuation">,</span> momentum<span class="token operator">=</span><span class="token number">0.9</span><span class="token punctuation">,</span>weight_decay<span class="token operator">=</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">4</span><span class="token punctuation">)</span>

    <span class="token keyword">for</span> e <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">200</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">for</span> i<span class="token punctuation">,</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span>y<span class="token punctuation">)</span> <span class="token keyword">in</span> enumerate<span class="token punctuation">(</span>dataLoader<span class="token punctuation">)</span><span class="token punctuation">:</span>
            x <span class="token operator">=</span> Variable<span class="token punctuation">(</span>x<span class="token punctuation">,</span>requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">.</span>float<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span>
            y <span class="token operator">=</span> Variable<span class="token punctuation">(</span>y<span class="token punctuation">)</span><span class="token punctuation">.</span>float<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span>
            y_pred <span class="token operator">=</span> shg<span class="token punctuation">.</span>forward<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
            <span class="token comment" spellcheck="true">#print(y_pred.shape,y.shape)</span>
            loss <span class="token operator">=</span> critical<span class="token punctuation">(</span>y_pred<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>y<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
            <span class="token comment" spellcheck="true">#print('loss : {}'.format(loss.data))</span>
            optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
            loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
            optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
            <span class="token keyword">if</span> i<span class="token operator">></span><span class="token number">2</span><span class="token punctuation">:</span>
                <span class="token keyword">break</span>
        <span class="token keyword">break</span></code></pre>
<p><strong>level</strong>: CVPR<br><strong>author</strong>: Shih-En Wei<br><strong>date</strong>:<br><strong>keyword</strong>:</p>
<ul>
<li>skeleton extract</li>
</ul>
<h2 id="Paper-CPM"><a href="#Paper-CPM" class="headerlink" title="Paper: CPM"></a>Paper: CPM</h2><div align="center">
<br>
<b>Convolutional Pose Machine</b>
</div>


<h4 id="Summary-11"><a href="#Summary-11" class="headerlink" title="Summary"></a>Summary</h4><ol>
<li>show a systematic design for how convolutional networks can be incorporated into the pose machine frame-work for learning image features and image-dependent spatial models for the task of pose estimation.</li>
<li><font color="red">CPM: consists of a sequence of convolutional networks that repeatly produce 2D belief maps for the location of each part, at each stage in a CPM, image features and the belief maps produced by the previous stage are used as input</font><ol>
<li>learn feature representations for both image and spatial context directly from data</li>
<li>a different architecture that allows for globally joint trainning with back propagation</li>
<li>efficiently handle large training datasets</li>
</ol>
</li>
<li><font color="red">large receptive fields on the belief maps are crucial for learning long range spatial relationships and result in improved accuracy.</font></li>
</ol>
<h4 id="Research-Object"><a href="#Research-Object" class="headerlink" title="Research Object"></a>Research Object</h4><p>previous work:</p>
<ul>
<li><p>classic approach:</p>
<ul>
<li><font color="red">pictorial structures:</font>  spatial correlations between parts of the body are expressed as a tree-structured graphical model with kinectmatic priors that couple connected limbs.</li>
<li><font color="red">Hierarchical models:</font> represent relationships between parts at different scales and size in a hierarchical tree structure.</li>
<li><font color="red">Non-tree models:</font> incorporate interactions that introduce loops to augment the tree structure with additional edges that capture symmetry, occlusion and long-range relationship.<font color="red">rely on approximate inference</font></li>
<li><font color="red">sequential prediction:</font> learn an implicit model with potentially complex interactions between variables by directly training an inference procedure.</li>
</ul>
</li>
</ul>
<h4 id="Methods-17"><a href="#Methods-17" class="headerlink" title="Methods"></a>Methods</h4><ul>
<li><strong>system overview</strong>:</li>
</ul>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200309105620041.png" alt=""></p>
<p>【<strong>Pose Machines</strong>】</p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200309105711936.png" alt="image-20200309105711936"></p>
<p><img src="../../../../MEGA/MEGAsync/actionPrediction/skeleton.assets/image-20200309105732538.png" alt="image-20200309105732538"></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200309105745261.png" alt="image-20200309105745261"></p>
<p><strong>【Convolutional Pose Machines】</strong></p>
<ul>
<li>Keypoint Localization Using Local Image Evidence:</li>
</ul>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200309105837745.png" alt=""></p>
<ul>
<li>Sequential Prediction with Learned Spatial Context Features:</li>
</ul>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200309105933784.png" alt=""></p>
<ul>
<li>Learning in Convolutional Pose Machines</li>
</ul>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200309110012428.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200309110025606.png" alt=""></p>
<h4 id="Conclusion-12"><a href="#Conclusion-12" class="headerlink" title="Conclusion"></a>Conclusion</h4><ul>
<li>learning implicit spatial models via a sequential composition of convolutional architectures</li>
<li>a systematic approach to designing and training such an architecture to learn both image features and image-dependent spatial models for structured presiction tasks, without the need for any graphical model style inference.</li>
</ul>
<h4 id="Notes-去加强了解-12"><a href="#Notes-去加强了解-12" class="headerlink" title="Notes 去加强了解"></a>Notes <font color="orange">去加强了解</font></h4><ul>
<li>code available: <a href="https://github.com/CMU-Perceptual-Computing-Lab/convolutional-pose-machines-release" target="_blank" rel="noopener">https://github.com/CMU-Perceptual-Computing-Lab/convolutional-pose-machines-release</a></li>
</ul>
<p><strong>level</strong>:  CVPR  CCF_A<br><strong>author</strong>: Charles R.Qi  Stanford University<br><strong>date</strong>:<br><strong>keyword</strong>:</p>
<ul>
<li>3D object detection, Point Cloud</li>
</ul>
<hr>
<h2 id="Paper-Frustum-PointNets"><a href="#Paper-Frustum-PointNets" class="headerlink" title="Paper: Frustum PointNets"></a>Paper: Frustum PointNets</h2><div align="center">
<br>
<b>Frustum PointNets for 3D Object Detection from RGB-D Data
</b>
</div>



<h4 id="Summary-12"><a href="#Summary-12" class="headerlink" title="Summary"></a>Summary</h4><ol>
<li>3D sensor data is often in the form of point clouds.</li>
</ol>
<h4 id="Research-Objective-14"><a href="#Research-Objective-14" class="headerlink" title="Research Objective"></a>Research Objective</h4><ul>
<li><strong>Application Area</strong>: autonomous driving, augmented reality</li>
<li><strong>Purpose</strong>:  how to effective localize objects in point clouds of large-scale scenes.</li>
</ul>
<h4 id="Proble-Statement-14"><a href="#Proble-Statement-14" class="headerlink" title="Proble Statement"></a>Proble Statement</h4><ul>
<li>study 3D object detection from RGB-D data in both indoor and outdoor scenes.</li>
<li>previous work focus on images or 3D voxels, often obscuring natural 3D patterns and invariances of 3D voxels.</li>
</ul>
<p>previous work:</p>
<ul>
<li>object detection and instance segmentation based on 2D image.</li>
<li>most existing works convert 3D point clouds to images by projection, or to volumetric grids by quantization and then apply convolutional networks.</li>
<li>3D object detection from RGB-D data<ul>
<li>front view image based methods:<ul>
<li>take monocular RGB images and shape priors or occlusion patterns to infer 3D bounding boxes</li>
<li>represent depth data as 2D maps.</li>
</ul>
</li>
<li>Bird’s eye view based methods:<ul>
<li>projects Li-DAR point cloud to bird’s eye view and trains RPN</li>
</ul>
</li>
<li>3D based methods:<ul>
<li>train 3D object classifiers by SVMs on hand-designed geometry features extracted from point cloud and localize objects using window search,</li>
</ul>
</li>
</ul>
</li>
<li>Deep Learning on Point Clouds:<ul>
<li>convert point clouds to images or volumetric forms before feature learning</li>
<li><font color="red">require quantitization of point clouds with certain voxel resolution</font></li>
</ul>
</li>
</ul>
<h4 id="Methods-18"><a href="#Methods-18" class="headerlink" title="Methods"></a>Methods</h4><ul>
<li><strong>Problem Formulation</strong>:</li>
</ul>
<p>input RGB-D data, classify and localize objects in 3D space.</p>
<ul>
<li><strong>system overview</strong>:</li>
</ul>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200307115907354.png" alt=""></p>
<p><strong>【Model 1】Frustum Proposal</strong></p>
<ul>
<li>with known camera projection matrix, a 2D bounding box can be lifted to a frustum(with near and far planes specified by depth sensor range) that defines a 3D search space for the object.     </li>
<li>using FPN ,pre-train the model weights on ImageNet classification and COCO object detection datasets and further fine-tune it on KITTI 2D object detection to classify and predict amodal 2D boxes.</li>
</ul>
<p><strong>【Model 2】3D Instance Segmentation</strong></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200307120512402.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200307120608061.png" alt=""></p>
<p><strong>【Model 3】3D Instance Segmentation PointNet</strong></p>
<ul>
<li>Similar to the case in 2D instance segmentation, depending on the position of the frustum, object points in on frustum may become cluttered or occlude points in another.</li>
<li>transform the point cloud into a local coordinate by subtracting XYZ values by its centroid. considering the bounding sphere size of a partial point cloud can be greatly affected by viewpoints and the real size of the point clouds helps the box size estimation.</li>
</ul>
<p><strong>【Model 4】Amodal 3D Box Estimation</strong></p>
<ul>
<li>learning-based 3D alignment by T-Net</li>
<li>amodal 3D box estimation pointnet</li>
</ul>
<h4 id="Evaluation-11"><a href="#Evaluation-11" class="headerlink" title="Evaluation"></a>Evaluation</h4><ul>
<li><strong>Environment</strong>:   <ul>
<li>Dataset: KITTI  , </li>
</ul>
</li>
</ul>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200307121349449.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200307121605990.png" alt=""></p>
<h4 id="Conclusion-13"><a href="#Conclusion-13" class="headerlink" title="Conclusion"></a>Conclusion</h4><ul>
<li>propose a novel framework Frustum PointNets for RGB-D data based 3D object detection </li>
<li>provide extensive quantitative evaluations to validate the design.</li>
</ul>
<h4 id="Notes-去加强了解-13"><a href="#Notes-去加强了解-13" class="headerlink" title="Notes 去加强了解"></a>Notes <font color="orange">去加强了解</font></h4><ul>
<li>KITTI 3D object detection:  <a href="http://www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmark=3d" target="_blank" rel="noopener">http://www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmark=3d</a>   contain datasets, many methods and compare.<font color="red">如果后面学习和使用到3D object detection 可以从这里学习</font></li>
<li>bird’s eye view detection benchmarks</li>
</ul>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200307122059591.png" alt=""></p>
<p><strong>level</strong>: ICCV   CCF_B<br><strong>author</strong>:         National Institute of Advanced Industrial Science and Technology<br><strong>date</strong>: 2017<br><strong>keyword</strong>:</p>
<ul>
<li>Spatio-Temporal ,action recognision</li>
</ul>
<hr>
<h2 id="Paper-3D-Resnet"><a href="#Paper-3D-Resnet" class="headerlink" title="Paper: 3D-Resnet"></a>Paper: 3D-Resnet</h2><div align="center">
<br>
<b>Learning Spatio-Temporal Features with 3D Residual Networks
for Action Recognition</b>
</div>


<h4 id="Summary-13"><a href="#Summary-13" class="headerlink" title="Summary"></a>Summary</h4><ol>
<li>exploring the effectiveness of ResNets with 3D convolutional kernels </li>
<li>学会使用作为基本的视频特征提取方法</li>
</ol>
<h4 id="Research-Objective-15"><a href="#Research-Objective-15" class="headerlink" title="Research Objective"></a>Research Objective</h4><ul>
<li><strong>Application Area</strong>: surveillance systems, video indexing, and human computer  interaction</li>
<li><strong>Purpose</strong>:  propose a 3D CNNs based on ResNets toward a better action representation</li>
</ul>
<h4 id="Proble-Statement-15"><a href="#Proble-Statement-15" class="headerlink" title="Proble Statement"></a>Proble Statement</h4><ul>
<li><p><strong>Action Recognition Database</strong>:</p>
<ul>
<li>HMDB51 [13]</li>
<li>UCF101 [16]</li>
<li>Kinetics human action video dataset [12]</li>
</ul>
</li>
<li><p>Residual block</p>
</li>
</ul>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191223101841019.png" alt=""></p>
<h4 id="Methods-19"><a href="#Methods-19" class="headerlink" title="Methods"></a>Methods</h4><ul>
<li><strong>network design</strong>:</li>
</ul>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191223101025170.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191223101347554.png" alt=""></p>
<h4 id="Notes-去加强了解-14"><a href="#Notes-去加强了解-14" class="headerlink" title="Notes 去加强了解"></a>Notes <font color="orange">去加强了解</font></h4><ul>
<li><input disabled="" type="checkbox"> code available: <a href="https://github.com/kenshohara/3D-ResNets" target="_blank" rel="noopener">https://github.com/kenshohara/3D-ResNets</a>.   需要做实验</li>
</ul>
<p><strong>level</strong>:  CVPR    CCF_A<br><strong>author</strong>: DUSHYANT<br><strong>date</strong>:  2017<br><strong>keyword</strong>:</p>
<ul>
<li>3D human pose estimation</li>
</ul>
<hr>
<h2 id="Paper-VNect"><a href="#Paper-VNect" class="headerlink" title="Paper: VNect"></a>Paper: VNect</h2><div align="center">
<br>
<b>Real-time 3D Human Pose Estimation with a Single RGB Camera</b>
</div>


<h4 id="Research-Objective-16"><a href="#Research-Objective-16" class="headerlink" title="Research Objective"></a>Research Objective</h4><ul>
<li><p><strong>Application Area</strong>:realtime motion-driven 3D game character control,self-immersion in 3D virtual and augmented reality and human-computer interaction.</p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191118101846038.png" alt=""></p>
</li>
<li><p><strong>Purpose</strong>:  <font color="red">stable 3D skeletal motion capture from a single camera in real-time</font></p>
</li>
</ul>
<h4 id="Proble-Statement-16"><a href="#Proble-Statement-16" class="headerlink" title="Proble Statement"></a>Proble Statement</h4><p>previous work:</p>
<ul>
<li>Multi-view: using multi-view setups markerless motion-capture solutions attain high accuracy. <font color="red">we combine discriminative pose estimation with kinematic fitting to succeed in our underconstrained setting</font></li>
<li>Monocular Depth-based: RGB-D sensors overcomes forward backward ambiguities in monocular pose estimation.</li>
<li>Monocular RGB:structure-from-motion techniques exploit motion cues in a batch of frames and also been applied to human motion estimation.</li>
<li>Given 2D joint locations ,existing approaches  use bone length and depth ordering constraints ,sparsity asumptions,joint limits,inter-penetration constraints,temporal dependencies and regression to create 3D  pose.<font color="red">sparse set of 2D locations losesimage evidence-&gt; discriminative methods</font></li>
<li>previous work only obtain temporally unstable coarse pose not directly usable in applications.</li>
</ul>
<h4 id="Methods-20"><a href="#Methods-20" class="headerlink" title="Methods"></a>Methods</h4><ul>
<li><strong>system overview</strong>:</li>
</ul>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191118103828661.png" alt=""></p>
<p>【Qustion 1】how to use CNN to regress Pose by single RGB image?</p>
<p>extending the 2D heatmap formulation to 3D using three additional location-maps(x,yz,) per joint j , captureing the root-relative locations (x,y,z) respectively.<img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191118110049931.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191121100234719.png" alt=""></p>
<h4 id="Contribution-1"><a href="#Contribution-1" class="headerlink" title="Contribution"></a>Contribution</h4><ul>
<li>the first real-time method to capture the full global 3D skeletal pose of a human in a stable ,temporally consistent manner using a single RGB camera.</li>
<li>novel fully convolutional pose formulation regresses 2D and 3D joint positions jointly in real time and doesn’t require tightly cropped input frames, and forgoes the need to perform expensive bounding box computations.</li>
<li>model-based kinematic skeleton fitting against the 2D/3D pose predictions to produce temporally stable joint angles of a metric global 3D skeleton in rea time.</li>
<li>more applicable for outdoor scenes ,community video and low quality commodity RGB cameras.</li>
</ul>
<h4 id="Notes-2"><a href="#Notes-2" class="headerlink" title="Notes"></a>Notes</h4><ul>
<li><input checked="" disabled="" type="checkbox"> <p>heatmap based bodyjoint detection formulation  ,heatmap is the distribution of confidence probability of body part. </p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191121094023973.png" alt=""></p>
</li>
</ul>
<h1 id=""><a href="#" class="headerlink" title=""></a></h1></font><script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script>
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        Author:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://liudongdong1.github.io" rel="external nofollow noreferrer">liudongdong1</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        Link:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://liudongdong1.github.io/2020/06/06/shi-jue-ai/video-understand/video-understand/">https://liudongdong1.github.io/2020/06/06/shi-jue-ai/video-understand/video-understand/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        Reprint policy:
                    </i>
                </span>
                <span class="reprint-info">
                    All articles in this blog are used except for special statements
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    reprint polocy. If reproduced, please indicate source
                    <a href="https://liudongdong1.github.io" target="_blank">liudongdong1</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>Copied successfully, please follow the reprint policy of this article</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">more</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/tags/VideoAnalyse/">
                                    <span class="chip bg-color">VideoAnalyse</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="qq,qzone,wechat,weibo,douban" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
                <style>
    #reward {
        margin: 40px 0;
        text-align: center;
    }

    #reward .reward-link {
        font-size: 1.4rem;
        line-height: 38px;
    }

    #reward .btn-floating:hover {
        box-shadow: 0 6px 12px rgba(0, 0, 0, 0.2), 0 5px 15px rgba(0, 0, 0, 0.2);
    }

    #rewardModal {
        width: 320px;
        height: 350px;
    }

    #rewardModal .reward-title {
        margin: 15px auto;
        padding-bottom: 5px;
    }

    #rewardModal .modal-content {
        padding: 10px;
    }

    #rewardModal .close {
        position: absolute;
        right: 15px;
        top: 15px;
        color: rgba(0, 0, 0, 0.5);
        font-size: 1.3rem;
        line-height: 20px;
        cursor: pointer;
    }

    #rewardModal .close:hover {
        color: #ef5350;
        transform: scale(1.3);
        -moz-transform:scale(1.3);
        -webkit-transform:scale(1.3);
        -o-transform:scale(1.3);
    }

    #rewardModal .reward-tabs {
        margin: 0 auto;
        width: 210px;
    }

    .reward-tabs .tabs {
        height: 38px;
        margin: 10px auto;
        padding-left: 0;
    }

    .reward-content ul {
        padding-left: 0 !important;
    }

    .reward-tabs .tabs .tab {
        height: 38px;
        line-height: 38px;
    }

    .reward-tabs .tab a {
        color: #fff;
        background-color: #ccc;
    }

    .reward-tabs .tab a:hover {
        background-color: #ccc;
        color: #fff;
    }

    .reward-tabs .wechat-tab .active {
        color: #fff !important;
        background-color: #22AB38 !important;
    }

    .reward-tabs .alipay-tab .active {
        color: #fff !important;
        background-color: #019FE8 !important;
    }

    .reward-tabs .reward-img {
        width: 210px;
        height: 210px;
    }
</style>

<div id="reward">
    <a href="#rewardModal" class="reward-link modal-trigger btn-floating btn-medium waves-effect waves-light red">赏</a>

    <!-- Modal Structure -->
    <div id="rewardModal" class="modal">
        <div class="modal-content">
            <a class="close modal-close"><i class="fas fa-times"></i></a>
            <h4 class="reward-title">你的赏识是我前进的动力</h4>
            <div class="reward-content">
                <div class="reward-tabs">
                    <ul class="tabs row">
                        <li class="tab col s6 alipay-tab waves-effect waves-light"><a href="#alipay">支付宝</a></li>
                        <li class="tab col s6 wechat-tab waves-effect waves-light"><a href="#wechat">微 信</a></li>
                    </ul>
                    <div id="alipay">
                        <img src="/medias/reward/alipay.jpg" class="reward-img" alt="支付宝打赏二维码">
                    </div>
                    <div id="wechat">
                        <img src="/medias/reward/wechat.png" class="reward-img" alt="微信打赏二维码">
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>

<script>
    $(function () {
        $('.tabs').tabs();
    });
</script>

            
        </div>
    </div>

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;Previous</div>
            <div class="card">
                <a href="/2020/06/07/shi-jue-ai/objecttracking/object-tracking/">
                    <div class="card-image">
                        
                        <img src="https://gitee.com/github-25970295/blogImage/raw/master/img/20210501132858.png" class="responsive-img" alt="Object Tracking">
                        
                        <span class="card-title">Object Tracking</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Paper: SiamRPN++

SiamRPN++: Evolution of Siamese Visual Tracking with Very DeepNetworks


#### Summary


propose a new 
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2020-06-07
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/%E8%A7%86%E8%A7%89AI/" class="post-category">
                                    视觉AI
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/Object-Tracking/">
                        <span class="chip bg-color">Object Tracking</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                Next&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/2020/06/06/shi-jue-ai/dataglove/data-glove-record/">
                    <div class="card-image">
                        
                        <img src="https://gitee.com/github-25970295/blogImage/raw/master/img/gloves-1268930__340.webp" class="responsive-img" alt="Data Glove Record">
                        
                        <span class="card-title">Data Glove Record</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            
动作捕捉(Motion capture)，简称动捕(Mocap)，又称运动捕捉。是指记录并处理人或其他物体动作的技术。它广泛应用于军事，娱乐，体育，医疗应用，计算机视觉以及机器人技术等诸多领域。

1. 介绍
​       在电影制作和
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2020-06-06
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/AIOT/" class="post-category">
                                    AIOT
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/Sense/">
                        <span class="chip bg-color">Sense</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/libs/codeBlock/codeBlockFuction.js"></script>

<!-- 代码语言 -->

<script type="text/javascript" src="/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/libs/codeBlock/codeShrink.js"></script>


<!-- 代码块折行 -->

<style type="text/css">
code[class*="language-"], pre[class*="language-"] { white-space: pre !important; }
</style>


    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;TOC</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // modify the toc link href to support Chinese.
        let i = 0;
        let tocHeading = 'toc-heading-';
        $('#toc-content a').each(function () {
            $(this).attr('href', '#' + tocHeading + (++i));
        });

        // modify the heading title id to support Chinese.
        i = 0;
        $('#articleContent').children('h2, h3, h4').each(function () {
            $(this).attr('id', tocHeading + (++i));
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>

    

</main>




    <!-- <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="463294659"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/libs/aplayer/APlayer.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/meting@2/dist/Meting.min.js"></script>

    
    <div class="container row center-align" style="margin-bottom: 0px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            <span id="year">2019</span>
            <a href="https://liudongdong1.github.io" target="_blank">liudongdong</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            <br>
            
            &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                class="white-color">1195.6k</span>&nbsp;字
            
            
            
            
            
            
            
            <br>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/liudongdong1/" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:3463264078@qq.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>














    <a href="https://blog.csdn.net/liudongdong19/" class="tooltipped" target="_blank" data-tooltip="关注我的CSDN: https://blog.csdn.net/liudongdong19/" data-position="top" data-delay="50">
        <i class="fab fa-csdn">C</i>
    </a>





</div>
    </div>
</footer>

<div class="progress-bar"></div>
 -->

    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;Search</span>
            <input type="search" id="searchInput" name="s" placeholder="Please enter a search keyword"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script src="/js/search.js"></script>
<script type="text/javascript">
$(function () {
    searchFunc("/search.xml", 'searchInput', 'searchResult');
});
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script type="text/javascript" src="/js/CFS.Snow.min.js"></script>
    <!-- 点击爆灯效果 -->
    <canvas class="fireworks" style="position: fixed;left: 0;top: 0;z-index: 1; pointer-events: none;" ></canvas> 
    <script type="text/javascript" src="//cdn.bootcss.com/animejs/2.2.0/anime.min.js"></script> 
    <script type="text/javascript" src="/js/fireworks.js"></script>
    <!--动态线条背景-->
    <script type="text/javascript"
        color="122 103 238" opacity='0.7' zIndex="-2" count="200" src="//cdn.bootcss.com/canvas-nest.js/1.0.0/canvas-nest.min.js">
    </script>
    <!-- 天气 -->
    <!-- weather -->
    <!-- weather -->
    <script type="text/javascript">
         WIDGET = {FID: 'knAMQaFanP'}
    </script>
    <script type="text/javascript" src="https://apip.weatherdt.com/float/static/js/r.js?v=1111"></script>
    <script type="text/javascript" src="https://apip.weatherdt.com/float/static/js/r.js?v=1111"></script>
    <script src="/libs/materialize/materialize.min.js"></script>
    <script src="/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/libs/aos/aos.js"></script>
    <script src="/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/js/matery.js"></script>

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    
    <script async src="/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    

    
    
    <script type="text/javascript" size="150" alpha='0.6'
        zIndex="-1" src="/libs/background/ribbon-refresh.min.js" async="async"></script>
    

    

    
    <script src="/libs/instantpage/instantpage.js" type="module"></script>
    
    <!-- {% include '_custom/custom.swig' %} -->

</body>

</html>
