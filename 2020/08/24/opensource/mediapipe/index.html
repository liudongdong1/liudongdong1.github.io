<!DOCTYPE HTML>
<html lang="en">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="MediaPipe, AIOT,Space&amp;Temporal Sequence Analysis,SpringBoot,liudongdong1,cloud">
    <meta name="description" content="">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>MediaPipe | DaybyDay</title>
    <link rel="icon" type="image/png" href="/favicon.png">

    <link rel="stylesheet" type="text/css" href="/libs/awesome/css/all.css">
    <link rel="stylesheet" type="text/css" href="/libs/materialize/materialize.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/aos/aos.css">
    <link rel="stylesheet" type="text/css" href="/libs/animate/animate.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/lightGallery/css/lightgallery.min.css">
    <link rel="stylesheet" type="text/css" href="/css/matery.css">
    <link rel="stylesheet" type="text/css" href="/css/my.css">

    <script src="/libs/jquery/jquery.min.js"></script>

<meta name="generator" content="Hexo 4.2.0"><link rel="alternate" href="/atom.xml" title="DaybyDay" type="application/atom+xml">
<link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css"></head>


<body>
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/" class="waves-effect waves-light">
                    
                    <img src="/medias/logo.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">DaybyDay</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>Index</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>Tags</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>Categories</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>Archives</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/" class="waves-effect waves-light">

      
      <i class="fas fa-user-circle" style="zoom: 0.6;"></i>
      
      <span>About</span>
      <i class="fas fa-chevron-down" aria-hidden="true" style="zoom: 0.6;"></i>
    </a>
    <ul class="sub-nav menus_item_child ">
      
      <li>
        <a href="/about">
          
          <i class="fas fa-user-circle" style="margin-top: -20px; zoom: 0.6;"></i>
          
          <span>about</span>
        </a>
      </li>
      
      <li>
        <a href="/resume">
          
          <i class="fa fa-user-secret" style="margin-top: -20px; zoom: 0.6;"></i>
          
          <span>resume</span>
        </a>
      </li>
      
    </ul>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/friends" class="waves-effect waves-light">
      
      <i class="fas fa-address-book" style="zoom: 0.6;"></i>
      
      <span>Friends</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/gallery" class="waves-effect waves-light">
      
      <i class="fas fa-camera" style="zoom: 0.6;"></i>
      
      <span>Galleries</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="Search" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/medias/logo.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">DaybyDay</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			Index
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			Tags
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			Categories
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			Archives
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="javascript:;">
			
				<i class="fa-fw fas fa-user-circle"></i>
			
			About
			<span class="m-icon"><i class="fas fa-chevron-right"></i></span>
		</a>
            <ul  style="background:  ;" >
              
                <li>

                  <a href="/about " style="margin-left:75px">
				  
				   <i class="fa fas fa-user-circle" style="position: absolute;left:50px" ></i>
			      
		          <span>about</span>
                  </a>
                </li>
              
                <li>

                  <a href="/resume " style="margin-left:75px">
				  
				   <i class="fa fa fa-user-secret" style="position: absolute;left:50px" ></i>
			      
		          <span>resume</span>
                  </a>
                </li>
              
            </ul>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/friends" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-address-book"></i>
			
			Friends
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/gallery" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-camera"></i>
			
			Galleries
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/liudongdong1" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/liudongdong1" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200715085515063.png')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">MediaPipe</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <link rel="stylesheet" href="/libs/tocbot/tocbot.css">
<style>
    #articleContent h1::before,
    #articleContent h2::before,
    #articleContent h3::before,
    #articleContent h4::before,
    #articleContent h5::before,
    #articleContent h6::before {
        display: block;
        content: " ";
        height: 100px;
        margin-top: -100px;
        visibility: hidden;
    }

    #articleContent :focus {
        outline: none;
    }

    .toc-fixed {
        position: fixed;
        top: 64px;
    }

    .toc-widget {
        width: 345px;
        padding-left: 20px;
    }

    .toc-widget .toc-title {
        margin: 35px 0 15px 0;
        padding-left: 17px;
        font-size: 1.5rem;
        font-weight: bold;
        line-height: 1.5rem;
    }

    .toc-widget ol {
        padding: 0;
        list-style: none;
    }

    #toc-content {
        height: calc(100vh - 250px);
        overflow: auto;
    }

    #toc-content ol {
        padding-left: 10px;
    }

    #toc-content ol li {
        padding-left: 10px;
    }

    #toc-content .toc-link:hover {
        color: #42b983;
        font-weight: 700;
        text-decoration: underline;
    }

    #toc-content .toc-link::before {
        background-color: transparent;
        max-height: 25px;

        position: absolute;
        right: 23.5vw;
        display: block;
    }

    .toc-fixed .toc-link::before{
        position: fixed!important;/*当toc的位置改为fixed时，.toc-link::before也要改为fixed*/
    }

    #toc-content .is-active-link {
        color: #42b983;
    }

    #toc-content .is-active-link::before {
        background-color: #42b983;
    }

    #floating-toc-btn {
        position: fixed;
        right: 15px;
        bottom: 76px;
        padding-top: 15px;
        margin-bottom: 0;
        z-index: 998;
    }

    #floating-toc-btn .btn-floating {
        width: 48px;
        height: 48px;
    }

    #floating-toc-btn .btn-floating i {
        line-height: 48px;
        font-size: 1.4rem;
    }
</style>
<div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/tags/%E5%BC%80%E6%BA%90%E6%A1%86%E6%9E%B6/">
                                <span class="chip bg-color">开源框架</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/categories/Framework/" class="post-category">
                                Framework
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>Publish Date:&nbsp;&nbsp;
                    2020-08-24
                </div>
                

                <!-- 
                    <i class="fa fa-pencil"></i> Author: liudongdong1
                  -->

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>Update Date:&nbsp;&nbsp;
                    2021-05-14
                </div>
                

                <!-- 
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>Word Count:&nbsp;&nbsp;
                    3.3k
                </div>
                 -->

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>Read Times:&nbsp;&nbsp;
                    18 Min
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>Read Count:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">
        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p><a href="https://google.github.io/mediapipe/" target="_blank" rel="noopener">MediaPipe</a> is the simplest way for researchers and developers to build world-class ML solutions and applications for mobile, desktop/cloud, web and IoT devices.</p>
</blockquote>
<h1 id="1-Introduce"><a href="#1-Introduce" class="headerlink" title="1. Introduce"></a>1. Introduce</h1><ol>
<li><strong>End-to-End acceleration</strong>: <em>built-in fast ML inference and processing accelerated even on common hardware</em></li>
<li><strong>Build one, deploy anywhere</strong>: <em>Unified solution works across Android, iOS, desktop/cloud, web and IoT</em></li>
<li><strong>Ready-to-use solutions:</strong> cutting-edge ML solutions demonstrating full power of the framework</li>
<li><strong>Free and Open Source</strong></li>
</ol>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/20200824115410.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/20200824115501.png" alt=""></p>
<h1 id="2-PaperReading"><a href="#2-PaperReading" class="headerlink" title="2.PaperReading"></a>2.PaperReading</h1><p><strong>level</strong>: CCF_A CVPR<br><strong>author</strong>:  <a href="mailto:Mediapipe@google.com">Mediapipe@google.com</a><br><strong>date</strong>: 2019.6.14<br><strong>keyword</strong>:</p>
<ul>
<li>Perception, Framework</li>
</ul>
<hr>
<h2 id="Paper-MediaPipe"><a href="#Paper-MediaPipe" class="headerlink" title="Paper: MediaPipe"></a>Paper: MediaPipe</h2><div align="center">
<br>
<b>MediaPipe: A Framework for Building Perception Pipelines
</b>
</div>



<h4 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h4><ol>
<li>propose a framework consists of three main parts:<ul>
<li>inference from sensory data;</li>
<li>a set of tools for performance evaluation;</li>
<li>a collection of re-usable inference and processing components called calculators.</li>
</ul>
</li>
<li>MediaPipe is targeted towards applications in the audio/video processing domain and not limited to the scope of modeling the performance of concurrent systems.</li>
</ol>
<h4 id="Research-Objective"><a href="#Research-Objective" class="headerlink" title="Research Objective"></a>Research Objective</h4><h4 id="Proble-Statement"><a href="#Proble-Statement" class="headerlink" title="Proble Statement"></a>Proble Statement</h4><ul>
<li>select and develop corresponding machine learning algorithms and models;</li>
<li>build a series of prototypes and demos;</li>
<li>balance resource consumption against the quality of the solutions;</li>
<li>identify and mitigate problematic cases;</li>
</ul>
<blockquote>
<p>Modifying a perception application to incorporate additional processing steps or inference models can be difficult due to excessive coupling between steps;</p>
<p>different platforms consuming time and involves optimizing inference and processing steps to run correctly and efficiently on a target device.</p>
</blockquote>
<h4 id="System-overview"><a href="#System-overview" class="headerlink" title="System overview"></a>System overview</h4><blockquote>
<p>MediaPipe allows to prototype a pipeline incrementally as a directed graph of components where each component is a calculator; The graph is specified using <strong>GraphConfig</strong> protocol buffer and then run using a Graph object; the calculators are connected by data Stream, each Stream represents a time-series of data Packets;</p>
</blockquote>
<p>【<strong>Module one</strong>】 <strong>Component</strong></p>
<ul>
<li><strong>Packets：</strong> consists of a numeric timestamp and a shared pointer to an immutable payload.</li>
<li><strong>Streams</strong>: carries a sequence of packets whose timestamps must be monotonically increasing. Each input stream receives a separate copy of the packets from  an output stream, and maintains its own queue to allow the receiving node to consume the packets.</li>
<li><strong>Side packets:</strong> a side-packets connection between nodes carries a single packet with an unspecified timestamp.</li>
<li><strong>Calculators:</strong> a calculator may receive zero or more output streams or its packets, comprise of four essential methods: <strong>GetContract(),Open(),Process(),Close()</strong>;</li>
<li><strong>Graph:</strong> the context of all processing, contains a collection of nodes joined by directed connections along which packets can flow, some constraints are as follows:<ul>
<li>each stream and side packet must be produced by one source;</li>
<li>the type of an input stream/side packet must be compatible with the type of the output stream/side packet to which it is connected;</li>
<li>each node’s connections are compatible with its contract.</li>
</ul>
</li>
<li><strong>GraphConfig:</strong> a specification that describes the topology and functionality of the graph</li>
</ul>
<h4 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a>Implementation</h4><blockquote>
<p>scheduling logic and powerful synchronization primitives to process time-series in a customizable fashion.</p>
</blockquote>
<p>【Scheduling】</p>
<ul>
<li>each graph has at least one scheduler queue, each scheduler has exactly one executor, nodes are statically assigned to a queue.</li>
<li>each node has a scheduling state, <strong>not ready, ready, running</strong>; </li>
<li>when a node becomes ready for execution, a task is added to the corresponding scheduler queue, the nodes are topologically sorted and assigned a priority based on the graph’s layout;</li>
</ul>
<p>【<strong>Synchronization</strong>】</p>
<blockquote>
<p>mediapipe graph execution is decentralized: there is no global clock, and different nodes can process data from different timestamps at the same time;</p>
</blockquote>
<ul>
<li>the packets pushed into a given stream must have monotonically increasing timestamps;</li>
<li>each stream has a timestamp bound, which is the lowest possible timestamp allowed for a new packet on the stream.</li>
</ul>
<p>【<strong>Input policies</strong>】</p>
<blockquote>
<p>Synchronization is handled locally on each node, using input policy specified by the node.</p>
</blockquote>
<ul>
<li>if packets with the same timestamp are provided on multiple input streams, they will always be processed together regardless of their arrival order in real time;</li>
<li>input set are processed in strictly ascending timestamp order;</li>
<li>no packets are dropped, and the processing is fully deterministic;</li>
<li>the node becomes ready to process data as soon as possible given the guarantees above.</li>
</ul>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/20200831085500.png" alt=""></p>
<p>【<strong>Flow Control</strong>】</p>
<blockquote>
<p>packets may be generated faster than they can be process, flow control is necessary to keep resource usage under control;</p>
</blockquote>
<ul>
<li>a simple back-pressure system: throttles the execution of upstream nodes when the packets buffered on a stream reach limit; by maintaining deterministic behavior and includes a deadlock avoidance system that relaxes configured limits;</li>
<li>a richer node-based system: consists of inserting special nodes which can drop packets according to real-time constraints;</li>
</ul>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/20200831090000.png" alt=""></p>
<p>【GPU support】</p>
<p>【Opaque buffer type】</p>
<p>【OpenGL support】</p>
<h4 id="Tools"><a href="#Tools" class="headerlink" title="Tools"></a>Tools</h4><ul>
<li><strong>Tracker</strong>：follow individual packets across a graph and records timing events along the way, recording a <strong>TraceEvent</strong> structure with several data fields event_time, packet_timestamp, packet_data_id, node_id, and stream_id;</li>
<li><strong>Visualizer</strong>: help to understand the topology and overall behavior of their pipelines:<ul>
<li>Timeline View</li>
<li>Graph view</li>
</ul>
</li>
</ul>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/20200831090454.png" alt=""></p>
<h4 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h4><ul>
<li>Object Detection</li>
</ul>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/20200831090533.png" alt=""></p>
<blockquote>
<ul>
<li>In the detection branch, a frame-selection node ﬁrst selects frames to go through detection based on limiting frequency or scene-change analysis, and passes them to the detector while dropping the irrelevant frames. </li>
<li>The objectdetection node consumes an ML model and the associated label map as input side packets, performs ML inference on the incoming selected frames using an inference engine (e.g., [12] or [2]) and outputs detection results.</li>
<li>the tracking branch updates earlier detections and advances their locations to the current camera frame.  </li>
<li>the detection-merging node compares results and merges them with detections from earlier frames removing duplicate results based on their location in the frame and/or class proximity. </li>
</ul>
</blockquote>
<ul>
<li>FaceLandmark<ul>
<li>demultiplexing node splits the packets in the input stream into interleaving subsets of packets, with subset going into a separate output stream;</li>
</ul>
</li>
</ul>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/20200831091200.png" alt=""></p>
<h4 id="Notes-去加强了解"><a href="#Notes-去加强了解" class="headerlink" title="Notes 去加强了解"></a>Notes <font color="orange">去加强了解</font></h4><ul>
<li>Beam[1]; Apache beam: An advanced uniﬁed programming model. </li>
<li>Dataflow[5];  Thedataﬂowmodel: Apracticalapproach to balancing correctness, latency, and cost in massive-scale, unbounded,out-of-orderdataprocessing</li>
<li>Gstream[8];  https: //gstreamer.freedesktop.org/, </li>
<li>opencv4.0(graph api[9])；OpenCV Graph API. Intel Corporation, 2018. </li>
</ul>
<p><strong>level</strong>:<br><strong>author</strong>: Valentin Bazarevsky (google research)<br><strong>date</strong>: 2020<br><strong>keyword</strong>:</p>
<ul>
<li>Pose estimation</li>
</ul>
<blockquote>
<p>Bazarevsky, Valentin, Ivan Grishchenko, Karthik Raveendran, Tyler Zhu, Fan Zhang, and Matthias Grundmann. “BlazePose: On-device Real-time Body Pose tracking.” <em>arXiv preprint arXiv:2006.10204</em> (2020).</p>
</blockquote>
<hr>
<h2 id="Paper-BlazePose"><a href="#Paper-BlazePose" class="headerlink" title="Paper: BlazePose"></a>Paper: BlazePose</h2><div align="center">
<br>
<b>BlazePose: On-device Real-time Body Pose tracking
</b>
</div>
#### Summary

<ol>
<li>blazepose, a lightweight convolutional neural network architecture for human pose estimation that is tailored for real-time inference on mobile devices.</li>
<li>produces 33 body keypoints for a single person and runs at over 30    frames per second on a Pixel 2 phone.</li>
</ol>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/20200821200224.png" alt=""></p>
<h4 id="Research-Objective-1"><a href="#Research-Objective-1" class="headerlink" title="Research Objective"></a>Research Objective</h4><ul>
<li><strong>Application Area</strong>: fitness tracking ; sign language recognition; Yoga;<ul>
<li><strong>Purpose</strong>:  estimate human pose from images or video with edge devices.</li>
</ul>
</li>
</ul>
<h4 id="Methods"><a href="#Methods" class="headerlink" title="Methods"></a>Methods</h4><ul>
<li><strong>system overview</strong>:</li>
</ul>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/20200821195628.png" alt=""></p>
<blockquote>
<p>a lightweight body pose detector followed by a pose tracker network, the tracker predicts keypoint coordinates, the presence of the person on the current frame, and the refined region of interest for the current frame, when the tracker indicates that there is no human present, we re-run the detector network on the next frame.</p>
</blockquote>
<p>【Person Detector】use a fast on-device face-detector as a proxy for a person detector. the middle point between the person’s hips, the size of the circle circumscribing the whole person, and incline(the angle between the lines connecting the two mid-shoulder and mid-hip points).</p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/20200821212805.png" alt=""></p>
<blockquote>
<ul>
<li><p>use the heatmap and offset loss only in the training stage and remove the corresponding output layers from the model before running the inference. use the heatmap to supervise the lightweight embedding, </p>
</li>
<li><p>stack a tiny encoder-decoder heatmap-based network and subsequent regression encoder network.</p>
</li>
<li><p>utilize skip-connections between all the stages of the network to achieve a balance between high and low-level features.</p>
</li>
<li><p>for invisible points, simulate occlusions during training and introduce a per-point visibility classifier that indicates whether a particular point is occluded and if the position prediction is deemed inaccurate.</p>
</li>
</ul>
</blockquote>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/20200821213938.png" alt=""></p>
<h4 id="Notes-去加强了解-1"><a href="#Notes-去加强了解-1" class="headerlink" title="Notes 去加强了解"></a>Notes <font color="orange">去加强了解</font></h4><ul>
<li><input disabled="" type="checkbox"> 运行代码，学习代码中模型结构，以及数据格式</li>
</ul>
<p><strong>level</strong>:<br><strong>author</strong>: Fan Zhang(google research)<br><strong>date</strong>: 2020<br><strong>keyword</strong>:</p>
<ul>
<li>hand pose estimation;</li>
</ul>
<blockquote>
<p>Zhang, Fan, et al. “MediaPipe Hands: On-device Real-time Hand Tracking.” <em>arXiv preprint arXiv:2006.10214</em> (2020).</p>
</blockquote>
<hr>
<h2 id="Paper-MediaPipe-Hands"><a href="#Paper-MediaPipe-Hands" class="headerlink" title="Paper: MediaPipe Hands"></a>Paper: MediaPipe Hands</h2><div align="center">
<br>
<b>MediaPipe Hands: On-device Real-time Hand Tracking
</b>
</div>
#### Summary

<ol>
<li>the pipeline consists of two models:<ul>
<li>a palm detector, that is providing a bounding box of a hand to</li>
<li>a hand landmark model, that is predicting the hand skeleton.</li>
</ul>
</li>
<li>an efficient two-stage hand tracking pipeline that can track multiple hands in real-time on mobile devices.</li>
<li>a hand pose estimation model that is capable of predicting 2.5D hand pose with only RGB input.</li>
<li>an open source hand tracking pipelines as a ready-to-go solution on variety of platforms, including android, ios, web, and desktop PCs.</li>
</ol>
<h4 id="Methods-1"><a href="#Methods-1" class="headerlink" title="Methods"></a>Methods</h4><ul>
<li><strong>system overview</strong>:</li>
</ul>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/20200821214746.png" alt=""></p>
<p><strong>【BlazePalm Detector】</strong></p>
<blockquote>
<ul>
<li>work across a variety of hand sizes with a large scale span </li>
<li>be able to detect occluded and self-occluded hands</li>
<li>the hands is dynamic, lack of contrast patterns.</li>
</ul>
</blockquote>
<ol>
<li>train a palm detector instead of a hand detector;</li>
<li>use an encoder-decoder feature extractro similar to FPN for larger scene-context awareness even for small objects.</li>
<li>minimize the focal loss during training to support a large amount of anchors resulting from the high scale variance.</li>
</ol>
<p><strong>【Hand Landmark Model】</strong></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/20200821220035.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/20200821220256.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/20200821220320.png" alt=""></p>
<h4 id="Notes-去加强了解-2"><a href="#Notes-去加强了解-2" class="headerlink" title="Notes 去加强了解"></a>Notes <font color="orange">去加强了解</font></h4><blockquote>
<p>2.5D游戏仅仅是在2D游戏基础上把视角横向旋转了45度。2.5D视角带来的最核心的问题是每个图片和其他图片之间的遮挡关系如何处理，才能更符合人类对3D世界的常识性认知呢，也就是用2D的方式来模拟3D。2D游戏的做法很简单粗暴。2D游戏世界中每一个物件都会用一个2维坐标来表示其位置，x表示其横向位置，y表示其纵向位置。<strong>当一个物件的y值越小，也就是其越靠近画面底部，则渲染顺序越靠后</strong>。就像一个画家在Photoshop上作画一样，离相机越近的图层要越后面画，才能盖住离相机远的图层，所以画家要从远到近地画。3D游戏的渲染，简单来说可以理解为将三维数据在二维平面上做投影的过程。所以所谓的3D游戏，呈现在玩家面前依然是一个二维的画面，三维空间中的物件移动表现在二维画面上，也就是二维坐标位置的移动而已。θ就是相机的俯仰角,投影线段 = 3D线段 * sin(俯仰角)。CD长度=3D世界中正方形CD长度 * sin(俯仰角)。遍历三角函数查找表，只有sin(30°)的分子分母都为整数，也就是说只有30°这个角度有可能让长宽都为整数，具体可参看尼文定理：。因此人们通常说的斜45度视角游戏只是人们通过臆测而给2.5D游戏取得俗名，准确来说我们应该称这类游戏叫做斜30度视角游戏。或者可以采用另一种对斜45度视角游戏的解释，斜45度指的是相机水平方向上（围绕世界空间Y轴）的旋转角度。</p>
<p><a href="http://matov.me/isometric-toolset/" target="_blank" rel="noopener">http://matov.me/isometric-toolset/</a>  能不能将2.5D坐标转化为3D坐标。</p>
</blockquote>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201101104102167.png" alt=""></p>
<ul>
<li><input checked="" disabled="" type="checkbox"> multi-handDetection pipeline<ul>
<li><strong>multi_hand_detection_gpu</strong></li>
</ul>
</li>
</ul>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200916212815466.png" alt=""></p>
<ul>
<li><strong>multi_hand_landmark_gpu</strong></li>
</ul>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200916214124663.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200916214222259.png" alt=""></p>
<ul>
<li><strong>multi_hand_renderer_gpu</strong></li>
</ul>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200916221843111.png" alt=""></p>
<p><strong>level</strong>:  CVPR<br><strong>author</strong>: Artsiom Ablavatski<br><strong>date</strong>: 2020<br><strong>keyword</strong>:</p>
<ul>
<li>iris tracking</li>
</ul>
<hr>
<h2 id="Paper-PupilTracking"><a href="#Paper-PupilTracking" class="headerlink" title="Paper: PupilTracking"></a>Paper: PupilTracking</h2><div align="center">
<br>
<b>Real-time Pupil Tracking from Monocular Video for Digital Puppetry</b>
</div>

<h4 id="Summary-1"><a href="#Summary-1" class="headerlink" title="Summary"></a>Summary</h4><ol>
<li>present a simple, real-time approach for pupil tracking from live video on mobile devices.</li>
<li>consists of two new component: a tiny neural network that predicts positions of the pupils in 2D, and a displacement-based estimation of pupil blend shape coefficients.</li>
<li>this methods can be used to accurately control the pupil movements of a virtual puppet, and lends liveliness and energy to it, run 50FPs on model phones.</li>
<li>detects 5 points of the pupil, outer iris circle and eye contour for each eye;</li>
</ol>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/20200831094224.png" alt=""></p>
<h4 id="System-Overview"><a href="#System-Overview" class="headerlink" title="System Overview"></a>System Overview</h4><p>【<strong>Neural network based eye landmarks</strong>】</p>
<blockquote>
<p>using a tiny neural network combined face mesh model to produces additional higher quality landmarks.</p>
</blockquote>
<ul>
<li>combine the corresponding landmarks (16 points of eye contour)from the face estimation pipeline with those from the eye reﬁnement network by <font color="red">replacing the x,y coordinates of the former while leaving z untouched</font>.</li>
<li>extend the face mesh with 5 pupil landmarks(pupil center and  4 points of outer iris circle ), with z coordinate set to the average of the z coordinate of the eye corners.</li>
</ul>
<p>【<strong>Displacement-based pupil blend shape estimation</strong>】基于位移的瞳孔混合变形估计</p>
<ul>
<li>refine the mesh to predict 4 blend shapes for the pupils:<font color="red"> pupils pointing outwards, inwards, upwards and downwards respectively.</font></li>
<li>by combine 3 displacement to obtain scalar value in the range of [0,1] for each pupil blend shape.  <font color="red"> 不明白这一步达到的效果是什么？</font></li>
</ul>
<blockquote>
<p>for the pupil pointing inwards, using the vertex of the pupil and vertex of eye corner, and measure the displacement $D_{current}$ between these two vertices and compare it to two empirically derived displacements $D_{neutral}$ , the displacement with the minimum activation of the blend shape and $D_{activated}$ the displacement measured using maximum activation of the blend shape. </p>
</blockquote>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/20200831100400.png" alt=""></p>
<p>【<strong>Real-time heuristics calibratin</strong>】</p>
<blockquote>
<p>the initial displacements are empirically estimated based on the representative face mesh dataset, but unable to model all person-specific variations.</p>
<p>employ the standard score calculation algorithm with a few modificaitons, the main idea of the filter is to check the displacement on every iteration and add it to a circular buffer of the trusted displacement if it falls within the specified confidence interval, and the calibrated displacement is calculated as an average of the trusted displacement, the standard deviation of these trusted displacements is used as the confidence interval in the next iteration.</p>
</blockquote>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/20200831102522.png" alt=""></p>
<h4 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a>Evaluation</h4><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/20200831102637.png" alt=""></p>
<h4 id="Notes-去加强了解-3"><a href="#Notes-去加强了解-3" class="headerlink" title="Notes 去加强了解"></a>Notes <font color="orange">去加强了解</font></h4><ul>
<li>human face model predicts a 468 vertex mesh [4]  Real-time facial surface geometry from monocular video on mobile gpus. </li>
<li>[1] predicts 5 locations in 2D(pupil center , 4 points of outer iris circle, and 16 points of eye contour)  Blazeface: Sub-milli second neural face detection on mobile gpus. arXiv preprint arXiv:1907.05047, 2019. 2 </li>
<li>[8]Mnasnet: Platform-aware neural architecture search for mobile</li>
</ul>
<h2 id="3-案例"><a href="#3-案例" class="headerlink" title="3. 案例"></a>3. 案例</h2><h3 id="3-1-Video-Reframing"><a href="#3-1-Video-Reframing" class="headerlink" title="3.1. Video Reframing"></a>3.1. Video Reframing</h3><p><a href="http://ai.googleblog.com/2020/02/autoflip-open-source-framework-for.html" target="_blank" rel="noopener">AutoFlip: An Open Source Framework for Intelligent Video Reframing</a></p>
<blockquote>
<p>AutoFlip provides a fully automatic solution to smart video reframing, making use of state-of-the-art ML-enabled object detection and tracking technologies to intelligently understand video content. AutoFlip detects changes in the composition that signify scene changes in order to isolate scenes for processing. Within each shot, video analysis is used to identify salient content before the scene is reframed by selecting a camera mode and path optimized for the contents.</p>
</blockquote>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/20200825153825.png" alt=""></p>
<ul>
<li><strong>Shot (Scene) Detection</strong>: </li>
</ul>
<blockquote>
<p>A scene or shot is a continuous sequence of video without cuts (or jumps). To detect the occurrence of a shot change, AutoFlip computes the color histogram of each frame and compares this with prior frames. If the distribution of frame colors changes at a different rate than a sliding historical window, a shot change is signaled. AutoFlip buffers the video until the scene is complete before making reframing decisions, in order to optimize the reframing for  entire scene.</p>
</blockquote>
<ul>
<li><strong>Video Content Analysis:</strong> utilize deep  learning-based object detection models to find interesting, salient content in the frame.</li>
<li><strong>Reframing:</strong> </li>
</ul>
<blockquote>
<p>AutoFlip automatically chooses an optimal refremingn strategy, stationary, paining or tracking, depending on the way obects behave during the scene.</p>
</blockquote>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/jitter_combined.gif" alt=""></p>
<blockquote>
<p><strong>Top:</strong> Camera paths resulting from following the bounding boxes from frame-to-frame. <strong>Bottom:</strong> Final smoothed camera paths generated using <a href="https://en.wikipedia.org/wiki/Norm_(mathematics)#Euclidean_norm" target="_blank" rel="noopener">Euclidean-norm</a> path formation. <strong>Left:</strong> Scene in which objects are moving around, requiring a tracking camera path. <strong>Right:</strong> Scene where objects stay close to the same position; a stationary camera covers the content for the full duration of the scene.</p>
</blockquote>
<h3 id="3-2-Real-Time-3D-Object-Detection"><a href="#3-2-Real-Time-3D-Object-Detection" class="headerlink" title="3.2. Real-Time 3D Object Detection"></a><strong>3.2.</strong> Real-Time 3D Object Detection</h3><blockquote>
<p>robotics, self-driving vehicles, image retrieval, and augmented reality. <a href="https://arxiv.org/abs/2003.03522" target="_blank" rel="noopener">built a single-stage model</a> to predict the pose and physical size of an object from a single RGB image</p>
</blockquote>
<ul>
<li><strong>Real-World 3D Training Data</strong>: With the arrival of <a href="https://developers.google.com/ar" target="_blank" rel="noopener">ARCore</a> and <a href="https://developer.apple.com/augmented-reality/" target="_blank" rel="noopener">ARKit</a>, <a href="https://arinsider.co/2019/05/13/arcore-reaches-400-million-devices/" target="_blank" rel="noopener">hundreds of millions</a> of smartphones now have AR capabilities and the ability to capture additional information during an AR session, including the camera pose, sparse <a href="https://en.wikipedia.org/wiki/Point_cloud" target="_blank" rel="noopener">3D point clouds</a>, estimated lighting, and planar surfaces.</li>
<li><strong>AR Synthetic Data Generation:</strong> AR Synthetic Data Generation, places virtual objects into scenes that have AR session data, which allows us to leverage camera poses, detected planar surfaces, and estimated lighting to generate placements that are physically probable and with lighting that matches the scene.</li>
</ul>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/20200825170636.png" alt="Network architecture and post-processing for 3D object detection."></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/20200825170655.png" alt=""></p>
<blockquote>
<p>Sample results of our network — [<strong>left</strong>] original 2D image with estimated bounding boxes, [<strong>middle</strong>] object detection by Gaussian distribution, [<strong>right</strong>] predicted segmentation mask.</p>
</blockquote>
<h3 id="3-3-Afred-Camera"><a href="#3-3-Afred-Camera" class="headerlink" title="3.3. Afred Camera"></a>3.3. Afred Camera</h3><blockquote>
<p> users are able to turn their spare phones into security cameras and monitors directly, which allows them to watch their homes, shops, pets anytime. </p>
</blockquote>
<ul>
<li><strong>Moving Object Detection:</strong>  continuously uses the device’s camera to monitor a target scene, once detected recording the video and send notifications to the device owner.</li>
<li><strong>Low-light Detection and Low-light Filter:</strong> calculate the average luminance of the scene, and conditionally process the incoming frames to intensify the brightness of the pixel.</li>
<li>*<em>Motion Detection: *</em> by calculating the difference between two frames with some additional tricks that take the movements detected in a few frames</li>
<li><strong>Area of Interest:</strong> manually mask out the area where they don’t want the camera to see.</li>
</ul>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/20200825173215.png" alt=""></p>
<h3 id="3-4-Iris-Tracking-and-Depth-Estimation"><a href="#3-4-Iris-Tracking-and-Depth-Estimation" class="headerlink" title="3.4. Iris Tracking and Depth Estimation"></a>3.4. Iris Tracking and Depth Estimation</h3><blockquote>
<p>A wide range of real-world applications, including computational photography (e.g., <a href="https://ai.googleblog.com/2019/12/improvements-to-portrait-mode-on-google.html" target="_blank" rel="noopener">portrait mode</a> and glint reflections) and <a href="https://ai.googleblog.com/2019/03/real-time-ar-self-expression-with.html" target="_blank" rel="noopener">augmented reality effects</a> (e.g., virtual avatars) rely on estimating eye position by tracking the iris</p>
</blockquote>
<blockquote>
<p>Iris tracking is a challenging task to solve on mobile devices, due to limited computing resources, variable light conditions and the presence of occlusions, such as hair or people squinting. Often, sophisticated specialized hardware is employed, limiting the range of devices on which the solution could be applied.</p>
</blockquote>
<ul>
<li>Depth-from-Iris from a single Image: <font color="red">by relying on the fact that the horizontal iris diameter of the human eye remains roughly constant at 11.7+-0.5 mm across a wide population, along with some simple geometric arguments.</font></li><font color="red">
</font></ul><font color="red">
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/20200825174114.png" alt=""></p>
</font><script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script>
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        Author:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://liudongdong1.github.io" rel="external nofollow noreferrer">liudongdong1</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        Link:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://liudongdong1.github.io/2020/08/24/opensource/mediapipe/">https://liudongdong1.github.io/2020/08/24/opensource/mediapipe/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        Reprint policy:
                    </i>
                </span>
                <span class="reprint-info">
                    All articles in this blog are used except for special statements
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    reprint polocy. If reproduced, please indicate source
                    <a href="https://liudongdong1.github.io" target="_blank">liudongdong1</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>Copied successfully, please follow the reprint policy of this article</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">more</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/tags/%E5%BC%80%E6%BA%90%E6%A1%86%E6%9E%B6/">
                                    <span class="chip bg-color">开源框架</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="qq,qzone,wechat,weibo,douban" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
                <style>
    #reward {
        margin: 40px 0;
        text-align: center;
    }

    #reward .reward-link {
        font-size: 1.4rem;
        line-height: 38px;
    }

    #reward .btn-floating:hover {
        box-shadow: 0 6px 12px rgba(0, 0, 0, 0.2), 0 5px 15px rgba(0, 0, 0, 0.2);
    }

    #rewardModal {
        width: 320px;
        height: 350px;
    }

    #rewardModal .reward-title {
        margin: 15px auto;
        padding-bottom: 5px;
    }

    #rewardModal .modal-content {
        padding: 10px;
    }

    #rewardModal .close {
        position: absolute;
        right: 15px;
        top: 15px;
        color: rgba(0, 0, 0, 0.5);
        font-size: 1.3rem;
        line-height: 20px;
        cursor: pointer;
    }

    #rewardModal .close:hover {
        color: #ef5350;
        transform: scale(1.3);
        -moz-transform:scale(1.3);
        -webkit-transform:scale(1.3);
        -o-transform:scale(1.3);
    }

    #rewardModal .reward-tabs {
        margin: 0 auto;
        width: 210px;
    }

    .reward-tabs .tabs {
        height: 38px;
        margin: 10px auto;
        padding-left: 0;
    }

    .reward-content ul {
        padding-left: 0 !important;
    }

    .reward-tabs .tabs .tab {
        height: 38px;
        line-height: 38px;
    }

    .reward-tabs .tab a {
        color: #fff;
        background-color: #ccc;
    }

    .reward-tabs .tab a:hover {
        background-color: #ccc;
        color: #fff;
    }

    .reward-tabs .wechat-tab .active {
        color: #fff !important;
        background-color: #22AB38 !important;
    }

    .reward-tabs .alipay-tab .active {
        color: #fff !important;
        background-color: #019FE8 !important;
    }

    .reward-tabs .reward-img {
        width: 210px;
        height: 210px;
    }
</style>

<div id="reward">
    <a href="#rewardModal" class="reward-link modal-trigger btn-floating btn-medium waves-effect waves-light red">赏</a>

    <!-- Modal Structure -->
    <div id="rewardModal" class="modal">
        <div class="modal-content">
            <a class="close modal-close"><i class="fas fa-times"></i></a>
            <h4 class="reward-title">你的赏识是我前进的动力</h4>
            <div class="reward-content">
                <div class="reward-tabs">
                    <ul class="tabs row">
                        <li class="tab col s6 alipay-tab waves-effect waves-light"><a href="#alipay">支付宝</a></li>
                        <li class="tab col s6 wechat-tab waves-effect waves-light"><a href="#wechat">微 信</a></li>
                    </ul>
                    <div id="alipay">
                        <img src="/medias/reward/alipay.jpg" class="reward-img" alt="支付宝打赏二维码">
                    </div>
                    <div id="wechat">
                        <img src="/medias/reward/wechat.png" class="reward-img" alt="微信打赏二维码">
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>

<script>
    $(function () {
        $('.tabs').tabs();
    });
</script>

            
        </div>
    </div>

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;Previous</div>
            <div class="card">
                <a href="/2020/08/25/dlminimalpra/model/anchorintroduce/">
                    <div class="card-image">
                        
                        <img src="https://gitee.com/github-25970295/blogImage/raw/master/img/laptop-disposable-cup-and-yellow-flowers-in-vase-on-table.jpg" class="responsive-img" alt="AnchorIntroduce">
                        
                        <span class="card-title">AnchorIntroduce</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            
在网络最后的输出中，对于每个grid cell产生3个bounding box，每个bounding box的输出有三类参数：一个是对象的box参数，一共是四个值，即box的中心点坐标（x,y）和box的宽和高（w,h）;一个是置信度，这
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2020-08-25
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/%E8%AF%AD%E8%A8%80%E6%A1%86%E6%9E%B6/" class="post-category">
                                    语言框架
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/pytorch/">
                        <span class="chip bg-color">pytorch</span>
                    </a>
                    
                    <a href="/tags/AI/">
                        <span class="chip bg-color">AI</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                Next&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/2020/08/21/shi-jue-ai/video-understand/visionnlpcommend/">
                    <div class="card-image">
                        
                        <img src="https://gitee.com/github-25970295/blogImage/raw/master/img/voice-recognition-speech-detect-deep-260nw-694633963.webp" class="responsive-img" alt="VisionNLPCommend">
                        
                        <span class="card-title">VisionNLPCommend</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            level: CCF_A  CVPRauthor: Amaia Salvador1(FaceBook Al Research)date: 2019keyword:

image understanding; information retr
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2020-08-21
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/%E8%A7%86%E8%A7%89AI/" class="post-category">
                                    视觉AI
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/VideoAnalyse/">
                        <span class="chip bg-color">VideoAnalyse</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/libs/codeBlock/codeBlockFuction.js"></script>

<!-- 代码语言 -->

<script type="text/javascript" src="/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/libs/codeBlock/codeShrink.js"></script>


<!-- 代码块折行 -->

<style type="text/css">
code[class*="language-"], pre[class*="language-"] { white-space: pre !important; }
</style>


    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;TOC</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // modify the toc link href to support Chinese.
        let i = 0;
        let tocHeading = 'toc-heading-';
        $('#toc-content a').each(function () {
            $(this).attr('href', '#' + tocHeading + (++i));
        });

        // modify the heading title id to support Chinese.
        i = 0;
        $('#articleContent').children('h2, h3, h4').each(function () {
            $(this).attr('id', tocHeading + (++i));
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>

    

</main>




    <!-- <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="463294659"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/libs/aplayer/APlayer.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/meting@2/dist/Meting.min.js"></script>

    
    <div class="container row center-align" style="margin-bottom: 0px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            <span id="year">2019</span>
            <a href="https://liudongdong1.github.io" target="_blank">liudongdong</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            <br>
            
            &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                class="white-color">1206.4k</span>&nbsp;字
            
            
            
            
            
            
            <span id="busuanzi_container_site_pv">
                |&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;<span id="busuanzi_value_site_pv"
                    class="white-color"></span>&nbsp;次
            </span>
            
            
            <span id="busuanzi_container_site_uv">
                |&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;<span id="busuanzi_value_site_uv"
                    class="white-color"></span>&nbsp;人
            </span>
            
            <br>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/liudongdong1/" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:3463264078@qq.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>














    <a href="https://blog.csdn.net/liudongdong19/" class="tooltipped" target="_blank" data-tooltip="关注我的CSDN: https://blog.csdn.net/liudongdong19/" data-position="top" data-delay="50">
        <i class="fab fa-csdn">C</i>
    </a>





</div>
    </div>
</footer>

<div class="progress-bar"></div>
 -->

    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;Search</span>
            <input type="search" id="searchInput" name="s" placeholder="Please enter a search keyword"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script src="/js/search.js"></script>
<script type="text/javascript">
$(function () {
    searchFunc("/search.xml", 'searchInput', 'searchResult');
});
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script type="text/javascript" src="/js/CFS.Snow.min.js"></script>
    <!-- 点击爆灯效果 -->
    <canvas class="fireworks" style="position: fixed;left: 0;top: 0;z-index: 1; pointer-events: none;" ></canvas> 
    <script type="text/javascript" src="//cdn.bootcss.com/animejs/2.2.0/anime.min.js"></script> 
    <script type="text/javascript" src="/js/fireworks.js"></script>
    <!--动态线条背景-->
    <script type="text/javascript"
        color="122 103 238" opacity='0.7' zIndex="-2" count="200" src="//cdn.bootcss.com/canvas-nest.js/1.0.0/canvas-nest.min.js">
    </script>
    <!-- 天气 -->
    <!-- weather -->
    <!-- weather -->
    <script type="text/javascript">
         WIDGET = {FID: 'knAMQaFanP'}
    </script>
    <script type="text/javascript" src="https://apip.weatherdt.com/float/static/js/r.js?v=1111"></script>
    <script type="text/javascript" src="https://apip.weatherdt.com/float/static/js/r.js?v=1111"></script>
    <script src="/libs/materialize/materialize.min.js"></script>
    <script src="/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/libs/aos/aos.js"></script>
    <script src="/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/js/matery.js"></script>

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    
    <script async src="/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    

    
    
    <script type="text/javascript" size="150" alpha='0.6'
        zIndex="-1" src="/libs/background/ribbon-refresh.min.js" async="async"></script>
    

    

    
    <script src="/libs/instantpage/instantpage.js" type="module"></script>
    
    <!-- {% include '_custom/custom.swig' %} -->

</body>

</html>
