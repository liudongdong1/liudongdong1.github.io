<!DOCTYPE HTML>
<html lang="en">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="PySpark, AIOT,Space&amp;Temporal Sequence Analysis,SpringBoot,liudongdong1,cloud">
    <meta name="description" content="">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>PySpark | DaybyDay</title>
    <link rel="icon" type="image/png" href="/favicon.png">

    <link rel="stylesheet" type="text/css" href="/libs/awesome/css/all.css">
    <link rel="stylesheet" type="text/css" href="/libs/materialize/materialize.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/aos/aos.css">
    <link rel="stylesheet" type="text/css" href="/libs/animate/animate.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/lightGallery/css/lightgallery.min.css">
    <link rel="stylesheet" type="text/css" href="/css/matery.css">
    <link rel="stylesheet" type="text/css" href="/css/my.css">

    <script src="/libs/jquery/jquery.min.js"></script>

<meta name="generator" content="Hexo 4.2.0"><link rel="alternate" href="/atom.xml" title="DaybyDay" type="application/atom+xml">
<link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css"></head>


<body>
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/" class="waves-effect waves-light">
                    
                    <img src="/medias/logo.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">DaybyDay</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>Index</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>Tags</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>Categories</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>Archives</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/" class="waves-effect waves-light">

      
      <i class="fas fa-user-circle" style="zoom: 0.6;"></i>
      
      <span>About</span>
      <i class="fas fa-chevron-down" aria-hidden="true" style="zoom: 0.6;"></i>
    </a>
    <ul class="sub-nav menus_item_child ">
      
      <li>
        <a href="/about">
          
          <i class="fas fa-user-circle" style="margin-top: -20px; zoom: 0.6;"></i>
          
          <span>about</span>
        </a>
      </li>
      
      <li>
        <a href="/resume">
          
          <i class="fa fa-user-secret" style="margin-top: -20px; zoom: 0.6;"></i>
          
          <span>resume</span>
        </a>
      </li>
      
    </ul>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/friends" class="waves-effect waves-light">
      
      <i class="fas fa-address-book" style="zoom: 0.6;"></i>
      
      <span>Friends</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/gallery" class="waves-effect waves-light">
      
      <i class="fas fa-camera" style="zoom: 0.6;"></i>
      
      <span>Galleries</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="Search" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/medias/logo.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">DaybyDay</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			Index
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			Tags
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			Categories
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			Archives
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="javascript:;">
			
				<i class="fa-fw fas fa-user-circle"></i>
			
			About
			<span class="m-icon"><i class="fas fa-chevron-right"></i></span>
		</a>
            <ul  style="background:  ;" >
              
                <li>

                  <a href="/about " style="margin-left:75px">
				  
				   <i class="fa fas fa-user-circle" style="position: absolute;left:50px" ></i>
			      
		          <span>about</span>
                  </a>
                </li>
              
                <li>

                  <a href="/resume " style="margin-left:75px">
				  
				   <i class="fa fa fa-user-secret" style="position: absolute;left:50px" ></i>
			      
		          <span>resume</span>
                  </a>
                </li>
              
            </ul>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/friends" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-address-book"></i>
			
			Friends
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/gallery" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-camera"></i>
			
			Galleries
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/liudongdong1" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/liudongdong1" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://gitee.com/github-25970295/blogImage/raw/master/img/4.jpeg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">PySpark</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <link rel="stylesheet" href="/libs/tocbot/tocbot.css">
<style>
    #articleContent h1::before,
    #articleContent h2::before,
    #articleContent h3::before,
    #articleContent h4::before,
    #articleContent h5::before,
    #articleContent h6::before {
        display: block;
        content: " ";
        height: 100px;
        margin-top: -100px;
        visibility: hidden;
    }

    #articleContent :focus {
        outline: none;
    }

    .toc-fixed {
        position: fixed;
        top: 64px;
    }

    .toc-widget {
        width: 345px;
        padding-left: 20px;
    }

    .toc-widget .toc-title {
        margin: 35px 0 15px 0;
        padding-left: 17px;
        font-size: 1.5rem;
        font-weight: bold;
        line-height: 1.5rem;
    }

    .toc-widget ol {
        padding: 0;
        list-style: none;
    }

    #toc-content {
        height: calc(100vh - 250px);
        overflow: auto;
    }

    #toc-content ol {
        padding-left: 10px;
    }

    #toc-content ol li {
        padding-left: 10px;
    }

    #toc-content .toc-link:hover {
        color: #42b983;
        font-weight: 700;
        text-decoration: underline;
    }

    #toc-content .toc-link::before {
        background-color: transparent;
        max-height: 25px;

        position: absolute;
        right: 23.5vw;
        display: block;
    }

    .toc-fixed .toc-link::before{
        position: fixed!important;/*当toc的位置改为fixed时，.toc-link::before也要改为fixed*/
    }

    #toc-content .is-active-link {
        color: #42b983;
    }

    #toc-content .is-active-link::before {
        background-color: #42b983;
    }

    #floating-toc-btn {
        position: fixed;
        right: 15px;
        bottom: 76px;
        padding-top: 15px;
        margin-bottom: 0;
        z-index: 998;
    }

    #floating-toc-btn .btn-floating {
        width: 48px;
        height: 48px;
    }

    #floating-toc-btn .btn-floating i {
        line-height: 48px;
        font-size: 1.4rem;
    }
</style>
<div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/tags/spark/">
                                <span class="chip bg-color">spark</span>
                            </a>
                        
                            <a href="/tags/stream/">
                                <span class="chip bg-color">stream</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/categories/Framewrok/" class="post-category">
                                Framewrok
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>Publish Date:&nbsp;&nbsp;
                    2020-07-13
                </div>
                

                <!-- 
                    <i class="fa fa-pencil"></i> Author: liudongdong1
                  -->

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>Update Date:&nbsp;&nbsp;
                    2021-06-21
                </div>
                

                <!-- 
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>Word Count:&nbsp;&nbsp;
                    4.8k
                </div>
                 -->

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>Read Times:&nbsp;&nbsp;
                    25 Min
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>Read Count:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">
        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>Apart from real-time and batch processing, Apache Spark supports interactive queries and iterative algorithms. Using PySpark, you can work with <strong>RDDs</strong> in Python programming language also. It is because of a library called <strong>Py4j</strong> that they are able to achieve this.</p>
</blockquote>
<blockquote>
<ul>
<li><strong>Resilient Distributed Dataset (RDD)</strong>: RDD is an immutable (read-only), fundamental <code>collection of elements or items</code> that can be operated on many devices at the same time (parallel processing). Each dataset in an RDD can be divided into logical portions, which are then executed on different nodes of a cluster.</li>
<li><strong>Directed Acyclic Graph (DAG)</strong>: DAG is the <code>scheduling layer of the Apache Spark architecture</code> that implements <strong>stage-oriented scheduling</strong>. Compared to MapReduce that creates a graph in two stages, Map and Reduce, Apache Spark can create DAGs that contain many stages.</li>
</ul>
</blockquote>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201119211239377.png" alt=""></p>
<h2 id="0-Architecture"><a href="#0-Architecture" class="headerlink" title="0. Architecture"></a>0. Architecture</h2><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201119211608027.png" alt="Basic"></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201119211639658.png" alt="standalone"></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201119211701746.png" alt="Yarn"></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201119211800146.png" alt="component"></p>
<h2 id="1-core-class"><a href="#1-core-class" class="headerlink" title="1. core class"></a>1. core class</h2><p><a href="https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.SparkContext" target="_blank" rel="noopener"><code>pyspark.SparkContext</code></a>Main entry point for Spark functionality.</p>
<p><a href="https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD" target="_blank" rel="noopener"><code>pyspark.RDD</code></a>A Resilient Distributed Dataset (RDD), the basic abstraction in Spark.</p>
<p><a href="https://spark.apache.org/docs/latest/api/python/pyspark.streaming.html#pyspark.streaming.StreamingContext" target="_blank" rel="noopener"><code>pyspark.streaming.StreamingContext</code></a>Main entry point for Spark Streaming functionality.</p>
<p><a href="https://spark.apache.org/docs/latest/api/python/pyspark.streaming.html#pyspark.streaming.DStream" target="_blank" rel="noopener"><code>pyspark.streaming.DStream</code></a>A Discretized Stream (DStream), the basic abstraction in Spark Streaming.</p>
<p><a href="https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.SparkSession" target="_blank" rel="noopener"><code>pyspark.sql.SparkSession</code></a>Main entry point for DataFrame and SQL functionality.</p>
<p><a href="https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame" target="_blank" rel="noopener"><code>pyspark.sql.DataFrame</code></a>A distributed collection of data grouped into named columns.</p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210225232741569.png" alt=""></p>
<figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">连接spark cluster</span></span><br><span class="line">from pyspark import SparkContext, SparkConf</span><br><span class="line">conf = SparkConf().setAppName("sparkAppExample")</span><br><span class="line">sc = SparkContext(conf=conf)</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">使用session</span></span><br><span class="line">from pyspark.sql import SparkSession</span><br><span class="line">spark = SparkSession.builder \</span><br><span class="line">          .master("local") \</span><br><span class="line">          .appName("Word Count") \</span><br><span class="line">          .config("spark.some.config.option", "some-value") \</span><br><span class="line">          .getOrCreate()</span><br><span class="line"><span class="meta">#</span><span class="bash"> 如果使用 hive table 则加上 .enableHiveSupport()</span></span><br><span class="line"><span class="meta">#</span><span class="bash">spark.sparkContext._conf.getAll()  <span class="comment"># check the config</span></span></span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkContext, SparkConf</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_sc</span><span class="params">()</span>:</span></span><br><span class="line">    sc_conf = SparkConf()</span><br><span class="line">    sc_conf.setMaster(<span class="string">'spark://master:7077'</span>)</span><br><span class="line">    sc_conf.setAppName(<span class="string">'my-app'</span>)</span><br><span class="line">    sc_conf.set(<span class="string">'spark.executor.memory'</span>, <span class="string">'2g'</span>)  <span class="comment">#executor memory是每个节点上占用的内存。每一个节点可使用内存</span></span><br><span class="line">    sc_conf.set(<span class="string">"spark.executor.cores"</span>, <span class="string">'4'</span>) <span class="comment">#spark.executor.cores：顾名思义这个参数是用来指定executor的cpu内核个数，分配更多的内核意味着executor并发能力越强，能够同时执行更多的task</span></span><br><span class="line">    sc_conf.set(<span class="string">'spark.cores.max'</span>, <span class="number">40</span>)    <span class="comment">#spark.cores.max：为一个application分配的最大cpu核心数，如果没有设置这个值默认为spark.deploy.defaultCores</span></span><br><span class="line">    sc_conf.set(<span class="string">'spark.logConf'</span>, <span class="literal">True</span>)    <span class="comment">#当SparkContext启动时，将有效的SparkConf记录为INFO。</span></span><br><span class="line">    print(sc_conf.getAll())</span><br><span class="line"></span><br><span class="line">    sc = SparkContext(conf=sc_conf)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> sc</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> pyspark.conf <span class="keyword">import</span> SparkConf</span><br><span class="line">conf=SparkConf()</span><br><span class="line">        conf.set(<span class="string">'spark.sql.execute.arrow.enabled'</span>,<span class="string">'true'</span>)</span><br><span class="line">        <span class="keyword">if</span> os.getenv(<span class="string">"APP_MODE"</span>) == <span class="string">'prod'</span>:</span><br><span class="line">            <span class="string">"""</span></span><br><span class="line"><span class="string">            集群环境</span></span><br><span class="line"><span class="string">            """</span></span><br><span class="line">            url = <span class="string">'spark://master:7077'</span></span><br><span class="line">            conf.setAppName(<span class="string">'prod-practice-info'</span>).setMaster(url).set(<span class="string">"spark.driver.maxResultSize"</span>, <span class="string">"12g"</span>).set(<span class="string">"spark.executor.memory"</span>, <span class="string">'4g'</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="string">"""</span></span><br><span class="line"><span class="string">            本地环境</span></span><br><span class="line"><span class="string">            """</span></span><br><span class="line">            print(<span class="string">"本地环境"</span>)</span><br><span class="line">            url = <span class="string">'local[*]'</span></span><br><span class="line">            conf.setAppName(<span class="string">'prod-practice-info'</span>).setMaster(url)</span><br><span class="line">        spark = SparkSession.builder. \</span><br><span class="line">            config(conf=conf).\</span><br><span class="line">            getOrCreate()</span><br></pre></td></tr></tbody></table></figure>

<h3 id="1-0-Submit-Model"><a href="#1-0-Submit-Model" class="headerlink" title="1.0. Submit Model"></a>1.0. Submit Model</h3><ul>
<li><code>--class</code>: The entry point for your application (e.g. <code>org.apache.spark.examples.SparkPi</code>)</li>
<li><code>--master</code>: The <a href="https://spark.apache.org/docs/latest/submitting-applications.html#master-urls" target="_blank" rel="noopener">master URL</a> for the cluster (e.g. <code>spark://23.195.26.187:7077</code>)</li>
<li><code>--deploy-mode</code>: Whether to deploy your driver on the worker nodes (<code>cluster</code>) or locally as an external client (<code>client</code>) (default: <code>client</code>) <strong>†</strong></li>
<li><code>--conf</code>: Arbitrary Spark configuration property in key=value format. For values that contain spaces wrap “key=value” in quotes (as shown). Multiple configurations should be passed as separate arguments. (e.g. <code>--conf &lt;key&gt;=&lt;value&gt; --conf &lt;key2&gt;=&lt;value2&gt;</code>)</li>
<li><code>application-jar</code>: Path to a bundled jar including your application and all dependencies. The URL must be globally visible inside of your cluster, for instance, an <code>hdfs://</code> path or a <code>file://</code> path that is present on all nodes.</li>
<li><code>application-arguments</code>: Arguments passed to the main method of your main class.</li>
</ul>
<figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Run application locally on 8 cores</span></span><br><span class="line">./bin/spark-submit \</span><br><span class="line">  --class org.apache.spark.examples.SparkPi \</span><br><span class="line">  --master local[8] \</span><br><span class="line">  /path/to/examples.jar \</span><br><span class="line">  100</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Run on a Spark standalone cluster <span class="keyword">in</span> client deploy mode</span></span><br><span class="line">./bin/spark-submit \</span><br><span class="line">  --class org.apache.spark.examples.SparkPi \</span><br><span class="line">  --master spark://207.184.161.138:7077 \</span><br><span class="line">  --executor-memory 20G \</span><br><span class="line">  --total-executor-cores 100 \</span><br><span class="line">  /path/to/examples.jar \</span><br><span class="line">  1000</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Run on a Spark standalone cluster <span class="keyword">in</span> cluster deploy mode with supervise</span></span><br><span class="line">./bin/spark-submit \</span><br><span class="line">  --class org.apache.spark.examples.SparkPi \</span><br><span class="line">  --master spark://207.184.161.138:7077 \</span><br><span class="line">  --deploy-mode cluster \</span><br><span class="line">  --supervise \</span><br><span class="line">  --executor-memory 20G \</span><br><span class="line">  --total-executor-cores 100 \</span><br><span class="line">  /path/to/examples.jar \</span><br><span class="line">  1000</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Run on a YARN cluster</span></span><br><span class="line">export HADOOP_CONF_DIR=XXX</span><br><span class="line">./bin/spark-submit \</span><br><span class="line">  --class org.apache.spark.examples.SparkPi \</span><br><span class="line">  --master yarn \</span><br><span class="line">  --deploy-mode cluster \  # can be client for client mode</span><br><span class="line">  --executor-memory 20G \</span><br><span class="line">  --num-executors 50 \</span><br><span class="line">  /path/to/examples.jar \</span><br><span class="line">  1000</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Run a Python application on a Spark standalone cluster</span></span><br><span class="line">./bin/spark-submit \</span><br><span class="line">  --master spark://207.184.161.138:7077 \</span><br><span class="line">  examples/src/main/python/pi.py \</span><br><span class="line">  1000</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Run on a Mesos cluster <span class="keyword">in</span> cluster deploy mode with supervise</span></span><br><span class="line">./bin/spark-submit \</span><br><span class="line">  --class org.apache.spark.examples.SparkPi \</span><br><span class="line">  --master mesos://207.184.161.138:7077 \</span><br><span class="line">  --deploy-mode cluster \</span><br><span class="line">  --supervise \</span><br><span class="line">  --executor-memory 20G \</span><br><span class="line">  --total-executor-cores 100 \</span><br><span class="line">  http://path/to/examples.jar \</span><br><span class="line">  1000</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Run on a Kubernetes cluster <span class="keyword">in</span> cluster deploy mode</span></span><br><span class="line">./bin/spark-submit \</span><br><span class="line">  --class org.apache.spark.examples.SparkPi \</span><br><span class="line">  --master k8s://xx.yy.zz.ww:443 \</span><br><span class="line">  --deploy-mode cluster \</span><br><span class="line">  --executor-memory 20G \</span><br><span class="line">  --num-executors 50 \</span><br><span class="line">  http://path/to/examples.jar \</span><br><span class="line">  1000</span><br></pre></td></tr></tbody></table></figure>

<h3 id="1-1-RDD"><a href="#1-1-RDD" class="headerlink" title="1.1. RDD"></a>1.1. RDD</h3><blockquote>
<p>支持两种类型的操作： 转化操作（transformation） 和行动操作（action）。转化操作会由一个RDD 生成一个新的RDD。行动操作是对的RDD 内容进行操作，它们会把最终求得的结果返回到驱动器程序，或者写入外部存储系统中。由于行动操作需要生成实际的输出，它们会强制执行那些求值必须用到的RDD 的转化操作。RDD的转化操作与行动操作不同，是惰性求值的，也就是在被调用行动操作之前Spark 不会开始计算。同样创建操作也是一样，数据并没有被立刻读取到内存中，只是记录了读取操作需要的相关信息。我理解为这与tensorflow的网络构建类似，我们之前编写的代码只是记录了整个操作过程的计算流程图，只有当计算操作被激活时，数据才会沿着之前定义的计算图进行计算.</p>
</blockquote>
<h4 id="1-1-1-RDD-creation"><a href="#1-1-1-RDD-creation" class="headerlink" title="1.1.1. RDD creation"></a>1.1.1. RDD creation</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#---  Creating a RDD from a file</span></span><br><span class="line"><span class="keyword">import</span> urllib</span><br><span class="line">f = urllib.urlretrieve (<span class="string">"http://kdd.ics.uci.edu/databases/kddcup99/kddcup.data_10_percent.gz"</span>, <span class="string">"kddcup.data_10_percent.gz"</span>)</span><br><span class="line">data_file = <span class="string">"./kddcup.data_10_percent.gz"</span></span><br><span class="line">raw_data = sc.textFile(data_file).cache()</span><br><span class="line"><span class="comment">#---  Creating and RDD using parallelize</span></span><br><span class="line">a = range(<span class="number">100</span>)</span><br><span class="line">data = sc.parallelize(a)</span><br><span class="line"><span class="comment">#---  makeRDD 操作</span></span><br><span class="line">val rdd02 = sc.makeRDD(Array(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>))</span><br></pre></td></tr></tbody></table></figure>




<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#----  create key-value data</span></span><br><span class="line">key_value_data = csv_data.map(<span class="keyword">lambda</span> x: (x[<span class="number">41</span>], x)) <span class="comment"># x[41] contains the network interaction tag</span></span><br><span class="line">durations_by_key = key_value_duration.reduceByKey(<span class="keyword">lambda</span> x, y: x + y)</span><br><span class="line">counts_by_key = key_value_data.countByKey()</span><br><span class="line"></span><br><span class="line">head_rows = raw_data.take(<span class="number">5</span>)<span class="comment"># 查看前面5个</span></span><br><span class="line">count=raw_data.count() <span class="comment"># 查看个数</span></span><br><span class="line"><span class="comment">#-------   sample</span></span><br><span class="line">raw_data_sample = raw_data.sample(<span class="literal">False</span>, <span class="number">0.1</span>, <span class="number">1234</span>)  <span class="comment"># # whether the sampling is done with replacement, sample size as a fraction, random seed;</span></span><br><span class="line">raw_data_sample = raw_data.takeSample(<span class="literal">False</span>, <span class="number">400000</span>, <span class="number">1234</span>)<span class="comment">#grab a sample of raw data from our RDD into local memory, number samples</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#----     set operation</span></span><br><span class="line">attack_raw_data = raw_data.subtract(normal_raw_data)</span><br><span class="line"></span><br><span class="line">csv_data = raw_data.map(<span class="keyword">lambda</span> x: x.split(<span class="string">","</span>))</span><br><span class="line"></span><br><span class="line">normal_raw_data = raw_data.filter(<span class="keyword">lambda</span> x: <span class="string">'normal.'</span> <span class="keyword">in</span> x)  <span class="comment">#count how many normal</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_interaction</span><span class="params">(line)</span>:</span></span><br><span class="line">    elems = line.split(<span class="string">","</span>)</span><br><span class="line">    tag = elems[<span class="number">41</span>]</span><br><span class="line">    <span class="keyword">return</span> (tag, elems)</span><br><span class="line">key_csv_data = raw_data.map(parse_interaction)</span><br><span class="line"></span><br><span class="line">all_raw_data = raw_data.collect()<span class="comment">#get all the elements in the RDD into memory for us to work with them.</span></span><br></pre></td></tr></tbody></table></figure>

<h4 id="1-1-2-TransformOp"><a href="#1-1-2-TransformOp" class="headerlink" title="1.1.2.  TransformOp"></a>1.1.2.  TransformOp</h4><blockquote>
<p>皆产生新的 RDD,且直保存运算逻辑，依赖原始 rdd</p>
</blockquote>
<h5 id="1-map-amp-mapValues"><a href="#1-map-amp-mapValues" class="headerlink" title="1. map &amp; mapValues"></a>1. map &amp; mapValues</h5><p><code>对于每个元素</code>都应用这个func</p>
<ul>
<li>入参：<ul>
<li>func表示需要应用到每个元素的方法</li>
<li>preservesPartitioning是否保持当前分区方式，默认重新分区</li>
</ul>
</li>
<li>返回：<ul>
<li><code>返回的结果是一个RDD</code></li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">rdd = sc.parallelize([<span class="string">"b"</span>, <span class="string">"a"</span>, <span class="string">"c"</span>])</span><br><span class="line">sorted(rdd.map(<span class="keyword">lambda</span> x: (x, <span class="number">1</span>)).collect()) <span class="comment">#[('a', 1), ('b', 1), ('c', 1)]</span></span><br></pre></td></tr></tbody></table></figure>

<p><code>对键值对中每个value都应用这个func</code>，并保持key不变</p>
<ul>
<li>入参：<ul>
<li>func表示需要应用到每个元素值上的方法</li>
</ul>
</li>
<li>返回：<ul>
<li>返回的结果是一个RDD</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = sc.parallelize([(<span class="string">"a"</span>, [<span class="string">"apple"</span>, <span class="string">"banana"</span>, <span class="string">"lemon"</span>]), (<span class="string">"b"</span>, [<span class="string">"grapes"</span>])])</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(x)</span>:</span> <span class="keyword">return</span> len(x)</span><br><span class="line">x.mapValues(f).collect() <span class="comment"># [('a', 3), ('b', 1)]</span></span><br></pre></td></tr></tbody></table></figure>

<h5 id="2-flatmap"><a href="#2-flatmap" class="headerlink" title="2. flatmap"></a>2. flatmap</h5><p>遍历全部元素，将传入方法应用到每个元素上，并将<code>最后结果展平（压成一个List）</code></p>
<ul>
<li>入参：<ul>
<li>func表示需要应用到每个元素的方法</li>
<li>preservesPartitioning是否保持当前分区方式，默认重新分区</li>
</ul>
</li>
<li>返回：<ul>
<li>返回的结果是一个RDD</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">rdd = sc.parallelize([<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line">sorted(rdd.flatMap(<span class="keyword">lambda</span> x: range(<span class="number">1</span>, x)).collect()) <span class="comment">#[1, 1, 1, 2, 2, 3]</span></span><br><span class="line">sorted(rdd.flatMap(<span class="keyword">lambda</span> x: [(x, x), (x, x)]).collect()) <span class="comment">#[(2, 2), (2, 2), (3, 3), (3, 3), (4, 4), (4, 4)]</span></span><br></pre></td></tr></tbody></table></figure>

<p>遍历某个元素的元素值，将传入方法应用到每个元素值上，并将最后结果展平（压成一个List）</p>
<ul>
<li>入参：<ul>
<li>func表示需要应用到每个元素值的方法</li>
</ul>
</li>
<li>返回：<ul>
<li>返回的结果是一个RDD</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = sc.parallelize([(<span class="string">"a"</span>, [<span class="string">"x"</span>, <span class="string">"y"</span>, <span class="string">"z"</span>]), (<span class="string">"b"</span>, [<span class="string">"p"</span>, <span class="string">"r"</span>])])</span><br><span class="line">x.flatMapValues(<span class="keyword">lambda</span> val: val).collect() <span class="comment"># [('a', 'x'), ('a', 'y'), ('a', 'z'), ('b', 'p'), ('b', 'r')]</span></span><br></pre></td></tr></tbody></table></figure>

<h5 id="3-filter"><a href="#3-filter" class="headerlink" title="3. filter"></a>3. filter</h5><p>遍历全部元素，筛选符合传入方法的元素</p>
<ul>
<li>入参：<ul>
<li>func表示需要应用到每个元素的筛选方法</li>
</ul>
</li>
<li>返回：<ul>
<li>返回的结果是一个RDD</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">rdd = sc.parallelize([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>])</span><br><span class="line">rdd.filter(<span class="keyword">lambda</span> x: x % <span class="number">2</span> == <span class="number">0</span>)</span><br><span class="line">print(rdd.collect()) <span class="comment"># [2, 4]</span></span><br><span class="line">kvRDD1.fiter(<span class="keyword">lambda</span> x:x[<span class="number">0</span>]&lt;<span class="number">5</span>) 根据 key过滤</span><br><span class="line">kvRDD1.filter(<span class="keyword">lambda</span> x:x[<span class="number">1</span>]&lt;<span class="number">5</span>) 根据值过滤</span><br></pre></td></tr></tbody></table></figure>

<h5 id="4-distinct"><a href="#4-distinct" class="headerlink" title="4. distinct"></a>4. distinct</h5><p>遍历全部元素，<code>并返回包含的不同元素的总数</code></p>
<ul>
<li>入参：<ul>
<li>numPartitions表示需要将此操作分割成多少个分区</li>
</ul>
</li>
<li>返回：<ul>
<li>返回的结果是一个Int</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(rddInt.distinct().collect())</span><br></pre></td></tr></tbody></table></figure>

<h5 id="5-cartesian"><a href="#5-cartesian" class="headerlink" title="5. cartesian"></a>5. cartesian</h5><p>返回自己与传入rdd的笛卡尔积</p>
<ul>
<li>入参：<ul>
<li>rdd表示一个rdd对象，可以存储不同数据类型 RDD</li>
</ul>
</li>
<li>返回：<ul>
<li>返回的结果是一个RDD</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">num_rdd = sc.parallelize([<span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line">str_rdd = sc.parallelize([<span class="string">'a'</span>, <span class="string">'y'</span>])</span><br><span class="line">result = num_rdd.cartesian(str_rdd)</span><br><span class="line">print(result.collect()) <span class="comment"># [(1, 'a'), (1, 'y'), (2, 'a'), (2, 'y')]</span></span><br></pre></td></tr></tbody></table></figure>

<h4 id="1-1-3-ActionOp"><a href="#1-1-3-ActionOp" class="headerlink" title="1.1.3. ActionOp"></a>1.1.3. ActionOp</h4><h5 id="1-collect"><a href="#1-collect" class="headerlink" title="1. collect"></a>1. collect</h5><p>将数据以List取回本地<br><a href="https://spark.apache.org/docs/latest/api/python/pyspark.html" target="_blank" rel="noopener">官网</a>提示，建议只在任务结束时在调用collect方法.</p>
<ul>
<li>返回：<ul>
<li>返回的结果是一个List</li>
</ul>
</li>
</ul>
<h5 id="2-count-amp-amp-take-num"><a href="#2-count-amp-amp-take-num" class="headerlink" title="2. count&amp;&amp;take(num)"></a>2. count&amp;&amp;take(num)</h5><h5 id="3-countByValue-amp-countByKey"><a href="#3-countByValue-amp-countByKey" class="headerlink" title="3.  countByValue&amp; countByKey"></a>3.  countByValue&amp; countByKey</h5><p>返回<code>每个key对应的元素数量</code></p>
<ul>
<li>返回：<ul>
<li>返回的结果是一个Dict</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">rdd = sc.parallelize([(<span class="string">"a"</span>, <span class="number">1</span>), (<span class="string">"b"</span>, <span class="number">1</span>), (<span class="string">"a"</span>, <span class="number">1</span>)])</span><br><span class="line">print(rdd.countByKey()) <span class="comment"># defaultdict(&lt;class 'int'&gt;, {'a': 2, 'b': 1})</span></span><br></pre></td></tr></tbody></table></figure>

<p>返回<code>每个value出现的次数</code></p>
<ul>
<li>返回：<ul>
<li>返回的结果是一个Dict</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">rdd2 = sc.parallelize([<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>], <span class="number">2</span>)</span><br><span class="line">print(rdd2.countByValue())  <span class="comment"># defaultdict(&lt;class 'int'&gt;, {1: 2, 2: 3})</span></span><br></pre></td></tr></tbody></table></figure>

<h5 id="4-reduce"><a href="#4-reduce" class="headerlink" title="4. reduce"></a>4. reduce</h5><p>对于每个元素值都应用这个func</p>
<ul>
<li>入参：<ul>
<li>func表示需要应用到每个元素的方法</li>
</ul>
</li>
<li>返回：<ul>
<li>返回的结果是一个Python obj, 与元素值得数据类型一致</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x = sc.parallelize([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">y = x.reduce(<span class="keyword">lambda</span> a, b : a + b )</span><br><span class="line">print(x.collect()) <span class="comment"># [1, 2, 3]</span></span><br><span class="line">print(y) <span class="comment"># 6</span></span><br></pre></td></tr></tbody></table></figure>

<h5 id="5-fold"><a href="#5-fold" class="headerlink" title="5. fold"></a>5. fold</h5><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 和reduce() 一样， 但是需要提供初始值</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>rdd = sc.parallelize([<span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">5</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>rdd.fold(<span class="number">0</span>, <span class="keyword">lambda</span> x, y: x + y)</span><br><span class="line"><span class="number">21</span></span><br></pre></td></tr></tbody></table></figure>

<h5 id="6-aggregate"><a href="#6-aggregate" class="headerlink" title="6. aggregate"></a>6. aggregate</h5><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 和reduce() 相似， 但是通常返回不同类型的函数</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>seqOp = (<span class="keyword">lambda</span> x, y: (x[<span class="number">0</span>] + y, x[<span class="number">1</span>] + <span class="number">1</span>))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>combOp = (<span class="keyword">lambda</span> x, y: (x[<span class="number">0</span>] + y[<span class="number">0</span>], x[<span class="number">1</span>] + y[<span class="number">1</span>]))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>sc.parallelize([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>]).aggregate((<span class="number">0</span>, <span class="number">0</span>), seqOp, combOp)</span><br><span class="line">(<span class="number">10</span>, <span class="number">4</span>)</span><br></pre></td></tr></tbody></table></figure>

<h5 id="7-foreach"><a href="#7-foreach" class="headerlink" title="7. foreach"></a>7. foreach</h5><p>用于遍历RDD中的元素,将函数func应用于每一个元素。</p>
<ul>
<li>入参：<ul>
<li>func表示需要应用到每个元素的方法, 但这个方法不会在客户端执行</li>
</ul>
</li>
<li>返回：<ul>
<li>返回的结果是一个RDD</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(x)</span>:</span> print(x)</span><br><span class="line">sc.parallelize([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]).foreach(f)</span><br></pre></td></tr></tbody></table></figure>

<h3 id="1-2-SQL-amp-DataFrames"><a href="#1-2-SQL-amp-DataFrames" class="headerlink" title="1.2. SQL&amp;DataFrames"></a>1.2. SQL&amp;DataFrames</h3><blockquote>
<ul>
<li><strong>Use of Input Optimization Engine</strong>: DataFrames <code>make use of the input optimization engines</code>, e.g., <strong>Catalyst Optimizer</strong>, to process data efficiently. We can use the same engine for all Python, Java, Scala, and R DataFrame APIs.</li>
<li><strong>Handling of Structured Data</strong>: DataFrames provide a<code>schematic view of data</code>. Here, the data has some meaning to it when it is being stored.</li>
<li><strong>Custom Memory Management</strong>: In <code>RDDs, the data is stored in memory</code>, whereas <code>DataFrames store data off-heap</code> (outside the main Java Heap space, but still inside RAM), which in turn reduces the garbage collection overload.</li>
<li><strong>Flexibility</strong>: <code>DataFrames, like RDDs</code>, can support various formats of data, such as CSV, <a href="https://intellipaat.com/blog/apache-cassandra-a-brief-intro/" target="_blank" rel="noopener">Cassandra</a>, etc.</li>
<li><strong>Scalability</strong>: <code>DataFrames can be integrated with various other [Big Data tools](https://intellipaat.com/blog/big-data-analytics-tools-performance-testing/),</code> and they allow processing megabytes to petabytes of data at once.</li>
</ul>
</blockquote>
<ul>
<li>pyspark.sql.SQLContext： DataFrame和SQL方法的主入口</li>
<li>pyspark.sql.DataFrame： 将分布式数据集分组到指定列名的数据框中</li>
<li>pyspark.sql.Column ：DataFrame中的列<code>Row(name="Alice", age=11).asDict() == {'name': 'Alice', 'age': 11}</code></li>
<li>pyspark.sql.Row： DataFrame数据的行</li>
<li>pyspark.sql.HiveContext： 访问Hive数据的主入口</li>
<li>pyspark.sql.GroupedData： 由DataFrame.groupBy()创建的聚合方法集</li>
<li>pyspark.sql.DataFrameNaFunctions： 处理丢失数据(空数据)的方法</li>
<li>pyspark.sql.DataFrameStatFunctions： 统计功能的方法<br> -pyspark.sql.functions DataFrame：可用的内置函数</li>
<li>pyspark.sql.types： 可用的数据类型列表</li>
<li>pyspark.sql.Window： 用于处理窗口函数</li>
</ul>
<h5 id="1-creation"><a href="#1-creation" class="headerlink" title="1. creation"></a>1. creation</h5><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#--- create from csv</span></span><br><span class="line">fifa_df = spark.read.csv(<span class="string">"path-of-file/fifa_players.csv"</span>, inferSchema = <span class="literal">True</span>, header = <span class="literal">True</span>)</span><br><span class="line"><span class="comment">#--- create from json</span></span><br><span class="line">val jsondata=spark.read.json(<span class="string">"file.json"</span>)</span><br><span class="line"><span class="comment">#--- create from exitRDD</span></span><br><span class="line">df=spark.createDataFrame(rdd).toDF(<span class="string">"key"</span>,<span class="string">"cube"</span>)</span><br><span class="line"><span class="comment">#--- create from dict</span></span><br><span class="line">df = spark.createDataFrame([{<span class="string">'name'</span>:<span class="string">'Alice'</span>,<span class="string">'age'</span>:<span class="number">1</span>},</span><br><span class="line">    {<span class="string">'name'</span>:<span class="string">'Polo'</span>,<span class="string">'age'</span>:<span class="number">1</span>}]) </span><br><span class="line"><span class="comment">#--- create from schema</span></span><br><span class="line">schema = StructType([</span><br><span class="line">    StructField(<span class="string">"id"</span>, LongType(), <span class="literal">True</span>),   </span><br><span class="line">    StructField(<span class="string">"name"</span>, StringType(), <span class="literal">True</span>),</span><br><span class="line">    StructField(<span class="string">"age"</span>, LongType(), <span class="literal">True</span>),</span><br><span class="line">    StructField(<span class="string">"eyeColor"</span>, StringType(), <span class="literal">True</span>)</span><br><span class="line">])</span><br><span class="line">df = spark.createDataFrame(csvRDD, schema)</span><br><span class="line"><span class="comment">#--- create from pandas</span></span><br><span class="line">colors = [<span class="string">'white'</span>,<span class="string">'green'</span>,<span class="string">'yellow'</span>,<span class="string">'red'</span>,<span class="string">'brown'</span>,<span class="string">'pink'</span>]</span><br><span class="line">color_df=pd.DataFrame(colors,columns=[<span class="string">'color'</span>])</span><br><span class="line">color_df[<span class="string">'length'</span>]=color_df[<span class="string">'color'</span>].apply(len)</span><br><span class="line"></span><br><span class="line">color_df=spark.createDataFrame(color_df)</span><br><span class="line">color_df.show()</span><br></pre></td></tr></tbody></table></figure>

<h5 id="2-show"><a href="#2-show" class="headerlink" title="2. show"></a>2. show</h5><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">df.printSchema()</span><br><span class="line">df.columns()</span><br><span class="line">df.count()</span><br><span class="line">df.describe(<span class="string">'column name'</span>).show() <span class="comment"># look at the summary of particular column</span></span><br><span class="line">df.dtypes <span class="comment">#将所有列名称及其数据类型作为列表返回。</span></span><br><span class="line"><span class="comment"># 查找每列出现次数占总的30%以上频繁项目</span></span><br><span class="line">df.stat.freqItems([<span class="string">"id"</span>, <span class="string">"gender"</span>], <span class="number">0.3</span>).show()</span><br><span class="line"><span class="comment"># ----  缺失值</span></span><br><span class="line"><span class="comment"># 计算每列空值数目</span></span><br><span class="line"><span class="keyword">for</span> col <span class="keyword">in</span> df.columns:</span><br><span class="line">    print(col, <span class="string">"\t"</span>, <span class="string">"with null values: "</span>, </span><br><span class="line">          df.filter(df[col].isNull()).count())</span><br></pre></td></tr></tbody></table></figure>

<h5 id="3-column-select"><a href="#3-column-select" class="headerlink" title="3. column select"></a>3. column select</h5><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">df.select(<span class="string">'column name'</span>,<span class="string">'name2'</span>).show()</span><br><span class="line">color_df.filter(color_df[<span class="string">'length'</span>]&gt;=<span class="number">4</span>).show()   <span class="comment"># filter方法</span></span><br><span class="line"><span class="comment"># 返回具有新指定列名的DataFrame</span></span><br><span class="line">df.toDF(<span class="string">'f1'</span>, <span class="string">'f2'</span>)</span><br><span class="line"></span><br><span class="line">first_row = df.head()</span><br><span class="line"><span class="comment"># Row(address=Row(city='Nanjing', country='China'), age=12, name='Li')</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 读取行内某一列的属性值</span></span><br><span class="line">first_row[<span class="string">'age'</span>]           <span class="comment"># 12</span></span><br><span class="line">first_row.age              <span class="comment"># 12</span></span><br><span class="line">getattr(first_row, <span class="string">'age'</span>)  <span class="comment"># 12</span></span><br><span class="line">first_row.address</span><br><span class="line"><span class="comment"># Row(city='Nanjing', country='China')</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># -------------- column -----------------------</span></span><br><span class="line"></span><br><span class="line">first_col = df[<span class="number">0</span>]</span><br><span class="line">first_col = df[<span class="string">'adress'</span>]</span><br><span class="line"><span class="comment"># Column&lt;b'address'&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># copy column[s]</span></span><br><span class="line">address_copy = first_col.alias(<span class="string">'address_copy'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># rename column / create new column</span></span><br><span class="line">df.withColumnRenamed(<span class="string">'age'</span>, <span class="string">'birth_age'</span>)</span><br><span class="line">df.withColumn(<span class="string">'age_copy'</span>, df[<span class="string">'age'</span>]).show(<span class="number">1</span>)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">+----------------+---+----+--------+</span></span><br><span class="line"><span class="string">|         address|age|name|age_copy|</span></span><br><span class="line"><span class="string">+----------------+---+----+--------+</span></span><br><span class="line"><span class="string">|[Nanjing, China]| 12|  Li|      12|</span></span><br><span class="line"><span class="string">+----------------+---+----+--------+</span></span><br><span class="line"><span class="string">only showing top 1 row</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line">df.withColumn(<span class="string">'age_over_18'</span>,df[<span class="string">'age'</span>] &gt; <span class="number">18</span>).show(<span class="number">1</span>)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">+----------------+---+----+-----------+</span></span><br><span class="line"><span class="string">|         address|age|name|age_over_18|</span></span><br><span class="line"><span class="string">+----------------+---+----+-----------+</span></span><br><span class="line"><span class="string">|[Nanjing, China]| 12|  Li|      false|</span></span><br><span class="line"><span class="string">+----------------+---+----+-----------+</span></span><br><span class="line"><span class="string">only showing top 1 row</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></tbody></table></figure>

<h5 id="4-filter"><a href="#4-filter" class="headerlink" title="4.  filter"></a>4.  filter</h5><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.filter(df.MathchID==<span class="string">'1111'</span>).show()</span><br></pre></td></tr></tbody></table></figure>

<h5 id="5-sort"><a href="#5-sort" class="headerlink" title="5. sort"></a>5. sort</h5><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df.orderBy(df.column)  <span class="comment"># default ascending</span></span><br><span class="line">color_df.sort(<span class="string">'column'</span>,ascending=<span class="literal">False</span>).show()</span><br></pre></td></tr></tbody></table></figure>

<h5 id="6-group"><a href="#6-group" class="headerlink" title="6. group"></a>6. group</h5><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.goupby(<span class="string">'columnname'</span>).count().show()</span><br></pre></td></tr></tbody></table></figure>

<p><img src="https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20201119164459904.png" alt=""></p>
<h5 id="7-SQL"><a href="#7-SQL" class="headerlink" title="7. SQL"></a>7. SQL</h5><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">df.registerTempTable(<span class="string">'tablename'</span>) <span class="comment"># 注册表</span></span><br><span class="line">sqlContext.sql(<span class="string">'select * from tablename'</span>)</span><br><span class="line"><span class="comment">#  方式二：</span></span><br><span class="line">df.createOrReplaceTempView(tablename)</span><br><span class="line">result=spark.sql(<span class="string">"sql sentences"</span>)</span><br><span class="line"><span class="comment">#方式三 sql function</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> functions <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> datetime <span class="keyword">as</span> dt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 装饰器使用</span></span><br><span class="line"><span class="meta">@F.udf()</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calculate_birth_year</span><span class="params">(age)</span>:</span></span><br><span class="line">    this_year = dt.datetime.today().year</span><br><span class="line">    birth_year = this_year - age</span><br><span class="line">    <span class="keyword">return</span> birth_year </span><br><span class="line"></span><br><span class="line">calculated_df = df.select(<span class="string">"*"</span>, calculate_birth_year(<span class="string">'age'</span>).alias(<span class="string">'birth_year'</span>))</span><br><span class="line">calculated_df .show(<span class="number">2</span>)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">+------------------+---+-------+----------+</span></span><br><span class="line"><span class="string">|           address|age|   name|birth_year|</span></span><br><span class="line"><span class="string">+------------------+---+-------+----------+</span></span><br><span class="line"><span class="string">|  [Nanjing, China]| 12|     Li|      2008|</span></span><br><span class="line"><span class="string">|[Los Angeles, USA]| 14|Richard|      2006|</span></span><br><span class="line"><span class="string">+------------------+---+-------+----------+</span></span><br><span class="line"><span class="string">only showing top 2 rows</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></tbody></table></figure>

<h5 id="8-drop"><a href="#8-drop" class="headerlink" title="8. drop"></a>8. drop</h5><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">df.drop(<span class="string">'column name'</span>)<span class="comment">#：cols - 要删除的列的字符串名称，要删除的列或要删除的列的字符串名称的列表。</span></span><br><span class="line"><span class="comment">#新增一列</span></span><br><span class="line">df.withColumn(colName, col)</span><br><span class="line"><span class="comment">#通过为原数据框添加一个新列或替换已存在的同名列而返回一个新数据框。colName —— 是一个字符串, 为新列的名字。必须是已存在的列的名字</span></span><br><span class="line"><span class="comment">#col —— 为这个新列的 Column 表达式。必须是含有列的表达式。如果不是它会报错 AssertionError: col should be Column</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 重新命名聚合后结果的列名(需要修改多个列名就跟多个：withColumnRenamed)</span></span><br><span class="line"><span class="comment"># 聚合之后不修改列名则会显示：count(member_name)</span></span><br><span class="line">df_res.agg({<span class="string">'member_name'</span>: <span class="string">'count'</span>, <span class="string">'income'</span>: <span class="string">'sum'</span>, <span class="string">'num'</span>: <span class="string">'sum'</span>})</span><br><span class="line">      .withColumnRenamed(<span class="string">"count(member_name)"</span>, <span class="string">"member_num"</span>).show()</span><br><span class="line">    </span><br><span class="line"><span class="comment">#修改数据类型</span></span><br><span class="line">df = df.withColumn(<span class="string">"height"</span>, df[<span class="string">"height"</span>].cast(IntegerType()))</span><br></pre></td></tr></tbody></table></figure>

<h5 id="9-collect"><a href="#9-collect" class="headerlink" title="9. collect"></a>9. collect</h5><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201119220742676.png" alt=""></p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">print(data1[<span class="number">0</span>][<span class="string">"words"</span>])</span><br><span class="line"></span><br><span class="line">df.collect() <span class="comment">#Row列表形式返回所有记录。</span></span><br><span class="line"><span class="comment"># 在只有一列的情况下可以用 [0] 来获取值</span></span><br><span class="line"><span class="comment"># 获取一列的所有值，或者多列的所有值</span></span><br><span class="line"><span class="comment"># collect()函数将分布式的dataframe转成local类型的 list-row格式</span></span><br><span class="line">rows= df.select(<span class="string">'col_1'</span>, <span class="string">'col_2'</span>).collect()</span><br><span class="line">value = [[ row.col_1, row.col_2 ] <span class="keyword">for</span> row <span class="keyword">in</span> rows ]</span><br><span class="line"><span class="comment">#获取第一行的多个值，返回普通python变量</span></span><br><span class="line"><span class="comment"># first() 返回的是 Row 类型，可以看做是dict类型，用 row.col_name 来获取值</span></span><br><span class="line">row = df.select(<span class="string">'col_1'</span>, <span class="string">'col_2'</span>).first()</span><br><span class="line">col_1_value = row.col_1</span><br><span class="line">col_2_value = row.col_2</span><br></pre></td></tr></tbody></table></figure>

<h5 id="10-RDD-amp-DF-amp-Pandas"><a href="#10-RDD-amp-DF-amp-Pandas" class="headerlink" title="10. RDD &amp;DF&amp;Pandas"></a>10. RDD &amp;DF&amp;Pandas</h5><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">rdd_df = df.rdd	  <span class="comment"># DF转RDD</span></span><br><span class="line">df = rdd_df.toDF()  <span class="comment"># RDD转DF</span></span><br><span class="line">pandas_df = spark_df.toPandas()	</span><br><span class="line">spark_df = sqlContext.createDataFrame(pandas_df)</span><br></pre></td></tr></tbody></table></figure>

<h5 id="11-UDF"><a href="#11-UDF" class="headerlink" title="11. UDF"></a>11. UDF</h5><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#udf(f=None, returnType=StringType)</span></span><br><span class="line"><span class="comment">#Parameters：</span></span><br><span class="line"><span class="comment">#  f – python函数（如果用作独立函数）</span></span><br><span class="line"><span class="comment">#  returnType – 用户定义函数的返回类型。</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> udf</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> StructType, StructField, IntegerType, FloatType, StringType, ArrayType</span><br><span class="line">stopwords = [k.strip() <span class="keyword">for</span> k <span class="keyword">in</span> open(<span class="string">'./data/stopwords.txt'</span>, encoding=<span class="string">'utf-8'</span>) <span class="keyword">if</span> k.strip() != <span class="string">''</span>]</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">clearTxt</span><span class="params">(line)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> line != <span class="string">''</span>:</span><br><span class="line">        line = line.strip()</span><br><span class="line">        <span class="comment">#去除文本中的英文和数字</span></span><br><span class="line">        line = re.sub(<span class="string">"[a-zA-Z0-9]"</span>,<span class="string">""</span>,line)</span><br><span class="line">         </span><br><span class="line">        <span class="comment">#去除文本中的中文符号和英文符号</span></span><br><span class="line">        line = re.sub(<span class="string">"[\s+\.\!\/_,$%^*(+\"\'；：“”．]+|[+——！，。？?、~@#￥%……&amp;*（）]+"</span>, <span class="string">""</span>, line)</span><br><span class="line">        <span class="keyword">return</span> line</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">'Empyt Line'</span></span><br><span class="line">cutWords_list=[]</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cutwordshandle</span><span class="params">(jobinfo)</span>:</span></span><br><span class="line">    jobinfo=clearTxt(jobinfo)</span><br><span class="line">    cutWords = [k <span class="keyword">for</span> k <span class="keyword">in</span> jieba.cut(jobinfo,cut_all=<span class="literal">False</span>) <span class="keyword">if</span> k <span class="keyword">not</span> <span class="keyword">in</span> stopwords]</span><br><span class="line">    <span class="comment">#print(len(cutWords_list))</span></span><br><span class="line">    cutWords_list.append(cutWords)</span><br><span class="line">    <span class="keyword">return</span> cutWords</span><br><span class="line"></span><br><span class="line">cutwords = udf(<span class="keyword">lambda</span> z: cutwordshandle(z), ArrayType(StringType()))</span><br><span class="line">sqlcontext.udf.register(<span class="string">"label"</span>, cutwords)</span><br><span class="line">df3 = df2.withColumn( <span class="string">'words'</span>,cutwords(<span class="string">'待遇'</span>))</span><br></pre></td></tr></tbody></table></figure>



<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SQLContext</span><br><span class="line">sqlContext = SQLContext(sc)</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> Row</span><br><span class="line">csv_data = raw_data.map(<span class="keyword">lambda</span> l: l.split(<span class="string">","</span>))</span><br><span class="line">row_data = csv_data.map(<span class="keyword">lambda</span> p: Row(</span><br><span class="line">    duration=int(p[<span class="number">0</span>]), </span><br><span class="line">    protocol_type=p[<span class="number">1</span>],</span><br><span class="line">    service=p[<span class="number">2</span>],</span><br><span class="line">    flag=p[<span class="number">3</span>],</span><br><span class="line">    src_bytes=int(p[<span class="number">4</span>]),</span><br><span class="line">    dst_bytes=int(p[<span class="number">5</span>])</span><br><span class="line">    )</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">interactions_df = sqlContext.createDataFrame(row_data)</span><br><span class="line">interactions_df.registerTempTable(<span class="string">"interactions"</span>)</span><br><span class="line"><span class="comment"># Select tcp network interactions with more than 1 second duration and no transfer from destination</span></span><br><span class="line">tcp_interactions = sqlContext.sql(<span class="string">"""</span></span><br><span class="line"><span class="string">    SELECT duration, dst_bytes FROM interactions WHERE protocol_type = 'tcp' AND duration &gt; 1000 AND dst_bytes = 0</span></span><br><span class="line"><span class="string">"""</span>)</span><br><span class="line">tcp_interactions.show()</span><br><span class="line"><span class="comment"># Output duration together with dst_bytes</span></span><br><span class="line">tcp_interactions_out = tcp_interactions.map(<span class="keyword">lambda</span> p: <span class="string">"Duration: {}, Dest. bytes: {}"</span>.format(p.duration, p.dst_bytes))</span><br><span class="line"><span class="keyword">for</span> ti_out <span class="keyword">in</span> tcp_interactions_out.collect():</span><br><span class="line">  <span class="keyword">print</span> ti_out</span><br><span class="line">interactions_df.printSchema()  <span class="comment"># printdata schema</span></span><br><span class="line"><span class="comment">#---   dataframe query</span></span><br><span class="line">interactions_df.select(<span class="string">"protocol_type"</span>, <span class="string">"duration"</span>, <span class="string">"dst_bytes"</span>).filter(interactions_df.duration&gt;<span class="number">1000</span>).filter(interactions_df.dst_bytes==<span class="number">0</span>).groupBy(<span class="string">"protocol_type"</span>).count().show()</span><br></pre></td></tr></tbody></table></figure>

<h5 id="11-1-Pandas-UDF"><a href="#11-1-Pandas-UDF" class="headerlink" title="11.1. Pandas UDF"></a>11.1. Pandas UDF</h5><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> udf</span><br><span class="line"><span class="comment"># 使用 udf 定义一个 row-at-a-time 的 udf</span></span><br><span class="line"><span class="meta">@udf('double')</span></span><br><span class="line"><span class="comment"># 输入/输出都是单个 double 类型的值</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plus_one</span><span class="params">(v)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> v + <span class="number">1</span></span><br><span class="line">df.withColumn(<span class="string">'v2'</span>, plus_one(df.v))</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> pandas_udf, PandasUDFType</span><br><span class="line"><span class="comment"># 使用 pandas_udf 定义一个 Pandas UDF</span></span><br><span class="line"><span class="meta">@pandas_udf('double', PandasUDFType.SCALAR)</span></span><br><span class="line"><span class="comment"># 输入/输出都是 double 类型的 pandas.Series</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pandas_plus_one</span><span class="params">(v)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> v + <span class="number">1</span></span><br><span class="line">df.withColumn(<span class="string">'v2'</span>, pandas_plus_one(df.v))</span><br><span class="line"></span><br><span class="line"><span class="comment">#最小二乘法举例</span></span><br><span class="line"><span class="keyword">import</span> statsmodels.api <span class="keyword">as</span> sm</span><br><span class="line"><span class="comment"># df has four columns: id, y, x1, x2</span></span><br><span class="line">group_column = <span class="string">'id'</span></span><br><span class="line">y_column = <span class="string">'y'</span></span><br><span class="line">x_columns = [<span class="string">'x1'</span>, <span class="string">'x2'</span>]</span><br><span class="line">schema = df.select(group_column, *x_columns).schema</span><br><span class="line"><span class="meta">@pandas_udf(schema, PandasUDFType.GROUPED_MAP)</span></span><br><span class="line"><span class="comment"># Input/output are both a pandas.DataFrame</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ols</span><span class="params">(pdf)</span>:</span></span><br><span class="line">    group_key = pdf[group_column].iloc[<span class="number">0</span>]</span><br><span class="line">    y = pdf[y_column]</span><br><span class="line">    X = pdf[x_columns]</span><br><span class="line">      X = sm.add_constant(X)</span><br><span class="line">    model = sm.OLS(y, X).fit()</span><br><span class="line">    <span class="keyword">return</span> pd.DataFrame([[group_key] + [model.params[i] <span class="keyword">for</span> i <span class="keyword">in</span>   x_columns]], columns=[group_column] + x_columns)</span><br><span class="line">beta = df.groupby(group_column).apply(ols)</span><br></pre></td></tr></tbody></table></figure>



<h5 id="12-schema"><a href="#12-schema" class="headerlink" title="12. schema"></a>12. schema</h5><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> StructField, MapType, StringType, IntegerType, StructType</span><br><span class="line"><span class="comment"># 常用的还包括 DateType 等</span></span><br><span class="line"></span><br><span class="line">people_schema= StructType([</span><br><span class="line">    StructField(<span class="string">'address'</span>, MapType(StringType(), StringType()), <span class="literal">True</span>),</span><br><span class="line">    StructField(<span class="string">'age'</span>, LongType(), <span class="literal">True</span>),</span><br><span class="line">    StructField(<span class="string">'name'</span>, StringType(), <span class="literal">True</span>),</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">df = spark.read.json(<span class="string">'people.json'</span>, schema=people_schema)</span><br><span class="line"></span><br><span class="line">df.show(<span class="number">1</span>)</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">+--------------------+---+----+</span></span><br><span class="line"><span class="string">|             address|age|name|</span></span><br><span class="line"><span class="string">+--------------------+---+----+</span></span><br><span class="line"><span class="string">|[country -&gt; China...| 12|  Li|</span></span><br><span class="line"><span class="string">+--------------------+---+----+</span></span><br><span class="line"><span class="string">only showing top 1 row</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line">df.dtypes</span><br><span class="line"><span class="comment"># [('address', 'map&lt;string,string&gt;'), ('age', 'bigint'), ('name', 'string')]</span></span><br></pre></td></tr></tbody></table></figure>

<h3 id="1-3-RDD-amp-DataFrame-amp-Dataset"><a href="#1-3-RDD-amp-DataFrame-amp-Dataset" class="headerlink" title="1.3.  RDD&amp;DataFrame&amp;Dataset"></a>1.3.  RDD&amp;DataFrame&amp;Dataset</h3><table>
<thead>
<tr>
<th><strong>Basis of Difference</strong></th>
<th><strong>Spark RDD</strong></th>
<th><strong>Spark DataFrame</strong></th>
<th><strong>Spark Dataset</strong></th>
</tr>
</thead>
<tbody><tr>
<td><strong>What is it?</strong></td>
<td>A low-level API</td>
<td>A high-level abstraction</td>
<td>A <code>combination</code> of both RDDs and DataFrames</td>
</tr>
<tr>
<td><strong>Input Optimization Engine</strong></td>
<td><code>Cannot make use of input optimization engines</code></td>
<td>Uses input optimization engines to generate logical queries</td>
<td>Uses Catalyst Optimizer for input optimization, as DataFrames do</td>
</tr>
<tr>
<td><strong>Data Representation</strong></td>
<td><code>Distributed across multiple nodes of a cluster</code></td>
<td><code>A collection of rows and named columns</code></td>
<td>An extension of DataFrames, providing the functionalities of both RDDs and DataFrames</td>
</tr>
<tr>
<td><strong>Benefit</strong></td>
<td>A simple API</td>
<td>Gives a schema for the distributed data</td>
<td><code>Improves memory usage</code></td>
</tr>
<tr>
<td><strong>Immutability and Interoperability</strong></td>
<td>Tracks data lineage information to recover the lost data</td>
<td>Once transformed into a DataFrame, not possible to get the domain object</td>
<td>Can regenerate RDDs</td>
</tr>
<tr>
<td><strong>Performance Limitation</strong></td>
<td>Java Serialization and Garbage Collection overheads</td>
<td>Offers huge performance improvement over RDDs</td>
<td>Operations are performed on serialized data to improve performance</td>
</tr>
</tbody></table>
<h3 id="1-4-Summary-statistics"><a href="#1-4-Summary-statistics" class="headerlink" title="1.4. Summary statistics"></a>1.4. Summary statistics</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.mllib.stat <span class="keyword">import</span> Statistics </span><br><span class="line"><span class="keyword">from</span> math <span class="keyword">import</span> sqrt </span><br><span class="line"></span><br><span class="line"><span class="comment"># Compute column summary statistics.</span></span><br><span class="line">summary = Statistics.colStats(vector_data)</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> <span class="string">"Duration Statistics:"</span></span><br><span class="line"><span class="keyword">print</span> <span class="string">" Mean: {}"</span>.format(round(summary.mean()[<span class="number">0</span>],<span class="number">3</span>))</span><br><span class="line"><span class="keyword">print</span> <span class="string">" St. deviation: {}"</span>.format(round(sqrt(summary.variance()[<span class="number">0</span>]),<span class="number">3</span>))</span><br><span class="line"><span class="keyword">print</span> <span class="string">" Max value: {}"</span>.format(round(summary.max()[<span class="number">0</span>],<span class="number">3</span>))</span><br><span class="line"><span class="keyword">print</span> <span class="string">" Min value: {}"</span>.format(round(summary.min()[<span class="number">0</span>],<span class="number">3</span>))</span><br><span class="line"><span class="keyword">print</span> <span class="string">" Total value count: {}"</span>.format(summary.count())</span><br><span class="line"><span class="keyword">print</span> <span class="string">" Number of non-zero values: {}"</span>.format(summary.numNonzeros()[<span class="number">0</span>])</span><br></pre></td></tr></tbody></table></figure>

<h3 id="1-4-ML"><a href="#1-4-ML" class="headerlink" title="1. 4. ML"></a>1. 4. ML</h3><ul>
<li><strong>mllib.classification</strong>: The spark.mllib package offers support for various methods to perform binary classification, regression analysis, and multiclass classification. Some of the most used algorithms in classifications are Naive Bayes, decision trees, etc.</li>
<li><strong>mllib.clustering</strong>: In clustering, you can perform the grouping of subsets of entities on the basis of some similarities in the elements or entities.</li>
<li><strong>mllib.linalg</strong>: This algorithm offers MLlib utilities to support linear algebra.</li>
<li><strong>mllib.recommendation</strong>: This algorithm is used for recommender systems to fill in the missing entries in any dataset.</li>
<li><strong>spark.mllib</strong>: This supports collaborative filtering, where Spark uses ALS (Alternating Least Squares) to predict the missing entries in the sets of descriptions of users and products.</li>
</ul>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201119215234237.png" alt=""></p>
<ul>
<li><input disabled="" type="checkbox"> <a href="https://spark.apache.org/docs/latest/ml-features" target="_blank" rel="noopener">https://spark.apache.org/docs/latest/ml-features</a>  后期学习使用这里面的api，但是ML基本算法必须熟练掌握；</li>
<li><input disabled="" type="checkbox"> 后面遇到比较好的代码，案例可以多多积累；</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib</span><br><span class="line">f = urllib.urlretrieve (<span class="string">"http://kdd.ics.uci.edu/databases/kddcup99/kddcup.data.gz"</span>, <span class="string">"kddcup.data.gz"</span>)</span><br><span class="line">data_file = <span class="string">"./kddcup.data.gz"</span></span><br><span class="line">raw_data = sc.textFile(data_file)</span><br><span class="line"><span class="keyword">print</span> <span class="string">"Train data size is {}"</span>.format(raw_data.count())</span><br><span class="line">ft = urllib.urlretrieve(<span class="string">"http://kdd.ics.uci.edu/databases/kddcup99/corrected.gz"</span>, <span class="string">"corrected.gz"</span>)</span><br><span class="line">test_data_file = <span class="string">"./corrected.gz"</span></span><br><span class="line">test_raw_data = sc.textFile(test_data_file)</span><br><span class="line"><span class="keyword">print</span> <span class="string">"Test data size is {}"</span>.format(test_raw_data.count())</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> pyspark.mllib.regression <span class="keyword">import</span> LabeledPoint</span><br><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> array</span><br><span class="line">csv_data = raw_data.map(<span class="keyword">lambda</span> x: x.split(<span class="string">","</span>))</span><br><span class="line">test_csv_data = test_raw_data.map(<span class="keyword">lambda</span> x: x.split(<span class="string">","</span>))</span><br><span class="line">protocols = csv_data.map(<span class="keyword">lambda</span> x: x[<span class="number">1</span>]).distinct().collect()</span><br><span class="line">services = csv_data.map(<span class="keyword">lambda</span> x: x[<span class="number">2</span>]).distinct().collect()</span><br><span class="line">flags = csv_data.map(<span class="keyword">lambda</span> x: x[<span class="number">3</span>]).distinct().collect()</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_labeled_point</span><span class="params">(line_split)</span>:</span></span><br><span class="line">    <span class="comment"># leave_out = [41]</span></span><br><span class="line">    clean_line_split = line_split[<span class="number">0</span>:<span class="number">41</span>]</span><br><span class="line">    <span class="comment"># convert protocol to numeric categorical variable</span></span><br><span class="line">    <span class="keyword">try</span>: </span><br><span class="line">        clean_line_split[<span class="number">1</span>] = protocols.index(clean_line_split[<span class="number">1</span>])</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        clean_line_split[<span class="number">1</span>] = len(protocols)  </span><br><span class="line">    <span class="comment"># convert service to numeric categorical variable</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        clean_line_split[<span class="number">2</span>] = services.index(clean_line_split[<span class="number">2</span>])</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        clean_line_split[<span class="number">2</span>] = len(services)</span><br><span class="line">    <span class="comment"># convert flag to numeric categorical variable</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        clean_line_split[<span class="number">3</span>] = flags.index(clean_line_split[<span class="number">3</span>])</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        clean_line_split[<span class="number">3</span>] = len(flags)</span><br><span class="line">    <span class="comment"># convert label to binary label</span></span><br><span class="line">    attack = <span class="number">1.0</span></span><br><span class="line">    <span class="keyword">if</span> line_split[<span class="number">41</span>]==<span class="string">'normal.'</span>:</span><br><span class="line">        attack = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">return</span> LabeledPoint(attack, array([float(x) <span class="keyword">for</span> x <span class="keyword">in</span> ]))</span><br><span class="line">training_data = csv_data.map(create_labeled_point)</span><br><span class="line">test_data = test_csv_data.map(create_labeled_point)</span><br><span class="line"><span class="comment">#--- training the classifier</span></span><br><span class="line"><span class="keyword">from</span> pyspark.mllib.tree <span class="keyword">import</span> DecisionTree, DecisionTreeModel</span><br><span class="line"><span class="keyword">from</span> time <span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="comment"># Build the model</span></span><br><span class="line">t0 = time()</span><br><span class="line">tree_model = DecisionTree.trainClassifier(training_data, numClasses=<span class="number">2</span>, </span><br><span class="line">                                          categoricalFeaturesInfo={<span class="number">1</span>: len(protocols), <span class="number">2</span>: len(services), <span class="number">3</span>: len(flags)}, impurity=<span class="string">'gini'</span>, maxDepth=<span class="number">4</span>, maxBins=<span class="number">100</span>)</span><br><span class="line">tt = time() - t0</span><br><span class="line"><span class="keyword">print</span> <span class="string">"Classifier trained in {} seconds"</span>.format(round(tt,<span class="number">3</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">#--- predict</span></span><br><span class="line">predictions = tree_model.predict(test_data.map(<span class="keyword">lambda</span> p: p.features))</span><br><span class="line">labels_and_preds = test_data.map(<span class="keyword">lambda</span> p: p.label).zip(predictions)</span><br><span class="line"></span><br><span class="line">t0 = time()</span><br><span class="line">test_accuracy = labels_and_preds.filter(<span class="keyword">lambda</span> (v, p): v == p).count() / float(test_data.count())</span><br><span class="line">tt = time() - t0</span><br><span class="line"><span class="keyword">print</span> <span class="string">"Prediction made in {} seconds. Test accuracy is {}"</span>.format(round(tt,<span class="number">3</span>), round(test_accuracy,<span class="number">4</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> <span class="string">"Learned classification tree model:"</span></span><br><span class="line"><span class="keyword">print</span> tree_model.toDebugString()</span><br></pre></td></tr></tbody></table></figure>

<p><img src="https://intellipaat.com/mediaFiles/2019/03/spark-and-rdd-cheat-sheet-1.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201119212743127.png" alt=""></p>
<ul>
<li><a href="https://github.com/jadianes/spark-py-notebooks" target="_blank" rel="noopener">https://github.com/jadianes/spark-py-notebooks</a></li>
<li><a href="https://www.cnblogs.com/sight-tech/p/12990579.html" target="_blank" rel="noopener">https://www.cnblogs.com/sight-tech/p/12990579.html</a></li>
<li><a href="https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame" target="_blank" rel="noopener">https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame</a></li>
<li><a href="https://intellipaat.com/mediaFiles/2019/03/PySpark-SQL-cheat-sheet.pdf" target="_blank" rel="noopener">https://intellipaat.com/mediaFiles/2019/03/PySpark-SQL-cheat-sheet.pdf</a></li>
<li><a href="https://www.cnblogs.com/liaowuhen1314/p/12792202.html" target="_blank" rel="noopener">https://www.cnblogs.com/liaowuhen1314/p/12792202.html</a></li>
<li><a href="https://spark.apache.org/docs/2.2.0/sql-programming-guide.html" target="_blank" rel="noopener">https://spark.apache.org/docs/2.2.0/sql-programming-guide.html</a></li>
</ul>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script>
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        Author:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://liudongdong1.github.io" rel="external nofollow noreferrer">liudongdong1</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        Link:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://liudongdong1.github.io/2020/07/13/yu-yan-kuang-jia/sparkhadoop/pyspark/">https://liudongdong1.github.io/2020/07/13/yu-yan-kuang-jia/sparkhadoop/pyspark/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        Reprint policy:
                    </i>
                </span>
                <span class="reprint-info">
                    All articles in this blog are used except for special statements
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    reprint polocy. If reproduced, please indicate source
                    <a href="https://liudongdong1.github.io" target="_blank">liudongdong1</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>Copied successfully, please follow the reprint policy of this article</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">more</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/tags/spark/">
                                    <span class="chip bg-color">spark</span>
                                </a>
                            
                                <a href="/tags/stream/">
                                    <span class="chip bg-color">stream</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="qq,qzone,wechat,weibo,douban" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
                <style>
    #reward {
        margin: 40px 0;
        text-align: center;
    }

    #reward .reward-link {
        font-size: 1.4rem;
        line-height: 38px;
    }

    #reward .btn-floating:hover {
        box-shadow: 0 6px 12px rgba(0, 0, 0, 0.2), 0 5px 15px rgba(0, 0, 0, 0.2);
    }

    #rewardModal {
        width: 320px;
        height: 350px;
    }

    #rewardModal .reward-title {
        margin: 15px auto;
        padding-bottom: 5px;
    }

    #rewardModal .modal-content {
        padding: 10px;
    }

    #rewardModal .close {
        position: absolute;
        right: 15px;
        top: 15px;
        color: rgba(0, 0, 0, 0.5);
        font-size: 1.3rem;
        line-height: 20px;
        cursor: pointer;
    }

    #rewardModal .close:hover {
        color: #ef5350;
        transform: scale(1.3);
        -moz-transform:scale(1.3);
        -webkit-transform:scale(1.3);
        -o-transform:scale(1.3);
    }

    #rewardModal .reward-tabs {
        margin: 0 auto;
        width: 210px;
    }

    .reward-tabs .tabs {
        height: 38px;
        margin: 10px auto;
        padding-left: 0;
    }

    .reward-content ul {
        padding-left: 0 !important;
    }

    .reward-tabs .tabs .tab {
        height: 38px;
        line-height: 38px;
    }

    .reward-tabs .tab a {
        color: #fff;
        background-color: #ccc;
    }

    .reward-tabs .tab a:hover {
        background-color: #ccc;
        color: #fff;
    }

    .reward-tabs .wechat-tab .active {
        color: #fff !important;
        background-color: #22AB38 !important;
    }

    .reward-tabs .alipay-tab .active {
        color: #fff !important;
        background-color: #019FE8 !important;
    }

    .reward-tabs .reward-img {
        width: 210px;
        height: 210px;
    }
</style>

<div id="reward">
    <a href="#rewardModal" class="reward-link modal-trigger btn-floating btn-medium waves-effect waves-light red">赏</a>

    <!-- Modal Structure -->
    <div id="rewardModal" class="modal">
        <div class="modal-content">
            <a class="close modal-close"><i class="fas fa-times"></i></a>
            <h4 class="reward-title">你的赏识是我前进的动力</h4>
            <div class="reward-content">
                <div class="reward-tabs">
                    <ul class="tabs row">
                        <li class="tab col s6 alipay-tab waves-effect waves-light"><a href="#alipay">支付宝</a></li>
                        <li class="tab col s6 wechat-tab waves-effect waves-light"><a href="#wechat">微 信</a></li>
                    </ul>
                    <div id="alipay">
                        <img src="/medias/reward/alipay.jpg" class="reward-img" alt="支付宝打赏二维码">
                    </div>
                    <div id="wechat">
                        <img src="/medias/reward/wechat.png" class="reward-img" alt="微信打赏二维码">
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>

<script>
    $(function () {
        $('.tabs').tabs();
    });
</script>

            
        </div>
    </div>

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;Previous</div>
            <div class="card">
                <a href="/2020/07/13/nlp/framework/nlp-pyspark/">
                    <div class="card-image">
                        
                        <img src="https://gitee.com/github-25970295/blogImage/raw/master/img/traffic-lights-in-city-at-night.jpg" class="responsive-img" alt="NLP_pyspark">
                        
                        <span class="card-title">NLP_pyspark</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            

1. Concept1.1. Estimators
The Estimators have a method called fit() which secures and trains a piece of data to such a
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2020-07-13
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/NLP/" class="post-category">
                                    NLP
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/pyspark/">
                        <span class="chip bg-color">pyspark</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                Next&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/2020/07/13/shi-jue-ai/dataset-record/">
                    <div class="card-image">
                        
                        <img src="https://gitee.com/github-25970295/blogImage/raw/master/img/33.jpeg" class="responsive-img" alt="DataSet_Record">
                        
                        <span class="card-title">DataSet_Record</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            
在以数据驱动的人工智能时代，本文用于平时学习或者阅读论文中所涉及到的开源数据集积累。dataset知识图谱

More:https://www.codetd.com/article/7219369

Dataset: https://ww
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2020-07-13
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/%E8%A7%86%E8%A7%89AI/" class="post-category">
                                    视觉AI
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/Dataset/">
                        <span class="chip bg-color">Dataset</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/libs/codeBlock/codeBlockFuction.js"></script>

<!-- 代码语言 -->

<script type="text/javascript" src="/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/libs/codeBlock/codeShrink.js"></script>


<!-- 代码块折行 -->

<style type="text/css">
code[class*="language-"], pre[class*="language-"] { white-space: pre !important; }
</style>


    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;TOC</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // modify the toc link href to support Chinese.
        let i = 0;
        let tocHeading = 'toc-heading-';
        $('#toc-content a').each(function () {
            $(this).attr('href', '#' + tocHeading + (++i));
        });

        // modify the heading title id to support Chinese.
        i = 0;
        $('#articleContent').children('h2, h3, h4').each(function () {
            $(this).attr('id', tocHeading + (++i));
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>

    

</main>




    <!-- <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="463294659"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/libs/aplayer/APlayer.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/meting@2/dist/Meting.min.js"></script>

    
    <div class="container row center-align" style="margin-bottom: 0px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            <span id="year">2019</span>
            <a href="https://liudongdong1.github.io" target="_blank">liudongdong</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            <br>
            
            &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                class="white-color">1206.4k</span>&nbsp;字
            
            
            
            
            
            
            <span id="busuanzi_container_site_pv">
                |&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;<span id="busuanzi_value_site_pv"
                    class="white-color"></span>&nbsp;次
            </span>
            
            
            <span id="busuanzi_container_site_uv">
                |&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;<span id="busuanzi_value_site_uv"
                    class="white-color"></span>&nbsp;人
            </span>
            
            <br>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/liudongdong1/" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:3463264078@qq.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>














    <a href="https://blog.csdn.net/liudongdong19/" class="tooltipped" target="_blank" data-tooltip="关注我的CSDN: https://blog.csdn.net/liudongdong19/" data-position="top" data-delay="50">
        <i class="fab fa-csdn">C</i>
    </a>





</div>
    </div>
</footer>

<div class="progress-bar"></div>
 -->

    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;Search</span>
            <input type="search" id="searchInput" name="s" placeholder="Please enter a search keyword"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script src="/js/search.js"></script>
<script type="text/javascript">
$(function () {
    searchFunc("/search.xml", 'searchInput', 'searchResult');
});
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script type="text/javascript" src="/js/CFS.Snow.min.js"></script>
    <!-- 点击爆灯效果 -->
    <canvas class="fireworks" style="position: fixed;left: 0;top: 0;z-index: 1; pointer-events: none;" ></canvas> 
    <script type="text/javascript" src="//cdn.bootcss.com/animejs/2.2.0/anime.min.js"></script> 
    <script type="text/javascript" src="/js/fireworks.js"></script>
    <!--动态线条背景-->
    <script type="text/javascript"
        color="122 103 238" opacity='0.7' zIndex="-2" count="200" src="//cdn.bootcss.com/canvas-nest.js/1.0.0/canvas-nest.min.js">
    </script>
    <!-- 天气 -->
    <!-- weather -->
    <!-- weather -->
    <script type="text/javascript">
         WIDGET = {FID: 'knAMQaFanP'}
    </script>
    <script type="text/javascript" src="https://apip.weatherdt.com/float/static/js/r.js?v=1111"></script>
    <script type="text/javascript" src="https://apip.weatherdt.com/float/static/js/r.js?v=1111"></script>
    <script src="/libs/materialize/materialize.min.js"></script>
    <script src="/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/libs/aos/aos.js"></script>
    <script src="/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/js/matery.js"></script>

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    
    <script async src="/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    

    
    
    <script type="text/javascript" size="150" alpha='0.6'
        zIndex="-1" src="/libs/background/ribbon-refresh.min.js" async="async"></script>
    

    

    
    <script src="/libs/instantpage/instantpage.js" type="module"></script>
    
    <!-- {% include '_custom/custom.swig' %} -->

</body>

</html>
