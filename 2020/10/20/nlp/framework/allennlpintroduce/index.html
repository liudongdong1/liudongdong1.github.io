<!DOCTYPE HTML>
<html lang="en">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="AllenNLPIntroduce, AIOT,Space&amp;Temporal Sequence Analysis,SpringBoot,liudongdong1,cloud">
    <meta name="description" content="">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>AllenNLPIntroduce | DaybyDay</title>
    <link rel="icon" type="image/png" href="/favicon.png">

    <link rel="stylesheet" type="text/css" href="/libs/awesome/css/all.css">
    <link rel="stylesheet" type="text/css" href="/libs/materialize/materialize.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/aos/aos.css">
    <link rel="stylesheet" type="text/css" href="/libs/animate/animate.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/lightGallery/css/lightgallery.min.css">
    <link rel="stylesheet" type="text/css" href="/css/matery.css">
    <link rel="stylesheet" type="text/css" href="/css/my.css">

    <script src="/libs/jquery/jquery.min.js"></script>

<meta name="generator" content="Hexo 4.2.0"><link rel="alternate" href="/atom.xml" title="DaybyDay" type="application/atom+xml">
<link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css"></head>


<body>
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/" class="waves-effect waves-light">
                    
                    <img src="/medias/logo.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">DaybyDay</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>Index</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>Tags</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>Categories</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>Archives</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/" class="waves-effect waves-light">

      
      <i class="fas fa-user-circle" style="zoom: 0.6;"></i>
      
      <span>About</span>
      <i class="fas fa-chevron-down" aria-hidden="true" style="zoom: 0.6;"></i>
    </a>
    <ul class="sub-nav menus_item_child ">
      
      <li>
        <a href="/about">
          
          <i class="fas fa-user-circle" style="margin-top: -20px; zoom: 0.6;"></i>
          
          <span>about</span>
        </a>
      </li>
      
      <li>
        <a href="/resume">
          
          <i class="fa fa-user-secret" style="margin-top: -20px; zoom: 0.6;"></i>
          
          <span>resume</span>
        </a>
      </li>
      
    </ul>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/friends" class="waves-effect waves-light">
      
      <i class="fas fa-address-book" style="zoom: 0.6;"></i>
      
      <span>Friends</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/gallery" class="waves-effect waves-light">
      
      <i class="fas fa-camera" style="zoom: 0.6;"></i>
      
      <span>Galleries</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="Search" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/medias/logo.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">DaybyDay</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			Index
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			Tags
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			Categories
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			Archives
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="javascript:;">
			
				<i class="fa-fw fas fa-user-circle"></i>
			
			About
			<span class="m-icon"><i class="fas fa-chevron-right"></i></span>
		</a>
            <ul  style="background:  ;" >
              
                <li>

                  <a href="/about " style="margin-left:75px">
				  
				   <i class="fa fas fa-user-circle" style="position: absolute;left:50px" ></i>
			      
		          <span>about</span>
                  </a>
                </li>
              
                <li>

                  <a href="/resume " style="margin-left:75px">
				  
				   <i class="fa fa fa-user-secret" style="position: absolute;left:50px" ></i>
			      
		          <span>resume</span>
                  </a>
                </li>
              
            </ul>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/friends" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-address-book"></i>
			
			Friends
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/gallery" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-camera"></i>
			
			Galleries
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/liudongdong1" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/liudongdong1" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://gitee.com/github-25970295/blogImage/raw/master/img/20210501113113.png')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">AllenNLPIntroduce</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <link rel="stylesheet" href="/libs/tocbot/tocbot.css">
<style>
    #articleContent h1::before,
    #articleContent h2::before,
    #articleContent h3::before,
    #articleContent h4::before,
    #articleContent h5::before,
    #articleContent h6::before {
        display: block;
        content: " ";
        height: 100px;
        margin-top: -100px;
        visibility: hidden;
    }

    #articleContent :focus {
        outline: none;
    }

    .toc-fixed {
        position: fixed;
        top: 64px;
    }

    .toc-widget {
        width: 345px;
        padding-left: 20px;
    }

    .toc-widget .toc-title {
        margin: 35px 0 15px 0;
        padding-left: 17px;
        font-size: 1.5rem;
        font-weight: bold;
        line-height: 1.5rem;
    }

    .toc-widget ol {
        padding: 0;
        list-style: none;
    }

    #toc-content {
        height: calc(100vh - 250px);
        overflow: auto;
    }

    #toc-content ol {
        padding-left: 10px;
    }

    #toc-content ol li {
        padding-left: 10px;
    }

    #toc-content .toc-link:hover {
        color: #42b983;
        font-weight: 700;
        text-decoration: underline;
    }

    #toc-content .toc-link::before {
        background-color: transparent;
        max-height: 25px;

        position: absolute;
        right: 23.5vw;
        display: block;
    }

    .toc-fixed .toc-link::before{
        position: fixed!important;/*当toc的位置改为fixed时，.toc-link::before也要改为fixed*/
    }

    #toc-content .is-active-link {
        color: #42b983;
    }

    #toc-content .is-active-link::before {
        background-color: #42b983;
    }

    #floating-toc-btn {
        position: fixed;
        right: 15px;
        bottom: 76px;
        padding-top: 15px;
        margin-bottom: 0;
        z-index: 998;
    }

    #floating-toc-btn .btn-floating {
        width: 48px;
        height: 48px;
    }

    #floating-toc-btn .btn-floating i {
        line-height: 48px;
        font-size: 1.4rem;
    }
</style>
<div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/tags/Model/">
                                <span class="chip bg-color">Model</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80/" class="post-category">
                                自然语言
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>Publish Date:&nbsp;&nbsp;
                    2020-10-20
                </div>
                

                <!-- 
                    <i class="fa fa-pencil"></i> Author: liudongdong1
                  -->

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>Update Date:&nbsp;&nbsp;
                    2021-06-21
                </div>
                

                <!-- 
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>Word Count:&nbsp;&nbsp;
                    2.2k
                </div>
                 -->

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>Read Times:&nbsp;&nbsp;
                    13 Min
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>Read Count:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">
        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>you can write your own script to construct the dataset reader and model and run the training loop, or you can write a configuration file and use the <code>allennlp train</code> command</p>
</blockquote>
<h2 id="1-Text-Classification"><a href="#1-Text-Classification" class="headerlink" title="1. Text Classification"></a>1. Text Classification</h2><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201022211754511.png" alt=""></p>
<table>
<thead>
<tr>
<th>Spam filtering</th>
<th>Detect and filter spam emails</th>
<th>Email</th>
<th>Spam / Not spam</th>
</tr>
</thead>
<tbody><tr>
<td>Sentiment analysis</td>
<td>Detect the polarity of text</td>
<td>Tweet, review</td>
<td>Positive / Negative</td>
</tr>
<tr>
<td>Topic detection</td>
<td>Detect the topic of text</td>
<td>News article, blog post</td>
<td>Business / Tech / Sports</td>
</tr>
</tbody></table>
<ul>
<li><strong>Reading Data</strong></li>
</ul>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201022212005872.png" alt=""></p>
<ul>
<li><strong>Model</strong></li>
</ul>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201022212412613.png" alt=""></p>
<h2 id="2-Train-with-script"><a href="#2-Train-with-script" class="headerlink" title="2. Train with script"></a>2. Train with script</h2><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> Dict, Iterable, List</span><br><span class="line"><span class="keyword">from</span> allennlp.data <span class="keyword">import</span> DatasetReader, Instance</span><br><span class="line"><span class="keyword">from</span> allennlp.data.fields <span class="keyword">import</span> LabelField, TextField</span><br><span class="line"><span class="keyword">from</span> allennlp.data.token_indexers <span class="keyword">import</span> TokenIndexer, SingleIdTokenIndexer</span><br><span class="line"><span class="keyword">from</span> allennlp.data.tokenizers <span class="keyword">import</span> Token, Tokenizer, WhitespaceTokenizer</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ClassificationTsvReader</span><span class="params">(DatasetReader)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,</span></span></span><br><span class="line"><span class="function"><span class="params">                 lazy: bool = False,</span></span></span><br><span class="line"><span class="function"><span class="params">                 tokenizer: Tokenizer = None,</span></span></span><br><span class="line"><span class="function"><span class="params">                 token_indexers: Dict[str, TokenIndexer] = None,</span></span></span><br><span class="line"><span class="function"><span class="params">                 max_tokens: int = None)</span>:</span></span><br><span class="line">        super().__init__(lazy)</span><br><span class="line">        self.tokenizer = tokenizer <span class="keyword">or</span> WhitespaceTokenizer()  <span class="comment">##？？</span></span><br><span class="line">        self.token_indexers = token_indexers <span class="keyword">or</span> {<span class="string">'tokens'</span>: SingleIdTokenIndexer()}  <span class="comment">##？？</span></span><br><span class="line">        self.max_tokens = max_tokens</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_read</span><span class="params">(self, file_path: str)</span> -&gt; Iterable[Instance]:</span></span><br><span class="line">        <span class="keyword">with</span> open(file_path, <span class="string">'r'</span>) <span class="keyword">as</span> lines:</span><br><span class="line">            <span class="keyword">for</span> line <span class="keyword">in</span> lines:</span><br><span class="line">                text, sentiment = line.strip().split(<span class="string">'\t'</span>)</span><br><span class="line">                tokens = self.tokenizer.tokenize(text)   <span class="comment">##？？得到的是什么</span></span><br><span class="line">                <span class="keyword">if</span> self.max_tokens:</span><br><span class="line">                    tokens = tokens[:self.max_tokens]</span><br><span class="line">                text_field = TextField(tokens, self.token_indexers)<span class="comment">##？？token_indexers 用来干什么</span></span><br><span class="line">                label_field = LabelField(sentiment)</span><br><span class="line">                fields = {<span class="string">'text'</span>: text_field, <span class="string">'label'</span>: label_field}</span><br><span class="line">                <span class="keyword">yield</span> Instance(fields)</span><br><span class="line"></span><br><span class="line">dataset_reader = ClassificationTsvReader(max_tokens=<span class="number">64</span>)</span><br><span class="line">instances = dataset_reader.read(<span class="string">"quick_start/data/movie_review/train.tsv"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> instance <span class="keyword">in</span> instances[:<span class="number">10</span>]:</span><br><span class="line">    print(instance)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SimpleClassifier</span><span class="params">(Model)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,</span></span></span><br><span class="line"><span class="function"><span class="params">                 vocab: Vocabulary,</span></span></span><br><span class="line"><span class="function"><span class="params">                 embedder: TextFieldEmbedder,</span></span></span><br><span class="line"><span class="function"><span class="params">                 encoder: Seq2VecEncoder)</span>:</span></span><br><span class="line">        super().__init__(vocab)</span><br><span class="line">        self.embedder = embedder</span><br><span class="line">        self.encoder = encoder</span><br><span class="line">        num_labels = vocab.get_vocab_size(<span class="string">"labels"</span>)</span><br><span class="line">        self.classifier = torch.nn.Linear(encoder.get_output_dim(), num_labels)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self,</span></span></span><br><span class="line"><span class="function"><span class="params">                text: Dict[str, torch.Tensor],</span></span></span><br><span class="line"><span class="function"><span class="params">                label: torch.Tensor)</span> -&gt; Dict[str, torch.Tensor]:</span></span><br><span class="line">        <span class="comment"># Shape: (batch_size, num_tokens, embedding_dim)</span></span><br><span class="line">        embedded_text = self.embedder(text)</span><br><span class="line">        <span class="comment"># Shape: (batch_size, num_tokens)</span></span><br><span class="line">        mask = util.get_text_field_mask(text)</span><br><span class="line">        <span class="comment"># Shape: (batch_size, encoding_dim)</span></span><br><span class="line">        encoded_text = self.encoder(embedded_text, mask)</span><br><span class="line">        <span class="comment"># Shape: (batch_size, num_labels)</span></span><br><span class="line">        logits = self.classifier(encoded_text)</span><br><span class="line">        <span class="comment"># Shape: (batch_size, num_labels)</span></span><br><span class="line">        probs = torch.nn.functional.softmax(logits, dim=<span class="number">-1</span>)</span><br><span class="line">        <span class="comment"># Shape: (1,)</span></span><br><span class="line">        loss = torch.nn.functional.cross_entropy(logits, label)</span><br><span class="line">        <span class="keyword">return</span> {<span class="string">'loss'</span>: loss, <span class="string">'probs'</span>: probs}</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_dataset_reader</span><span class="params">()</span> -&gt; DatasetReader:</span></span><br><span class="line">    <span class="keyword">return</span> ClassificationTsvReader()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">read_data</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    reader: DatasetReader</span></span></span><br><span class="line"><span class="function"><span class="params">)</span> -&gt; Tuple[Iterable[Instance], Iterable[Instance]]:</span></span><br><span class="line">    print(<span class="string">"Reading data"</span>)</span><br><span class="line">    training_data = reader.read(<span class="string">"quick_start/data/movie_review/train.tsv"</span>)</span><br><span class="line">    validation_data = reader.read(<span class="string">"quick_start/data/movie_review/dev.tsv"</span>)</span><br><span class="line">    <span class="keyword">return</span> training_data, validation_data</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_vocab</span><span class="params">(instances: Iterable[Instance])</span> -&gt; Vocabulary:</span></span><br><span class="line">    print(<span class="string">"Building the vocabulary"</span>)</span><br><span class="line">    <span class="keyword">return</span> Vocabulary.from_instances(instances)  <span class="comment">#？？ </span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_model</span><span class="params">(vocab: Vocabulary)</span> -&gt; Model:</span></span><br><span class="line">    print(<span class="string">"Building the model"</span>)</span><br><span class="line">    vocab_size = vocab.get_vocab_size(<span class="string">"tokens"</span>)</span><br><span class="line">    embedder = BasicTextFieldEmbedder(   <span class="comment">##？？</span></span><br><span class="line">        {<span class="string">"tokens"</span>: Embedding(embedding_dim=<span class="number">10</span>, num_embeddings=vocab_size)})</span><br><span class="line">    encoder = BagOfEmbeddingsEncoder(embedding_dim=<span class="number">10</span>)  <span class="comment">#？？</span></span><br><span class="line">    <span class="keyword">return</span> SimpleClassifier(vocab, embedder, encoder)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run_training_loop</span><span class="params">()</span>:</span></span><br><span class="line">    dataset_reader = build_dataset_reader()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># These are a subclass of pytorch Datasets, with some allennlp-specific</span></span><br><span class="line">    <span class="comment"># functionality added.</span></span><br><span class="line">    train_data, dev_data = read_data(dataset_reader)</span><br><span class="line"></span><br><span class="line">    vocab = build_vocab(train_data + dev_data)</span><br><span class="line">    model = build_model(vocab)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># This is the allennlp-specific functionality in the Dataset object;</span></span><br><span class="line">    <span class="comment"># we need to be able convert strings in the data to integers, and this</span></span><br><span class="line">    <span class="comment"># is how we do it.</span></span><br><span class="line">    train_data.index_with(vocab)   <span class="comment">#？？</span></span><br><span class="line">    dev_data.index_with(vocab)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># These are again a subclass of pytorch DataLoaders, with an</span></span><br><span class="line">    <span class="comment"># allennlp-specific collate function, that runs our indexing and</span></span><br><span class="line">    <span class="comment"># batching code.</span></span><br><span class="line">    train_loader, dev_loader = build_data_loaders(train_data, dev_data)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># You obviously won't want to create a temporary file for your training</span></span><br><span class="line">    <span class="comment"># results, but for execution in binder for this guide, we need to do this.</span></span><br><span class="line">    <span class="keyword">with</span> tempfile.TemporaryDirectory() <span class="keyword">as</span> serialization_dir:</span><br><span class="line">        trainer = build_trainer(</span><br><span class="line">            model,</span><br><span class="line">            serialization_dir,</span><br><span class="line">            train_loader,</span><br><span class="line">            dev_loader</span><br><span class="line">        )</span><br><span class="line">        print(<span class="string">"Starting training"</span>)</span><br><span class="line">        trainer.train()</span><br><span class="line">        print(<span class="string">"Finished training"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># The other `build_*` methods are things we've seen before, so they are</span></span><br><span class="line"><span class="comment"># in the setup section above.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_data_loaders</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    train_data: torch.utils.data.Dataset,</span></span></span><br><span class="line"><span class="function"><span class="params">    dev_data: torch.utils.data.Dataset,</span></span></span><br><span class="line"><span class="function"><span class="params">)</span> -&gt; Tuple[allennlp.data.DataLoader, allennlp.data.DataLoader]:</span></span><br><span class="line">    <span class="comment"># Note that DataLoader is imported from allennlp above, *not* torch.</span></span><br><span class="line">    <span class="comment"># We need to get the allennlp-specific collate function, which is</span></span><br><span class="line">    <span class="comment"># what actually does indexing and batching.</span></span><br><span class="line">    train_loader = DataLoader(train_data, batch_size=<span class="number">8</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line">    dev_loader = DataLoader(dev_data, batch_size=<span class="number">8</span>, shuffle=<span class="literal">False</span>)</span><br><span class="line">    <span class="keyword">return</span> train_loader, dev_loader</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_trainer</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    model: Model,</span></span></span><br><span class="line"><span class="function"><span class="params">    serialization_dir: str,</span></span></span><br><span class="line"><span class="function"><span class="params">    train_loader: DataLoader,</span></span></span><br><span class="line"><span class="function"><span class="params">    dev_loader: DataLoader</span></span></span><br><span class="line"><span class="function"><span class="params">)</span> -&gt; Trainer:</span></span><br><span class="line">    parameters = [</span><br><span class="line">        [n, p]</span><br><span class="line">        <span class="keyword">for</span> n, p <span class="keyword">in</span> model.named_parameters() <span class="keyword">if</span> p.requires_grad</span><br><span class="line">    ]</span><br><span class="line">    optimizer = AdamOptimizer(parameters)</span><br><span class="line">    trainer = GradientDescentTrainer(</span><br><span class="line">        model=model,</span><br><span class="line">        serialization_dir=serialization_dir,</span><br><span class="line">        data_loader=train_loader,</span><br><span class="line">        validation_data_loader=dev_loader,</span><br><span class="line">        num_epochs=<span class="number">5</span>,</span><br><span class="line">        optimizer=optimizer,</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">return</span> trainer</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">run_training_loop()</span><br></pre></td></tr></tbody></table></figure>

<h2 id="3-Train-with-allennlp"><a href="#3-Train-with-allennlp" class="headerlink" title="3. Train with allennlp"></a>3. Train with allennlp</h2><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tempfile</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> Dict, Iterable, List</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> allennlp.data <span class="keyword">import</span> DatasetReader, Instance, Vocabulary</span><br><span class="line"><span class="keyword">from</span> allennlp.data.fields <span class="keyword">import</span> LabelField, TextField</span><br><span class="line"><span class="keyword">from</span> allennlp.data.token_indexers <span class="keyword">import</span> TokenIndexer, SingleIdTokenIndexer</span><br><span class="line"><span class="keyword">from</span> allennlp.data.tokenizers <span class="keyword">import</span> Token, Tokenizer, WhitespaceTokenizer</span><br><span class="line"><span class="keyword">from</span> allennlp.models <span class="keyword">import</span> Model</span><br><span class="line"><span class="keyword">from</span> allennlp.modules <span class="keyword">import</span> TextFieldEmbedder, Seq2VecEncoder</span><br><span class="line"><span class="keyword">from</span> allennlp.nn <span class="keyword">import</span> util</span><br><span class="line"><span class="keyword">from</span> allennlp.training.metrics <span class="keyword">import</span> CategoricalAccuracy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@DatasetReader.register("classification-tsv")</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ClassificationTsvReader</span><span class="params">(DatasetReader)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,</span></span></span><br><span class="line"><span class="function"><span class="params">                 lazy: bool = False,</span></span></span><br><span class="line"><span class="function"><span class="params">                 tokenizer: Tokenizer = None,</span></span></span><br><span class="line"><span class="function"><span class="params">                 token_indexers: Dict[str, TokenIndexer] = None,</span></span></span><br><span class="line"><span class="function"><span class="params">                 max_tokens: int = None)</span>:</span></span><br><span class="line">        super().__init__(lazy)</span><br><span class="line">        self.tokenizer = tokenizer <span class="keyword">or</span> WhitespaceTokenizer()</span><br><span class="line">        self.token_indexers = token_indexers <span class="keyword">or</span> {<span class="string">'tokens'</span>: SingleIdTokenIndexer()}</span><br><span class="line">        self.max_tokens = max_tokens</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">text_to_instance</span><span class="params">(self,</span></span></span><br><span class="line"><span class="function"><span class="params">                         tokens: List[Token],</span></span></span><br><span class="line"><span class="function"><span class="params">                         label: str = None)</span> -&gt; Instance:</span></span><br><span class="line">        <span class="keyword">if</span> self.max_tokens:</span><br><span class="line">            tokens = tokens[:self.max_tokens]</span><br><span class="line">        text_field = TextField(tokens, self.token_indexers)</span><br><span class="line">        fields = {<span class="string">'text'</span>: text_field}</span><br><span class="line">        <span class="keyword">if</span> label:</span><br><span class="line">            fields[<span class="string">'label'</span>] = LabelField(label)</span><br><span class="line">        <span class="keyword">return</span> Instance(fields)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_read</span><span class="params">(self, file_path: str)</span> -&gt; Iterable[Instance]:</span></span><br><span class="line">        <span class="keyword">with</span> open(file_path, <span class="string">'r'</span>) <span class="keyword">as</span> lines:</span><br><span class="line">            <span class="keyword">for</span> line <span class="keyword">in</span> lines:</span><br><span class="line">                text, sentiment = line.strip().split(<span class="string">'\t'</span>)</span><br><span class="line">                tokens = self.tokenizer.tokenize(text)</span><br><span class="line">                <span class="keyword">yield</span> self.text_to_instance(tokens, sentiment)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@Model.register("simple_classifier")</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SimpleClassifier</span><span class="params">(Model)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,</span></span></span><br><span class="line"><span class="function"><span class="params">                 vocab: Vocabulary,</span></span></span><br><span class="line"><span class="function"><span class="params">                 embedder: TextFieldEmbedder,</span></span></span><br><span class="line"><span class="function"><span class="params">                 encoder: Seq2VecEncoder)</span>:</span></span><br><span class="line">        super().__init__(vocab)</span><br><span class="line">        self.embedder = embedder</span><br><span class="line">        self.encoder = encoder</span><br><span class="line">        num_labels = vocab.get_vocab_size(<span class="string">"labels"</span>)</span><br><span class="line">        self.classifier = torch.nn.Linear(encoder.get_output_dim(), num_labels)</span><br><span class="line">        self.accuracy = CategoricalAccuracy()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self,</span></span></span><br><span class="line"><span class="function"><span class="params">                text: Dict[str, torch.Tensor],</span></span></span><br><span class="line"><span class="function"><span class="params">                label: torch.Tensor = None)</span> -&gt; Dict[str, torch.Tensor]:</span></span><br><span class="line">        print(<span class="string">"In model.forward(); printing here just because binder is so slow"</span>)</span><br><span class="line">        <span class="comment"># Shape: (batch_size, num_tokens, embedding_dim)</span></span><br><span class="line">        embedded_text = self.embedder(text)</span><br><span class="line">        <span class="comment"># Shape: (batch_size, num_tokens)</span></span><br><span class="line">        mask = util.get_text_field_mask(text)</span><br><span class="line">        <span class="comment"># Shape: (batch_size, encoding_dim)</span></span><br><span class="line">        encoded_text = self.encoder(embedded_text, mask)</span><br><span class="line">        <span class="comment"># Shape: (batch_size, num_labels)</span></span><br><span class="line">        logits = self.classifier(encoded_text)</span><br><span class="line">        <span class="comment"># Shape: (batch_size, num_labels)</span></span><br><span class="line">        probs = torch.nn.functional.softmax(logits)</span><br><span class="line">        <span class="comment"># Shape: (1,)</span></span><br><span class="line">        output = {<span class="string">'probs'</span>: probs}</span><br><span class="line">        <span class="keyword">if</span> label <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            self.accuracy(logits, label)</span><br><span class="line">            output[<span class="string">'loss'</span>] = torch.nn.functional.cross_entropy(logits, label)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_metrics</span><span class="params">(self, reset: bool = False)</span> -&gt; Dict[str, float]:</span></span><br><span class="line">        <span class="keyword">return</span> {<span class="string">"accuracy"</span>: self.accuracy.get_metric(reset)}</span><br><span class="line"></span><br><span class="line">config = {</span><br><span class="line">    <span class="string">"dataset_reader"</span> : {</span><br><span class="line">        <span class="string">"type"</span>: <span class="string">"classification-tsv"</span>,</span><br><span class="line">        <span class="string">"token_indexers"</span>: {</span><br><span class="line">            <span class="string">"tokens"</span>: {</span><br><span class="line">                <span class="string">"type"</span>: <span class="string">"single_id"</span></span><br><span class="line">            }</span><br><span class="line">        }</span><br><span class="line">    },</span><br><span class="line">    <span class="string">"train_data_path"</span>: <span class="string">"quick_start/data/movie_review/train.tsv"</span>,</span><br><span class="line">    <span class="string">"validation_data_path"</span>: <span class="string">"quick_start/data/movie_review/dev.tsv"</span>,</span><br><span class="line">    <span class="string">"model"</span>: {</span><br><span class="line">        <span class="string">"type"</span>: <span class="string">"simple_classifier"</span>,</span><br><span class="line">        <span class="string">"embedder"</span>: {</span><br><span class="line">            <span class="string">"token_embedders"</span>: {</span><br><span class="line">                <span class="string">"tokens"</span>: {</span><br><span class="line">                    <span class="string">"type"</span>: <span class="string">"embedding"</span>,</span><br><span class="line">                    <span class="string">"embedding_dim"</span>: <span class="number">10</span></span><br><span class="line">                }</span><br><span class="line">            }</span><br><span class="line">        },</span><br><span class="line">        <span class="string">"encoder"</span>: {</span><br><span class="line">            <span class="string">"type"</span>: <span class="string">"bag_of_embeddings"</span>,</span><br><span class="line">            <span class="string">"embedding_dim"</span>: <span class="number">10</span></span><br><span class="line">        }</span><br><span class="line">    },</span><br><span class="line">    <span class="string">"data_loader"</span>: {</span><br><span class="line">        <span class="string">"batch_size"</span>: <span class="number">8</span>,</span><br><span class="line">        <span class="string">"shuffle"</span>: <span class="literal">True</span></span><br><span class="line">    },</span><br><span class="line">    <span class="string">"trainer"</span>: {</span><br><span class="line">        <span class="string">"optimizer"</span>: <span class="string">"adam"</span>,</span><br><span class="line">        <span class="string">"num_epochs"</span>: <span class="number">5</span></span><br><span class="line">    }</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tempfile.TemporaryDirectory() <span class="keyword">as</span> serialization_dir:</span><br><span class="line">    config_filename = serialization_dir + <span class="string">"/training_config.json"</span></span><br><span class="line">    <span class="keyword">with</span> open(config_filename, <span class="string">'w'</span>) <span class="keyword">as</span> config_file:</span><br><span class="line">        json.dump(config, config_file)</span><br><span class="line">    <span class="keyword">from</span> allennlp.commands.train <span class="keyword">import</span> train_model_from_file</span><br><span class="line">    <span class="comment"># Instead of this python code, you would typically just call</span></span><br><span class="line">    <span class="comment"># allennlp train [config_file] -s [serialization_dir]</span></span><br><span class="line">    train_model_from_file(config_filename,</span><br><span class="line">                          serialization_dir,</span><br><span class="line">                          file_friendly_logging=<span class="literal">True</span>,</span><br><span class="line">                          force=<span class="literal">True</span>)</span><br><span class="line"><span class="comment">#allennlp train my_text_classifier.jsonnet -s model --include-package my_text_classifier</span></span><br></pre></td></tr></tbody></table></figure>

<h2 id="4-Unlabeled-Prediction"><a href="#4-Unlabeled-Prediction" class="headerlink" title="4. Unlabeled Prediction"></a>4. Unlabeled Prediction</h2><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tempfile</span><br><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> Dict, Iterable, List, Tuple</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> allennlp</span><br><span class="line"><span class="keyword">from</span> allennlp.common <span class="keyword">import</span> JsonDict</span><br><span class="line"><span class="keyword">from</span> allennlp.data <span class="keyword">import</span> DataLoader, DatasetReader, Instance</span><br><span class="line"><span class="keyword">from</span> allennlp.data <span class="keyword">import</span> Vocabulary</span><br><span class="line"><span class="keyword">from</span> allennlp.data.fields <span class="keyword">import</span> LabelField, TextField</span><br><span class="line"><span class="keyword">from</span> allennlp.data.token_indexers <span class="keyword">import</span> TokenIndexer, SingleIdTokenIndexer</span><br><span class="line"><span class="keyword">from</span> allennlp.data.tokenizers <span class="keyword">import</span> Token, Tokenizer, WhitespaceTokenizer</span><br><span class="line"><span class="keyword">from</span> allennlp.models <span class="keyword">import</span> Model</span><br><span class="line"><span class="keyword">from</span> allennlp.modules <span class="keyword">import</span> TextFieldEmbedder, Seq2VecEncoder</span><br><span class="line"><span class="keyword">from</span> allennlp.modules.text_field_embedders <span class="keyword">import</span> BasicTextFieldEmbedder</span><br><span class="line"><span class="keyword">from</span> allennlp.modules.token_embedders <span class="keyword">import</span> Embedding</span><br><span class="line"><span class="keyword">from</span> allennlp.modules.seq2vec_encoders <span class="keyword">import</span> BagOfEmbeddingsEncoder</span><br><span class="line"><span class="keyword">from</span> allennlp.predictors <span class="keyword">import</span> Predictor</span><br><span class="line"><span class="keyword">from</span> allennlp.nn <span class="keyword">import</span> util</span><br><span class="line"><span class="keyword">from</span> allennlp.training.metrics <span class="keyword">import</span> CategoricalAccuracy</span><br><span class="line"><span class="keyword">from</span> allennlp.training.trainer <span class="keyword">import</span> Trainer, GradientDescentTrainer</span><br><span class="line"><span class="keyword">from</span> allennlp.training.optimizers <span class="keyword">import</span> AdamOptimizer</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ClassificationTsvReader</span><span class="params">(DatasetReader)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,</span></span></span><br><span class="line"><span class="function"><span class="params">                 lazy: bool = False,</span></span></span><br><span class="line"><span class="function"><span class="params">                 tokenizer: Tokenizer = None,</span></span></span><br><span class="line"><span class="function"><span class="params">                 token_indexers: Dict[str, TokenIndexer] = None,</span></span></span><br><span class="line"><span class="function"><span class="params">                 max_tokens: int = None)</span>:</span></span><br><span class="line">        super().__init__(lazy)</span><br><span class="line">        self.tokenizer = tokenizer <span class="keyword">or</span> WhitespaceTokenizer()</span><br><span class="line">        self.token_indexers = token_indexers <span class="keyword">or</span> {<span class="string">'tokens'</span>: SingleIdTokenIndexer()}</span><br><span class="line">        self.max_tokens = max_tokens</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">text_to_instance</span><span class="params">(self, text: str, label: str = None)</span> -&gt; Instance:</span></span><br><span class="line">        tokens = self.tokenizer.tokenize(text)</span><br><span class="line">        <span class="keyword">if</span> self.max_tokens:</span><br><span class="line">            tokens = tokens[:self.max_tokens]</span><br><span class="line">        text_field = TextField(tokens, self.token_indexers)</span><br><span class="line">        fields = {<span class="string">'text'</span>: text_field}</span><br><span class="line">        <span class="keyword">if</span> label:</span><br><span class="line">            fields[<span class="string">'label'</span>] = LabelField(label)</span><br><span class="line">        <span class="keyword">return</span> Instance(fields)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_read</span><span class="params">(self, file_path: str)</span> -&gt; Iterable[Instance]:</span></span><br><span class="line">        <span class="keyword">with</span> open(file_path, <span class="string">'r'</span>) <span class="keyword">as</span> lines:</span><br><span class="line">            <span class="keyword">for</span> line <span class="keyword">in</span> lines:</span><br><span class="line">                text, sentiment = line.strip().split(<span class="string">'\t'</span>)</span><br><span class="line">                <span class="keyword">yield</span> self.text_to_instance(text, sentiment)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SimpleClassifier</span><span class="params">(Model)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,</span></span></span><br><span class="line"><span class="function"><span class="params">                 vocab: Vocabulary,</span></span></span><br><span class="line"><span class="function"><span class="params">                 embedder: TextFieldEmbedder,</span></span></span><br><span class="line"><span class="function"><span class="params">                 encoder: Seq2VecEncoder)</span>:</span></span><br><span class="line">        super().__init__(vocab)</span><br><span class="line">        self.embedder = embedder</span><br><span class="line">        self.encoder = encoder</span><br><span class="line">        num_labels = vocab.get_vocab_size(<span class="string">"labels"</span>)</span><br><span class="line">        self.classifier = torch.nn.Linear(encoder.get_output_dim(), num_labels)</span><br><span class="line">        self.accuracy = CategoricalAccuracy()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self,</span></span></span><br><span class="line"><span class="function"><span class="params">                text: Dict[str, torch.Tensor],</span></span></span><br><span class="line"><span class="function"><span class="params">                label: torch.Tensor = None)</span> -&gt; Dict[str, torch.Tensor]:</span></span><br><span class="line">        <span class="comment"># Shape: (batch_size, num_tokens, embedding_dim)</span></span><br><span class="line">        embedded_text = self.embedder(text)</span><br><span class="line">        <span class="comment"># Shape: (batch_size, num_tokens)</span></span><br><span class="line">        mask = util.get_text_field_mask(text)</span><br><span class="line">        <span class="comment"># Shape: (batch_size, encoding_dim)</span></span><br><span class="line">        encoded_text = self.encoder(embedded_text, mask)</span><br><span class="line">        <span class="comment"># Shape: (batch_size, num_labels)</span></span><br><span class="line">        logits = self.classifier(encoded_text)</span><br><span class="line">        <span class="comment"># Shape: (batch_size, num_labels)</span></span><br><span class="line">        probs = torch.nn.functional.softmax(logits)</span><br><span class="line">        output = {<span class="string">'probs'</span>: probs}</span><br><span class="line">        <span class="keyword">if</span> label <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            self.accuracy(logits, label)</span><br><span class="line">            <span class="comment"># Shape: (1,)</span></span><br><span class="line">            output[<span class="string">'loss'</span>] = torch.nn.functional.cross_entropy(logits, label)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_metrics</span><span class="params">(self, reset: bool = False)</span> -&gt; Dict[str, float]:</span></span><br><span class="line">        <span class="keyword">return</span> {<span class="string">"accuracy"</span>: self.accuracy.get_metric(reset)}</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_metrics</span><span class="params">(self, reset: bool = False)</span> -&gt; Dict[str, float]:</span></span><br><span class="line">        <span class="keyword">return</span> {<span class="string">"accuracy"</span>: self.accuracy.get_metric(reset)}</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_dataset_reader</span><span class="params">()</span> -&gt; DatasetReader:</span></span><br><span class="line">    <span class="keyword">return</span> ClassificationTsvReader()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">read_data</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    reader: DatasetReader</span></span></span><br><span class="line"><span class="function"><span class="params">)</span> -&gt; Tuple[Iterable[Instance], Iterable[Instance]]:</span></span><br><span class="line">    print(<span class="string">"Reading data"</span>)</span><br><span class="line">    training_data = reader.read(<span class="string">"quick_start/data/movie_review/train.tsv"</span>)</span><br><span class="line">    validation_data = reader.read(<span class="string">"quick_start/data/movie_review/dev.tsv"</span>)</span><br><span class="line">    <span class="keyword">return</span> training_data, validation_data</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_vocab</span><span class="params">(instances: Iterable[Instance])</span> -&gt; Vocabulary:</span></span><br><span class="line">    print(<span class="string">"Building the vocabulary"</span>)</span><br><span class="line">    <span class="keyword">return</span> Vocabulary.from_instances(instances)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_model</span><span class="params">(vocab: Vocabulary)</span> -&gt; Model:</span></span><br><span class="line">    print(<span class="string">"Building the model"</span>)</span><br><span class="line">    vocab_size = vocab.get_vocab_size(<span class="string">"tokens"</span>)</span><br><span class="line">    embedder = BasicTextFieldEmbedder(</span><br><span class="line">        {<span class="string">"tokens"</span>: Embedding(embedding_dim=<span class="number">10</span>, num_embeddings=vocab_size)})</span><br><span class="line">    encoder = BagOfEmbeddingsEncoder(embedding_dim=<span class="number">10</span>)</span><br><span class="line">    <span class="keyword">return</span> SimpleClassifier(vocab, embedder, encoder)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_data_loaders</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    train_data: torch.utils.data.Dataset,</span></span></span><br><span class="line"><span class="function"><span class="params">    dev_data: torch.utils.data.Dataset,</span></span></span><br><span class="line"><span class="function"><span class="params">)</span> -&gt; Tuple[allennlp.data.DataLoader, allennlp.data.DataLoader]:</span></span><br><span class="line">    <span class="comment"># Note that DataLoader is imported from allennlp above, *not* torch.</span></span><br><span class="line">    <span class="comment"># We need to get the allennlp-specific collate function, which is</span></span><br><span class="line">    <span class="comment"># what actually does indexing and batching.</span></span><br><span class="line">    train_loader = DataLoader(train_data, batch_size=<span class="number">8</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line">    dev_loader = DataLoader(dev_data, batch_size=<span class="number">8</span>, shuffle=<span class="literal">False</span>)</span><br><span class="line">    <span class="keyword">return</span> train_loader, dev_loader</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_trainer</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    model: Model,</span></span></span><br><span class="line"><span class="function"><span class="params">    serialization_dir: str,</span></span></span><br><span class="line"><span class="function"><span class="params">    train_loader: DataLoader,</span></span></span><br><span class="line"><span class="function"><span class="params">    dev_loader: DataLoader</span></span></span><br><span class="line"><span class="function"><span class="params">)</span> -&gt; Trainer:</span></span><br><span class="line">    parameters = [</span><br><span class="line">        [n, p]</span><br><span class="line">        <span class="keyword">for</span> n, p <span class="keyword">in</span> model.named_parameters() <span class="keyword">if</span> p.requires_grad</span><br><span class="line">    ]</span><br><span class="line">    optimizer = AdamOptimizer(parameters)</span><br><span class="line">    trainer = GradientDescentTrainer(</span><br><span class="line">        model=model,</span><br><span class="line">        serialization_dir=serialization_dir,</span><br><span class="line">        data_loader=train_loader,</span><br><span class="line">        validation_data_loader=dev_loader,</span><br><span class="line">        num_epochs=<span class="number">5</span>,</span><br><span class="line">        optimizer=optimizer,</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">return</span> trainer</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run_training_loop</span><span class="params">()</span>:</span></span><br><span class="line">    dataset_reader = build_dataset_reader()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># These are a subclass of pytorch Datasets, with some allennlp-specific</span></span><br><span class="line">    <span class="comment"># functionality added.</span></span><br><span class="line">    train_data, dev_data = read_data(dataset_reader)</span><br><span class="line"></span><br><span class="line">    vocab = build_vocab(train_data + dev_data)</span><br><span class="line">    model = build_model(vocab)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># This is the allennlp-specific functionality in the Dataset object;</span></span><br><span class="line">    <span class="comment"># we need to be able convert strings in the data to integers, and this</span></span><br><span class="line">    <span class="comment"># is how we do it.</span></span><br><span class="line">    train_data.index_with(vocab)</span><br><span class="line">    dev_data.index_with(vocab)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># These are again a subclass of pytorch DataLoaders, with an</span></span><br><span class="line">    <span class="comment"># allennlp-specific collate function, that runs our indexing and</span></span><br><span class="line">    <span class="comment"># batching code.</span></span><br><span class="line">    train_loader, dev_loader = build_data_loaders(train_data, dev_data)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># You obviously won't want to create a temporary file for your training</span></span><br><span class="line">    <span class="comment"># results, but for execution in binder for this guide, we need to do this.</span></span><br><span class="line">    <span class="keyword">with</span> tempfile.TemporaryDirectory() <span class="keyword">as</span> serialization_dir:</span><br><span class="line">        trainer = build_trainer(</span><br><span class="line">            model,</span><br><span class="line">            serialization_dir,</span><br><span class="line">            train_loader,</span><br><span class="line">            dev_loader</span><br><span class="line">        )</span><br><span class="line">        trainer.train()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> model, dataset_reader</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SentenceClassifierPredictor</span><span class="params">(Predictor)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, sentence: str)</span> -&gt; JsonDict:</span></span><br><span class="line">        <span class="keyword">return</span> self.predict_json({<span class="string">"sentence"</span>: sentence})</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_json_to_instance</span><span class="params">(self, json_dict: JsonDict)</span> -&gt; Instance:</span></span><br><span class="line">        sentence = json_dict[<span class="string">"sentence"</span>]</span><br><span class="line">        <span class="keyword">return</span> self._dataset_reader.text_to_instance(sentence)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># We've copied the training loop from an earlier example, with updated model</span></span><br><span class="line"><span class="comment"># code, above in the Setup section. We run the training loop to get a trained</span></span><br><span class="line"><span class="comment"># model.</span></span><br><span class="line">model, dataset_reader = run_training_loop()</span><br><span class="line">vocab = model.vocab</span><br><span class="line">predictor = SentenceClassifierPredictor(model, dataset_reader)</span><br><span class="line"></span><br><span class="line">output = predictor.predict(<span class="string">'A good movie!'</span>)</span><br><span class="line">print([(vocab.get_token_from_index(label_id, <span class="string">'labels'</span>), prob)</span><br><span class="line">       <span class="keyword">for</span> label_id, prob <span class="keyword">in</span> enumerate(output[<span class="string">'probs'</span>])])</span><br><span class="line">output = predictor.predict(<span class="string">'This was a monstrous waste of time.'</span>)</span><br><span class="line">print([(vocab.get_token_from_index(label_id, <span class="string">'labels'</span>), prob)</span><br><span class="line">       <span class="keyword">for</span> label_id, prob <span class="keyword">in</span> enumerate(output[<span class="string">'probs'</span>])])</span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#部署到web应用</span></span><br><span class="line">python allennlp-server/server_simple.py \</span><br><span class="line">    --archive-path model/model.tar.gz \</span><br><span class="line">    --predictor sentence_classifier \</span><br><span class="line">    --field-name sentence</span><br><span class="line">    --include-package my_text_classifier</span><br></pre></td></tr></tbody></table></figure>

<h2 id="5-API"><a href="#5-API" class="headerlink" title="5. API"></a>5. API</h2><h3 id="5-1-DataReading"><a href="#5-1-DataReading" class="headerlink" title="5.1. DataReading"></a>5.1. DataReading</h3><ul>
<li><strong>Field&amp;&amp;Instance</strong></li>
</ul>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201022222221588.png" alt=""></p>
<blockquote>
<p>A <code>Field</code> contains one piece of data for one example that is passed through your model. <code>Fields</code> get converted to tensors in a model, either as an input or an output, after being converted to IDs and batched &amp; padded.</p>
</blockquote>
<ul>
<li><strong>Dataset readers</strong></li>
</ul>
<blockquote>
<p><a href="https://github.com/allenai/allennlp/blob/master/allennlp/data/dataset_readers/text_classification_json.py" target="_blank" rel="noopener">Text classification</a></p>
<p><a href="https://github.com/allenai/allennlp/blob/master/allennlp/data/dataset_readers/sequence_tagging.py" target="_blank" rel="noopener">Sequence labeling</a></p>
<p><a href="https://github.com/allenai/allennlp-models/blob/master/allennlp_models/lm/dataset_readers/simple_language_modeling.py" target="_blank" rel="noopener">Language modeling</a></p>
<p><a href="https://github.com/allenai/allennlp-models/blob/master/allennlp_models/pair_classification/dataset_readers/snli.py" target="_blank" rel="noopener">Natural language inference</a></p>
<p><a href="https://github.com/allenai/allennlp-models/blob/master/allennlp_models/structured_prediction/dataset_readers/srl.py" target="_blank" rel="noopener">Semantic role labeling</a></p>
<p><a href="https://github.com/allenai/allennlp-models/blob/master/allennlp_models/generation/dataset_readers/seq2seq.py" target="_blank" rel="noopener">Seq2Seq tasks</a></p>
<p><a href="https://github.com/allenai/allennlp-models/blob/master/allennlp_models/structured_prediction/dataset_readers/penn_tree_bank.py" target="_blank" rel="noopener">Constituency parsing</a> and <a href="https://github.com/allenai/allennlp-models/blob/master/allennlp_models/structured_prediction/dataset_readers/universal_dependencies.py" target="_blank" rel="noopener">dependency parsing</a></p>
<p><a href="https://github.com/allenai/allennlp-models/tree/master/allennlp_models/rc/dataset_readers" target="_blank" rel="noopener">Reading comprehension</a></p>
<p><a href="https://github.com/allenai/allennlp-semparse" target="_blank" rel="noopener">Semantic parsing</a></p>
</blockquote>
<ul>
<li><strong>Vocabulary</strong></li>
</ul>
<blockquote>
<p><code>Vocabulary</code> manages different mappings using a concept called <em>namespaces</em>. Each namespace is a distinct mapping from strings to integers, so strings in different namespaces are treated separately. </p>
</blockquote>
<blockquote>
<p> create a <code>Vocabulary</code> object is to pass a collection of <code>Instances</code> to the <code>Vocabulary.from_instances()</code> method. This will count all strings in the <code>Instances</code> that need to be mapped to integers, then use those counts to decide what strings should be in the vocabulary. </p>
</blockquote>
<h3 id="5-2-Text-Representation"><a href="#5-2-Text-Representation" class="headerlink" title="5.2. Text Representation"></a>5.2. Text Representation</h3><blockquote>
<ul>
<li>GloVe or word2vec embeddings</li>
<li>Character CNNs</li>
<li>POS tag embeddings</li>
<li>Combination of GloVe and character CNNs</li>
<li>wordpieces and BERT</li>
</ul>
</blockquote>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201022230601867.png" alt=""></p>
<ul>
<li><strong>GloVe</strong></li>
</ul>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201022230658192.png" alt=""></p>
<blockquote>
<ol>
<li>Tokenizer (Text → Tokens)</li>
<li>TextField, TokenIndexer, and Vocabulary (Tokens → Ids)</li>
<li>TextFieldEmbedder (Ids → Vectors)</li>
</ol>
</blockquote>
<ul>
<li><strong>Tokenizers</strong></li>
</ul>
<blockquote>
<ul>
<li>Characters (“AllenNLP is great” → <code>["A", "l", "l", "e", "n", "N", "L", "P", " ", "i", "s", " ", "g", "r", "e", "a", "t"]</code>)</li>
<li>Wordpieces (“AllenNLP is great” → <code>["Allen", "##NL", "##P", "is", "great"]</code>)</li>
<li>Words (“AllenNLP is great” → <code>["AllenNLP", "is", "great"]</code>)</li>
</ul>
</blockquote>
<ul>
<li><strong>TokenIndexers</strong></li>
</ul>
<blockquote>
<p>Each <code>TokenIndexer</code> knows how to convert a <code>Token</code> into a representation that can be encoded by a corresponding piece of the model. This could be just mapping the token to an index in some vocabulary, or it could be breaking up the token into characters or wordpieces and representing the token by a sequence of indexed characters</p>
</blockquote>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script>
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        Author:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://liudongdong1.github.io" rel="external nofollow noreferrer">liudongdong1</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        Link:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://liudongdong1.github.io/2020/10/20/nlp/framework/allennlpintroduce/">https://liudongdong1.github.io/2020/10/20/nlp/framework/allennlpintroduce/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        Reprint policy:
                    </i>
                </span>
                <span class="reprint-info">
                    All articles in this blog are used except for special statements
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    reprint polocy. If reproduced, please indicate source
                    <a href="https://liudongdong1.github.io" target="_blank">liudongdong1</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>Copied successfully, please follow the reprint policy of this article</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">more</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/tags/Model/">
                                    <span class="chip bg-color">Model</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="qq,qzone,wechat,weibo,douban" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
                <style>
    #reward {
        margin: 40px 0;
        text-align: center;
    }

    #reward .reward-link {
        font-size: 1.4rem;
        line-height: 38px;
    }

    #reward .btn-floating:hover {
        box-shadow: 0 6px 12px rgba(0, 0, 0, 0.2), 0 5px 15px rgba(0, 0, 0, 0.2);
    }

    #rewardModal {
        width: 320px;
        height: 350px;
    }

    #rewardModal .reward-title {
        margin: 15px auto;
        padding-bottom: 5px;
    }

    #rewardModal .modal-content {
        padding: 10px;
    }

    #rewardModal .close {
        position: absolute;
        right: 15px;
        top: 15px;
        color: rgba(0, 0, 0, 0.5);
        font-size: 1.3rem;
        line-height: 20px;
        cursor: pointer;
    }

    #rewardModal .close:hover {
        color: #ef5350;
        transform: scale(1.3);
        -moz-transform:scale(1.3);
        -webkit-transform:scale(1.3);
        -o-transform:scale(1.3);
    }

    #rewardModal .reward-tabs {
        margin: 0 auto;
        width: 210px;
    }

    .reward-tabs .tabs {
        height: 38px;
        margin: 10px auto;
        padding-left: 0;
    }

    .reward-content ul {
        padding-left: 0 !important;
    }

    .reward-tabs .tabs .tab {
        height: 38px;
        line-height: 38px;
    }

    .reward-tabs .tab a {
        color: #fff;
        background-color: #ccc;
    }

    .reward-tabs .tab a:hover {
        background-color: #ccc;
        color: #fff;
    }

    .reward-tabs .wechat-tab .active {
        color: #fff !important;
        background-color: #22AB38 !important;
    }

    .reward-tabs .alipay-tab .active {
        color: #fff !important;
        background-color: #019FE8 !important;
    }

    .reward-tabs .reward-img {
        width: 210px;
        height: 210px;
    }
</style>

<div id="reward">
    <a href="#rewardModal" class="reward-link modal-trigger btn-floating btn-medium waves-effect waves-light red">赏</a>

    <!-- Modal Structure -->
    <div id="rewardModal" class="modal">
        <div class="modal-content">
            <a class="close modal-close"><i class="fas fa-times"></i></a>
            <h4 class="reward-title">你的赏识是我前进的动力</h4>
            <div class="reward-content">
                <div class="reward-tabs">
                    <ul class="tabs row">
                        <li class="tab col s6 alipay-tab waves-effect waves-light"><a href="#alipay">支付宝</a></li>
                        <li class="tab col s6 wechat-tab waves-effect waves-light"><a href="#wechat">微 信</a></li>
                    </ul>
                    <div id="alipay">
                        <img src="/medias/reward/alipay.jpg" class="reward-img" alt="支付宝打赏二维码">
                    </div>
                    <div id="wechat">
                        <img src="/medias/reward/wechat.png" class="reward-img" alt="微信打赏二维码">
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>

<script>
    $(function () {
        $('.tabs').tabs();
    });
</script>

            
        </div>
    </div>

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;Previous</div>
            <div class="card">
                <a href="/2020/10/20/nlp/wordembedding/">
                    <div class="card-image">
                        
                        <img src="https://gitee.com/github-25970295/blogImage/raw/master/img/view-of-coffee-beans.jpg" class="responsive-img" alt="WordEmbedding">
                        
                        <span class="card-title">WordEmbedding</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            
TEXT processing deals with humongous amount of text to perform different range of tasks like clustering in the google s
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2020-10-20
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/NLP/" class="post-category">
                                    NLP
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/embedding/">
                        <span class="chip bg-color">embedding</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                Next&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/2020/10/19/nlp/relationextraction/">
                    <div class="card-image">
                        
                        <img src="https://gitee.com/github-25970295/blogImage/raw/master/img/79.jpeg" class="responsive-img" alt="RelationExtraction">
                        
                        <span class="card-title">RelationExtraction</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            1. Challenges
数据规模问题：人工精准地标注句子级别的数据代价十分高昂，需要耗费大量的时间和人力。在实际场景中，面向数以千计的关系、数以千万计的实体对、以及数以亿计的句子，依靠人工标注训练数据几乎是不可能完成的任务。
远程监督：
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2020-10-19
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/NLP/" class="post-category">
                                    NLP
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/NLP/">
                        <span class="chip bg-color">NLP</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/libs/codeBlock/codeBlockFuction.js"></script>

<!-- 代码语言 -->

<script type="text/javascript" src="/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/libs/codeBlock/codeShrink.js"></script>


<!-- 代码块折行 -->

<style type="text/css">
code[class*="language-"], pre[class*="language-"] { white-space: pre !important; }
</style>


    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;TOC</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // modify the toc link href to support Chinese.
        let i = 0;
        let tocHeading = 'toc-heading-';
        $('#toc-content a').each(function () {
            $(this).attr('href', '#' + tocHeading + (++i));
        });

        // modify the heading title id to support Chinese.
        i = 0;
        $('#articleContent').children('h2, h3, h4').each(function () {
            $(this).attr('id', tocHeading + (++i));
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>

    

</main>




    <!-- <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="463294659"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/libs/aplayer/APlayer.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/meting@2/dist/Meting.min.js"></script>

    
    <div class="container row center-align" style="margin-bottom: 0px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            <span id="year">2019</span>
            <a href="https://liudongdong1.github.io" target="_blank">liudongdong</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            <br>
            
            &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                class="white-color">1206.4k</span>&nbsp;字
            
            
            
            
            
            
            <span id="busuanzi_container_site_pv">
                |&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;<span id="busuanzi_value_site_pv"
                    class="white-color"></span>&nbsp;次
            </span>
            
            
            <span id="busuanzi_container_site_uv">
                |&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;<span id="busuanzi_value_site_uv"
                    class="white-color"></span>&nbsp;人
            </span>
            
            <br>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/liudongdong1/" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:3463264078@qq.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>














    <a href="https://blog.csdn.net/liudongdong19/" class="tooltipped" target="_blank" data-tooltip="关注我的CSDN: https://blog.csdn.net/liudongdong19/" data-position="top" data-delay="50">
        <i class="fab fa-csdn">C</i>
    </a>





</div>
    </div>
</footer>

<div class="progress-bar"></div>
 -->

    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;Search</span>
            <input type="search" id="searchInput" name="s" placeholder="Please enter a search keyword"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script src="/js/search.js"></script>
<script type="text/javascript">
$(function () {
    searchFunc("/search.xml", 'searchInput', 'searchResult');
});
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script type="text/javascript" src="/js/CFS.Snow.min.js"></script>
    <!-- 点击爆灯效果 -->
    <canvas class="fireworks" style="position: fixed;left: 0;top: 0;z-index: 1; pointer-events: none;" ></canvas> 
    <script type="text/javascript" src="//cdn.bootcss.com/animejs/2.2.0/anime.min.js"></script> 
    <script type="text/javascript" src="/js/fireworks.js"></script>
    <!--动态线条背景-->
    <script type="text/javascript"
        color="122 103 238" opacity='0.7' zIndex="-2" count="200" src="//cdn.bootcss.com/canvas-nest.js/1.0.0/canvas-nest.min.js">
    </script>
    <!-- 天气 -->
    <!-- weather -->
    <!-- weather -->
    <script type="text/javascript">
         WIDGET = {FID: 'knAMQaFanP'}
    </script>
    <script type="text/javascript" src="https://apip.weatherdt.com/float/static/js/r.js?v=1111"></script>
    <script type="text/javascript" src="https://apip.weatherdt.com/float/static/js/r.js?v=1111"></script>
    <script src="/libs/materialize/materialize.min.js"></script>
    <script src="/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/libs/aos/aos.js"></script>
    <script src="/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/js/matery.js"></script>

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    
    <script async src="/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    

    
    
    <script type="text/javascript" size="150" alpha='0.6'
        zIndex="-1" src="/libs/background/ribbon-refresh.min.js" async="async"></script>
    

    

    
    <script src="/libs/instantpage/instantpage.js" type="module"></script>
    
    <!-- {% include '_custom/custom.swig' %} -->

</body>

</html>
