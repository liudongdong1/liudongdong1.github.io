<!DOCTYPE HTML>
<html lang="en">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="WordEmbedding, AIOT，Space&amp;Temporal Sequence Analysis，SpringBoot，liudongdong1 .etc">
    <meta name="description" content="">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>WordEmbedding | DaybyDay</title>
    <link rel="icon" type="image/png" href="/favicon.png">

    <link rel="stylesheet" type="text/css" href="/libs/awesome/css/all.css">
    <link rel="stylesheet" type="text/css" href="/libs/materialize/materialize.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/aos/aos.css">
    <link rel="stylesheet" type="text/css" href="/libs/animate/animate.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/lightGallery/css/lightgallery.min.css">
    <link rel="stylesheet" type="text/css" href="/css/matery.css">
    <link rel="stylesheet" type="text/css" href="/css/my.css">

    <script src="/libs/jquery/jquery.min.js"></script>

<meta name="generator" content="Hexo 4.2.0"><link rel="alternate" href="/atom.xml" title="DaybyDay" type="application/atom+xml">
<link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css"></head>


<body>
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/" class="waves-effect waves-light">
                    
                    <img src="/medias/logo.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">DaybyDay</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>Index</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>Tags</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>Categories</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>Archives</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/" class="waves-effect waves-light">

      
      <i class="fas fa-user-circle" style="zoom: 0.6;"></i>
      
      <span>About</span>
      <i class="fas fa-chevron-down" aria-hidden="true" style="zoom: 0.6;"></i>
    </a>
    <ul class="sub-nav menus_item_child ">
      
      <li>
        <a href="/about">
          
          <i class="fas fa-user-circle" style="margin-top: -20px; zoom: 0.6;"></i>
          
          <span>about</span>
        </a>
      </li>
      
      <li>
        <a href="/resume">
          
          <i class="fa fa-user-secret" style="margin-top: -20px; zoom: 0.6;"></i>
          
          <span>resume</span>
        </a>
      </li>
      
    </ul>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/friends" class="waves-effect waves-light">
      
      <i class="fas fa-address-book" style="zoom: 0.6;"></i>
      
      <span>Friends</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="Search" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/medias/logo.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">DaybyDay</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			Index
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			Tags
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			Categories
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			Archives
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="javascript:;">
			
				<i class="fa-fw fas fa-user-circle"></i>
			
			About
			<span class="m-icon"><i class="fas fa-chevron-right"></i></span>
		</a>
            <ul  style="background:  ;" >
              
                <li>

                  <a href="/about " style="margin-left:75px">
				  
				   <i class="fa fas fa-user-circle" style="position: absolute;left:50px" ></i>
			      
		          <span>about</span>
                  </a>
                </li>
              
                <li>

                  <a href="/resume " style="margin-left:75px">
				  
				   <i class="fa fa fa-user-secret" style="position: absolute;left:50px" ></i>
			      
		          <span>resume</span>
                  </a>
                </li>
              
            </ul>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/friends" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-address-book"></i>
			
			Friends
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/liudongdong1" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/liudongdong1" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://gitee.com/github-25970295/blogImage/raw/master/img/view-of-coffee-beans.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">WordEmbedding</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <link rel="stylesheet" href="/libs/tocbot/tocbot.css">
<style>
    #articleContent h1::before,
    #articleContent h2::before,
    #articleContent h3::before,
    #articleContent h4::before,
    #articleContent h5::before,
    #articleContent h6::before {
        display: block;
        content: " ";
        height: 100px;
        margin-top: -100px;
        visibility: hidden;
    }

    #articleContent :focus {
        outline: none;
    }

    .toc-fixed {
        position: fixed;
        top: 64px;
    }

    .toc-widget {
        width: 345px;
        padding-left: 20px;
    }

    .toc-widget .toc-title {
        margin: 35px 0 15px 0;
        padding-left: 17px;
        font-size: 1.5rem;
        font-weight: bold;
        line-height: 1.5rem;
    }

    .toc-widget ol {
        padding: 0;
        list-style: none;
    }

    #toc-content {
        height: calc(100vh - 250px);
        overflow: auto;
    }

    #toc-content ol {
        padding-left: 10px;
    }

    #toc-content ol li {
        padding-left: 10px;
    }

    #toc-content .toc-link:hover {
        color: #42b983;
        font-weight: 700;
        text-decoration: underline;
    }

    #toc-content .toc-link::before {
        background-color: transparent;
        max-height: 25px;

        position: absolute;
        right: 23.5vw;
        display: block;
    }

    .toc-fixed .toc-link::before{
        position: fixed!important;/*当toc的位置改为fixed时，.toc-link::before也要改为fixed*/
    }

    #toc-content .is-active-link {
        color: #42b983;
    }

    #toc-content .is-active-link::before {
        background-color: #42b983;
    }

    #floating-toc-btn {
        position: fixed;
        right: 15px;
        bottom: 76px;
        padding-top: 15px;
        margin-bottom: 0;
        z-index: 998;
    }

    #floating-toc-btn .btn-floating {
        width: 48px;
        height: 48px;
    }

    #floating-toc-btn .btn-floating i {
        line-height: 48px;
        font-size: 1.4rem;
    }
</style>
<div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/tags/embedding/">
                                <span class="chip bg-color">embedding</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/categories/NLP/" class="post-category">
                                NLP
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>Publish Date:&nbsp;&nbsp;
                    2020-10-20
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>Update Date:&nbsp;&nbsp;
                    2021-06-21
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>Word Count:&nbsp;&nbsp;
                    5.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>Read Times:&nbsp;&nbsp;
                    23 Min
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>Read Count:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">
        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p><strong>TEXT processing</strong> deals with humongous amount of text to perform different range of tasks like clustering in the google search example, classification in the second and Machine Translation. How to create a representation for words that capture their <em>meanings</em>, <em>semantic relationships</em> and the different types of contexts they are used in.</p>
</blockquote>
<blockquote>
<ul>
<li>作为 Embedding 层嵌入到深度模型中，实现将高维稀疏特征到低维稠密特征的转换（如 Wide&amp;Deep、DeepFM 等模型）；</li>
<li>作为预训练的 Embedding 特征向量，与其他特征向量拼接后，一同作为深度学习模型输入进行训练（如 FNN）；</li>
<li>在召回层中，通过计算用户和物品的 Embedding 向量相似度，作为召回策略（比 Youtube 推荐模型等）；</li>
<li>实时计算用户和物品的 Embedding 向量，并将其作为实时特征输入到深度学习模型中（比 Airbnb 的 embedding 应用）。</li>
</ul>
</blockquote>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200721081514640.png" alt=""></p>
<h2 id="1-Item-Embedding"><a href="#1-Item-Embedding" class="headerlink" title="1. Item Embedding"></a>1. Item Embedding</h2><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200721081648881.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200721083502813.png" alt=""></p>
<h2 id="2-Img-Embedding"><a href="#2-Img-Embedding" class="headerlink" title="2. Img Embedding"></a>2. Img Embedding</h2><blockquote>
<p>图片作为文章的门面特征，对推荐也很重要，可以通过 resnet 得到图片的向量，还可以通过 image caption  得到对一张图片的中文描述，对于娱乐类的新闻，还可以利用 facenet 识别出组图中，哪一张包含明星，对于动漫类类的新闻可以利用 OCR 识别出漫画里的文字，对于年龄，性别有明显倾向的场景还可以利用 resnet 改变图片的风格。</p>
</blockquote>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200721084331320.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200721084622194.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200721084850357.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200721090031252.png" alt=""></p>
<h2 id="3-Methods"><a href="#3-Methods" class="headerlink" title="3. Methods"></a>3. Methods</h2><h3 id="3-1-Frequency-based-Embedding"><a href="#3-1-Frequency-based-Embedding" class="headerlink" title="3.1. Frequency based Embedding"></a>3.1. Frequency based Embedding</h3><ul>
<li><strong>Count Vector:</strong> using top numbers words based on frequency and then prepare a dictionary.</li>
</ul>
<blockquote>
<p>Consider a Corpus C of D documents {d1,d2…..dD} and N unique tokens extracted out of the corpus C. The N tokens will form our dictionary and the size of the Count Vector matrix M will be given by D X N. Each row in the matrix M contains the frequency of tokens in document D(i).</p>
</blockquote>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200716234329746.png" alt=""></p>
<p>Prediction based Embedding</p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200716234619560.png" alt=""></p>
<ul>
<li><p><strong>TF-IDF vectorization:</strong>takes into account not just the occurrence of a word in a single document but in the entire corpus. (1）搜索引擎；（2）关键词提取；（3）文本相似性；（4）文本摘要</p>
<ul>
<li>TF词频</li>
</ul>
<p>$$<br>tf_{ij}=n_{i,j}/\sum_kn_{k,j}\<br>$$</p>
<p>$n_{i,j}$ 是该词再文件$d_j$ 中出现的次数，分母是文件$d_j$中所有词汇出现的次数和。</p>
<ul>
<li>IDF逆向文件频率：某一特定词语的IDF，可以由<strong>总文件数目除以包含该词语的文件的数目</strong>，<strong>再将得到的商取对数得到</strong>。</li>
</ul>
<p>$$<br>idf_i=log|D|/|{j:t_i\epsilon d_j}|<br>$$</p>
<p>|D|是预料库中文件总数，$|{j:t_i\epsilon d_j}|$ 表示包含词语$t_i$ 的文件数目。<br>$$<br>TF-IDF=TF*IDF<br>$$</p>
</li>
<li><h4 id="Co-Occurrence-Matrix-with-a-fixed-context-window"><a href="#Co-Occurrence-Matrix-with-a-fixed-context-window" class="headerlink" title="Co-Occurrence Matrix with a fixed context window"></a>Co-Occurrence Matrix with a fixed context window</h4><blockquote>
<p>Similar words tend to occur together and will have similar context for example .Co-occurrence matrix is decomposed using techniques like PCA, SVD etc. into factors and combination of these factors forms the word vector representation.</p>
</blockquote>
<ul>
<li>Co-occurrence – For a given corpus, the co-occurrence of a pair of words say w1 and w2 is the number of times they have appeared together in a Context Window.</li>
<li>Context Window – Context window is specified by a number and the direction. </li>
</ul>
</li>
</ul>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200717101404690.png" alt=""></p>
<ul>
<li>Advantage:<ul>
<li><code>preserves the semantic relationship</code> between words,</li>
<li>uses SVD at its core, producing more accurate word vector representations.</li>
<li>uses factorization which is a well-defined problem and can be efficiently solved.</li>
<li><code>computed once and can be use anytime once computed</code>.</li>
</ul>
</li>
<li><strong>Disadvantages:</strong> requires huge memory to store the co-occurrence matrix.</li>
</ul>
<h3 id="3-2-Bag-of-Words-BOW"><a href="#3-2-Bag-of-Words-BOW" class="headerlink" title="3.2. Bag of Words(BOW)"></a>3.2. Bag of Words(BOW)</h3><blockquote>
<ol>
<li>Tokenize the text into sentences</li>
<li>Tokenize sentences into words</li>
<li>Remove punctuation or stop words</li>
<li>Convert the words to lower text</li>
<li><code>Create the frequency distribution of words</code></li>
</ol>
</blockquote>
<pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true">#Creating frequency distribution of words using nltk</span>
<span class="token keyword">from</span> nltk<span class="token punctuation">.</span>tokenize <span class="token keyword">import</span> sent_tokenize
<span class="token keyword">from</span> nltk<span class="token punctuation">.</span>tokenize <span class="token keyword">import</span> word_tokenize
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>feature_extraction<span class="token punctuation">.</span>text <span class="token keyword">import</span> CountVectorizer
text<span class="token operator">=</span><span class="token triple-quoted-string string">"""Achievers are not afraid of Challenges, rather they relish them, thrive in them, use them. Challenges makes is stronger.
        Challenges makes us uncomfortable. If you get comfortable with uncomfort then you will grow. Challenge the challenge """</span>
<span class="token comment" spellcheck="true">#Tokenize the sentences from the text corpus</span>
tokenized_text<span class="token operator">=</span>sent_tokenize<span class="token punctuation">(</span>text<span class="token punctuation">)</span>
<span class="token comment" spellcheck="true">#using CountVectorizer and removing stopwords in english language</span>
cv1<span class="token operator">=</span> CountVectorizer<span class="token punctuation">(</span>lowercase<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>stop_words<span class="token operator">=</span><span class="token string">'english'</span><span class="token punctuation">)</span>
<span class="token comment" spellcheck="true">#fitting the tonized senetnecs to the countvectorizer</span>
text_counts<span class="token operator">=</span>cv1<span class="token punctuation">.</span>fit_transform<span class="token punctuation">(</span>tokenized_text<span class="token punctuation">)</span>
<span class="token comment" spellcheck="true"># printing the vocabulary and the frequency distribution pf vocabulary in tokinzed sentences</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>cv1<span class="token punctuation">.</span>vocabulary_<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>text_counts<span class="token punctuation">.</span>toarray<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201024214303559.png" alt=""></p>
<blockquote>
<p>each document is represented as a word-count vector, these counts can be <code>binary counts or absolute counts</code>, but the size equal to the size(Voc);</p>
<ul>
<li>huge amount of weights;</li>
<li>computationally intensive;</li>
<li><code>lack of meaningful relations and no consideration for order of words</code>;</li>
</ul>
</blockquote>
<h3 id="3-3-静态向量"><a href="#3-3-静态向量" class="headerlink" title="3.3. 静态向量"></a>3.3. 静态向量</h3><blockquote>
<p> <code>translate large sparse vectors into a lower-dimensional space that preserves semantic relationships</code>.</p>
</blockquote>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201024215025229.png" alt=""></p>
<h4 id="3-3-1-Latent-Semantic-Analysis-LSA"><a href="#3-3-1-Latent-Semantic-Analysis-LSA" class="headerlink" title="3.3.1.  Latent Semantic Analysis (LSA)"></a>3.3.1.  Latent Semantic Analysis (LSA)</h4><h4 id="3-3-2-Latent-Dirichlet-Allocation-LDA"><a href="#3-3-2-Latent-Dirichlet-Allocation-LDA" class="headerlink" title="3.3.2.  Latent Dirichlet Allocation (LDA)"></a>3.3.2.  Latent Dirichlet Allocation (LDA)</h4><h4 id="3-3-3-PCA"><a href="#3-3-3-PCA" class="headerlink" title="3.3.3. PCA"></a>3.3.3. PCA</h4><h4 id="3-3-4-Word2Vector-CBOW-Skip-ngram"><a href="#3-3-4-Word2Vector-CBOW-Skip-ngram" class="headerlink" title="3.3.4. Word2Vector(CBOW, Skip-ngram)"></a>3.3.4. Word2Vector(CBOW, Skip-ngram)</h4><p>In paper:”<strong>Efficient Estimation of Word Representations in Vector Space</strong>“:  the training complexity is proportional to $O=E<em>T</em>Q$; E is the epoch; T is the number of words in the training set, and Q is the defined further for each model architecture;</p>
<ul>
<li><p><strong>NNLM(Feedforward Neural Net Language Model):</strong> consists of input, projection, hidden and output layers; N previous words are encoded using 1-of-V coding, V: the vocabulary size; $N<em>D:$ the projection layer dimensionality; H: the hidden  size; the computational complexity per each example: $Q=N*D+N</em> D* H+H* V$</p>
</li>
<li><p><strong>Recurrent Neural Net Language Model(RNNLM):</strong> the word representations D have the same dimensionality as the hidden layer H, $Q=H<em>H+H</em>V$;</p>
</li>
<li><p><strong>New Log-linear Models:</strong>  neural network language model can be successfully trained in two steps: first <code>continuous word vectors are learned using simple model</code>, and then <code>N-gram NNLM is trained on the top of these distributed representations of words</code>;</p>
<ul>
<li><strong><code>Continuous Bag-of-Words Models</code>:</strong> similar to the feed forward NNLM, where the non-linear hidden layer is removed and the projection layer is shared for all words, all words projected into the same position(their vectors are averaged), the order of words in the history does not influence the projection. $Q=N<em>D+D</em>log_2(V)$;</li>
</ul>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201025125100209.png" alt=""></p>
<ul>
<li><strong><code>Continuous Skip-gram Model</code>:</strong> instead of predicting the current word based on the context, it tries to maximize classification of a word based on another word in the same sentence. In other words: use each current word as an input to a log-linear classifier with continuous projection layer and predict words within a certain range before and after the current word; $Q=C<em>(D+D</em>log_2(V))$; C: the maximum distance of the words;</li>
</ul>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201025123844292.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201025123923520.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201025124909812.png" alt=""></p>
</li>
</ul>
<p>  <img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201025122012098.png" alt=""></p>
<blockquote>
<p>New model architectures. The CBOW architecture predicts the current word based on the context, and the Skip-gram predicts surrounding words given the current word. </p>
<ul>
<li>support algebraic operations with the vector representation of words, Vector=vector(“biggest”)-vector(“big”)+vector(“small”)=vector(“smallest”)</li>
<li>when train high dimensional word vectors on large amount data, the resulting vectors can be used to answer very subtle semantic relationships between words, which is good for machine translation, information retrieval and question answering systems;</li>
</ul>
</blockquote>
<p>  <img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201025122757936.png" alt=""></p>
<p>  <img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201025122930252.png" alt=""></p>
<p>  <img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201025123038373.png" alt=""></p>
<h4 id="3-3-5-GloVe-Embedding"><a href="#3-3-5-GloVe-Embedding" class="headerlink" title="3.3.5. GloVe Embedding"></a>3.3.5. GloVe Embedding</h4><blockquote>
<p>In paper: “GloVe: Global Vectors for Word Representation”: </p>
<ul>
<li>analyze and make explicit the model properties needed for such regularities to emerge in word vectors, and propose a new global log-bilinear regression model that combines the advantages of the two major model families in the literature: <code>global matrix factorization and local context window methods</code>;</li>
<li>efficiently leverages statistical information by training only on the nonzero elements in a <code>word-word co-occurence matrix</code>;</li>
<li>open source: <a href="http://nlp.stanford.edu/projects/glove/" target="_blank" rel="noopener">http://nlp.stanford.edu/projects/glove/</a>.</li>
</ul>
</blockquote>
<ul>
<li>$X$:  the matrix of word-word co-occurrence counts, $X_{ij}$: the number of times word j occurs in the context of word i;</li>
<li>$X_i=\sum_k X_{ik}$ : the number of times any word appears in the context of word i;</li>
<li>$P_{ij}=P(j|i)=x_{ij}/X_i$: the probability that word j appear in the context of word i;</li>
</ul>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201025131020291.png" alt=""></p>
<blockquote>
<p>The ratio is better able to distinguish relevant words (solid and gas) from irrelevant words (water and fashion) and it is also better able to discriminate between the two relevant words.</p>
</blockquote>
<ul>
<li>$F(w_i, w_j, w_k)=P_{ik}/P_{jk}$:   w: the word vector; $w_k$: the separate context word vectors;</li>
<li>loss function:  <img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201025134201555.png" alt=""></li>
</ul>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201025131931244.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201025132137995.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201025132259638.png" alt=""></p>
<p><strong>改良 SkipGram</strong></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201025132437364.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201025132513525.png" alt=""></p>
<blockquote>
<ul>
<li>word2vec是局部语料库训练的，其特征提取是基于滑窗的；而glove的滑窗是为了构建co-occurance matrix，是基于全局语料的，可见glove需要事先统计共现概率；因此，word2vec可以进行在线学习，glove则需要统计固定语料信息。</li>
<li>word2vec是无监督学习，同样由于不需要人工标注；glove通常被认为是无监督学习，但实际上glove还是有label的，即共现次数<img src="https://www.zhihu.com/equation?tex=log%28X_%7Bij%7D%29" alt="[公式]">。</li>
<li>word2vec损失函数实质上是带权重的交叉熵，权重固定；glove的损失函数是最小平方损失函数，权重可以做映射变换。</li>
<li>总体来看，<strong>glove可以被看作是更换了目标函数和权重函数的全局word2vec</strong>。</li>
</ul>
</blockquote>
<h4 id="3-3-6-FastText"><a href="#3-3-6-FastText" class="headerlink" title="3.3.6. FastText"></a>3.3.6. FastText</h4><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201126120608705.png" alt=""></p>
<h3 id="3-4-动态向量"><a href="#3-4-动态向量" class="headerlink" title="3.4.  动态向量"></a>3.4.  动态向量</h3><blockquote>
<p>由于静态向量表示中每个词被表示成一个固定的向量，无法有效解决一词多义的问题。在动态向量表示中，模型不再是向量对应关系，而是一个训练好的模型。</p>
</blockquote>
<h4 id="3-4-1-ELMo"><a href="#3-4-1-ELMo" class="headerlink" title="3.4.1. ELMo"></a>3.4.1. ELMo</h4><ul>
<li>(1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy);</li>
<li>Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pretrained on a large text corpus.</li>
<li>ELMo（Embeddings from Language Models）是2018年3月发表，获得了NAACL18的Best Paper</li>
</ul>
<blockquote>
<ol>
<li>预训练biLM模型，通常由两层bi-LSTM组成，之间用residual connection连接起来。</li>
<li>在任务语料上fine tuning上一步得到的biLM模型，这里可以看做是biLM的domain transfer。</li>
<li><code>利用ELMo提取word embedding</code>，将word embedding<code>作为输入</code>来对任务进行训练。</li>
</ol>
</blockquote>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201126120802633.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201126121055516.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201126121352272.png" alt=""></p>
<h4 id="3-4-2-GPT"><a href="#3-4-2-GPT" class="headerlink" title="3.4.2. GPT"></a>3.4.2. GPT</h4><blockquote>
<p>GPT-1（Generative Pre-Training）是OpenAI在2018年提出的，采用pre-training和fine-tuning的下游统一框架，将预训练和finetune的结构进行了统一，解决了之前两者分离的使用的不确定性（例如ELMo）。此外，GPT使用了Transformer结构克服了LSTM不能捕获远距离信息的缺点。GPT主要分为两个阶段：pre-training和fine-tuning.</p>
</blockquote>
<ul>
<li><strong>Pre-training</strong></li>
</ul>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201126132635179.png" alt=""></p>
<ul>
<li><strong>Fine-tuning</strong> :采用无监督学习预训练好模型后后，可以把模型模型迁移到新的任务中，并根据新任务来调整模型的参数。</li>
</ul>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201126132849556.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201126132916870.png" alt=""></p>
<h4 id="3-4-3-BERT-Bidirectional-Encoder-Representations-from-Transformers"><a href="#3-4-3-BERT-Bidirectional-Encoder-Representations-from-Transformers" class="headerlink" title="3.4.3. BERT(Bidirectional Encoder Representations from Transformers)"></a>3.4.3. BERT(Bidirectional Encoder Representations from Transformers)</h4><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201126133039218.png" alt=""></p>
<blockquote>
<p>BERT进一步增强了词向的型泛化能力，充分描述字符级、词级、句子级甚至句间的关系特征。BERT的输入的编码向量（长度为512）是3种Embedding特征element-wise和.</p>
</blockquote>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201126133152652.png" alt=""></p>
<ul>
<li><p><strong>Input Features:</strong></p>
<ul>
<li>Token Embedding (WordPiece)：将<code>单词划分成一组有限的公共词单元</code>，能在单词的有效性和字符的灵活性之间取得一个折中的平衡。如图中的“playing”被拆分成了“play”和“ing”；</li>
<li>Segment Embedding：用于<code>区分两个句子</code>，如B是否是A的下文（对话场景，问答场景等）。对于<code>句子对，第一个句子的特征值是0，第二个句子的特征值是1</code>；</li>
<li>Position Embedding：<code>将单词的位置信息编码成特征向量</code>，Position embedding能有效将单词的位置关系引入到模型中，提升模型对句子理解能力；</li>
<li><code>[CLS]</code>表示该特征用于分类模型，对非分类模型，该符合可以省去。<code>[SEP]</code>表示分句符号，用于断开输入语料中的两个句子。</li>
</ul>
</li>
<li><p><strong>Masked Language Model(MLM)</strong></p>
</li>
</ul>
<blockquote>
<p>是指在<code>训练时随机从输入语料中mask掉一些单词</code>，然后通过该词上下文来预测它（非常像让模型来做完形填空）。80%<code>概率直接替换为</code>[MASK]； <code>10%</code>概率替换为其他任意Token； <code>10%</code>概率保留为原始Token 。</p>
</blockquote>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201126133616522.png" alt=""></p>
<ul>
<li><strong>Next Sentence Predictions（NSP）</strong></li>
</ul>
<blockquote>
<p>BERT采用NSP任务来增强模型对句子关系的理解，即给出两个句子A、B，模型预测B是否是A的下一句。</p>
</blockquote>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201126133832576.png" alt=""></p>
<ul>
<li><strong>Fine-tuning</strong><ul>
<li>句对关系判断：第一个起始符号[CLS]经过编码后，增加Softmax层，即可用于分类；</li>
<li>单句分类任务：实现同“句对关系判断”；</li>
<li><code>问答类任务</code>：问答系统输入文本序列的question和包含answer的段落，并在序列中标记answer，让BERT模型学习标记answer开始和结束的向量来训练模型；</li>
<li>序列标准任务：识别系统输入标记好实体类别（人、组织、位置、其他无名实体）文本序列进行微调训练，识别实体类别时，将序列的每个Token向量送到预测NER标签的分类层进行识别。</li>
</ul>
</li>
</ul>
<h4 id="3-4-5-UniLM"><a href="#3-4-5-UniLM" class="headerlink" title="3.4.5. UniLM"></a>3.4.5. UniLM</h4><p>给定一个输入序列$x=x_1…x_n$，UniLM 通过下图的方式获取每个词条的基于上下文的向量表示。整个预训练过程利用单向的语言建模（unidirectional LM），双向的语言建模（bidirectional LM）和 Seq2Seq 语言建模（sequence-to-sequence LM）优化共享的 Transformer 网络。</p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201128145004244.png" alt=""></p>
<h3 id="3-5-Graph-Embedding"><a href="#3-5-Graph-Embedding" class="headerlink" title="3.5. Graph Embedding"></a>3.5. Graph Embedding</h3><blockquote>
<p>Graph Embedding是一种将图结构数据映射为低微稠密向量的过程，从而捕捉到图的拓扑结构、顶点与顶点的关系、以及其他的信息。目前，Graph Embedding方法大致可以分为两大类：1）浅层图模型；2）深度图模型。<strong>图嵌入（Graph / Network Embedding）</strong>和<strong>图神经网络（Graph Neural Networks, GNN）</strong>是两个类似的研究领域。<code>图嵌入旨在将图的节点表示成一个低维向量空间</code>，同时<code>保留网络的拓扑结构和节点信息</code>，以便在后续的图分析任务中可以直接使用现有的机器学习算法。</p>
</blockquote>
<h4 id="3-5-1-浅层图"><a href="#3-5-1-浅层图" class="headerlink" title="3.5.1. 浅层图"></a>3.5.1. 浅层图</h4><blockquote>
<p>浅层图模型主要是采用<code>random-walk + skip-gram</code>模式的embedding方法。主要是通过在图中采用<code>随机游走策略来生成多条节点列表</code>，然后将<code>每个列表相当于含有多个单词（图中的节点）的句子</code>，再用<code>skip-gram模型</code>来训练每个<code>节点的向量</code>。这些方法主要包括<code>DeepWalk、Node2vec、Metapath2vec</code>等。</p>
</blockquote>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201128145849897.png" alt=""></p>
<h5 id="1-DeepWalk"><a href="#1-DeepWalk" class="headerlink" title="1. DeepWalk"></a>1. DeepWalk</h5><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201126134723219.png" alt=""></p>
<p>到达节点后，下一步遍历其邻居节点的概率：</p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201126134840114.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201126134933080.png" alt=""></p>
<h5 id="2-Node2vec"><a href="#2-Node2vec" class="headerlink" title="2. Node2vec"></a>2. Node2vec</h5><blockquote>
<p>该模型通过调整random walk权重的方法使得节点的embedding向量更倾向于体现网络的同质性或结构性。Node2vec无法指定游走路径，且仅适用于解决只包含一种类型节点的同构网络，无法有效表示包含多种类型节点和边类型的复杂网络。</p>
</blockquote>
<ul>
<li><code>同质性</code>：指得是<code>距离相近的节点的embedding向量应近似</code>，如下图中，与节点相连的节点s1、s2、s3和s4的embedding向量应相似。为了使embedding向量能够表达网络的同质性，需要让随机游走更倾向于<code>DFS</code>，因为DFS更有可能通过多次跳转，到达远方的节点上，使游走序列集中在一个较大的集合内部，使得在一个集合内部的节点具有更高的相似性，从而表达图的同质性。</li>
<li><code>结构性</code>：<code>结构相似的节点的embedding向量应近似</code>，如下图中，与节点结构相似的节点的embedding向量应相似。为了表达结构性，需要随机游走更倾向于<code>BFS</code>，因为BFS会更多的在当前节点的邻域中游走，相当于对当前节点的网络结构进行扫描，从而使得embedding向量能刻画节点邻域的结构信息。</li>
</ul>
<h5 id="3-Metapath2vec"><a href="#3-Metapath2vec" class="headerlink" title="3. Metapath2vec"></a><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201126135506214.png" alt="">3. Metapath2vec</h5><blockquote>
<p>主要是在随机游走上使用基于meta-path的random walk来构建节点序列，然后用Skip-gram模型来完成顶点的Embedding。</p>
</blockquote>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201126140116288.png" alt=""></p>
<h5 id="4-APP"><a href="#4-APP" class="headerlink" title="4. APP"></a>4. APP</h5><blockquote>
<p>DeepWalk，node2vec 等，都无法保留图中的非对称信息。然而非对称性在很多问题，例如：社交网络中的链路预测、电商中的推荐等，中至关重要。</p>
</blockquote>
<h4 id="3-5-2-深度图"><a href="#3-5-2-深度图" class="headerlink" title="3.5.2. 深度图"></a>3.5.2. 深度图</h4><p><img src="https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20210608161356940.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogpictureV2/raw/master/propa_step.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogpictureV2/raw/master/table-1623202159387.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogpictureV2/raw/master/gnn_table.png" alt=""></p>
<blockquote>
<p>将图与深度模型结合，实现end-to-end训练模型，从而在图中提取拓扑图的空间特征。主要分为四大类：<code>Graph Convolution Networks (GCN)</code>，<code>Graph Attention Networks (GAT)</code>，<code>Graph AutoEncoder (GAE)</code>和<code>Graph Generative Networks (GGN)</code>。</p>
</blockquote>
<ul>
<li>基于<strong>spatial domain</strong>：基于<code>空域卷积</code>的方法直接<code>将卷积操作定义在每个结点的连接关系上</code>，跟传统的卷积神经网络中的卷积更相似一些。主要有两个问题：1）按照什么条件去<code>找</code>中心节点的邻居，也就是如何确定receptive field；2）按照什么方式<code>处理</code>包含不同数目邻居的特征。</li>
<li>基于<strong>spectral domain</strong>：借助卷积定理可以通过定义<code>频谱域上</code>的<code>内积</code>操作来得到<code>空间域图</code>上的卷积操作。</li>
</ul>
<h5 id="0-GNN"><a href="#0-GNN" class="headerlink" title="0. GNN"></a>0. GNN</h5><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201128151414174.png" alt=""></p>
<p>令 $H,O,X,X_n$  分别表示为 状态， 输出，特征，所有节点特征的向量:<br>$$<br>\begin{align}<br>H&amp;7=F(H,X)\<br>O&amp;=G(H,X_n)\<br>H^{t+1}&amp;=F(H^t,X) \<br>loss&amp;=\sum{i=1}^p(t_i-o_i)<br>\end{align}<br>$$</p>
<h5 id="1-GCN"><a href="#1-GCN" class="headerlink" title="1. GCN"></a>1. GCN</h5><blockquote>
<p>核心思想在于学习一个函数 $f$，通过聚合节点 $v_i$ <code>自身的特征</code> $X_i$ 和<code>邻居的特征</code> $X_j $获得节点的表示，其中 $j∈N(v_i) $为节点的邻居。</p>
</blockquote>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201128151958945.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201128152016958.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201128152040805.png" alt=""></p>
<h6 id="1-1-基于频谱（Spectral-Methods）"><a href="#1-1-基于频谱（Spectral-Methods）" class="headerlink" title="1.1. 基于频谱（Spectral Methods）"></a>1.1. 基于频谱（Spectral Methods）</h6><blockquote>
<p>(1)<code>对图结构的小小扰动将会导致不同的特征基</code>;(2)特征分解需要<code>较为庞大的计算代价</code>;(3)学习到<code>的滤波器是针对特定问题的,不能够将其进行推广到更丰富的图结构上</code>.ChebNet及其一阶近似是局部卷积操作,从而可以在图的不同位置共享相同的滤波器参数.  基于频谱方法的一个关键缺陷是其需要将整个图的信息载入内存中,这使得其在大规模的图结构(如大规模的社交网络分析)上不能有效的进行应用.</p>
</blockquote>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201128152717815.png" alt=""></p>
<h6 id="1-频谱卷积神经网络"><a href="#1-频谱卷积神经网络" class="headerlink" title=".1. 频谱卷积神经网络"></a>.1. 频谱卷积神经网络</h6><p><img src="https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20210608160028439.png" alt=""></p>
<h6 id="2-契比雪夫频谱卷积网络-ChebNet"><a href="#2-契比雪夫频谱卷积网络-ChebNet" class="headerlink" title=".2. 契比雪夫频谱卷积网络(ChebNet)"></a>.2. 契比雪夫频谱卷积网络(ChebNet)</h6><p><img src="https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20210608160042565.png" alt=""></p>
<h6 id="3-ChebNet的一阶近似"><a href="#3-ChebNet的一阶近似" class="headerlink" title=".3. ChebNet的一阶近似"></a>.3. ChebNet的一阶近似</h6><p><img src="https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20210608160110144.png" alt=""></p>
<h6 id="1-2-基于空间的方法（Spatial-Methods）"><a href="#1-2-基于空间的方法（Spatial-Methods）" class="headerlink" title="1.2. 基于空间的方法（Spatial Methods）"></a>1.2. 基于空间的方法（Spatial Methods）</h6><blockquote>
<p>基于空间的方法通过节点的空间关系来定义图卷积操作。为了将图像和图关联起来，可以将图像视为一个特殊形式的图，每个像素点表示一个节点.</p>
</blockquote>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201128152918580.png" alt=""></p>
<h5 id="2-GRN"><a href="#2-GRN" class="headerlink" title="2. GRN"></a>2. GRN</h5><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201128153113989.png" alt=""></p>
<p>节点 $v$ 首先从邻居汇总信息，其中 $A_v $为图邻接矩阵$ A$ 的子矩阵表示节点$ v$ 及其邻居的连接。类似 GRU 的更新函数，通过结合其他节点和上一时间的信息更新节点的隐状态。$a $用于获取节点 $v $邻居的信息，$z $和 $r$ 分别为更新和重置门。</p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201128153323524.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201128153337702.png" alt=""></p>
<h5 id="3-Graph-Attention"><a href="#3-Graph-Attention" class="headerlink" title="3. Graph Attention"></a>3. Graph Attention</h5><blockquote>
<p>与 GCN 对于节点所有的邻居平等对待相比，注意力机制可以<code>为每个邻居分配不同的注意力评分</code>，从而识别更重要的邻居。</p>
</blockquote>
<h5 id="4-GraphSAGE"><a href="#4-GraphSAGE" class="headerlink" title="4. GraphSAGE"></a>4. <strong>GraphSAGE</strong></h5><blockquote>
<p>GraphSAGE（Graph SAmple and aggreGatE）是基于空间域方法，其思想与基于频谱域方法相反，是直接在图上定义卷积操作，对<code>空间上相邻的节点</code>上进行运算。其计算流程主要分为三部：</p>
<ul>
<li><code>对图中每个节点领节点进行采样</code></li>
<li>根据<code>聚合函数聚合邻居节点信息（特征）</code></li>
<li>得到图中<code>各节点的embedding向量</code>，供下游任务使用</li>
</ul>
</blockquote>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201126145200343.png" alt=""></p>
<h5 id="5-DNGR"><a href="#5-DNGR" class="headerlink" title="5. DNGR"></a>5. DNGR</h5><blockquote>
<p>一种利用基于 Stacked Denoising Autoencoder（SDAE）提取特征的网络表示学习算法。</p>
</blockquote>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201128150733829.png" alt=""></p>
<h4 id="3-Research-Area"><a href="#3-Research-Area" class="headerlink" title="3. Research Area"></a>3. Research Area</h4><blockquote>
<p>图分析任务可以大致抽象为以下四类: ( a )节点分类，( b )链接预测，( c )聚类，以及( d )可视化。</p>
</blockquote>
<h4 id="1-姿态识别-amp-预测"><a href="#1-姿态识别-amp-预测" class="headerlink" title=".1. 姿态识别&amp;预测"></a>.1. 姿态识别&amp;预测</h4><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201128150910711.png" alt=""></p>
<h4 id="2-超图"><a href="#2-超图" class="headerlink" title=".2. 超图"></a>.2. 超图</h4><p><img src="https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20210608161529742.png" alt=""></p>
<h4 id="3-图构建"><a href="#3-图构建" class="headerlink" title=".3. 图构建"></a>.3. 图构建</h4><p><img src="https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20210608161558819.png" alt=""></p>
<h4 id="4-子图嵌入"><a href="#4-子图嵌入" class="headerlink" title=".4. 子图嵌入"></a>.4. 子图嵌入</h4><blockquote>
<p>图嵌入（Graph Embedding，也叫Network Embedding）是一种将图数据（通常为高维稠密的矩阵）映射为低微稠密向量的过程，能够很好地解决图数据难以高效输入机器学习算法的问题。图嵌入是将属性图转换为向量或向量集。嵌入应该捕获图的<strong>拓扑结构、顶点到顶点的关系以及关于图、子图和顶点的其他相关信息</strong>。</p>
<ul>
<li><p>节点的分布式表示；节点之间的相似性表示链接强度； 编码网络信息并生成节点表示</p>
</li>
<li><p>顶点嵌入:每个顶点(节点)用其自身的向量表示进行编码。这种嵌入一般用于在顶点层次上执行可视化或预测。比如，在2D平面上显示顶点，或者基于顶点相似性预测新的连接。</p>
</li>
<li><p>图嵌入:用单个向量表示整个图。这种嵌入用于在图的层次上做出预测，可者想要比较或可视化整个图。例如，比较化学结构。</p>
</li>
</ul>
</blockquote>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201128150931910.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201128150955897.png" alt=""></p>
<h2 id="4-视文分析"><a href="#4-视文分析" class="headerlink" title="4. 视文分析"></a>4. 视文分析</h2><h3 id="4-1-预训练模型"><a href="#4-1-预训练模型" class="headerlink" title="4.1. 预训练模型"></a>4.1. 预训练模型</h3><ul>
<li><strong>模型越来越大。</strong>比如 Transformer 的层数变化，从12层的 Base 模型到24层的 Large 模型。导致模型的参数越来越大，比如 GPT 110 M，到 GPT-2 是1.5 Billion，图灵是 17 Billion，而 GPT-3 达到了惊人的 175 Billion。一般而言模型大了，其能力也会越来越强，但是训练代价确实非常大。</li>
<li><strong>预训练方法也在不断增加</strong>，从自回归 LM，到自动编码的各种方法，以及各种多任务训练等。</li>
<li><strong>，还有从语言、多语言到多模态不断演进</strong>。最后就是模型压缩，使之能在实际应用中经济的使用，比如在手机端。这就涉及到知识蒸馏和 teacher-student models，把大模型作为 teacher，让一个小模型作为 student 来学习，接近大模型的能力，但是模型的参数减少很多。</li>
</ul>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201126150533100.png" alt=""></p>
<h4 id="4-1-1-LayoutLM"><a href="#4-1-1-LayoutLM" class="headerlink" title="4.1.1. LayoutLM"></a>4.1.1. LayoutLM</h4><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201126150648546.png" alt=""></p>
<ul>
<li><p><code>二维位置嵌入 2-D Position Embedding</code>：根据 <code>OCR</code> 获得的文本边界框 (Bounding Box)，能获取文本在文档中的具体位置。在将对应坐标转化为虚拟坐标之后，则可以计算该坐标对应在 <code>x、y、w、h</code> 四个 Embedding 子层的表示，最终的<code>2-D Position Embedding 为四个子层的 Embedding 之和</code>。</p>
</li>
<li><p><code>图嵌入 Image Embedding</code>：将每个文本相应的边界框 (Bounding Box) 当作 Faster R-CNN 中的候选框（Proposal），从而提取对应的局部特征。其特别之处在于，由于 <code>[CLS] 符号用于表示整个输入文本的语义</code>，所以同样<code>使用整张文档图像作为该位置的 Image Embedding</code>，从而保持模态对齐。</p>
</li>
<li><p>预训练：</p>
<ul>
<li><code>掩码视觉语言模型</code>（Masked Visual-Language Model，MVLM）：大量实验已经证明 MLM 能够在预训练阶段有效地进行自监督学习。研究员们在此基础上进行了修改：<code>在遮盖当前词之后，保留对应的 2-D Position Embedding 暗示，让模型预测对应的词</code>。在这种方法下，模型根据已有的上下文和对应的视觉暗示预测被掩码的词，从而让模型更好地学习文本位置和文本语义的模态对齐关系。</li>
<li><code>多标签文档分类</code>（Multi-label Document Classification，MDC）：MLM 能够有效的表示词级别的信息，但是对于文档级的表示，还需要将文档级的预训练任务引入更高层的语义信息。在预训练阶段研究员们使用的 <code>IIT-CDIP 数据集为每个文档提供了多标签的文档类型标注</code>，并引入 <code>MDC 多标签文档分类任务</code>。该任务使得模型可以利用这些监督信号，聚合相应的文档类别并捕捉文档类型信息，从而获得更有效的高层语义表示。</li>
</ul>
</li>
</ul>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201126151137411.png" alt=""></p>
<h2 id="5-学习链接"><a href="#5-学习链接" class="headerlink" title="5. 学习链接"></a>5. 学习链接</h2><ul>
<li><p><a href="https://zhuanlan.zhihu.com/p/39562499" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/39562499</a></p>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/101179171" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/101179171</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s?__biz=MzIyNDY5NjEzNQ==&amp;mid=2247486604&amp;idx=2&amp;sn=4805abb34e3a94243bb182ec74fff550&amp;chksm=e80a4ea4df7dc7b244e39535ae6adf34eece9d10e428e07d52c9675d2629fe111c58e158127c&amp;scene=126&amp;sessionid=1606356123&amp;key=d29f68c0cc2770c7a295460b63af1f2b438bd8d34e43fd923ffab1503f5419358cb801a0ec1cb521815173b06edbb8b4391e82bac9ca99bd324a470299662d0074b19f1c710ebb605f21ad6653f596049aeffc3ecf67018c3a6e1f06e4d967a80aa1117ee0badb2637c6f98ab2b9a0158f340990857e17d4ffc094708a5f04b9&amp;ascene=1&amp;uin=MzE0ODMxOTQzMQ%3D%3D&amp;devicetype=Windows+10+x64&amp;version=6300002f&amp;lang=zh_CN&amp;exportkey=A6O1Vab3FB0dKfbgjPLgUOI%3D&amp;pass_ticket=dWuSAMKgl2YK7zg1wPn7XPBZPohIpbR0IPLY%2Fi1CvZ%2B0Hp9NIxue%2FHPzD4K1r4vD&amp;wx_header=0" target="_blank" rel="noopener">用万字长文聊一聊 Embedding 技术 (qq.com)</a></p>
</li>
<li><p>“Document Visual Question Answering”：<a href="https://medium.com/@anishagunjal7/document-visual-question-answering-e6090f3bddee" target="_blank" rel="noopener">https://medium.com/@anishagunjal7/document-visual-question-answering-e6090f3bddee</a></p>
</li>
<li><p>LayoutLM 论文：<a href="https://arxiv.org/abs/1912.13318" target="_blank" rel="noopener">https://arxiv.org/abs/1912.13318</a></p>
</li>
<li><p>LayoutLM 代码&amp;模型：<a href="https://aka.ms/layoutlm" target="_blank" rel="noopener">https://aka.ms/layoutlm</a></p>
</li>
<li><p><a href="https://leovan.me/cn/2020/04/graph-embedding-and-gnn/" target="_blank" rel="noopener">https://leovan.me/cn/2020/04/graph-embedding-and-gnn/</a></p>
</li>
</ul>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script>
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        Author:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://liudongdong1.github.io" rel="external nofollow noreferrer">liudongdong1</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        Link:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://liudongdong1.github.io/2020/10/20/nlp/wordembedding/">https://liudongdong1.github.io/2020/10/20/nlp/wordembedding/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        Reprint policy:
                    </i>
                </span>
                <span class="reprint-info">
                    All articles in this blog are used except for special statements
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    reprint polocy. If reproduced, please indicate source
                    <a href="https://liudongdong1.github.io" target="_blank">liudongdong1</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>Copied successfully, please follow the reprint policy of this article</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">more</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/tags/embedding/">
                                    <span class="chip bg-color">embedding</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="qq,qzone,wechat,weibo,douban" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
                <style>
    #reward {
        margin: 40px 0;
        text-align: center;
    }

    #reward .reward-link {
        font-size: 1.4rem;
        line-height: 38px;
    }

    #reward .btn-floating:hover {
        box-shadow: 0 6px 12px rgba(0, 0, 0, 0.2), 0 5px 15px rgba(0, 0, 0, 0.2);
    }

    #rewardModal {
        width: 320px;
        height: 350px;
    }

    #rewardModal .reward-title {
        margin: 15px auto;
        padding-bottom: 5px;
    }

    #rewardModal .modal-content {
        padding: 10px;
    }

    #rewardModal .close {
        position: absolute;
        right: 15px;
        top: 15px;
        color: rgba(0, 0, 0, 0.5);
        font-size: 1.3rem;
        line-height: 20px;
        cursor: pointer;
    }

    #rewardModal .close:hover {
        color: #ef5350;
        transform: scale(1.3);
        -moz-transform:scale(1.3);
        -webkit-transform:scale(1.3);
        -o-transform:scale(1.3);
    }

    #rewardModal .reward-tabs {
        margin: 0 auto;
        width: 210px;
    }

    .reward-tabs .tabs {
        height: 38px;
        margin: 10px auto;
        padding-left: 0;
    }

    .reward-content ul {
        padding-left: 0 !important;
    }

    .reward-tabs .tabs .tab {
        height: 38px;
        line-height: 38px;
    }

    .reward-tabs .tab a {
        color: #fff;
        background-color: #ccc;
    }

    .reward-tabs .tab a:hover {
        background-color: #ccc;
        color: #fff;
    }

    .reward-tabs .wechat-tab .active {
        color: #fff !important;
        background-color: #22AB38 !important;
    }

    .reward-tabs .alipay-tab .active {
        color: #fff !important;
        background-color: #019FE8 !important;
    }

    .reward-tabs .reward-img {
        width: 210px;
        height: 210px;
    }
</style>

<div id="reward">
    <a href="#rewardModal" class="reward-link modal-trigger btn-floating btn-medium waves-effect waves-light red">赏</a>

    <!-- Modal Structure -->
    <div id="rewardModal" class="modal">
        <div class="modal-content">
            <a class="close modal-close"><i class="fas fa-times"></i></a>
            <h4 class="reward-title">你的赏识是我前进的动力</h4>
            <div class="reward-content">
                <div class="reward-tabs">
                    <ul class="tabs row">
                        <li class="tab col s6 alipay-tab waves-effect waves-light"><a href="#alipay">支付宝</a></li>
                        <li class="tab col s6 wechat-tab waves-effect waves-light"><a href="#wechat">微 信</a></li>
                    </ul>
                    <div id="alipay">
                        <img src="/" class="reward-img" alt="支付宝打赏二维码">
                    </div>
                    <div id="wechat">
                        <img src="/medias/reward/wechat.png" class="reward-img" alt="微信打赏二维码">
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>

<script>
    $(function () {
        $('.tabs').tabs();
    });
</script>

            
        </div>
    </div>

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;Previous</div>
            <div class="card">
                <a href="/2020/10/20/dlminimalpra/pytorch/pytorchsegmentcode/">
                    <div class="card-image">
                        
                        <img src="https://gitee.com/github-25970295/blogImage/raw/master/img/tourists-in-sea.jpg" class="responsive-img" alt="PytorchSegmentCode">
                        
                        <span class="card-title">PytorchSegmentCode</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            0. 基础配置0.1. 设置随机种子def set_seeds(seed, cuda):
    """ Set Numpy and PyTorch seeds.
    """
    np.random.seed(seed)
    t
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2020-10-20
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/Demo/" class="post-category">
                                    Demo
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/Pytorch-Code/">
                        <span class="chip bg-color">Pytorch Code</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                Next&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/2020/10/20/nlp/framework/allennlpintroduce/">
                    <div class="card-image">
                        
                        <img src="https://gitee.com/github-25970295/blogImage/raw/master/img/20210501113113.png" class="responsive-img" alt="AllenNLPIntroduce">
                        
                        <span class="card-title">AllenNLPIntroduce</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            
you can write your own script to construct the dataset reader and model and run the training loop, or you can write a c
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2020-10-20
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80/" class="post-category">
                                    自然语言
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/Model/">
                        <span class="chip bg-color">Model</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/libs/codeBlock/codeBlockFuction.js"></script>

<!-- 代码语言 -->

<script type="text/javascript" src="/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/libs/codeBlock/codeShrink.js"></script>


<!-- 代码块折行 -->

<style type="text/css">
code[class*="language-"], pre[class*="language-"] { white-space: pre !important; }
</style>


    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;TOC</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // modify the toc link href to support Chinese.
        let i = 0;
        let tocHeading = 'toc-heading-';
        $('#toc-content a').each(function () {
            $(this).attr('href', '#' + tocHeading + (++i));
        });

        // modify the heading title id to support Chinese.
        i = 0;
        $('#articleContent').children('h2, h3, h4').each(function () {
            $(this).attr('id', tocHeading + (++i));
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>

    

</main>




    <!-- <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="463294659"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/libs/aplayer/APlayer.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/meting@2/dist/Meting.min.js"></script>

    
    <div class="container row center-align" style="margin-bottom: 0px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            <span id="year">2019</span>
            <a href="https://liudongdong1.github.io" target="_blank">liudongdong</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            <br>
            
            &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                class="white-color">718.6k</span>&nbsp;字
            
            
            
            
            
            
            <span id="busuanzi_container_site_pv">
                |&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;<span id="busuanzi_value_site_pv"
                    class="white-color"></span>&nbsp;次
            </span>
            
            
            <span id="busuanzi_container_site_uv">
                |&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;<span id="busuanzi_value_site_uv"
                    class="white-color"></span>&nbsp;人
            </span>
            
            <br>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/liudongdong1" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:3463264078@qq.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>













    <a href="/atom.xml" class="tooltipped" target="_blank" data-tooltip="RSS 订阅" data-position="top" data-delay="50">
        <i class="fas fa-rss"></i>
    </a>


    <a href="https://blog.csdn.net/liudongdong19/" class="tooltipped" target="_blank" data-tooltip="关注我的CSDN: https://blog.csdn.net/liudongdong19/" data-position="top" data-delay="50">
        <i class="fab fa-csdn">C</i>
    </a>





</div>
    </div>
</footer>

<div class="progress-bar"></div>
 -->

    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;Search</span>
            <input type="search" id="searchInput" name="s" placeholder="Please enter a search keyword"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script src="/js/search.js"></script>
<script type="text/javascript">
$(function () {
    searchFunc("/search.xml", 'searchInput', 'searchResult');
});
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script type="text/javascript" src="/js/CFS.Snow.min.js"></script>
    <!-- 点击爆灯效果 -->
    <canvas class="fireworks" style="position: fixed;left: 0;top: 0;z-index: 1; pointer-events: none;" ></canvas> 
    <script type="text/javascript" src="//cdn.bootcss.com/animejs/2.2.0/anime.min.js"></script> 
    <script type="text/javascript" src="/js/fireworks.js"></script>
    <!--动态线条背景-->
    <script type="text/javascript"
        color="122 103 238" opacity='0.7' zIndex="-2" count="200" src="//cdn.bootcss.com/canvas-nest.js/1.0.0/canvas-nest.min.js">
    </script>
    <!-- 天气 -->
    <!-- weather -->
    <!-- weather -->
    <script type="text/javascript">
         WIDGET = {FID: 'knAMQaFanP'}
    </script>
    <script type="text/javascript" src="https://apip.weatherdt.com/float/static/js/r.js?v=1111"></script>
    <script type="text/javascript" src="https://apip.weatherdt.com/float/static/js/r.js?v=1111"></script>
    <script src="/libs/materialize/materialize.min.js"></script>
    <script src="/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/libs/aos/aos.js"></script>
    <script src="/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/js/matery.js"></script>

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    
    <script async src="/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    

    
    
    <script type="text/javascript" size="150" alpha='0.6'
        zIndex="-1" src="/libs/background/ribbon-refresh.min.js" async="async"></script>
    

    

    
    <script src="/libs/instantpage/instantpage.js" type="module"></script>
    

</body>

</html>
