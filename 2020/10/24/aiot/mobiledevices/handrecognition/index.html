<!DOCTYPE HTML>
<html lang="en">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="HandRecognition, AIOT,Space&amp;Temporal Sequence Analysis,SpringBoot,liudongdong1,cloud">
    <meta name="description" content="">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>HandRecognition | DaybyDay</title>
    <link rel="icon" type="image/png" href="/favicon.png">

    <link rel="stylesheet" type="text/css" href="/libs/awesome/css/all.css">
    <link rel="stylesheet" type="text/css" href="/libs/materialize/materialize.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/aos/aos.css">
    <link rel="stylesheet" type="text/css" href="/libs/animate/animate.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/lightGallery/css/lightgallery.min.css">
    <link rel="stylesheet" type="text/css" href="/css/matery.css">
    <link rel="stylesheet" type="text/css" href="/css/my.css">

    <script src="/libs/jquery/jquery.min.js"></script>

<meta name="generator" content="Hexo 4.2.0"><link rel="alternate" href="/atom.xml" title="DaybyDay" type="application/atom+xml">
<link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css"></head>


<body>
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/" class="waves-effect waves-light">
                    
                    <img src="/medias/logo.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">DaybyDay</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>Index</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>Tags</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>Categories</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>Archives</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/" class="waves-effect waves-light">

      
      <i class="fas fa-user-circle" style="zoom: 0.6;"></i>
      
      <span>About</span>
      <i class="fas fa-chevron-down" aria-hidden="true" style="zoom: 0.6;"></i>
    </a>
    <ul class="sub-nav menus_item_child ">
      
      <li>
        <a href="/about">
          
          <i class="fas fa-user-circle" style="margin-top: -20px; zoom: 0.6;"></i>
          
          <span>about</span>
        </a>
      </li>
      
      <li>
        <a href="/resume">
          
          <i class="fa fa-user-secret" style="margin-top: -20px; zoom: 0.6;"></i>
          
          <span>resume</span>
        </a>
      </li>
      
    </ul>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/friends" class="waves-effect waves-light">
      
      <i class="fas fa-address-book" style="zoom: 0.6;"></i>
      
      <span>Friends</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/gallery" class="waves-effect waves-light">
      
      <i class="fas fa-camera" style="zoom: 0.6;"></i>
      
      <span>Galleries</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="Search" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/medias/logo.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">DaybyDay</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			Index
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			Tags
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			Categories
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			Archives
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="javascript:;">
			
				<i class="fa-fw fas fa-user-circle"></i>
			
			About
			<span class="m-icon"><i class="fas fa-chevron-right"></i></span>
		</a>
            <ul  style="background:  ;" >
              
                <li>

                  <a href="/about " style="margin-left:75px">
				  
				   <i class="fa fas fa-user-circle" style="position: absolute;left:50px" ></i>
			      
		          <span>about</span>
                  </a>
                </li>
              
                <li>

                  <a href="/resume " style="margin-left:75px">
				  
				   <i class="fa fa fa-user-secret" style="position: absolute;left:50px" ></i>
			      
		          <span>resume</span>
                  </a>
                </li>
              
            </ul>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/friends" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-address-book"></i>
			
			Friends
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/gallery" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-camera"></i>
			
			Galleries
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/liudongdong1" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/liudongdong1" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://gitee.com/github-25970295/blogImage/raw/master/img/30.jpeg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">HandRecognition</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <link rel="stylesheet" href="/libs/tocbot/tocbot.css">
<style>
    #articleContent h1::before,
    #articleContent h2::before,
    #articleContent h3::before,
    #articleContent h4::before,
    #articleContent h5::before,
    #articleContent h6::before {
        display: block;
        content: " ";
        height: 100px;
        margin-top: -100px;
        visibility: hidden;
    }

    #articleContent :focus {
        outline: none;
    }

    .toc-fixed {
        position: fixed;
        top: 64px;
    }

    .toc-widget {
        width: 345px;
        padding-left: 20px;
    }

    .toc-widget .toc-title {
        margin: 35px 0 15px 0;
        padding-left: 17px;
        font-size: 1.5rem;
        font-weight: bold;
        line-height: 1.5rem;
    }

    .toc-widget ol {
        padding: 0;
        list-style: none;
    }

    #toc-content {
        height: calc(100vh - 250px);
        overflow: auto;
    }

    #toc-content ol {
        padding-left: 10px;
    }

    #toc-content ol li {
        padding-left: 10px;
    }

    #toc-content .toc-link:hover {
        color: #42b983;
        font-weight: 700;
        text-decoration: underline;
    }

    #toc-content .toc-link::before {
        background-color: transparent;
        max-height: 25px;

        position: absolute;
        right: 23.5vw;
        display: block;
    }

    .toc-fixed .toc-link::before{
        position: fixed!important;/*当toc的位置改为fixed时，.toc-link::before也要改为fixed*/
    }

    #toc-content .is-active-link {
        color: #42b983;
    }

    #toc-content .is-active-link::before {
        background-color: #42b983;
    }

    #floating-toc-btn {
        position: fixed;
        right: 15px;
        bottom: 76px;
        padding-top: 15px;
        margin-bottom: 0;
        z-index: 998;
    }

    #floating-toc-btn .btn-floating {
        width: 48px;
        height: 48px;
    }

    #floating-toc-btn .btn-floating i {
        line-height: 48px;
        font-size: 1.4rem;
    }
</style>
<div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/tags/IOT/">
                                <span class="chip bg-color">IOT</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/categories/AIOT/" class="post-category">
                                AIOT
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>Publish Date:&nbsp;&nbsp;
                    2020-10-24
                </div>
                

                <!-- 
                    <i class="fa fa-pencil"></i> Author: liudongdong1
                  -->

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>Update Date:&nbsp;&nbsp;
                    2022-03-24
                </div>
                

                <!-- 
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>Word Count:&nbsp;&nbsp;
                    7.1k
                </div>
                 -->

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>Read Times:&nbsp;&nbsp;
                    44 Min
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>Read Count:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">
        <div class="card-content article-card-content">
            <div id="articleContent">
                <h3 id="1-IMU，肌电信号"><a href="#1-IMU，肌电信号" class="headerlink" title="1. IMU，肌电信号"></a>1. IMU，肌电信号</h3><h4 id="1-1-Serendipity"><a href="#1-1-Serendipity" class="headerlink" title="1.1. Serendipity"></a>1.1. Serendipity</h4><blockquote>
<p>Wen, Hongyi, Julian Ramos Rojas, and Anind K. Dey. “Serendipity: Finger gesture recognition using an off-the-shelf smartwatch.” <em>Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems</em>. 2016. 107</p>
</blockquote>
<h5 id="1-1-1-Relative"><a href="#1-1-1-Relative" class="headerlink" title="1.1.1. Relative"></a>1.1.1. <strong>Relative</strong></h5><ul>
<li><strong>Expanding interaction space</strong>: <ul>
<li><code>SkinWatch</code> [9] provides gesture input by sensing <code>deformation of skin.</code> <code>Abracadabra</code>[4] enables off-the-screen fine motor control by <code>placing a magnet on the interacting finger</code>.</li>
<li>require use of other hand and specialized sensors;</li>
</ul>
</li>
<li><strong>Muscle-based Interface</strong>:<ul>
<li>Saponas et al. [11] used an <code>electromyography (EMG) sensor on an armband to convert muscle movement to finger gestures</code>.</li>
<li>Dementyev et al. [3] developed <code>a device with an array of force sensitive resistors (FSR)</code> worn around the wrist to classify gestures from subtle tendon movement. </li>
<li>not easy to integrate current wearable devices;</li>
</ul>
</li>
</ul>
<h5 id="1-1-2-Contribution"><a href="#1-1-2-Contribution" class="headerlink" title="1.1.2.  Contribution"></a>1.1.2.  Contribution</h5><ul>
<li>recognizing unremarkable and fine-motor finger gestures using <code>integrated motion sensors (accelerometer and gyroscope)</code> in off-the-shelf smartwatches.</li>
<li>distinguish 5 fine-motor gestures like <code>pinching, tapping and rubbing fingers</code> with an average f1-score of 87%.</li>
<li>感觉和顾涛老师的那篇工作很像。</li>
</ul>
<h5 id="1-1-3-Experiment"><a href="#1-1-3-Experiment" class="headerlink" title="1.1.3. Experiment"></a>1.1.3. Experiment</h5><ul>
<li>Samsung Galaxy Gear smartwatch:<ul>
<li>accelerometer, gyroscope, rotation and gravity sensors.</li>
<li>collected data for each gesture for a length of 10 seconds, sampling rate of 50Hz;</li>
</ul>
</li>
</ul>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201111093031032.png" alt=""></p>
<h4 id="1-2-RotoSwype"><a href="#1-2-RotoSwype" class="headerlink" title="1.2. RotoSwype"></a>1.2. RotoSwype</h4><blockquote>
<p>Gupta, A., Ji, C., Yeo, H. S., Quigley, A., &amp; Vogel, D. (2019, May). Rotoswype: Word-gesture typing using a ring. In <em>Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems</em> (pp. 1-12).  13</p>
</blockquote>
<h5 id="1-2-1-Relative"><a href="#1-2-1-Relative" class="headerlink" title="1.2.1. Relative"></a>1.2.1. Relative</h5><h5 id="1-2-2-Contribution"><a href="#1-2-2-Contribution" class="headerlink" title="1.2.2. Contribution"></a>1.2.2. Contribution</h5><ul>
<li><p>word-gesture typing using the orientation of a ring worn on the index finger.</p>
</li>
<li><p>using two arm positions: with the <code>hand raised up with the palm parallel to the ground</code>; and with the <code>hand resting at the side with the palm facing the body</code>.</p>
</li>
<li><p>Work flow:</p>
<ol>
<li><p>rotate the hand, the pointer is on the first letter of the desired word, click the ring button to begin the word-gesture input.</p>
</li>
<li><p>rotate the hand, so that the pointer traces a path over the last characters in order;</p>
</li>
<li><p>when the pointer is over the last character, click the ring’s button to end input.</p>
</li>
<li><p>select a different word from the word suggestion box, rotate the hand to bring the pointer over the desired suggestion and click the ring’s button.</p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201111095422501.png" alt=""></p>
</li>
</ol>
</li>
</ul>
<h5 id="1-2-3-Experiment"><a href="#1-2-3-Experiment" class="headerlink" title="1.2.3. Experiment"></a>1.2.3. Experiment</h5><ul>
<li>MPU6050 [34], a <code>6-axis motion-tracking sensor</code>  to provide the absolute orientation in terms of roll and pitch angles.</li>
</ul>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201111094349206.png" alt=""></p>
<h4 id="1-3-Sensing-Fine-Grained-Hand-Activity-with-Smartwatches"><a href="#1-3-Sensing-Fine-Grained-Hand-Activity-with-Smartwatches" class="headerlink" title="1.3. Sensing Fine-Grained Hand Activity with Smartwatches"></a>1.3. Sensing Fine-Grained Hand Activity with Smartwatches</h4><blockquote>
<p>Laput, Gierad, and Chris Harrison. “Sensing fine-grained hand activity with smartwatches.” In <em>Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems</em>, pp. 1-13. 2019.</p>
</blockquote>
<h5 id="1-3-1-Relative"><a href="#1-3-1-Relative" class="headerlink" title="1.3.1. Relative"></a>1.3.1. Relative</h5><h5 id="1-3-2-Contribution"><a href="#1-3-2-Contribution" class="headerlink" title="1.3.2. Contribution"></a>1.3.2. Contribution</h5><ul>
<li>demonstrating 95.2% accuracy across 25 hand activities.</li>
<li>draw an important distinction between <code>hand actions</code> versus <code>hand activities</code>. </li>
<li><code>wrist</code> is also the perfect vantage point to capture bio-acoustic information produced as a byproduct of most hand activities .</li>
</ul>
<h5 id="1-3-3-Experiment"><a href="#1-3-3-Experiment" class="headerlink" title="1.3.3. Experiment"></a>1.3.3. Experiment</h5><ul>
<li><p>started with a <code>50 participant</code>, in-the-wild study, which <code>captured hand activity labels over nearly 1000 worn hours</code>.</p>
</li>
<li><p>LG G watch captures both gross <code>orientation and movement of the hands</code>, as well as higher-fidelity, bio-acoustic information resulting from hand activities</p>
</li>
</ul>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201111102402829.png" alt=""></p>
<ul>
<li><strong>Features</strong>:  Y-axis is spectral power from 0 to 128 Hz. X-axis is time (3 seconds).</li>
</ul>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201111102722948.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201111101416946.png" alt=""></p>
<h4 id="1-4-Findroidhr"><a href="#1-4-Findroidhr" class="headerlink" title="1.4. Findroidhr"></a>1.4. Findroidhr</h4><blockquote>
<p>Zhang, Y., Gu, T., Luo, C., Kostakos, V., &amp; Seneviratne, A. (2018). Findroidhr: Smartwatch gesture input with optical heartrate monitor. <em>Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies</em>, <em>2</em>(1), 1-42. 7</p>
</blockquote>
<h5 id="1-4-1-Relative"><a href="#1-4-1-Relative" class="headerlink" title="1.4.1. Relative"></a>1.4.1. Relative</h5><h5 id="1-4-2-Contribution"><a href="#1-4-2-Contribution" class="headerlink" title="1.4.2. Contribution"></a>1.4.2. Contribution</h5><ul>
<li><p>demostrate how the noise captured by PPG-based heart rate monitors can in fact be mapped to specific hand gestures;</p>
</li>
<li><p>first use brute-force strategy to explore PPG signal features from time-frequency domain, and perform 3 well-developed feature selection algorithms combined with 2 widely used classifiers.</p>
</li>
<li><p>based on the fact t the hand gestures instantly make a change to the blood volume of hands;</p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201024184557463.png" alt=""></p>
</li>
</ul>
<h5 id="1-4-3-Experiment"><a href="#1-4-3-Experiment" class="headerlink" title="1.4.3. Experiment"></a>1.4.3. Experiment</h5><ul>
<li><code>Tizen Wearable OS system architecture</code>, we can not intercept or generate the system touch or button events to replace them with FinDroidHR. Instead, we have built our own application and simulated those events in this application to evaluate FinDroidHR.</li>
</ul>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201024185538854.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201024190027255.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201024190622276.png" alt=""></p>
<h4 id="1-5-FinD"><a href="#1-5-FinD" class="headerlink" title="1.5. FinD"></a>1.5. FinD</h4><blockquote>
<p>Ishikawa, H., Takahashi, W., &amp; Tobe, Y. (2018, September). FinD: Detection of Finger Movement using Smart Watch. In <em>Proceedings of the 5th international Workshop on Sensor-based Activity Recognition and Interaction</em> (pp. 1-3).</p>
</blockquote>
<h5 id="1-5-1-Contribution"><a href="#1-5-1-Contribution" class="headerlink" title="1.5.1. Contribution"></a>1.5.1. Contribution</h5><ul>
<li>using the acceleration signals obtained with Commercial Off-The-Shelf (COTS) smart watches. </li>
<li>developed a system to identify the moved finger called FinD using the acceleration signals obtained with Commercial Off-TheShelf (COTS) smart watches.</li>
</ul>
<h5 id="1-5-2-Experiment"><a href="#1-5-2-Experiment" class="headerlink" title="1.5.2. Experiment"></a>1.5.2. Experiment</h5><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201111103606617.png" alt=""></p>
<h4 id="1-6-SignSpeaker"><a href="#1-6-SignSpeaker" class="headerlink" title="1.6. SignSpeaker"></a>1.6. SignSpeaker</h4><blockquote>
<p>Hou, J., Li, X. Y., Zhu, P., Wang, Z., Wang, Y., Qian, J., &amp; Yang, P. (2019, August). Signspeaker: A real-time, high-precision smartwatch-based sign language translator. In <em>The 25th Annual International Conference on Mobile Computing and Networking</em> (pp. 1-15).  10</p>
</blockquote>
<h5 id="1-6-1-Challenge"><a href="#1-6-1-Challenge" class="headerlink" title="1..6.1. Challenge"></a>1..6.1. Challenge</h5><ul>
<li>FIngerspelling: to spell out words, which involves fine-grained finger tracking<ul>
<li>coarse, noisy sensing data(from acc, gry);</li>
</ul>
</li>
<li>Signs: there is likely a wide range of unpredictable movements between two signs, of which the movement production depends on its prior and following signs.</li>
<li>Diversity of individuals and devices.</li>
</ul>
<h5 id="1-6-2-Contribution"><a href="#1-6-2-Contribution" class="headerlink" title="1.6.2. Contribution"></a>1.6.2. Contribution</h5><ul>
<li><p>propose SignSpeaker - a real-time, robust, and user-friendly <code>American sign language recognition (ASLR)</code>system with affordable and portable commodity mobile devices. </p>
</li>
<li><p>the average translation time is <code>approximately 1.1 seconds for a sentence with eleven words</code>. The <code>average detection ratio and reliability of sign recognition are 99.2% and 99.5%</code>, respectively. The average word error rate of continuous sentence recognition is 1.04% on average.</p>
</li>
<li><p>use a long short-term memory (<code>LSTM)</code> model [14] trained with <code>connectionist temporal classification</code> (CTC)[10].</p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201111104944805.png" alt=""></p>
</li>
</ul>
<h5 id="1-6-3-Experiment"><a href="#1-6-3-Experiment" class="headerlink" title="1.6.3. Experiment"></a>1.6.3. Experiment</h5><ul>
<li><p><code>Huawei Watch (with Android 6.0 Wear OS)</code></p>
</li>
<li><p><code>sensing data collection application is implemented in JAVA for Android platform over the smartwatch</code></p>
</li>
<li><p><strong>Features</strong>:</p>
<ul>
<li>Time-Domain: include mean and standard deviation.</li>
<li>Frequency-Domain: each sign is composed of different gestures and hand motions, each of them has unique frequency features.</li>
</ul>
</li>
<li><p>using Kalman filter and moving average filters to removed noise.  $T × S_d × F_d$: sample size, number of sensors, frequency feature sizes;</p>
</li>
<li><p>leverage the power of LSTM networks to learn the semantic representation of given series data</p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201111110152372.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201111110245274.png" alt="image-20201111110245274"></p>
</li>
</ul>
<h4 id="1-7-Shopping-Gesture-Recognition"><a href="#1-7-Shopping-Gesture-Recognition" class="headerlink" title="1.7.  Shopping Gesture Recognition"></a>1.7.  Shopping Gesture Recognition</h4><h5 id="1-7-1-Relative"><a href="#1-7-1-Relative" class="headerlink" title="1.7.1. Relative"></a>1.7.1. Relative</h5><ul>
<li>understand the <code>browsing behavior</code> and <code>purchase pattern</code> of the shoppers inside the physical stores;</li>
<li><code>targeted advertising or recommendations</code>: based on longer term shopper profiles;</li>
<li><code>proactive retail help</code> to assist the shoppers who are confused in choosing between two items;</li>
<li><code>smart reminders</code> that can remind the shoppers to pick up an item in the shopping list that they might have missed</li>
</ul>
<h5 id="1-7-2-Contribution"><a href="#1-7-2-Contribution" class="headerlink" title="1.7.2. Contribution"></a>1.7.2. Contribution</h5><ul>
<li><p>showed the design and initial prototype of frameworks for reliably infer shopper’s in-store interactions and behavior by just observing their hand and foot movement inside a store.</p>
</li>
<li><p>the hand gestures and locomotive pattern of the shopper inside a store is identified by appropriately mining the sensor data from shopper’s personal smart phone and wearable devices;</p>
</li>
<li><p><code>viterbi decoding</code> to predict the sequence of hand activities that are occurring within an aisle.</p>
</li>
<li><p><code>landmarking based on sensor features</code> to identify the aisle and nonaisle zones and</p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201111122039890.png" alt=""></p>
</li>
</ul>
<h5 id="1-7-3-Challenges"><a href="#1-7-3-Challenges" class="headerlink" title="1.7.3. Challenges"></a>1.7.3. Challenges</h5><ul>
<li>determine discriminative features that can identify the gestures;</li>
<li>reliably demarcate the (start, end) times of individual interactions ;</li>
<li>accurate recognition of shopper’s interaction based on the features generated from the sensor data;</li>
</ul>
<h4 id="1-8-AirContour（轮廓）"><a href="#1-8-AirContour（轮廓）" class="headerlink" title="1.8.  AirContour（轮廓）"></a>1.8.  AirContour（轮廓）</h4><blockquote>
<p>Yin, Yafeng, Lei Xie, Tao Gu, Yijia Lu, and Sanglu Lu. “AirContour: Building Contour-based Model for In-Air Writing Gesture Recognition.” <em>ACM Transactions on Sensor Networks (TOSN)</em> 15, no. 4 (2019): 1-25.  2</p>
</blockquote>
<h5 id="1-8-1-Relative"><a href="#1-8-1-Relative" class="headerlink" title="1.8.1. Relative"></a>1.8.1. Relative</h5><ul>
<li>to recognize in-air writing gestures that occur in 3D space with more degrees of freedom while guaranteeing user experience</li>
<li>Parate et al. recognise <code>smoking gestures</code> and sessions with a wristband;</li>
<li>Blank et al. present a system for <code>table tennis stroke detection and classifycation</code> by attaching inertial sensors to table-tennis rackets.</li>
<li>Thomaz et al. using a 3-axis acc in a smart-watch to infer <code>eating moments</code>;</li>
<li>Xu et al. using a manual <code>toothbrush modified by attaching small magnets</code> to the handle and an off-the-shelf smart wartch.</li>
<li>uTrack convert the thumb and fingers into a continuous input system.</li>
<li>Bashir et al. use a pen equipped with inertial sensors and apply DTW to recognize handwritten characters.</li>
</ul>
<h5 id="1-8-2-Contribution"><a href="#1-8-2-Contribution" class="headerlink" title="1.8.2. Contribution"></a>1.8.2. Contribution</h5><ul>
<li>use an off-the-shelf wrist-worn device (e.g., smartwatch) to collect sensor data, and our basic idea is to build a 3D contour model for each gesture and utilize the contour feature to recognize gestures as characters；</li>
<li>AC-Vec, and an offline approach, AC-CNN, to recognize 2D contours as characters. The experimental results show that AC-Vec and AC-CNN achieve an accuracy of 91.6% and 94.3%, respectively, for gesture/character recognition, and both outperform the existing approaches.</li>
<li>using PCA to project 3D contour in 2D contour;</li>
</ul>
<h5 id="1-8-3-Challenge"><a href="#1-8-3-Challenge" class="headerlink" title="1.8.3. Challenge"></a>1.8.3. Challenge</h5><ul>
<li>contour distortion caused by different viewing angles, contour difference caused by different writing directions, and contour distribution across different planes, making it difficult to recognize 3D contours as 2D characters.</li>
</ul>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201111161413986.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201111161601438.png" alt=""></p>
<ul>
<li>pre-process the sensor data by data offset correction, noise removal, coordinate system transformation.</li>
<li>transform the sensor data from the device coordinate system to the fixed earth coordinate frame.</li>
<li>for uncontrollable accumulated error of continuous double integral, introduce segmented intergral and velocity compensation for contour calculation.<ul>
<li>utilize the gyroscope data close to zero to split the writing process into multiple segments.</li>
<li>reset the velocity at the start and the end of the segment to zero to suppress the velocity drifts.</li>
<li>use the velocity compensation to mitigate the computation error of velocity, and calculate the gesture contour.   5,17</li>
</ul>
</li>
</ul>
<h5 id="1-8-3-Experiment"><a href="#1-8-3-Experiment" class="headerlink" title="1.8.3. Experiment"></a>1.8.3. Experiment</h5><ul>
<li><a href="https://pcitem.jd.hk/55310797031.html" target="_blank" rel="noopener">LG Watch Urbane</a>  5400</li>
</ul>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201111131926129.png" alt=""></p>
<ul>
<li>Vector-sequence -based recognition approaches:</li>
</ul>
<p>$$<br>\vec{n_{d_i}}=(x^<code>_{p_i}-x_{p_0},y^</code><em>{p_i}-y</em>{p_0}) \<br>a , feature , vector=(\vec{n_{d_1}},…,\vec{n_{d_i}},…)<br>$$</p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201111162726188.png" alt=""></p>
<ul>
<li>CNN based methods:</li>
</ul>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201111163655300.png" alt=""></p>
<h4 id="1-9-GestEar"><a href="#1-9-GestEar" class="headerlink" title="1.9. GestEar"></a>1.9. GestEar</h4><blockquote>
<p>Becker, V., Fessler, L., &amp; Sörös, G. (2019, September). GestEar: combining audio and motion sensing for gesture recognition on smartwatches. In <em>Proceedings of the 23rd International Symposium on Wearable Computers</em> (pp. 10-19).</p>
</blockquote>
<h5 id="1-9-1-Relative"><a href="#1-9-1-Relative" class="headerlink" title="1.9.1. Relative"></a>1.9.1. Relative</h5><ul>
<li>Serendipity [19] processes the recordings from the accelerometer and the gyroscope of a smartwatch to <code>classify five finger gestures.</code></li>
<li>Winkler et al. [21] developed a multimodal approach to r<code>ecognize hand interactions with projections on a surface by visually tracking the fingers and detecting finger touches from the surface’s vibrations</code> employing a smartphone’s accelerometer. </li>
<li>FingerSound [25] employs a <code>microphone incorporated in a ring</code> for the thumb to sense unistroke gestures of the thumb scratching against the inner side of the hand.</li>
<li>SoundCraft [5] aims at <code>classifying audio signatures from natural hand gestures such as snapping or clapping</code> using a smartwatch. </li>
</ul>
<h5 id="1-9-2-Contribution"><a href="#1-9-2-Contribution" class="headerlink" title="1.9.2. Contribution"></a>1.9.2. Contribution</h5><ul>
<li>classify sound-emitting gestures based on motion and audio together , takes the <code>audio, acceleration and gyroscope</code> data as separate inputs, computes individual features for each, and fuses them within the network.</li>
<li>run locally on resource-constrained devices, which achieves a user-independent recognition accuracy of 97.2% for nine distinct gestures.</li>
<li>generate a large corpus of synthetic training data from the recorded real samples.</li>
</ul>
<h5 id="1-9-3-Experiment"><a href="#1-9-3-Experiment" class="headerlink" title="1.9.3. Experiment"></a>1.9.3. Experiment</h5><ul>
<li>device control( sound or light systems); music control while on the go,  unlocking a personal smart phone, replacing the doorbell button by simply knocking at the door;</li>
<li>record mono audio from the microphone at a smapling rate of 22050 Hz and three-axes measurements from the gyroscope and the accelerometer(with gravity subtraction) at a sampling rate of 200Hz. </li>
<li>collect data using app which display the gesture the user has to perform.</li>
</ul>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201111165538160.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201111181249045.png" alt=""></p>
<h4 id="1-10-Demo-Smartwatch-based-Shopping-Gesture-Recognition"><a href="#1-10-Demo-Smartwatch-based-Shopping-Gesture-Recognition" class="headerlink" title="1.10.  Demo: Smartwatch based Shopping Gesture Recognition"></a>1.10.  Demo: Smartwatch based Shopping Gesture Recognition</h4><blockquote>
<p>Kajiwara, D., &amp; Murao, K. (2019, September). Gesture recognition method with acceleration data weighted by sEMG. In <em>Adjunct Proceedings of the 2019 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2019 ACM International Symposium on Wearable Computers</em> (pp. 741-745).</p>
</blockquote>
<h5 id="1-10-2-Contribution"><a href="#1-10-2-Contribution" class="headerlink" title="1.10.2. Contribution"></a>1.10.2. Contribution</h5><ul>
<li>Three axis acceleration data and sEMG were collected for three types of baseball pitching forms: overarm, sidearm, and underarm beginning from three types of preliminary motions: no windup, quick, and windup.</li>
<li>The sEMG is required only for training data collection.</li>
</ul>
<h5 id="1-10-3-Experiment"><a href="#1-10-3-Experiment" class="headerlink" title="1.10.3. Experiment"></a>1.10.3. Experiment</h5><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201111185345426.png" alt=""></p>
<h4 id="1-1-11-continuous-gestures"><a href="#1-1-11-continuous-gestures" class="headerlink" title="1.1.11. continuous gestures"></a>1.1.11. continuous gestures</h4><blockquote>
<p>Watanabe, Hikaru, et al. “A recognition method for continuous gestures with an accelerometer.” <em>Proceedings of the 2016 ACM International Joint Conference on Pervasive and Ubiquitous Computing: Adjunct</em>. 2016.</p>
</blockquote>
<h3 id="2-Sound-based"><a href="#2-Sound-based" class="headerlink" title="2. Sound-based"></a>2. Sound-based</h3><h4 id="2-1-SpiroSonic"><a href="#2-1-SpiroSonic" class="headerlink" title="2.1. SpiroSonic"></a>2.1. SpiroSonic</h4><blockquote>
<p>Song X, Yang B, Yang G, et al. SpiroSonic: monitoring human lung function via acoustic sensing on commodity smartphones[C]//Proceedings of the 26th Annual International Conference on Mobile Computing and Networking. 2020: 1-14. [<a href="https://www.youtube.com/watch?v=SDiboSXJr28&amp;list=TLPQMjkxMTIwMjGPrdDXYiSKow&amp;index=8" target="_blank" rel="noopener">video</a>] [<a href="chrome-extension://ikhdkkncnoglghljlkmcimlnlhkeamad/pdf-viewer/web/viewer.html?file=https%3A%2F%2Fsites.pitt.edu%2F~weigao%2Fpublications%2Fmobicom20.pdf#=&amp;zoom=140">pdf</a>]</p>
</blockquote>
<ul>
<li>present SpiroSonic, a new system design that uses commodity smartphones to support complete, accuracy yet reliable spirometry tests in regular home settings.</li>
<li>accurate, easy to use, lightweight, adaptive: precisely remove the impact from various environmental and human factors, and allows flexible variations of smartphones’ position and orientation.</li>
<li>experimental aspect: power efficiency, different clothes, surroundings environment, impact of smartphone models, different smartphone positions, measurement accuracy, effectiveness of Irrelevant Motion removal.</li>
</ul>
<p><img src="../../../../picture/image-20211129162231122.png" alt=" Comparison of technologies measuring human lung function out of clinic"></p>
<p><img src="../../../../picture/image-20211129162035216.png" alt=""></p>
<p><img src="../../../../picture/image-20211129162312070.png" alt=""></p>
<h4 id="2-2-Ear-AR"><a href="#2-2-Ear-AR" class="headerlink" title="2.2. Ear-AR"></a>2.2. Ear-AR</h4><blockquote>
<p>Yang Z, Wei Y L, Shen S, et al. Ear-ar: indoor acoustic augmented reality on earphones[C]//Proceedings of the 26th Annual International Conference on Mobile Computing and Networking. 2020: 1-14.</p>
</blockquote>
<ul>
<li>jointly use the inertial sensors (IMUs) in earphone and smartphones to estimate a user’s indoor location and gazing orientation</li>
<li>play 3D sounds in the earphones and exploit the human’s responses to recalibrate errors in location and orientation.</li>
</ul>
<p><img src="../../../../picture/image-20211129163943515.png" alt=""></p>
<p><img src="../../../../picture/image-20211129164123155.png" alt=""></p>
<h4 id="2-3-EarSense"><a href="#2-3-EarSense" class="headerlink" title="2.3. EarSense"></a>2.3. EarSense</h4><blockquote>
<p>Prakash J, Yang Z, Wei Y L, et al. EarSense: earphones as a teeth activity sensor[C]//Proceedings of the 26th Annual International Conference on Mobile Computing and Networking. 2020: 1-13. [<a href="https://www.youtube.com/watch?v=DZLJ_lCG8RU&amp;list=TLPQMjkxMTIwMjGPrdDXYiSKow&amp;index=17" target="_blank" rel="noopener">video</a>]</p>
</blockquote>
<ul>
<li>pre-knowledge: teh actions of the teeth, namely tapping and sliding, produce vibrations in the jaw and skull. The vabrations are strong enough to propagate to the edge of the face and produce vibratory signals at an earphone.</li>
<li>recognise teeth related gestures using ear/headphones to enhance human-to-human interface.</li>
<li>challenges ranges from weak signals, distortions due to different teeth compositions, lack of timing resolution, spectral dispersion, etc.</li>
</ul>
<p><img src="../../../../picture/image-20211129170803545.png" alt=""></p>
<h3 id="3-MMwave-based-光学，毫米波"><a href="#3-MMwave-based-光学，毫米波" class="headerlink" title="3. MMwave-based(光学，毫米波)"></a>3. MMwave-based(光学，毫米波)</h3><h4 id="3-1-Soli"><a href="#3-1-Soli" class="headerlink" title="3.1.  Soli"></a>3.1.  Soli</h4><blockquote>
<p>Lien, Jaime, Nicholas Gillian, M. Emre Karagozler, Patrick Amihood, Carsten Schwesig, Erik Olson, Hakim Raja, and Ivan Poupyrev. “Soli: Ubiquitous gesture sensing with millimeter wave radar.” <em>ACM Transactions on Graphics (TOG)</em> 35, no. 4 (2016): 1-19.   378</p>
</blockquote>
<h5 id="3-1-2-Contribution"><a href="#3-1-2-Contribution" class="headerlink" title="3.1.2. Contribution"></a>3.1.2. Contribution</h5><ul>
<li>presents Soli, a new, robust, high-resolution, low-power, miniature gesture sensing technology for human-computer interaction based on <code>millimeter-wave radar</code>.</li>
<li>a radar-based sensor optimized for human-computer interaction, building the sensor architecture from the ground up with the inclusion of <code>radar design principles</code>, <code>high temporal resolution gesture tracking</code>, a <code>hardware abstraction layer (HAL)</code>, a <code>solidate radar chip and system architecture</code>, interaction models and gesture vocabularies, and gesture recognition.</li>
</ul>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201116162307105.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201116162527982.png" alt=""></p>
<h5 id="3-1-3-Experiment"><a href="#3-1-3-Experiment" class="headerlink" title="3.1.3. Experiment"></a>3.1.3. Experiment</h5><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201116162641142.png" alt=""></p>
<h3 id="4-肌电based"><a href="#4-肌电based" class="headerlink" title="4. 肌电based"></a>4. 肌电based</h3><h4 id="4-1-ConvLSTM-AM"><a href="#4-1-ConvLSTM-AM" class="headerlink" title="4.1. ConvLSTM-AM"></a>4.1. ConvLSTM-AM</h4><blockquote>
<p>Hoang Truong, Phuc Nguyen, Anh Nguyen, Nam Bui and Tam Vu.  “Capacitive Sensing 3D-printed Wristband for Enriched Hand Gesture Recognition” <em>Proceedings of the 2017 Workshop on Wearable Systems and Applications,</em> 2017.</p>
</blockquote>
<h5 id="4-1-1-Relative"><a href="#4-1-1-Relative" class="headerlink" title="4.1.1. Relative"></a>4.1.1. <strong>Relative</strong></h5><ul>
<li><strong>traditional machine learning</strong>:   <ul>
<li>On the basis of gesture recognition of electromyographic signals, traditional machine learning algorithms include <code>support vector machine (SVM)[13,14], k-nearest Neighbor [15], Linear discriminanalysis (LDA)[16-18], Hidden Markov Model (HMM)[19]</code>.</li>
</ul>
</li>
<li><strong>deep learning algorithm</strong>:<ul>
<li>To further analyze the characteristics of sEMG signals , a <code>CNN-RNN</code> framework has been proposed that contains <code>AM</code> to improve the accuracy of features[10].</li>
</ul>
</li>
</ul>
<h5 id="4-1-2-Contribution"><a href="#4-1-2-Contribution" class="headerlink" title="4.1.2.  Contribution"></a>4.1.2.  Contribution</h5><ul>
<li><p>a system based on Convolutional long-term memory (ConvLSTM)-Attention Mechanism (AM)  is proposed to maintain the spatial and time characteristics of surface EMG signals</p>
</li>
<li><p>distinguish 8 actions like <code>Rock, 5-StThm, Scissors, 3-StThm, 3-Scsr, Paper, Palm-Spn and Palm-Tr</code> with an average f1-score of 92.8%.</p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img//a1.png" alt=""></p>
</li>
</ul>
<h5 id="4-1-3-Experiment"><a href="#4-1-3-Experiment" class="headerlink" title="4.1.3. Experiment"></a>4.1.3. Experiment</h5><ul>
<li><p>Myo-armband developed by Thalmic Labs:</p>
<ul>
<li><p>a <code>sEMG sensor of eight electrodes with a sampling frequency of 200 Hz</code></p>
</li>
<li><p>eight men aged between 20 and 24 years old</p>
</li>
<li><p>each action for 1s, an object an action to do 20 groups</p>
<img src="https://gitee.com/github-25970295/blogImage/raw/master/img/a2.png" alt="a2" style="zoom:80%;">

</li>
</ul>
</li>
</ul>
<h4 id="4-2-LMA"><a href="#4-2-LMA" class="headerlink" title="4.2. LMA"></a>4.2. LMA</h4><blockquote>
<p>Nicholas Ward, Francisco Bernardo, Miguel Ortiz, Atau Tanaka.  “Designing and measuring gesture using Laban Movement Analysis and Electromyogram” Proceedings of the 2016 ACM International Joint Conference on Pervasive and Ubiquitous Computing: Adjunct, 2016.</p>
</blockquote>
<h5 id="4-2-1-Relative"><a href="#4-2-1-Relative" class="headerlink" title="4.2.1. Relative"></a>4.2.1. <strong>Relative</strong></h5><h5 id="4-2-2-Contribution"><a href="#4-2-2-Contribution" class="headerlink" title="4.2.2.  Contribution"></a>4.2.2.  Contribution</h5><ul>
<li>present an exploratory study that uses Laban Movement Analysis as a framework for designing gesture, and electromyogram (EMG) signals for measuring gestural output.</li>
</ul>
<h5 id="4-2-3-Experiment"><a href="#4-2-3-Experiment" class="headerlink" title="4.2.3. Experiment"></a>4.2.3. Experiment</h5><ul>
<li><p>The commercial Myo device:</p>
<ul>
<li><p>acquired <code>8 channels of EMG data in a circular formatio</code>n around the dominant forearm of each participant and <code>IMUdata consisting of 3-axis accelerometer, 3-axis gyroscope</code></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/a3.png" alt=""></p>
</li>
<li><p>participants were asked to devise a simple one-arm movement phrase that explored</p>
<p>specifific polarities of two or three LMA qualities</p>
</li>
<li><p>found that the LMA vocabulary was useful in allowing for a systematic exploration of movement</p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/a4.png" alt=""></p>
</li>
</ul>
</li>
</ul>
<blockquote>
<p>Wu Y, Kakaraparthi V, Li Z, et al. BioFace-3D: continuous 3d facial reconstruction through lightweight single-ear biosensors[C]//Proceedings of the 27th Annual International Conference on Mobile Computing and Networking. 2021: 350-363.  [<a href="https://www.youtube.com/watch?v=o1L0hD1YloY&amp;list=TLPQMjkxMTIwMjGPrdDXYiSKow&amp;index=1" target="_blank" rel="noopener">video</a>]</p>
</blockquote>
<h4 id="4-3-BioFace-3D"><a href="#4-3-BioFace-3D" class="headerlink" title="4.3. BioFace-3D"></a>4.3. BioFace-3D</h4><ul>
<li>facial landmark tracking and 3D reconstruction–&gt;(human-computer interactions, facial expression analysis, emotion recognition)</li>
<li>propose the first single-earpiece lightweight biasensing system, BioFace3D that unobtrusively, continuously and reliably sense the entire facial movements, track 2D facial landmarks and further render 3D facial animations.</li>
<li>cross-modal transfer learning model to transfer the knowledge embodied in a high-grade visual facial landmark detection model to the low-grade biasignal domain</li>
<li>identify optimal biosensor placement positions on the face to maintain a minimized obtrusiveness level of BioFace-3D to wearer.</li>
</ul>
<p><img src="../../../../picture/image-20211129155420414.png" alt="System Overview"></p>
<p><img src="../../../../picture/image-20211129154304577.png" alt=""></p>
<p><img src="../../../../picture/image-20211129155521239.png" alt=""></p>
<p><img src="../../../../picture/image-20211129155321119.png" alt=""></p>
<p><img src="../../../../picture/image-20211129155348059.png" alt=""></p>
<h3 id="5-RFID-based"><a href="#5-RFID-based" class="headerlink" title="5. RFID-based"></a>5. RFID-based</h3><h4 id="5-1-RF-IDraw"><a href="#5-1-RF-IDraw" class="headerlink" title="5.1. RF-IDraw"></a>5.1. <strong>RF-IDraw</strong></h4><blockquote>
<p>Deepak Vasisht, Jue Wang, and Dina Katabi.  “RF-IDraw: Virtual Touch Screen in the Air Using RF Signals” <em>Proceedings of the 6th annual workshop on Wireless of the students, by the students, for the students,</em> 2014.</p>
</blockquote>
<h5 id="5-1-1-Relative"><a href="#5-1-1-Relative" class="headerlink" title="5.1.1. Relative"></a>5.1.1. <strong>Relative</strong></h5><ul>
<li>[10, 7] enable gesture recognition based on RF signals, but are not sufficient because of the limited set of gestures that can be recognized and the training required for the gestures.</li>
</ul>
<h5 id="5-1-2-Contribution"><a href="#5-1-2-Contribution" class="headerlink" title="5.1.2.  Contribution"></a>5.1.2.  Contribution</h5><ul>
<li><p>RF-IDraw achieves <code>trajectory tracing</code> accuracy of a few centimeters </p>
</li>
<li><p>explore the idea of <code>varying the antenna separation to adapt the resolution</code> as opposed</p>
<p>to increasing the number of antennas</p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/b2.png" alt=""></p>
</li>
</ul>
<h5 id="5-1-3-Experiment"><a href="#5-1-3-Experiment" class="headerlink" title="5.1.3. Experiment"></a>5.1.3. Experiment</h5><ul>
<li><p><code>two separate ThingMagic RFID readers</code>:</p>
<ul>
<li><p>One set of 4 antennas is placed at the corners of a square of side 8λ and another set of   4 antennas is placed along two edges of the square with a separation of λ/2</p>
</li>
<li><p>user stands 2-5m away from the system</p>
<img src="https://gitee.com/github-25970295/blogImage/raw/master/img/b1.png" style="zoom:80%;">

</li>
</ul>
</li>
</ul>
<h4 id="5-2-RF-Glove"><a href="#5-2-RF-Glove" class="headerlink" title="5.2. RF-Glove"></a>5.2. <strong>RF-Glove</strong></h4><blockquote>
<p>Lei Xie, Chuyu Wang, Alex X. Liu, Jianqiang Sun, and Sanglu Lu.  “Multi-Touch in the Air: Concurrent Micromovement Recognition Using RF Signals” <em>IEEE/ACM Transactions on Networking Archive IEEE/ACM Transactions on Networking</em>, 2018.</p>
</blockquote>
<h5 id="5-2-1-Relative"><a href="#5-2-1-Relative" class="headerlink" title="5.2.1. Relative"></a>5.2.1. <strong>Relative</strong></h5><ul>
<li><p>RFID-Based Localization</p>
<ul>
<li>RSSI (Received Signal Strength) information [13], [14]</li>
<li>phase valuey [11], [12], [15]–[21]</li>
<li>PinIt exploits multi-path effect to accurately locate RFIDs by using synthetic aperture<br>radar created via antenna motion to extract multi-path profiles for accurate localization [11].</li>
<li><code>Tagoram</code> exploits tag mobility to build a <code>virtual antenna array</code>, and uses differential augmented hologram to facilitate the instant tracking of a mobile RFID tag [12].</li>
<li>Spatial-Temporal Phase Profiling (STPP) is proposed for the relative localization of RFID tags [22].</li>
<li>the polarization properties of the RF waves is exploited to achieve fine-grained pose sensing[20]</li>
<li>RFID-Based Motion Tracking<ul>
<li>Lin <em>et al.</em> [29] proposed a 3D human-computer interaction system called <code>Tagball</code>, where multiple <code>passive tags are attached to a controlling ball</code> to detect the motions of ball rotation from users</li>
<li><code>RF-IDraw [10]</code> uses a 2-dimensional array of RFID antennas to track the movement trajectory of one finger attached with an RFID tag, so that it can reconstruct the trajectory shape of the specified finger.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h5 id="5-2-2-Contribution"><a href="#5-2-2-Contribution" class="headerlink" title="5.2.2.  Contribution"></a>5.2.2.  Contribution</h5><ul>
<li>RF-glove, a system that recognizes concurrent multiple finger micromovement using RF signals, so as to realize the vision of “multi-touch in the air.” </li>
</ul>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201116154436260.png" alt=""></p>
<img src="https://gitee.com/github-25970295/blogImage/raw/master/img/b3.png" style="zoom:80%;">


<ul>
<li>To trade off between accuracy and robustness in terms of matching resolution, we propose a two phase approach, including <code>coarse-grained filtering and finegerained matching.</code></li>
</ul>
<h5 id="5-2-3-Experiment"><a href="#5-2-3-Experiment" class="headerlink" title="5.2.3. Experiment"></a>5.2.3. Experiment</h5><ul>
<li><p>ImpinJ [34] R420 reader, three Laird S9028 RFID antennas, and five Alien 9640 general purpose tags:</p>
<ul>
<li><p>three antennas on a flat plane with the antenna pairs in a mutually orthogonal approach, distance between adjacent antennas to 60cm, distance between the antenna plane and the operation plane to about 1.5<em>∼</em>2m</p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/b4.png" alt=""></p>
</li>
<li><p>Evaluate the Positioning Performance</p>
<p>The average positioning error of our positioning method is 25cm, whereas the average positioning error of the hologram-based solution is 30cm</p>
<img src="https://gitee.com/github-25970295/blogImage/raw/master/img/b6.png" style="zoom:80%;">

<ul>
<li><p>Evaluate the Micromovement Recognition Performance</p>
<p>The average accuracy of Phase Profifiles is 92.1%, whereas the average accuracy of RF-IDraw-based solution is 80.6%. </p>
<img src="https://gitee.com/github-25970295/blogImage/raw/master/img/b5.png" style="zoom:67%;">

</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="5-3-InDexMo"><a href="#5-3-InDexMo" class="headerlink" title="5.3. InDexMo"></a>5.3. <strong>InDexMo</strong></h4><blockquote>
<p>Rong-Hao Liang, Shun-Yao Yang and Bing-Yu Chen.  “InDexMo: Exploring Finger-Worn RFID Motion Tracking for Activity Recognition on Tagged Objects” <em>Proceedings of the 23rd International Symposium on Wearable Computers,</em> 2019.</p>
</blockquote>
<h5 id="5-3-1-Relative"><a href="#5-3-1-Relative" class="headerlink" title="5.3.1. Relative"></a>5.3.1. <strong>Relative</strong></h5><h5 id="5-3-2-Contribution"><a href="#5-3-2-Contribution" class="headerlink" title="5.3.2.  Contribution"></a>5.3.2.  Contribution</h5><ul>
<li>propose an index-finger-worn device that consists of a <code>short-range (∼2cm) RFID reader</code> and a pair of <code>two inertial measurement units (IMUs)</code>, which are mounted at the locations where an artificial nail and a ring are worn </li>
</ul>
<img src="https://gitee.com/github-25970295/blogImage/raw/master/img/b7.png" style="zoom:80%;">

<p>  ≥90% accuracy, with 500 ms recognition time</p>
<h5 id="5-3-3-Experiment"><a href="#5-3-3-Experiment" class="headerlink" title="5.3.3. Experiment"></a>5.3.3. Experiment</h5><ul>
<li><p>a Mifare 13.56MHz RFID reader,  a 15×20 mm2 2 cm-range antenna,  a nine-axis MPU9250 IMU at both the fingernail and root locations,  the 10-turn, 0.7 mm pitch inward spiral antenna, which is made by 0.29 mm enameled wire:</p>
</li>
<li><p>Ten users (7 males, 3 females) ranging from 22 to 25 years old (M = 23.4; SD = 1.35) wear the</p>
<p>sensing glove and perform given tasks using handy utensils in a simulated kitchen</p>
</li>
<li><p>The tasks were touching the cup lid (T1), holding the cup(T2), lifting the plate (T3), pouring out water from the teapot (T4), using the fork (T5), using the knife (T6), spooning up water from the cup (T7), and stirring water using the spoon (T8)</p>
<img src="https://gitee.com/github-25970295/blogImage/raw/master/img/b8.png" style="zoom:80%;">

</li>
</ul>
<h3 id="6-WIFI-based"><a href="#6-WIFI-based" class="headerlink" title="6. WIFI-based"></a>6. WIFI-based</h3><h4 id="6-1-FingerDraw"><a href="#6-1-FingerDraw" class="headerlink" title="6.1. FingerDraw"></a>6.1. FingerDraw</h4><blockquote>
<p>Dan Wu, Ruiyang Gao, YouweiI Zeng, JinyiI Liu, and Leye Wang.  “FingerDraw: Sub-wavelength Level Finger Motion Tracking with WiFi Signals” <em>Proceedings of the ACM on Interactive Mobile Wearable and Ubiquitous Technologies,</em>2020.</p>
</blockquote>
<h5 id="6-1-1-Relative"><a href="#6-1-1-Relative" class="headerlink" title="6.1.1. Relative"></a>6.1.1. <strong>Relative</strong></h5><ul>
<li><p><strong>Non-WiFi-based gesture recognition</strong>: </p>
<ul>
<li>Imaging-based systems (such as Xbox Kinect [29], and leap motion [9]) use monochromatic infrared cameras and LEDs to build body-depth imaging.</li>
<li>LLAP [42] uses the <code>phase change of the continuous wave sound signal</code> for motion sensing. </li>
<li>FingerIO [23] transmits specially modulated OFDM signals and locates finger based on the change of the echo profiles. </li>
<li>Strata [49] combines the frame-based approach and the phase-based approach.</li>
<li><code>Radar based</code> systems such as <code>Google Soli [11]</code> show the ability to track minor finger movement by constructing Doppler profile using 60GHz radar signals.</li>
<li><code>WiSee [25]</code>uses specialized devices <code>(USRP) with OFDM modulated signals</code> to extract the Doppler shift of the movement to recognize gestures. </li>
<li><code>WiTrack [2]</code> also uses custom<code>radar transmissions</code>to detect pointing gestures. Such systems, however, require embedded chips to capture and process radar signals.</li>
<li>RF-IDraw [39] traces a finger with RFID tag attached using multiple antenna arrays to enable a virtual touch screen application. RF-finger [36] enables device-free finger tracking using specially arranged tag arrays with one RFID antenna.</li>
<li>require dedicated hardware which are not cost-effective, and they may work on specific occasions, but their further deployment is limited;</li>
</ul>
</li>
<li><p><strong>WiFi-based gesture recognition</strong>:</p>
<ul>
<li><p><code>WiFinger [32]</code> extracts patterns in time domain using <code>principal component identification</code> and <code>compares waveform shapes with Dynamic Time Warping (DTW)</code> to identify different gestures. </p>
</li>
<li><p><code>WiGest [1] and WiAG [35]</code> extract frequency distributions using <code>Discrete Waveform Transform (DWT)</code>to recognize different hand gestures. </p>
</li>
<li><p><code>WiMu [34]</code> uses<code>frequency features of short time Fourier transform (STFT)</code> and generate virtual samples to enable multi-user gesture recognition.</p>
</li>
<li><p>Widar 3.0 use multiple WiFi device pairs to build cross-domain body-coordinate velocity profile from Doppler information for gesture recognition.</p>
</li>
<li><p>leverage on patterns, they can only recognize a pre-defined gesture set, not for arbitrary finger motion tracking;</p>
</li>
</ul>
</li>
<li><p><strong>WiFi-based gesture recognition</strong>:</p>
<ul>
<li><code>WiDraw [31]</code> shows signal <code>Angle-of-Arrival (AoA</code>) can be used to track hand motions.</li>
<li><code>QGesture [48], WiDance [27] and Doppler-MUSIC [18]</code> sense target motion by <code>estimating the length change of the reflected path with both CSI amplitude and phase information.</code></li>
<li>require target to move at least several wavelengths for the motion to be correctly extracted;</li>
</ul>
</li>
</ul>
<h5 id="6-1-2-Contribution"><a href="#6-1-2-Contribution" class="headerlink" title="6.1.2.  Contribution"></a>6.1.2.  Contribution</h5><ul>
<li><p>This paper introduces FingerDraw, the first sub-wavelength level finger motion tracking system using commodity WiFi devices, without attaching any sensor to finger.</p>
</li>
<li><p>the overall median tracking accuracy is 1<em>.</em>27 cm, and the recognition of drawing ten digits in the air achieves an average accuracy of over 93.0%.</p>
<img src="https://gitee.com/github-25970295/blogImage/raw/master/img/c2.png" style="zoom:80%;">

</li>
</ul>
<h5 id="6-1-3-Experiment"><a href="#6-1-3-Experiment" class="headerlink" title="6.1.3. Experiment"></a>6.1.3. Experiment</h5><ul>
<li><p>three GigaByte mini PCs as WiFi transceivers equipped with an Intel 5300 wireless card and three omnidirectional antennas:</p>
<ul>
<li><p>The frequency of WiFi signal is set to 5<em>.</em>24 GHz and the bandwidth is set to 20 MHz. The transmitter sends 500 packets per second at a transmitting power of 15 dBm.</p>
</li>
<li><p>an office (4 m × 5 m), a meeting room(6 m × 8 m), and an empty hall (8 m × 12 m)</p>
</li>
<li><p>7 females and 13 males, aging from 21 to 40.</p>
<img src="https://gitee.com/github-25970295/blogImage/raw/master/img/c3.png" style="zoom:67%;">
</li>
<li><p>the overall median tracking accuracy is 1<em>.</em>27 cm, and the recognition of drawing ten digits in the air achieves an average accuracy of over 93.0%.</p>
<img src="https://gitee.com/github-25970295/blogImage/raw/master/img/c4.png" style="zoom:80%;">

<img src="https://gitee.com/github-25970295/blogImage/raw/master/img/c1.png" style="zoom:67%;">

</li>
</ul>
</li>
</ul>
<h4 id="6-2-WiAG"><a href="#6-2-WiAG" class="headerlink" title="6.2. WiAG"></a>6.2. WiAG</h4><blockquote>
<p>Aditya Virmani and Muhammad Shahzad.  “Position and Orientation Agnostic<br>Gesture Recognition Using WiFi” <em>Proceedings of the 15th Annual International Conference on Mobile Systems, Applications, and Services</em>, 2017.</p>
</blockquote>
<h5 id="6-2-1-Relative"><a href="#6-2-1-Relative" class="headerlink" title="6.2.1. Relative"></a>6.2.1. <strong>Relative</strong></h5><ul>
<li><p>specialized-hardware (SH) based (slightly more accurate)</p>
<ul>
<li>use <code>software defined radios (SDRs)</code>, often along with specialized antennas or custom analog circuits, to capture changes in the wireless channel metrics due to human movements.</li>
</ul>
</li>
<li><p>commodity-devices (CD) based (wider acceptance)</p>
<ul>
<li>implemented using commercially available devices, such as commodity laptops, and use the WiFi NICs in those devices to capture changes in the wireless channel metrics. </li>
</ul>
<h5 id="6-2-2-Contribution"><a href="#6-2-2-Contribution" class="headerlink" title="6.2.2.  Contribution"></a>6.2.2.  Contribution</h5></li>
<li><p>present a novel translation function that enables position and orientation agnostic gesture recognition without requiring the user to provide training samples in all possible configurations</p>
</li>
<li><p>present a novel configuration estimation scheme that <code>automatically identifies the position and orientation of the user.</code></p>
</li>
<li><p>implemented and extensively evaluated WiAG using commodity WiFi devices</p>
</li>
<li><p>when user’s configuration is not the same at runtime as at the time of providing training samples, our translation function significantly improves the accuracy of gesture recognition from just 51.4% to 91.4%</p>
</li>
</ul>
<h5 id="6-2-3-Experiment"><a href="#6-2-3-Experiment" class="headerlink" title="6.2.3. Experiment"></a>6.2.3. Experiment</h5><ul>
<li><p>a Thinkpad X200 laptop equipped with an Intel 5300 WiFi NIC attached to <code>two omni-directional antennas:</code></p>
<ul>
<li><p>a sampling rate of 100 samples/sec</p>
</li>
<li><p>1427 samples from 10 volunteers for 6 gestures at 5 different positions on 8 different days</p>
</li>
<li><p>a 25ft×16ft room that contains usual furniture including 7 chairs and 3 tables</p>
<img src="https://gitee.com/github-25970295/blogImage/raw/master/img/c7.png" style="zoom: 80%;">
</li>
<li><p>Accuracy with Change in Orientation:</p>
<img src="https://gitee.com/github-25970295/blogImage/raw/master/img/c6.png" style="zoom:80%;">
</li>
<li><p>Accuracy with Change in Orientation and Position:</p>
<img src="https://gitee.com/github-25970295/blogImage/raw/master/img/c5.png" style="zoom:80%;">


</li>
</ul>
</li>
</ul>
<h4 id="6-3-WiMU"><a href="#6-3-WiMU" class="headerlink" title="6.3. WiMU"></a>6.3. WiMU</h4><blockquote>
<p>Raghav H. Venkatnarayan, Griffin Page, Muhammad Shahzad. “Multi-User Gesture Recognition Using WiFi” <em>Proceedings of the 16th Annual International Conference on Mobile Systems, Applications, and Services</em>, 2018.</p>
</blockquote>
<h5 id="6-3-1-Relative"><a href="#6-3-1-Relative" class="headerlink" title="6.3.1. Relative"></a>6.3.1. <strong>Relative</strong></h5><h5 id="6-3-2-Contribution"><a href="#6-3-2-Contribution" class="headerlink" title="6.3.2.  Contribution"></a>6.3.2.  Contribution</h5><ul>
<li><p>propose a method to generate virtual samples without requiring users to provide training samples for all possible combinations of gestures and without requiring to separate the contribution of each user’s movements from the net measurements of wireless channel metrics</p>
</li>
<li><p>propose a method to automatically identify the number of gestures as well as the start and end times of different gestures when multiple users perform them simultaneously</p>
</li>
<li><p>present our implementation and extensive evaluation of WiMU on commodity WiFi devices</p>
</li>
<li><p>WiMU recognizes 2, 3, 4, 5, and 6 simultaneously performed gestures with average accuracies of 95.0, 94.6, 93.6, 92.6, and 90.9 percent, respectively</p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/c8.png" alt=""></p>
</li>
</ul>
<h5 id="6-3-3-Experiment"><a href="#6-3-3-Experiment" class="headerlink" title="6.3.3. Experiment"></a>6.3.3. Experiment</h5><ul>
<li><p>a commodity PC (8 core Intel Xeon processor, 16GB RAM):</p>
<ul>
<li><p>used the tool presented in [15] to acquire CSI measurements in the 5GHz band from an Intel 5300 WiFi NIC connected with <em>N_{Rx}</em> = 3 omnidirectional antennas</p>
</li>
<li><p>used NETGEAR R6700 access point (AP) using <em>N_{T x}</em> = 1 omnidirectional</p>
<p>antenna and pinged it every 1ms to achieve a 1000 samples/sec sampling rate</p>
</li>
<li><p>3 males and 7 females, with ages ranging from 20 to 30, heights from 5’ 5” to 6’ 3”,</p>
<p>and weights from 135lb to 220lb</p>
</li>
<li><p>600 for gestures performed individually and 450 for gestures performed simultaneously</p>
<img src="https://gitee.com/github-25970295/blogImage/raw/master/img/c9.png" style="zoom:80%;">

</li>
</ul>
</li>
</ul>
<h4 id="6-4-Widar3-0"><a href="#6-4-Widar3-0" class="headerlink" title="6.4. Widar3.0"></a>6.4. Widar3.0</h4><blockquote>
<p>Yue Zheng, Yi Zhang, Kun Qian, Guidong Zhang, Yunhao Liu, Chenshu Wu, Zheng Yang. “Zero-Effort Cross-Domain Gesture Recognition with Wi-Fi” <em>Proceedings of the 17th Annual International Conference on Mobile Systems, Applications, and Services,</em> 2019.</p>
</blockquote>
<h5 id="6-4-1-Relative"><a href="#6-4-1-Relative" class="headerlink" title="6.4.1. Relative"></a>6.4.1. <strong>Relative</strong></h5><ul>
<li><p>Model-based wireless localization</p>
<ul>
<li><p>On the signal side</p>
<ul>
<li><p>existing approaches extract various parameters of signals reflected or shadowed by human, including <code>DFS [26, 32, 44], ToF [2–4, 21], AoA/AoD [2, 5, 21, 25] and attenuation [7, 41]</code>.</p>
</li>
<li><p>WiTrack [3, 4] develops <code>FMCW radar with wide bandwidth</code> to accurately estimate ToFs of reflected signals.</p>
</li>
<li><p>WiDeo [21] customizes <code>full-duplex Wi-Fi to jointly estimate ToFs and AoAs of major reflectors.</code></p>
</li>
<li><p>Widar2.0 [33] improves resolution by jointly estimating ToF, AoA and DFS</p>
</li>
</ul>
</li>
<li><p>On the human side</p>
<ul>
<li>existing model-based works only tracks coarse human <code>motion status, such as location [4, 41], velocity [26, 32], gait [43, 49] and figure [2, 19]</code></li>
</ul>
</li>
</ul>
</li>
<li><p>Learning-based wireless activity recognition</p>
<ul>
<li><p>existing approaches extract signal features, either statistical [14, 15, 23, 28, 30, 45, 49] or physical [6, 31, 34, 38, 39, 44, 51, 52] ones, and map them to discrete activities</p>
</li>
<li><p><code>E-eyes [45]</code> is a pioneer work to use <code>strength distribution of commercial Wi-Fi signal</code>s and <code>KNN</code> to recognize human activities.</p>
</li>
<li><p>Niu et al. [30] uses<code>signal waveforms</code> for fine-grained gesture recognition. The physical methods take a step further to extract features with clear physical meanings. </p>
</li>
<li><p>CARM [44] calculates power distribution of DFS components as learning features of</p>
<p>HMM model. </p>
</li>
<li><p>WIMU [38] further segments DFS power profile for multi-person activity recognition. </p>
</li>
<li><p>virtually generating features for target domains [39, 40, 50, 53] </p>
<ul>
<li>WiAG [39] derives translation functions between CSIs from different domains, and generates virtual training data accordingly. CrossSense [50] adopts the idea of transfer learning, and proposes a roaming model to translate signal features between domains. （domain-dependent， require training of classifier for each individual domain）</li>
</ul>
</li>
<li><p>developing domain-independent features [9, 20, 37]</p>
<ul>
<li>EI [20] incorporates an adversarial network to obtain domain-independent features from CSI. （require extra data samples from the target domain）</li>
</ul>
</li>
</ul>
<h5 id="6-4-2-Contribution"><a href="#6-4-2-Contribution" class="headerlink" title="6.4.2.  Contribution"></a>6.4.2.  Contribution</h5></li>
<li><p>present a novel domain-independent feature that captures body coordinate velocity profiles of human gestures at the lower signal level</p>
</li>
<li><p>develop a <em>one-fits-all</em> model on the basis of domain-independent BVP and a learning method that fully exploits spatial-temporal characteristics of BVP</p>
</li>
<li><p>though trained only once, Widar3.0 achieves on average 89.7%, 82.6%, and 92.4% recognition accuracy across locations, orientations, and environments, respectively</p>
</li>
<li><p>Widar3.0 is the first zero-effort cross-domain gesture recognition via Wi-Fi</p>
</li>
</ul>
<img src="https://gitee.com/github-25970295/blogImage/raw/master/img/c10.png" style="zoom:80%;">

<p> ​           </p>
<h5 id="6-4-3-Experiment"><a href="#6-4-3-Experiment" class="headerlink" title="6.4.3. Experiment"></a>6.4.3. Experiment</h5><ul>
<li><p><code>one transmitter and at least three receivers:</code></p>
<ul>
<li><p>All transceivers are off-the-shelf mini-desktops (physical size 170mm  × 170mm) equipped with an Intel 5300 wire less NIC.</p>
</li>
<li><p>Linux CSI Tool [18] </p>
</li>
<li><p>on channel 165 at 5.825 GHz </p>
</li>
<li><p>broadcasts Wi-Fi packets at a rate of 1,000 packets per second</p>
</li>
<li><p>2m × 2m square, 12 males and 4 females(22 to 28), different heights (varying from 185 cm to 155 cm) and somatotypes</p>
<img src="https://gitee.com/github-25970295/blogImage/raw/master/img/c13.png" style="zoom:67%;">
</li>
<li><p>gesture data from 5 locations and 5 orientations in each sensing area</p>
</li>
<li><p>six gestures: 12,000 gesture samples (16 users × 5 positions × 5 orientations × 6 gestures × 5 instances)</p>
<img src="https://gitee.com/github-25970295/blogImage/raw/master/img/c11.png" style="zoom:67%;">
</li>
<li><p>number 0 ∼ 9 in the horizontal plane,  5,000 samples (2 users × 5 positions × 5 orientations × 10 gestures × 10 instances)</p>
<img src="https://gitee.com/github-25970295/blogImage/raw/master/img/c12.png" style="zoom:67%;">
</li>
<li><p>though trained only once, Widar3.0 achieves on average 89.7%, 82.6%, and 92.4% recognition accuracy across locations, orientations, and environments, respectively</p>
</li>
</ul>
</li>
</ul>
<h4 id="6-5-RF-SCG"><a href="#6-5-RF-SCG" class="headerlink" title="6.5. RF-SCG"></a>6.5. RF-SCG</h4><blockquote>
<p>Ha U, Assana S, Adib F. Contactless seismocardiography via deep learning radars[C]//Proceedings of the 26th Annual International Conference on Mobile Computing and Networking. 2020: 1-14. [<a href="https://www.youtube.com/watch?v=36rO7BaU724&amp;list=TLPQMjkxMTIwMjGPrdDXYiSKow&amp;index=4" target="_blank" rel="noopener">video</a>]</p>
</blockquote>
<ul>
<li>a system that can capture SCG recordings without requiring any contact with the human body, by analyzing the reflections of milimeter-wave radar signals off the human body.</li>
<li>the system includes a 4F cardiac Beamformer that can focus on the reflections of the human heart and a deep learning pipeline to transfer these reflections into SCG waveforms.</li>
</ul>
<p><img src="../../../../picture/image-20211129160607285.png" alt=""></p>
<p><img src="../../../../picture/image-20211129160643952.png" alt=""></p>
<p><img src="../../../../picture/image-20211129160655200.png" alt=""></p>
<h3 id="7-4G-based"><a href="#7-4G-based" class="headerlink" title="7. 4G-based"></a>7. 4G-based</h3><blockquote>
<p>Weiyan Chen, Kai Niu, Dan Wu，Deng Zhao，Leye Wang and Daqing Zhang. “Demo: A Contactless Gesture Interaction System Using LTE (4G) Signals” <em>Adjunct Proceedings of the 2019 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2019 ACM International Symposium on Wearable Computers,</em> 2019.</p>
</blockquote>
<h5 id="7-1-1-Relative"><a href="#7-1-1-Relative" class="headerlink" title="7.1.1. Relative"></a>7.1.1. <strong>Relative</strong></h5><h5 id="7-1-2-Contribution"><a href="#7-1-2-Contribution" class="headerlink" title="7.1.2.  Contribution"></a>7.1.2.  Contribution</h5><ul>
<li><p>present an LTE-based contactless gesture interaction system to recognize various hand gestures around a 4G terminal like mobile phone, which can be used to control the switch, channel and volume of a TV set remotely without holding any devices</p>
<img src="https://gitee.com/github-25970295/blogImage/raw/master/img/d1.png" style="zoom:67%;">


</li>
</ul>
<h5 id="7-1-3-Experiment"><a href="#7-1-3-Experiment" class="headerlink" title="7.1.3. Experiment"></a>7.1.3. Experiment</h5><ul>
<li><p>a <code>USRP B210 platform</code> equipped with <code>two antennas</code> and works on 2.165GHz band with 1.4MHz bandwidth.:</p>
<ul>
<li><p>The 4G terminal receives CSI at the rate of 100 samples per second.</p>
</li>
<li><p>four hand gestures</p>
<img src="https://gitee.com/github-25970295/blogImage/raw/master/img/d2.png" style="zoom:67%;">

<img src="https://gitee.com/github-25970295/blogImage/raw/master/img/d3.png" style="zoom: 80%;">

</li>
</ul>
</li>
</ul>
<h3 id="8-电容-based"><a href="#8-电容-based" class="headerlink" title="8. 电容-based"></a>8. 电容-based</h3><blockquote>
<p>Hoang Truong, Phuc Nguyen, Anh Nguyen, Nam Bui and Tam Vu. “Capacitive Sensing 3D-printed Wristband for Enriched Hand Gesture Recognition” <em>Proceedings of the 2017 Workshop on Wearable Systems and Applications</em>, 2017.</p>
</blockquote>
<h5 id="8-1-1-Relative"><a href="#8-1-1-Relative" class="headerlink" title="8.1.1. Relative"></a>8.1.1. <strong>Relative</strong></h5><ul>
<li><p>Sensor and gesture recognition with wearable devices</p>
<ul>
<li>Myo product [17]，tomo [25] and tomo2 [26]</li>
</ul>
</li>
<li><p>Enriched gesture recognition</p>
<ul>
<li>TypingRing [13] is a ring-like device allowing user to type on any surface so that input text from a group of 3 letters using 3 consecutive fifingers</li>
</ul>
</li>
<li><p>Capacitive sensing based gesture recognition</p>
<ul>
<li>GestureWrist [14] utilized capacitive measurement of wrist shape changes and forearm movement with the usage of a system with 6 receiver electrodes.</li>
</ul>
<h5 id="8-1-2-Contribution"><a href="#8-1-2-Contribution" class="headerlink" title="8.1.2.  Contribution"></a>8.1.2.  Contribution</h5><ul>
<li><p>design a wearable-form hand gesture recognition system using <code>capacitive sensing technique</code></p>
</li>
<li><p>release an open API of our designed system for future applications</p>
</li>
<li><p>envision our system open API will be available for developers to customize vast range of hand</p>
<p>gesture and integrate the wristband into various applications</p>
<img src="https://gitee.com/github-25970295/blogImage/raw/master/img/e1.png" style="zoom:80%;">

</li>
</ul>
<h5 id="8-1-3-Experiment"><a href="#8-1-3-Experiment" class="headerlink" title="8.1.3. Experiment"></a>8.1.3. Experiment</h5></li>
<li><p>Hardware design</p>
<ul>
<li><p>3D printed wristband mold</p>
</li>
<li><p>Flexible circuit sensor board</p>
</li>
<li><p>Micro-controller unit (MCU) and BLE module</p>
<img src="https://gitee.com/github-25970295/blogImage/raw/master/img/e2.png" style="zoom: 80%;">
</li>
<li><p>Software stack</p>
<ul>
<li>Support vector machine (SVM), apply the radial basis kernel functions for SVM classififier for hand gesture training</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>   <img src="https://gitee.com/github-25970295/blogImage/raw/master/img/e3.png" alt=""></p>
<h3 id="9-光学-based"><a href="#9-光学-based" class="headerlink" title="9. 光学-based"></a>9. 光学-based</h3><h4 id="9-1-LiGest"><a href="#9-1-LiGest" class="headerlink" title="9.1.  LiGest"></a>9.1.  LiGest</h4><blockquote>
<p>Raghav H. Venkatnarayan and Muhammad Shahzad. “Gesture Recognition Using Ambient Light” <em>Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies</em>, 2018.</p>
</blockquote>
<h5 id="9-1-1-Relative"><a href="#9-1-1-Relative" class="headerlink" title="9.1.1. Relative"></a>9.1.1. <strong>Relative</strong></h5><ul>
<li><p>Light based Human Sensing</p>
<ul>
<li>LiSense [58] and StarLight [60], which <code>reconstruct user’s posture in the form of stick figures using visible light.</code></li>
<li>CeilingSee [108] proposed to <code>estimate the number of occupants in a room</code> using ceiling-mounted LEDs as sensors. </li>
<li>GestureLite [51], which leverages <code>ambient light to recognize hand gestures</code> in “an environment with a reasonably consistent lighting scheme”</li>
</ul>
</li>
<li><p>Camera based Gesture Recognition</p>
</li>
<li><p>Wearable Sensors based Gesture Recognition</p>
</li>
<li><p>Sound based Gesture Recognition</p>
<ul>
<li>Sound based sensing and gesture recognition systems leverage changes in the arrival times of reflections of sound waves or changes in the Doppler shift in the reflected sound waves caused by the changes in human posture to recognize gestures [40, 75, 98].</li>
</ul>
</li>
<li><p>RF based Gesture Recognition</p>
<h5 id="9-1-2-Contribution"><a href="#9-1-2-Contribution" class="headerlink" title="9.1.2.  Contribution"></a>9.1.2.  Contribution</h5><ul>
<li>proposed a gesture recognition system that works using ambient light generated from any type of light sources, including LEDs and fluorescent lamps, and without requiring any control over the light sources</li>
<li>developed techniques that make ambient light based gesture recognition systems agnostic to lighting conditions as well as to the position and orientation of user</li>
<li>collected a comprehensive real world dataset of 15175 samples from 20 users in 9 positions, 4 orientations, 11 lighting conditions, and 2 different environments. The dataset will be made available to the  research community after publication</li>
<li>implemented and extensively evaluated LiGest using commodity light sensors</li>
<li>LiGest achieves an <em>average</em> accuracy of 96.36%, which is similar to (and often higher than) the average accuracies achieved by RF based gesture recognition systems[24, 52, 78].</li>
</ul>
<h5 id="9-1-3-Experiment"><a href="#9-1-3-Experiment" class="headerlink" title="9.1.3. Experiment"></a>9.1.3. Experiment</h5></li>
<li><p>a sensing platform that comprised of a portable andsturdy 6ft × 6ft mat and <em>N</em> = 36 light sensors placed on it in the form of a square grid with an even spacing of 1ft between adjacent sensors</p>
<ul>
<li>cheap off-the-shelf light sensors, namely TSL237 [20]</li>
</ul>
</li>
<li><p>To connect the sensors to the central server, we interfaced the 36 sensors to 18 Arduino Uno boards, <em>i.e.</em>, 2 sensors per board.</p>
<ul>
<li>a total of 15175 samples for five different gestures (punch, hug, clap, step, and jump) from 20 volunteers (14 males and 6 females) with ages ranging from 22 to 28 years and heights ranging from 150cm to 180cm.</li>
</ul>
</li>
</ul>
<img src="https://gitee.com/github-25970295/blogImage/raw/master/img/f1.png" style="zoom:67%;">

<img src="https://gitee.com/github-25970295/blogImage/raw/master/img/f2.png" style="zoom:80%;">

<img src="https://gitee.com/github-25970295/blogImage/raw/master/img/f3.png" style="zoom:67%;">

<h4 id="9-2-SolarGest"><a href="#9-2-SolarGest" class="headerlink" title="9.2. SolarGest"></a>9.2. SolarGest</h4><blockquote>
<p>Dong Ma, Guohao Lan, Mahbub Hassan, Wen Hu, Mushfika B. Upama, Ashraf Uddin and Moustafa Youssef. “SolarGest: Ubiquitous and Battery-free Gesture Recognition using Solar Cells” <em>The 25th Annual International Conference on Mobile Computing and Networking</em>, 2019.</p>
</blockquote>
<h5 id="9-2-1-Relative"><a href="#9-2-1-Relative" class="headerlink" title="9.2.1. Relative"></a>9.2.1. <strong>Relative</strong></h5><ul>
<li>Gesture Recognition</li>
<li><code>Solar Energy Harvesting based Sensing</code><ul>
<li>utilize the solar cell as a light indicator to perform indoor positioning [46]</li>
<li>A recent work [40] employed an array of photodiodes around a smartwatch for the dual-use of <code>solar energy harvesting</code> as well as <code>gesture recognition</code>;</li>
<li>the most relevant work is [19], in which the authors utilized an opaque solar cell to identify three hand gestures. However, SolarGest differs in three aspects</li>
</ul>
</li>
</ul>
<h5 id="9-2-2-Contribution"><a href="#9-2-2-Contribution" class="headerlink" title="9.2.2.  Contribution"></a>9.2.2.  Contribution</h5><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201116161609512.png" alt=""></p>
<ul>
<li><p>develop a model to <code>simulate photocurrent waveforms</code> produced by arbitrary hand gestures in both <em>vertical</em> and <em>horizontal</em> planes relative to the solar panel.</p>
<ul>
<li><p>propose a general machine learning framework to detect any type of gestures</p>
</li>
<li><p>design an end-to-end signal processing pipeline to protect SolarGest performance against variations in operating conditions</p>
</li>
<li><p>SolarGest can detect gestures with an accuracy of 96%, which is comparable to that achieved with light sensors</p>
</li>
<li><p>SolarGest consumes 44% less power compared to systems that detect gestures using light sensors</p>
</li>
</ul>
</li>
</ul>
<h5 id="9-2-3-Experiment"><a href="#9-2-3-Experiment" class="headerlink" title="9.2.3.  Experiment"></a>9.2.3.  Experiment</h5><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201116161452426.png" alt=""></p>
<img src="https://gitee.com/github-25970295/blogImage/raw/master/img/f4.png" style="zoom: 50%;">

<img src="https://gitee.com/github-25970295/blogImage/raw/master/img/f5.png" style="zoom: 50%;">

<img src="https://gitee.com/github-25970295/blogImage/raw/master/img/f6.png" style="zoom: 50%;">

<ul>
<li><p>detect six gestures with 96% accuracy under typical use scenarios while consuming 44% less power compared to light sensor based approach</p>
</li>
<li><p>power measurements</p>
<ul>
<li><p>MCU Power Measurement</p>
</li>
<li><p>Light sensor Power Measurement</p>
</li>
<li><p>Overall System Power Saving</p>
<img src="https://gitee.com/github-25970295/blogImage/raw/master/img/f7.png" style="zoom:67%;">

</li>
</ul>
</li>
</ul>
<h3 id="10-多普勒-based"><a href="#10-多普勒-based" class="headerlink" title="10. 多普勒-based"></a>10. 多普勒-based</h3><h4 id="10-1-Multiwave"><a href="#10-1-Multiwave" class="headerlink" title="10.1.  Multiwave"></a>10.1.  Multiwave</h4><blockquote>
<p>Corey Pittman, Conner Brooks, Pamela Wisniewski and Joseph J. LaViola Jr. “Multiwave: Doppler Effect Based Gesture Recognition in Multiple Dimensions” <em>Proceedings of the 2016 CHI Conference Extended Abstracts on Human Factors in Computing Systems</em>, 2016.</p>
</blockquote>
<h5 id="10-1-1-Relative"><a href="#10-1-1-Relative" class="headerlink" title="10.1.1. Relative"></a>10.1.1. <strong>Relative</strong></h5><h5 id="10-1-2-Contribution"><a href="#10-1-2-Contribution" class="headerlink" title="10.1.2.  Contribution"></a>10.1.2.  Contribution</h5><ul>
<li><p>constructed an acoustic, gesture-based recognition system called Multiwave, which leverages the Doppler Effect to translate multidimensional movements into user interface commands</p>
</li>
<li><p>showed a method of transforming extracted motion information into Euclidean space to generalize gestures for any given speaker geometry</p>
<h5 id="10-1-3-Experiment"><a href="#10-1-3-Experiment" class="headerlink" title="10.1.3. Experiment"></a>10.1.3. Experiment</h5></li>
</ul>
<ul>
<li>a 55 inch HDTV, a stereo speaker system, and a low cost USB microphone with no processing enabled, each of which was connected to a PC with a Intel Xeon dual core processor and 12 GB of RAM</li>
</ul>
<img src="https://gitee.com/github-25970295/blogImage/raw/master/img/f8.png" style="zoom: 50%;">

<ul>
<li><p>9 male, 1 female, ages ranged from 19 to 27 with a median age of 21</p>
</li>
<li><p>the duration of the study ranged from 45 to 60 minutes</p>
</li>
<li><p>The overall accuracy for swipe and tap gestures in each confifiguration is shown.</p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/f9.png" alt=""></p>
</li>
</ul>
<h4 id="10-2-Fabriccio"><a href="#10-2-Fabriccio" class="headerlink" title="10.2.  Fabriccio"></a>10.2.  Fabriccio</h4><blockquote>
<p>Te-Yen Wu, Shutong Qi, Junchi Chen, MuJie Shang, Jun Gong, Teddy Seyed and Xing-Dong Yang. “Fabriccio: Touchless Gestural Input on Interactive Fabrics” <em>Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems</em>, 2020.</p>
</blockquote>
<h5 id="10-2-1-Relative"><a href="#10-2-1-Relative" class="headerlink" title="10.2.1. Relative"></a>10.2.1. <strong>Relative</strong></h5><ul>
<li><p>Sensing Input on Interactive Fabric</p>
<ul>
<li>With current technologies, input techniques through interactive fabrics includes touch [38, 42, 47, 50], deformation [35, 39, 59], and object recognition [18, 43] using sensing techniques based on capacitance [21, 33, 39, 40, 41], resistance [37, 38, 43, 58, 66], and inductance [18]. </li>
</ul>
</li>
<li><p>Sensing Touchless Gestural Input</p>
<ul>
<li>Sensing techniques for touchless gestural input can be divided into those based on vision [52, 55, 60], radio frequency [30, 54, 57, 43, 49, 62, 69, 71, 73], pyroelectric infrared [19] and acoustics [20, 37, 64]</li>
</ul>
</li>
<li><p>Textile Antennas</p>
<ul>
<li>Textile antennas made of conductive threads are an emerging technology in electrical engineering with applications primarily targeting wireless communication [6, 7, 9, 25, 36, 46, 65, 66], health monitoring [26, 46, 70], object identification [13, 51], and energy harvesting [17, 32].</li>
</ul>
<h5 id="10-2-2-Contribution"><a href="#10-2-2-Contribution" class="headerlink" title="10.2.2.  Contribution"></a>10.2.2.  Contribution</h5><ul>
<li><p>a touchless gesture sensing technique for interactive fabrics that uses the Doppler effect</p>
</li>
<li><p>a prototype demonstrating technical feasibility</p>
</li>
<li><p>a study evaluating the accuracy our sensing technique</p>
</li>
<li><p>several applications to demonstrate the unique interactions enabled by our technique</p>
<img src="https://gitee.com/github-25970295/blogImage/raw/master/img/f10.png" style="zoom:67%;">

</li>
</ul>
<h5 id="10-2-3-Experiment"><a href="#10-2-3-Experiment" class="headerlink" title="10.2.3. Experiment"></a>10.2.3. Experiment</h5></li>
<li><p>Hardware</p>
<ul>
<li><p>Conductive Thread Options</p>
<ul>
<li>estimate the performance of four conductive threads using simulation(no noticeable difference)</li>
</ul>
</li>
<li><p>Fabricating Textile Antennas</p>
<p>​                                             <img src="https://gitee.com/github-25970295/blogImage/raw/master/img/f11.png" style="zoom:67%;"></p>
</li>
</ul>
</li>
</ul>
<ul>
<li><p>Customized Sensing Board </p>
<ul>
<li><p>a Doppler sensor board and a data collection board </p>
<img src="https://gitee.com/github-25970295/blogImage/raw/master/img/f12.png" style="zoom:67%;">
</li>
</ul>
</li>
<li><p>Wire Connection</p>
</li>
</ul>
<ul>
<li><p>Software</p>
<ul>
<li><p>Signal Processing &amp; Featurization </p>
<ul>
<li>a low-pass filter at 100Hz to remove background noise</li>
<li>the features for machine learning were extracted in both frequency and time domains </li>
</ul>
</li>
<li><p>Machine Learning</p>
<ul>
<li>the Random Forest from Scikit-learn with a forest size of 100 and the maximum depth of 30  on a Microsoft Surface laptop</li>
</ul>
</li>
</ul>
</li>
<li><p>evaluation</p>
<ul>
<li>Ten right-handed participants (average age: 21.6, 6 females)</li>
<li>perform a gesture toward the textile sensor, roughly in a distance between 5 to 10 cm using right hand</li>
<li>each gesture was repeated 10 times each session</li>
<li>A 10-minute break was given between sessions</li>
</ul>
</li>
<li><p>3300 samples (10 participants × 11 gestures × 10 repetitions × 3 sessions) </p>
<pre><code>&lt;img src="https://gitee.com/github-25970295/blogImage/raw/master/img/f13.png" style="zoom: 80%;" /&gt;  </code></pre></li>
<li><p>-The study result yielded a 92.8% cross-validation accuracy and 85.2% leave-one-session-out(train using data from the first two session, test for data from last session) accuracy.</p>
</li>
</ul>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script>
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        Author:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://liudongdong1.github.io" rel="external nofollow noreferrer">liudongdong1</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        Link:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://liudongdong1.github.io/2020/10/24/aiot/mobiledevices/handrecognition/">https://liudongdong1.github.io/2020/10/24/aiot/mobiledevices/handrecognition/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        Reprint policy:
                    </i>
                </span>
                <span class="reprint-info">
                    All articles in this blog are used except for special statements
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    reprint polocy. If reproduced, please indicate source
                    <a href="https://liudongdong1.github.io" target="_blank">liudongdong1</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>Copied successfully, please follow the reprint policy of this article</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">more</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/tags/IOT/">
                                    <span class="chip bg-color">IOT</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="qq,qzone,wechat,weibo,douban" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
                <style>
    #reward {
        margin: 40px 0;
        text-align: center;
    }

    #reward .reward-link {
        font-size: 1.4rem;
        line-height: 38px;
    }

    #reward .btn-floating:hover {
        box-shadow: 0 6px 12px rgba(0, 0, 0, 0.2), 0 5px 15px rgba(0, 0, 0, 0.2);
    }

    #rewardModal {
        width: 320px;
        height: 350px;
    }

    #rewardModal .reward-title {
        margin: 15px auto;
        padding-bottom: 5px;
    }

    #rewardModal .modal-content {
        padding: 10px;
    }

    #rewardModal .close {
        position: absolute;
        right: 15px;
        top: 15px;
        color: rgba(0, 0, 0, 0.5);
        font-size: 1.3rem;
        line-height: 20px;
        cursor: pointer;
    }

    #rewardModal .close:hover {
        color: #ef5350;
        transform: scale(1.3);
        -moz-transform:scale(1.3);
        -webkit-transform:scale(1.3);
        -o-transform:scale(1.3);
    }

    #rewardModal .reward-tabs {
        margin: 0 auto;
        width: 210px;
    }

    .reward-tabs .tabs {
        height: 38px;
        margin: 10px auto;
        padding-left: 0;
    }

    .reward-content ul {
        padding-left: 0 !important;
    }

    .reward-tabs .tabs .tab {
        height: 38px;
        line-height: 38px;
    }

    .reward-tabs .tab a {
        color: #fff;
        background-color: #ccc;
    }

    .reward-tabs .tab a:hover {
        background-color: #ccc;
        color: #fff;
    }

    .reward-tabs .wechat-tab .active {
        color: #fff !important;
        background-color: #22AB38 !important;
    }

    .reward-tabs .alipay-tab .active {
        color: #fff !important;
        background-color: #019FE8 !important;
    }

    .reward-tabs .reward-img {
        width: 210px;
        height: 210px;
    }
</style>

<div id="reward">
    <a href="#rewardModal" class="reward-link modal-trigger btn-floating btn-medium waves-effect waves-light red">赏</a>

    <!-- Modal Structure -->
    <div id="rewardModal" class="modal">
        <div class="modal-content">
            <a class="close modal-close"><i class="fas fa-times"></i></a>
            <h4 class="reward-title">你的赏识是我前进的动力</h4>
            <div class="reward-content">
                <div class="reward-tabs">
                    <ul class="tabs row">
                        <li class="tab col s6 alipay-tab waves-effect waves-light"><a href="#alipay">支付宝</a></li>
                        <li class="tab col s6 wechat-tab waves-effect waves-light"><a href="#wechat">微 信</a></li>
                    </ul>
                    <div id="alipay">
                        <img src="/medias/reward/alipay.jpg" class="reward-img" alt="支付宝打赏二维码">
                    </div>
                    <div id="wechat">
                        <img src="/medias/reward/wechat.png" class="reward-img" alt="微信打赏二维码">
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>

<script>
    $(function () {
        $('.tabs').tabs();
    });
</script>

            
        </div>
    </div>

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;Previous</div>
            <div class="card">
                <a href="/2020/10/24/aiot/mobiledevices/smartwatchrelative/">
                    <div class="card-image">
                        
                        <img src="https://gitee.com/github-25970295/blogImage/raw/master/img/30.jpeg" class="responsive-img" alt="SmartWatchRelative">
                        
                        <span class="card-title">SmartWatchRelative</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            level:author: Gutaodate: 2018

Zhang, Yu, et al. “Findroidhr: Smartwatch gesture input with optical heartrate monitor.” 
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2020-10-24
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/AIOT/" class="post-category">
                                    AIOT
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/IOT/">
                        <span class="chip bg-color">IOT</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                Next&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/2020/10/22/kuang-jia-she-ji/designpattern/uml-ge-lei-guan-xi-jie-shao/">
                    <div class="card-image">
                        
                        <img src="http://static.blinkfox.com/20181022-organ.jpg" class="responsive-img" alt="UML各类关系介绍">
                        
                        <span class="card-title">UML各类关系介绍</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            
泛化 \=** 实现 *&gt;* 组合 *&gt;* 聚合 *&gt;* 关联 *&gt;* 依赖**


继承关系（generalization）继承关系用一条带空心箭头的直线表示。如下图所示（A继承自B）：

继承指的是一个类（称为
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2020-10-22
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/%E8%AF%AD%E8%A8%80%E6%A1%86%E6%9E%B6/" class="post-category">
                                    语言框架
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/Java/">
                        <span class="chip bg-color">Java</span>
                    </a>
                    
                    <a href="/tags/%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%E7%BC%96%E7%A8%8B/">
                        <span class="chip bg-color">面向对象编程</span>
                    </a>
                    
                    <a href="/tags/UML/">
                        <span class="chip bg-color">UML</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/libs/codeBlock/codeBlockFuction.js"></script>

<!-- 代码语言 -->

<script type="text/javascript" src="/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/libs/codeBlock/codeShrink.js"></script>


<!-- 代码块折行 -->

<style type="text/css">
code[class*="language-"], pre[class*="language-"] { white-space: pre !important; }
</style>


    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;TOC</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // modify the toc link href to support Chinese.
        let i = 0;
        let tocHeading = 'toc-heading-';
        $('#toc-content a').each(function () {
            $(this).attr('href', '#' + tocHeading + (++i));
        });

        // modify the heading title id to support Chinese.
        i = 0;
        $('#articleContent').children('h2, h3, h4').each(function () {
            $(this).attr('id', tocHeading + (++i));
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>

    

</main>




    <!-- <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="463294659"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/libs/aplayer/APlayer.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/meting@2/dist/Meting.min.js"></script>

    
    <div class="container row center-align" style="margin-bottom: 0px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            <span id="year">2019</span>
            <a href="https://liudongdong1.github.io" target="_blank">liudongdong</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            <br>
            
            &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                class="white-color">1413.4k</span>&nbsp;字
            
            
            
            
            
            
            
            <br>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/liudongdong1/" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:3463264078@qq.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>














    <a href="https://blog.csdn.net/liudongdong19/" class="tooltipped" target="_blank" data-tooltip="关注我的CSDN: https://blog.csdn.net/liudongdong19/" data-position="top" data-delay="50">
        <i class="fab fa-csdn">C</i>
    </a>





</div>
    </div>
</footer>

<div class="progress-bar"></div>
 -->

    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;Search</span>
            <input type="search" id="searchInput" name="s" placeholder="Please enter a search keyword"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script src="/js/search.js"></script>
<script type="text/javascript">
$(function () {
    searchFunc("/search.xml", 'searchInput', 'searchResult');
});
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script type="text/javascript" src="/js/CFS.Snow.min.js"></script>
    <!-- 点击爆灯效果 -->
    <canvas class="fireworks" style="position: fixed;left: 0;top: 0;z-index: 1; pointer-events: none;" ></canvas> 
    <script type="text/javascript" src="//cdn.bootcss.com/animejs/2.2.0/anime.min.js"></script> 
    <script type="text/javascript" src="/js/fireworks.js"></script>
    <!--动态线条背景-->
    <script type="text/javascript"
        color="122 103 238" opacity='0.7' zIndex="-2" count="200" src="//cdn.bootcss.com/canvas-nest.js/1.0.0/canvas-nest.min.js">
    </script>
    <!-- 天气 -->
    <!-- weather -->
    <!-- weather -->
    <script type="text/javascript">
         WIDGET = {FID: 'knAMQaFanP'}
    </script>
    <script type="text/javascript" src="https://apip.weatherdt.com/float/static/js/r.js?v=1111"></script>
    <script type="text/javascript" src="https://apip.weatherdt.com/float/static/js/r.js?v=1111"></script>
    <script src="/libs/materialize/materialize.min.js"></script>
    <script src="/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/libs/aos/aos.js"></script>
    <script src="/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/js/matery.js"></script>

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    
    <script async src="/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    

    
    
    <script type="text/javascript" size="150" alpha='0.6'
        zIndex="-1" src="/libs/background/ribbon-refresh.min.js" async="async"></script>
    

    

    
    <script src="/libs/instantpage/instantpage.js" type="module"></script>
    
    <!-- {% include '_custom/custom.swig' %} -->

</body>

</html>
