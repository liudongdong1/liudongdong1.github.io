<!DOCTYPE html>
<html itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">
  <head>
    
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
    <meta name="robots" content="noodp" />
    <title>PoetryGen - DAY By DAY</title><meta name="author" content="LiuDongdong">
<meta name="author-link" content="https://liudongdong1.github.io/">
<meta name="description" content="AI和文学艺术不断交融，产生了很多有趣的研究方向，如自动绘画生成、诗歌生成、音乐生成、小说生成等。这些研究在学术界和普通人群中都引起了热烈的" /><meta name="keywords" content='Demo, NLP' /><meta itemprop="name" content="PoetryGen">
<meta itemprop="description" content="AI和文学艺术不断交融，产生了很多有趣的研究方向，如自动绘画生成、诗歌生成、音乐生成、小说生成等。这些研究在学术界和普通人群中都引起了热烈的"><meta itemprop="datePublished" content="2022-04-27T08:56:09+00:00" />
<meta itemprop="dateModified" content="2023-12-31T15:45:21+08:00" />
<meta itemprop="wordCount" content="4685"><meta itemprop="image" content="https://liudongdong1.github.io/logo.png"/>
<meta itemprop="keywords" content="Demo,NLP," /><meta property="og:title" content="PoetryGen" />
<meta property="og:description" content="AI和文学艺术不断交融，产生了很多有趣的研究方向，如自动绘画生成、诗歌生成、音乐生成、小说生成等。这些研究在学术界和普通人群中都引起了热烈的" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://liudongdong1.github.io/poetrygen/" /><meta property="og:image" content="https://liudongdong1.github.io/logo.png"/><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-04-27T08:56:09+00:00" />
<meta property="article:modified_time" content="2023-12-31T15:45:21+08:00" />
<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://liudongdong1.github.io/logo.png"/>

<meta name="twitter:title" content="PoetryGen"/>
<meta name="twitter:description" content="AI和文学艺术不断交融，产生了很多有趣的研究方向，如自动绘画生成、诗歌生成、音乐生成、小说生成等。这些研究在学术界和普通人群中都引起了热烈的"/>
<meta name="application-name" content="DAY By DAY">
<meta name="apple-mobile-web-app-title" content="DAY By DAY"><meta name="theme-color" data-light="#f8f8f8" data-dark="#252627" content="#f8f8f8"><meta name="msapplication-TileColor" content="#da532c"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" />
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"><link rel="manifest" href="/site.webmanifest"><link rel="canonical" href="https://liudongdong1.github.io/poetrygen/" /><link rel="prev" href="https://liudongdong1.github.io/docker-operaction/" /><link rel="next" href="https://liudongdong1.github.io/spdk-reactor%E4%BB%8B%E7%BB%8D/" /><link rel="stylesheet" href="/css/style.min.css"><link rel="stylesheet" href="/lib/fontawesome-free/all.min.css"><link rel="stylesheet" href="/lib/animate/animate.min.css"><script type="application/ld+json">
  {
    "@context": "http://schema.org",
    "@type": "BlogPosting",
    "headline": "PoetryGen",
    "inLanguage": "zh-CN",
    "mainEntityOfPage": {
      "@type": "WebPage",
      "@id": "https:\/\/liudongdong1.github.io\/poetrygen\/"
    },"genre": "posts","keywords": "Demo, NLP","wordcount":  4685 ,
    "url": "https:\/\/liudongdong1.github.io\/poetrygen\/","datePublished": "2022-04-27T08:56:09+00:00","dateModified": "2023-12-31T15:45:21+08:00","license": "This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.","publisher": {
      "@type": "Organization",
      "name": "LiuDongdong","logo": "https:\/\/liudongdong1.github.io\/images\/person.png"},"author": {
        "@type": "Person",
        "name": "liudongdong1"
      },"description": ""
  }
  </script></head>
  <body data-header-desktop="auto" data-header-mobile="auto"><script>(window.localStorage?.getItem('theme') ? localStorage.getItem('theme') === 'dark' : ('auto' === 'auto' ? window.matchMedia('(prefers-color-scheme: dark)').matches : 'auto' === 'dark')) && document.body.setAttribute('data-theme', 'dark');</script><div class="wrapper"><script type="text/javascript"
        async
        src="https://cdnjs.cloudflare.com/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$']],
    displayMath: [['$$','$$']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
});

MathJax.Hub.Queue(function() {
    
    
    
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>

<style>
code.has-jax {
    font: inherit;
    font-size: 100%;
    background: inherit;
    border: inherit;
    color: #515151;
}
</style>
<header class="desktop animate__faster" id="header-desktop">
  <div class="header-wrapper" data-github-corner="right">
    <div class="header-title">
      <a href="/" title="DAY By DAY"><img
    class="lazyload logo"
    src="/svg/loading.min.svg"
    data-src="/fixit.min.svg"
    data-srcset="/fixit.min.svg, /fixit.min.svg 1.5x, /fixit.min.svg 2x"
    data-sizes="auto"
    alt="DAY By DAY"
    title="DAY By DAY"/><span class="header-title-text"></span></a><span id="typeit-header-subtitle-desktop" class="typeit header-subtitle"></span></div>
    <nav>
      <ul class="menu"><li class="menu-item">
              <a
                class="menu-link"
                href="/posts/"
                
                
              ><i class="fa-solid fa-archive fa-fw fa-sm" aria-hidden="true"></i> 所有文章</a></li><li class="menu-item">
              <a
                class="menu-link"
                href="/categories/"
                
                
              ><i class="fa-solid fa-th fa-fw fa-sm" aria-hidden="true"></i> 分类</a></li><li class="menu-item">
              <a
                class="menu-link"
                href="/tags/"
                
                
              ><i class="fa-solid fa-tags fa-fw fa-sm" aria-hidden="true"></i> 标签</a></li><li class="menu-item">
              <a
                class="menu-link"
                href="/friends/"
                title="友情链接"
                
              ><i class="fa-solid fa-users fa-fw fa-sm" aria-hidden="true"></i> 友链</a></li><li class="menu-item">
              <a
                class="menu-link"
                href="/about/"
                
                
              ><i class="fa-solid fa-info-circle fa-fw fa-sm" aria-hidden="true"></i> 关于</a></li><li class="menu-item delimiter"></li><li class="menu-item language">
            <span role="button" aria-label="选择语言" title="选择语言">简体中文<i class="dropdown-icon fa-solid fa-chevron-down" aria-hidden="true"></i>
            </span>
            <ul class="sub-menu"><li class="menu-item">没有更多翻译</li></ul>
          </li><li class="menu-item search" id="search-desktop">
            <input type="text" placeholder="搜索文章标题或内容 ..." id="search-input-desktop">
            <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-desktop" title="搜索">
              <i class="fa-solid fa-search fa-fw" aria-hidden="true"></i>
            </a>
            <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-desktop" title="清空">
              <i class="fa-solid fa-times-circle fa-fw" aria-hidden="true"></i>
            </a>
            <span class="search-button search-loading" id="search-loading-desktop">
              <i class="fa-solid fa-spinner fa-fw fa-spin" aria-hidden="true"></i>
            </span>
          </li><li class="menu-item theme-switch" title="切换主题">
          <i class="fa-solid fa-adjust fa-fw" aria-hidden="true"></i>
        </li>
      </ul>
    </nav>
  </div>
</header><header class="mobile animate__faster" id="header-mobile">
  <div class="header-container">
    <div class="header-wrapper">
      <div class="header-title">
        <a href="/" title="DAY By DAY"><img
    class="lazyload logo"
    src="/svg/loading.min.svg"
    data-src="/fixit.min.svg"
    data-srcset="/fixit.min.svg, /fixit.min.svg 1.5x, /fixit.min.svg 2x"
    data-sizes="auto"
    alt="/fixit.min.svg"
    title="/fixit.min.svg"/><span class="header-title-text"></span></a><span id="typeit-header-subtitle-mobile" class="typeit header-subtitle"></span></div>
      <div class="menu-toggle" id="menu-toggle-mobile">
        <span></span><span></span><span></span>
      </div>
    </div>
    <nav>
      <ul class="menu" id="menu-mobile"><li class="search-wrapper">
            <div class="search mobile" id="search-mobile">
              <input type="text" placeholder="搜索文章标题或内容 ..." id="search-input-mobile">
              <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-mobile" title="搜索">
                <i class="fa-solid fa-search fa-fw" aria-hidden="true"></i>
              </a>
              <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-mobile" title="清空">
                <i class="fa-solid fa-times-circle fa-fw" aria-hidden="true"></i>
              </a>
              <span class="search-button search-loading" id="search-loading-mobile">
                <i class="fa-solid fa-spinner fa-fw fa-spin" aria-hidden="true"></i>
              </span>
            </div>
            <a href="javascript:void(0);" class="search-cancel" id="search-cancel-mobile">
              取消
            </a>
          </li><li
              class="menu-item"
            ><a
                  class="menu-link"
                  href="/posts/"
                  
                  
                ><i class="fa-solid fa-archive fa-fw fa-sm" aria-hidden="true"></i> 所有文章</a></li><li
              class="menu-item"
            ><a
                  class="menu-link"
                  href="/categories/"
                  
                  
                ><i class="fa-solid fa-th fa-fw fa-sm" aria-hidden="true"></i> 分类</a></li><li
              class="menu-item"
            ><a
                  class="menu-link"
                  href="/tags/"
                  
                  
                ><i class="fa-solid fa-tags fa-fw fa-sm" aria-hidden="true"></i> 标签</a></li><li
              class="menu-item"
            ><a
                  class="menu-link"
                  href="/friends/"
                  title="友情链接"
                  
                ><i class="fa-solid fa-users fa-fw fa-sm" aria-hidden="true"></i> 友链</a></li><li
              class="menu-item"
            ><a
                  class="menu-link"
                  href="/about/"
                  
                  
                ><i class="fa-solid fa-info-circle fa-fw fa-sm" aria-hidden="true"></i> 关于</a></li><li
              class="menu-item text-center"
            ><a
                  class="menu-link"
                  href="/"
                  title="GitHub"
                  
                ><i class='fa-brands fa-github fa-fw' aria-hidden='true'></i> </a></li><li class="menu-item theme-switch" title="切换主题">
          <i class="fa-solid fa-adjust fa-fw" aria-hidden="true"></i>
        </li><li class="menu-item language">
            <span role="button" aria-label="选择语言" title="选择语言">简体中文<i class="dropdown-icon fa-solid fa-chevron-down" aria-hidden="true"></i>
            </span>
            <select class="language-select" onchange="location = this.value;"><option disabled>没有更多翻译</option></select>
          </li></ul>
    </nav>
  </div>
</header><div class="search-dropdown desktop">
    <div id="search-dropdown-desktop"></div>
  </div>
  <div class="search-dropdown mobile">
    <div id="search-dropdown-mobile"></div>
  </div><main class="container" data-page-style="normal"><aside class="toc" id="toc-auto"><h2 class="toc-title">目录 <i class="toc-icon fa-solid fa-angle-down fa-fw"></i></h2>
      <div class="toc-content" id="toc-content-auto"></div></aside>

  <aside class="aside-custom" id="aside-sakana">
    

<div class="sakana-widget">
  <div class="sakana-item" id="takina-widget"></div>
  <div class="sakana-item" id="chisato-widget"></div>
</div>
<script>
  function initSakanaWidget() {
    const takina = SakanaWidget.getCharacter('takina')
    SakanaWidget.registerCharacter('takina-slow', takina);
    new SakanaWidget({
      character: 'takina-slow',
      controls: false,
      autoFit: true,
      stroke: {
        color: "#b4b4b4",
        width: 2
      }
    }).mount('#takina-widget');

    const chisato = SakanaWidget.getCharacter('chisato')
    SakanaWidget.registerCharacter('chisato-slow', chisato);
    new SakanaWidget({
      character: 'chisato-slow',
      controls: false,
      autoFit: true,
      stroke: {
        color: "#b4b4b4",
        width: 2
      }
    }).mount('#chisato-widget');
  }
</script>
<script async onload="initSakanaWidget()" src="https://cdn.jsdelivr.net/npm/sakana-widget@2.3.0/lib/sakana.min.js">
</script></aside>

  <article class="page single">
    <div class="header"><h1 class="single-title animate__animated animate__flipInX">
        <span>PoetryGen</span>
      </h1></div><div class="post-meta">
      <div class="post-meta-line"><span class="post-author"><span class="author"><i class="fa-solid fa-user-circle" aria-hidden="true"></i>
      liudongdong1</span></span>
          <span class="post-category">收录于 <a href="/categories/"><i class="fa-regular fa-folder fa-fw"></i>&nbsp;Categories</a>&ensp;<a href="/categories/nlp/"><i class="fa-regular fa-folder fa-fw"></i>&nbsp;NLP</a></span></div>
      <div class="post-meta-line"><span title=2022-04-27&#32;08:56:09>
            <i class="fa-regular fa-calendar-alt fa-fw"></i>&nbsp;<time datetime="2022-04-27" >2022-04-27</time>
          </span>&nbsp;<i class="fa-solid fa-pencil-alt fa-fw"></i>&nbsp;约 4685 字&nbsp;
        <i class="fa-regular fa-clock fa-fw"></i>&nbsp;预计阅读 10 分钟&nbsp;<span id="busuanzi_container_page_pv" class="busuanzi_visitors comment-visitors" data-flag-title="PoetryGen">
            <i class="fa-regular fa-eye fa-fw"></i>&nbsp;<span id="busuanzi_value_page_pv">-</span>&nbsp;次阅读
          </span>&nbsp;</div>
    </div><div class="featured-image"><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://cdn.pixabay.com/photo/2014/12/22/19/59/macbook-577758__340.jpg"
    data-srcset="https://cdn.pixabay.com/photo/2014/12/22/19/59/macbook-577758__340.jpg, https://cdn.pixabay.com/photo/2014/12/22/19/59/macbook-577758__340.jpg 1.5x, https://cdn.pixabay.com/photo/2014/12/22/19/59/macbook-577758__340.jpg 2x"
    data-sizes="auto"
    alt="https://cdn.pixabay.com/photo/2014/12/22/19/59/macbook-577758__340.jpg"
    title="https://cdn.pixabay.com/photo/2014/12/22/19/59/macbook-577758__340.jpg"/></div><div class="details toc" id="toc-static" kept="true">
        <div class="details-summary toc-title">
          <span>目录</span>
          <span><i class="details-icon fa-solid fa-angle-right"></i></span>
        </div>
        <div class="details-content toc-content" id="toc-content-static"><nav id="TableOfContents">
  <ul>
    <li>
      <ul>
        <li><a href="#1-技术发展">1. 技术发展</a></li>
        <li><a href="#2-九歌团队httpsgithubcomthunlp-aipoet">2. <a href="https://github.com/THUNLP-AIPoet/">九歌团队</a></a></li>
        <li><a href="#3-相关案例">3. 相关案例</a></li>
        <li><a href="#4-代码阅读">4. 代码阅读</a></li>
        <li><a href="#resource">Resource</a></li>
      </ul>
    </li>
  </ul>
</nav></div>
      </div><div
      class="content"
      id="content"
      
      
    ><blockquote>
<p>AI和文学艺术不断交融，产生了很多有趣的研究方向，如<code>自动绘画生成</code>、<code>诗歌生成</code>、<code>音乐生成</code>、<code>小说生成</code>等。这些研究在学术界和普通人群中都引起了热烈的讨论，并且具有娱乐、教育、辅助文艺研究等广泛的应用价值。1.<code>中文古典诗歌(绝句、宋词等)生成</code>，2.<code>中文对联生成</code>，3.<code>中文现代诗生成</code>，4.<code>外文诗生成</code>，5.<code>多模态诗歌生成</code>, 6.<code>诗歌自动分析</code>, 7.<code>诗歌自动翻译</code>, 8. Demo及Survey</p>
</blockquote>
<h3 id="1-技术发展">1. 技术发展</h3>
<h4 id="1-传统方法">.1. 传统方法</h4>
<ul>
<li><strong>Word Salada（词语沙拉）</strong>：是最早期的诗歌生成模型，被称作只是<code>简单将词语进行随机组合和堆砌而不考虑语义语法要求</code>。</li>
<li><strong>基于模板和模式的方法</strong>：<code>基于模板的方法类似于完形填空</code>，将一首现有诗歌挖去一些词，作为模板，再用一些其他词进行替换，产生新的诗歌。这种方法生成的诗歌在语法上有所提升，但是灵活性太差。因此后来出现了<code>基于模式的方法，通过对每个位置词的词性，韵律平仄进行限制</code>，来进行诗歌生成。</li>
<li><strong>基于遗传算法的方法</strong>：周昌乐等[1]提出并应用到宋词生成上。这里将<code>诗歌生成看成状态空间搜索问题</code>。先从随机诗句开始，然后借助人工定义的诗句评估函数，不断进行评估，进化的迭代，最终得到诗歌。这种方法在单句上有较好的结果，但是<code>句子之间缺乏语义连贯性</code>。</li>
<li><strong>基于摘要生成的方法</strong>：严睿等[2]将<code>诗歌生成看成给定写作意图的摘要生成问题</code>，同时加入了诗歌相关的一些<code>优化约束</code>。</li>
<li><strong>基于统计机器翻译的方法</strong>：MSRA的何晶和周明[3]将诗歌生成看成一个<code>机器翻译问题</code>，将<code>上一句看成源语言，下一句看成目标语言</code>，用统计机器翻译模型进行翻译，并加上平仄押韵等约束，得到下一句。通过不断重复这个过程，得到一首完整的诗歌。</li>
</ul>
<h4 id="2-深度学习">.2. 深度学习</h4>
<h5 id="1-rnnlm">.1. RNNLM</h5>
<blockquote>
<p>基于RNN语言模型[1]的方法，将<code>诗歌的整体内容，作为训练语料</code>送给RNN语言模型进行训练。训练完成后，<code>先给定一些初始内容</code>，然后就可以按照语言模型<code>输出的概率分布进行采样得到下一个词</code>，不断重复这个过程就产生完整的诗歌。</p>
</blockquote>
<h5 id="2-rnnpg">.2. RNNPG</h5>
<blockquote>
<p>基于RNN语言模型[2]的方法，将诗歌的整体内容，作为训练语料送给RNN语言模型进行训练。训练完成后，先给定一些初始内容，然后就可以按照语言模型输出的概率分布进行采样得到下一个词，不断重复这个过程就产生完整的诗歌。</p>
</blockquote>
<ul>
<li><strong>Convolutional Sentence Model（CSM）</strong>：CNN模型，用于获取一句话的<code>向量表示</code>。</li>
<li><strong>Recurrent Context Model (RCM)</strong>：<code>句子级别的RNN</code>，根据历史生成句子的向量，输出下一个要生成句子的Context向量。</li>
<li><strong>Recurrent Generation Model (RGM)</strong>：<code>字符级别RNN</code>，根据RCM输出的Context向量和该句之前已经生成的字符，输出下一个字符的概率分布。解码的时候根据RGM模型输出的概率和语言模型概率加权以后，生成下一句诗歌，由人工规则保证押韵。</li>
</ul>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210523103709542.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210523103709542.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210523103709542.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210523103709542.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210523103709542.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210523103709542.png"/></p>
<h5 id="3-attention-base-model">.3. Attention-base model</h5>
<blockquote>
<p>模型[3]是<code>基于attention的encoder-decoder框架</code>，将<code>历史已经生成的内容作为源语言</code>，将下一句话作为目标语言进行翻译。需要用户提供第一句话，然后由第一句生成第二句，第一，二句生成第三句，并不断重复这个过程，直到生成完整诗歌。基于Attention机制配合LSTM，可以学习更长的诗歌，同时在一定程度上，可以保证前后语义的连贯性。</p>
</blockquote>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210523103833003.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210523103833003.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210523103833003.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210523103833003.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210523103833003.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210523103833003.png"/></p>
<h5 id="4-planning-based-neural-network-pg">.4. Planning based Neural Network PG</h5>
<blockquote>
<p>模型[5]不需要专家知识，是一个<code>端到端的模型</code>。它试图模仿人类开始写作前，先规划一个写作大纲的过程。整个诗歌生成框架由两部分组成：<code>规划模型和生成模型</code>。</p>
<ul>
<li><strong>规划模型</strong>：将代表用户写作意图的Query作为输入，生成一个写作大纲。<code>写作大纲是一个由主题词组成的序列</code>，第i个主题词代表第i句的主题。</li>
<li><strong>生成模型</strong>：基于encoder-decoder框架。有两个encoder,<code>其中一个encoder将主题词作为输入</code>，<code>另外一个encoder将历史生成的句子拼在一起作为输入</code>，<code>由decoder生成下一句话</code>。decoder生成的时候，利用Attention机制，对主题词和历史生成内容的向量一起做打分，由模型来决定生成的过程中各部分的重要性。</li>
</ul>
</blockquote>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210523104102116.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210523104102116.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210523104102116.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210523104102116.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210523104102116.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210523104102116.png"/></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210523104130020.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210523104130020.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210523104130020.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210523104130020.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210523104130020.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210523104130020.png"/></p>
<h5 id="5-rnn-with-iterative-polishing-shema">.5. RNN with Iterative Polishing Shema</h5>
<blockquote>
<p>模型[4]基于encoder-decoder框架。<code>encoder阶段，用户提供一个Query作为自己的写作意图,由CNN模型获取Query的向量表示</code>。decoder阶段，使用了<code>hierarchical的RNN生成框架，由句子级别和词级别两个RNN组成。</code></p>
<ul>
<li><strong>句子级别RNN</strong>：输入句子向量表示，<code>输出下一个句子的Context向量</code>。</li>
<li><strong>字符级别RNN</strong>：<code>输入Context向量和历史生成字符</code>，输出<code>下一个字符的概率分布</code>。当一句生成结束的时候，字符级别RNN的最后一个向量，作为表示这个句子的向量，送给句子级别RNN。</li>
</ul>
</blockquote>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210523104417394.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210523104417394.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210523104417394.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210523104417394.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210523104417394.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210523104417394.png"/></p>
<h5 id="6-generating-topical-poetry">.6. Generating Topical Poetry</h5>
<blockquote>
<p>模型[6]基于encoder-decoder框架，分为两步。先根据<code>用户输入的关键词得到每句话的最后一个词</code>，这些词都押韵且与用户输入相关。<code>再将这些押韵词作为一个序列，送给encoder,由decoder生成整个诗歌</code>。这种机制一方面保证了押韵，另外一方面，和之前提到的规划模型类似，在一定程度上避免了主题漂移问题。</p>
</blockquote>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210523104620302.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210523104620302.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210523104620302.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210523104620302.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210523104620302.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210523104620302.png"/></p>
<h5 id="7-seqgan">.7. SeqGAN</h5>
<blockquote>
<p>模型[7]将图像中的<code>对抗生成网络</code>，用到文本生成上。<code>生成网络是一个RNN，直接生成整首诗歌</code>。而<code>判别网络是一个CNN。用于判断这首诗歌是人写的</code>，还是机器生成的，并通过强化学习的方式，将梯度回传给生成网络。</p>
</blockquote>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210523104730231.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210523104730231.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210523104730231.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210523104730231.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210523104730231.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210523104730231.png"/></p>
<h5 id="8-gpt2-chinesehttpsgithubcommorizeyaogpt2-chinese">.8. <a href="https://github.com/Morizeyao/GPT2-Chinese"target="_blank" rel="external nofollow noopener noreferrer">GPT2-Chinese**<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></h5>
<blockquote>
<p>中文的GPT2训练代码，使用BERT的Tokenizer。可以写诗，新闻，小说，或是训练通用语言模型。支持字为单位或是分词模式。支持大语料训练。</p>
</blockquote>
<h3 id="2-九歌团队httpsgithubcomthunlp-aipoet">2. <a href="https://github.com/THUNLP-AIPoet/"target="_blank" rel="external nofollow noopener noreferrer">九歌团队<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></h3>
<h4 id="1-介绍">.1. 介绍</h4>
<blockquote>
<p>“九歌”是清华大学自然语言处理与社会人文计算实验室（THUNLP）在负责人<code>孙茂松教授</code>带领下研发的中文诗歌自动生成系统。作为目前最好的中文诗歌生成系统之一，“九歌”曾于2017年登上央视一套大型科技类挑战节目《机智过人》第一季的舞台，与当代优秀青年诗人同台竞技比拼诗词创作。2017年上线至今，“九歌”已累计为用户创作超过1000万首诗词，并荣获全国计算语言学学术会议最佳系统展示奖(2017，2019)和最佳论文奖(2018)。</p>
</blockquote>
<h4 id="3-开源模型">.3. 开源模型</h4>
<ul>
<li>WMPoetry</li>
</ul>
<blockquote>
<p>基于Memory Network的诗歌生成模型。该模型支持多关键词输入，并将中文古典诗歌的格律拆解为字级别的格式embeding，能够较好地控制生成诗歌的格律和韵脚，并提升诗歌的上下文关联性和扣题程度。相关论文发表于IJCAI 2018。</p>
</blockquote>
<ul>
<li>StylisticPoetry</li>
</ul>
<blockquote>
<p>基于互信息解耦的无监督风格诗歌生成模型。该模型无需任何标注数据，能够自动将生成的诗歌划分为用户指定的任意数量个不同风格。 相关论文发表于EMNLP 2018。</p>
</blockquote>
<ul>
<li>MixPoet</li>
</ul>
<blockquote>
<p>基于对抗因素混合的半监督风格诗歌生成模型。该模型利用少量标注数据，通过组合不同的影响因素，创造出多种可控的诗歌风格。相关论文发表于AAAI 2020。</p>
</blockquote>
<ul>
<li>预训练资源BERT-CCPoem</li>
</ul>
<blockquote>
<p>AIPoet基于超过90万首古诗文训练的BERT模型，该模型能提供任何一首古典诗词的任何一个句子的向量表示，可广泛应用于古典诗词智能检索与推荐、风格分析及情感计算等诸多下游任务。</p>
</blockquote>
<h4 id="4-开源数据集">.4. 开源数据集</h4>
<ul>
<li>中文古典诗歌数据集THU-CCPC：包含约13万首中文绝句(已划分训练、测试、开发集)，可用于相关模型的训练。</li>
<li>中文格律及韵律数据集THU-CRRD：包含整理好的平声字表、仄声字表以及平水韵表，可用于诗歌生成以及诗歌自动分析研究。</li>
<li>中文诗歌细粒度情感标注语料THU-FSPC：包含5,000首人工标注的绝句，每首诗包含诗歌整体以及每一句的情感标签。可用于训练情感可控的诗歌生成模型，以及进行诗歌情感自动分析。</li>
<li>中文诗歌质量标注数据集THU-PQED：包含173首古人诗作，每一首诗附有诗歌质量不同侧面(如通顺性、上下文连贯性等)的人工评分。可用于诗歌评价指标分析和研究。</li>
</ul>
<blockquote>
<p>• 数据集共分为训练集、验证集及测试集三部分。</p>
<p>• 训练集和验证集每行均代表一首完整的古诗，体裁为七言绝句（每句7 字，一共4 句）。</p>
<p>• 测试集中的每行为一个样本，只有古诗的第一句话，要求模型能以古诗的所给的第一句为输入来生成剩余的三句。</p>
</blockquote>
<h3 id="3-相关案例">3. 相关案例</h3>
<ul>
<li>animalize / QuanTangshi <em>离线全唐诗 Android</em></li>
<li>justdark / pytorch-poetry-gen <em>a char-RNN based on pytorch</em></li>
<li>Clover27 / ancient-Chinese-poem-generator <em>Ancient-Chinese-Poem-Generator</em></li>
<li>chinese-poetry / poetry-calendar <em>诗词周历</em></li>
<li>chenyuntc / pytorch-book <em>简体唐诗生成(char-RNN), 可生成藏头诗,自定义诗歌意境,前缀等</em></li>
<li>okcy1016 / poetry-desktop <em>诗词桌面</em></li>
<li>huangjianke / weapp-poem <em>诗词墨客 小程序版</em></li>
<li><a href="https://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650408997&amp;idx=1&amp;sn=93395c083d85cf15490cf36cb5251a0f&amp;scene=21#wechat_redirect"target="_blank" rel="external nofollow noopener noreferrer">风云三尺剑，花鸟一床书&mdash;对联数据集和自动对联机器人<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></li>
<li><a href="https://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650409170&amp;idx=1&amp;sn=852dba6972fd26e91f0c2fff9f458a4a&amp;scene=21#wechat_redirect"target="_blank" rel="external nofollow noopener noreferrer">自动对联活动获奖结果以及机器对联赏析<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></li>
<li><a href="https://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&amp;mid=2650410297&amp;idx=1&amp;sn=cda7099455083fbd412d0fdcb41acbea&amp;scene=21#wechat_redirect"target="_blank" rel="external nofollow noopener noreferrer">&ldquo;自动作诗机&quot;上线，代码和数据都是公开的<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></li>
</ul>
<h3 id="4-代码阅读">4. 代码阅读</h3>
<h4 id="1-tfversionrnn">.1. TfversionRNN</h4>
<ul>
<li>poetry.py</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> numpy <span style="color:#66d9ef">as</span> np
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">Poetry</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self):
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>poetry_file <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;poetry.txt&#39;</span>   <span style="color:#75715e">#存储诗词文件</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>poetry_list <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>_get_poetry()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>poetry_vectors, self<span style="color:#f92672">.</span>word_to_int, self<span style="color:#f92672">.</span>int_to_word <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>_gen_poetry_vectors()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>batch_size <span style="color:#f92672">=</span> <span style="color:#ae81ff">64</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>chunk_size <span style="color:#f92672">=</span> len(self<span style="color:#f92672">.</span>poetry_vectors) <span style="color:#f92672">//</span> self<span style="color:#f92672">.</span>batch_size
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">_get_poetry</span>(self):
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">with</span> open(self<span style="color:#f92672">.</span>poetry_file, <span style="color:#e6db74">&#34;r&#34;</span>, encoding<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;utf-8&#39;</span>) <span style="color:#66d9ef">as</span> f:
</span></span><span style="display:flex;"><span>            poetry_list <span style="color:#f92672">=</span> [line <span style="color:#66d9ef">for</span> line <span style="color:#f92672">in</span> f]
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> poetry_list
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">_gen_poetry_vectors</span>(self):
</span></span><span style="display:flex;"><span>        words <span style="color:#f92672">=</span> sorted(set(<span style="color:#e6db74">&#39;&#39;</span><span style="color:#f92672">.</span>join(self<span style="color:#f92672">.</span>poetry_list)<span style="color:#f92672">+</span><span style="color:#e6db74">&#39; &#39;</span>))  <span style="color:#75715e">#所有的words集合</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># 每一个字符分配一个索引 为后续诗词向量化做准备</span>
</span></span><span style="display:flex;"><span>        int_to_word <span style="color:#f92672">=</span> {i: word <span style="color:#66d9ef">for</span> i, word <span style="color:#f92672">in</span> enumerate(words)}
</span></span><span style="display:flex;"><span>        word_to_int <span style="color:#f92672">=</span> {v: k <span style="color:#66d9ef">for</span> k, v <span style="color:#f92672">in</span> int_to_word<span style="color:#f92672">.</span>items()}
</span></span><span style="display:flex;"><span>        to_int <span style="color:#f92672">=</span> <span style="color:#66d9ef">lambda</span> word: word_to_int<span style="color:#f92672">.</span>get(word)
</span></span><span style="display:flex;"><span>        poetry_vectors <span style="color:#f92672">=</span> [list(map(to_int, poetry)) <span style="color:#66d9ef">for</span> poetry <span style="color:#f92672">in</span> self<span style="color:#f92672">.</span>poetry_list]
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> poetry_vectors, word_to_int, int_to_word
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">batch</span>(self):
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># 生成器</span>
</span></span><span style="display:flex;"><span>        start <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>        end <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>batch_size
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> _ <span style="color:#f92672">in</span> range(self<span style="color:#f92672">.</span>chunk_size):
</span></span><span style="display:flex;"><span>            batches <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>poetry_vectors[start:end]
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># 输入数据 按每块数据中诗句最大长度初始化数组，缺失数据补全</span>
</span></span><span style="display:flex;"><span>            x_batch <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>full((self<span style="color:#f92672">.</span>batch_size, max(map(len, batches))), self<span style="color:#f92672">.</span>word_to_int[<span style="color:#e6db74">&#39; &#39;</span>], np<span style="color:#f92672">.</span>int32)
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">for</span> row <span style="color:#f92672">in</span> range(self<span style="color:#f92672">.</span>batch_size): x_batch[row, :len(batches[row])] <span style="color:#f92672">=</span> batches[row]
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># 标签数据 根据上一个字符预测下一个字符 所以这里y_batch数据应为x_batch数据向后移一位</span>
</span></span><span style="display:flex;"><span>            y_batch <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>copy(x_batch)
</span></span><span style="display:flex;"><span>            y_batch[:, :<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>], y_batch[:, <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>] <span style="color:#f92672">=</span> x_batch[:, <span style="color:#ae81ff">1</span>:], x_batch[:, <span style="color:#ae81ff">0</span>]
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">yield</span> x_batch, y_batch
</span></span><span style="display:flex;"><span>            start <span style="color:#f92672">+=</span> self<span style="color:#f92672">.</span>batch_size
</span></span><span style="display:flex;"><span>            end <span style="color:#f92672">+=</span> self<span style="color:#f92672">.</span>batch_size
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">if</span> __name__ <span style="color:#f92672">==</span> <span style="color:#e6db74">&#39;__main__&#39;</span>:
</span></span><span style="display:flex;"><span>    data <span style="color:#f92672">=</span> Poetry()<span style="color:#f92672">.</span>batch()
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> x, y <span style="color:#f92672">in</span> data:
</span></span><span style="display:flex;"><span>        print(x)
</span></span></code></pre></div><ul>
<li>model</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> os
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> datetime
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> random
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> tensorflow.compat.v1 <span style="color:#66d9ef">as</span> tf
</span></span><span style="display:flex;"><span>tf<span style="color:#f92672">.</span>disable_v2_behavior()
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> numpy <span style="color:#66d9ef">as</span> np
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> poetry <span style="color:#f92672">import</span> Poetry
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">PoetryModel</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self):
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># 诗歌生成</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>poetry <span style="color:#f92672">=</span> Poetry()
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># 单个cell训练序列个数</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>batch_size <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>poetry<span style="color:#f92672">.</span>batch_size
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># 所有出现字符的数量</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>word_len <span style="color:#f92672">=</span> len(self<span style="color:#f92672">.</span>poetry<span style="color:#f92672">.</span>word_to_int)
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># 隐层的数量</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>rnn_size <span style="color:#f92672">=</span> <span style="color:#ae81ff">128</span>
</span></span><span style="display:flex;"><span>    <span style="color:#a6e22e">@staticmethod</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">embedding_variable</span>(inputs, rnn_size, word_len):
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">with</span> tf<span style="color:#f92672">.</span>variable_scope(<span style="color:#e6db74">&#39;embedding&#39;</span>):
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># 这里选择使用cpu进行embedding</span>
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">with</span> tf<span style="color:#f92672">.</span>device(<span style="color:#e6db74">&#34;/cpu:0&#34;</span>):
</span></span><span style="display:flex;"><span>                <span style="color:#75715e"># 默认使用&#39;glorot_uniform_initializer&#39;初始化，来自源码说明:</span>
</span></span><span style="display:flex;"><span>                <span style="color:#75715e"># If initializer is `None` (the default), the default initializer passed in</span>
</span></span><span style="display:flex;"><span>                <span style="color:#75715e"># the variable scope will be used. If that one is `None` too, a</span>
</span></span><span style="display:flex;"><span>                <span style="color:#75715e"># `glorot_uniform_initializer` will be used.</span>
</span></span><span style="display:flex;"><span>                <span style="color:#75715e"># 这里实际上是根据字符数量分别生成state_size长度的向量</span>
</span></span><span style="display:flex;"><span>                embedding <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>get_variable(<span style="color:#e6db74">&#39;embedding&#39;</span>, [word_len, rnn_size])
</span></span><span style="display:flex;"><span>                <span style="color:#75715e"># 根据inputs序列中每一个字符对应索引 在embedding中寻找对应向量,即字符转为连续向量:[字]==&gt;[1]==&gt;[0,1,0]</span>
</span></span><span style="display:flex;"><span>                lstm_inputs <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>embedding_lookup(embedding, inputs)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> lstm_inputs
</span></span><span style="display:flex;"><span>    <span style="color:#a6e22e">@staticmethod</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">soft_max_variable</span>(rnn_size, word_len):
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># 共享变量</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">with</span> tf<span style="color:#f92672">.</span>variable_scope(<span style="color:#e6db74">&#39;soft_max&#39;</span>):
</span></span><span style="display:flex;"><span>            w <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>get_variable(<span style="color:#e6db74">&#34;w&#34;</span>, [rnn_size, word_len])
</span></span><span style="display:flex;"><span>            b <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>get_variable(<span style="color:#e6db74">&#34;b&#34;</span>, [word_len])
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> w, b
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">rnn_graph</span>(self, batch_size, rnn_size, word_len, lstm_inputs, keep_prob):
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># cell.state_size ==&gt; 128</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># 基础cell 也可以选择其他基本cell类型</span>
</span></span><span style="display:flex;"><span>        lstm <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>rnn_cell<span style="color:#f92672">.</span>BasicLSTMCell(num_units<span style="color:#f92672">=</span>rnn_size)
</span></span><span style="display:flex;"><span>        drop <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>rnn_cell<span style="color:#f92672">.</span>DropoutWrapper(lstm, output_keep_prob<span style="color:#f92672">=</span>keep_prob)
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># 多层cell 前一层cell作为后一层cell的输入</span>
</span></span><span style="display:flex;"><span>        cell <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>rnn_cell<span style="color:#f92672">.</span>MultiRNNCell([drop] <span style="color:#f92672">*</span> <span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># 初始状态生成(h0) 默认为0</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># initial_state.shape ==&gt; (64, 128)</span>
</span></span><span style="display:flex;"><span>        initial_state <span style="color:#f92672">=</span> cell<span style="color:#f92672">.</span>zero_state(batch_size, tf<span style="color:#f92672">.</span>float32)
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># 使用dynamic_rnn自动进行时间维度推进 且 可以使用不同长度的时间维度</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># 因为我们使用的句子长度不一致</span>
</span></span><span style="display:flex;"><span>        lstm_outputs, final_state <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>dynamic_rnn(cell, lstm_inputs, initial_state<span style="color:#f92672">=</span>initial_state)
</span></span><span style="display:flex;"><span>        seq_output <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>concat(lstm_outputs, <span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>reshape(seq_output, [<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, rnn_size])
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># softmax计算概率</span>
</span></span><span style="display:flex;"><span>        w, b <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>soft_max_variable(rnn_size, word_len)
</span></span><span style="display:flex;"><span>        logits <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>matmul(x, w) <span style="color:#f92672">+</span> b
</span></span><span style="display:flex;"><span>        prediction <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>softmax(logits, name<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;predictions&#39;</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> logits, prediction, initial_state, final_state
</span></span><span style="display:flex;"><span>    <span style="color:#a6e22e">@staticmethod</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">loss_graph</span>(word_len, targets, logits):
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># 将y序列按序列值转为one_hot向量</span>
</span></span><span style="display:flex;"><span>        y_one_hot <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>one_hot(targets, word_len)
</span></span><span style="display:flex;"><span>        y_reshaped <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>reshape(y_one_hot, [<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, word_len])
</span></span><span style="display:flex;"><span>        loss <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>reduce_mean(tf<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>softmax_cross_entropy_with_logits(logits<span style="color:#f92672">=</span>logits, labels<span style="color:#f92672">=</span>y_reshaped))
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> loss
</span></span><span style="display:flex;"><span>    <span style="color:#a6e22e">@staticmethod</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">optimizer_graph</span>(loss, learning_rate):
</span></span><span style="display:flex;"><span>        grad_clip <span style="color:#f92672">=</span> <span style="color:#ae81ff">5</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># 使用clipping gradients</span>
</span></span><span style="display:flex;"><span>        tvars <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>trainable_variables()
</span></span><span style="display:flex;"><span>        grads, _ <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>clip_by_global_norm(tf<span style="color:#f92672">.</span>gradients(loss, tvars), grad_clip)
</span></span><span style="display:flex;"><span>        train_op <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>train<span style="color:#f92672">.</span>AdamOptimizer(learning_rate)
</span></span><span style="display:flex;"><span>        optimizer <span style="color:#f92672">=</span> train_op<span style="color:#f92672">.</span>apply_gradients(zip(grads, tvars))
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> optimizer
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">train</span>(self, epoch):
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># 输入句子长短不一致 用None自适应</span>
</span></span><span style="display:flex;"><span>        inputs <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>placeholder(tf<span style="color:#f92672">.</span>int32, shape<span style="color:#f92672">=</span>(self<span style="color:#f92672">.</span>batch_size, <span style="color:#66d9ef">None</span>), name<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;inputs&#39;</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># 输出为预测某个字后续字符 故输出也不一致</span>
</span></span><span style="display:flex;"><span>        targets <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>placeholder(tf<span style="color:#f92672">.</span>int32, shape<span style="color:#f92672">=</span>(self<span style="color:#f92672">.</span>batch_size, <span style="color:#66d9ef">None</span>), name<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;targets&#39;</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># 防止过拟合</span>
</span></span><span style="display:flex;"><span>        keep_prob <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>placeholder(tf<span style="color:#f92672">.</span>float32, name<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;keep_prob&#39;</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># 将输入字符对应索引转化为变量</span>
</span></span><span style="display:flex;"><span>        lstm_inputs <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>embedding_variable(inputs, self<span style="color:#f92672">.</span>rnn_size, self<span style="color:#f92672">.</span>word_len)
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># rnn模型</span>
</span></span><span style="display:flex;"><span>        logits, _, initial_state, final_state <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>rnn_graph(self<span style="color:#f92672">.</span>batch_size, self<span style="color:#f92672">.</span>rnn_size, self<span style="color:#f92672">.</span>word_len, lstm_inputs, keep_prob)
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># 损失</span>
</span></span><span style="display:flex;"><span>        loss <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>loss_graph(self<span style="color:#f92672">.</span>word_len, targets, logits)
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># 优化</span>
</span></span><span style="display:flex;"><span>        learning_rate <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>Variable(<span style="color:#ae81ff">0.0</span>, trainable<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>        optimizer <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>optimizer_graph(loss, learning_rate)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># 开始训练</span>
</span></span><span style="display:flex;"><span>        saver <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>train<span style="color:#f92672">.</span>Saver()
</span></span><span style="display:flex;"><span>        sess <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>Session()
</span></span><span style="display:flex;"><span>        sess<span style="color:#f92672">.</span>run(tf<span style="color:#f92672">.</span>global_variables_initializer())
</span></span><span style="display:flex;"><span>        step <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>        new_state <span style="color:#f92672">=</span> sess<span style="color:#f92672">.</span>run(initial_state)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(epoch):
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># 训练数据生成器</span>
</span></span><span style="display:flex;"><span>            batches <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>poetry<span style="color:#f92672">.</span>batch()
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># 随模型进行训练 降低学习率</span>
</span></span><span style="display:flex;"><span>            sess<span style="color:#f92672">.</span>run(tf<span style="color:#f92672">.</span>assign(learning_rate, <span style="color:#ae81ff">0.001</span> <span style="color:#f92672">*</span> (<span style="color:#ae81ff">0.97</span> <span style="color:#f92672">**</span> i)))
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">for</span> batch_x, batch_y <span style="color:#f92672">in</span> batches:
</span></span><span style="display:flex;"><span>                feed <span style="color:#f92672">=</span> {inputs: batch_x, targets: batch_y, initial_state: new_state, keep_prob: <span style="color:#ae81ff">0.5</span>}
</span></span><span style="display:flex;"><span>                batch_loss, _, new_state <span style="color:#f92672">=</span> sess<span style="color:#f92672">.</span>run([loss, optimizer, final_state], feed_dict<span style="color:#f92672">=</span>feed)
</span></span><span style="display:flex;"><span>                print(datetime<span style="color:#f92672">.</span>datetime<span style="color:#f92672">.</span>now()<span style="color:#f92672">.</span>strftime(<span style="color:#e6db74">&#39;</span><span style="color:#e6db74">%c</span><span style="color:#e6db74">&#39;</span>), <span style="color:#e6db74">&#39; i:&#39;</span>, i, <span style="color:#e6db74">&#39;step:&#39;</span>, step, <span style="color:#e6db74">&#39; batch_loss:&#39;</span>, batch_loss)
</span></span><span style="display:flex;"><span>                step <span style="color:#f92672">+=</span> <span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>        model_path <span style="color:#f92672">=</span> os<span style="color:#f92672">.</span>getcwd() <span style="color:#f92672">+</span> os<span style="color:#f92672">.</span>sep <span style="color:#f92672">+</span> <span style="color:#e6db74">&#34;poetry.model&#34;</span>
</span></span><span style="display:flex;"><span>        saver<span style="color:#f92672">.</span>save(sess, model_path, global_step<span style="color:#f92672">=</span>step)
</span></span><span style="display:flex;"><span>        sess<span style="color:#f92672">.</span>close()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">gen</span>(self, poem_len):
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">to_word</span>(weights):
</span></span><span style="display:flex;"><span>            t <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>cumsum(weights)
</span></span><span style="display:flex;"><span>            s <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>sum(weights)
</span></span><span style="display:flex;"><span>            sample <span style="color:#f92672">=</span> int(np<span style="color:#f92672">.</span>searchsorted(t, np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>rand(<span style="color:#ae81ff">1</span>) <span style="color:#f92672">*</span> s))
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">return</span> self<span style="color:#f92672">.</span>poetry<span style="color:#f92672">.</span>int_to_word[sample]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># 输入</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># 句子长短不一致 用None自适应</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>batch_size <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>        inputs <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>placeholder(tf<span style="color:#f92672">.</span>int32, shape<span style="color:#f92672">=</span>(self<span style="color:#f92672">.</span>batch_size, <span style="color:#ae81ff">1</span>), name<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;inputs&#39;</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># 防止过拟合</span>
</span></span><span style="display:flex;"><span>        keep_prob <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>placeholder(tf<span style="color:#f92672">.</span>float32, name<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;keep_prob&#39;</span>)
</span></span><span style="display:flex;"><span>        lstm_inputs <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>embedding_variable(inputs, self<span style="color:#f92672">.</span>rnn_size, self<span style="color:#f92672">.</span>word_len)
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># rnn模型</span>
</span></span><span style="display:flex;"><span>        _, prediction, initial_state, final_state <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>rnn_graph(self<span style="color:#f92672">.</span>batch_size, self<span style="color:#f92672">.</span>rnn_size, self<span style="color:#f92672">.</span>word_len, lstm_inputs, keep_prob)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        saver <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>train<span style="color:#f92672">.</span>Saver()
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">with</span> tf<span style="color:#f92672">.</span>Session() <span style="color:#66d9ef">as</span> sess:
</span></span><span style="display:flex;"><span>            sess<span style="color:#f92672">.</span>run(tf<span style="color:#f92672">.</span>global_variables_initializer())
</span></span><span style="display:flex;"><span>            saver<span style="color:#f92672">.</span>restore(sess, tf<span style="color:#f92672">.</span>train<span style="color:#f92672">.</span>latest_checkpoint(<span style="color:#e6db74">&#39;.&#39;</span>))
</span></span><span style="display:flex;"><span>            new_state <span style="color:#f92672">=</span> sess<span style="color:#f92672">.</span>run(initial_state)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># 在所有字中随机选择一个作为开始</span>
</span></span><span style="display:flex;"><span>            x <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>zeros((<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>))
</span></span><span style="display:flex;"><span>            x[<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>] <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>poetry<span style="color:#f92672">.</span>word_to_int[self<span style="color:#f92672">.</span>poetry<span style="color:#f92672">.</span>int_to_word[random<span style="color:#f92672">.</span>randint(<span style="color:#ae81ff">1</span>, self<span style="color:#f92672">.</span>word_len<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>)]]
</span></span><span style="display:flex;"><span>            feed <span style="color:#f92672">=</span> {inputs: x, initial_state: new_state, keep_prob: <span style="color:#ae81ff">1</span>}
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            predict, new_state <span style="color:#f92672">=</span> sess<span style="color:#f92672">.</span>run([prediction, final_state], feed_dict<span style="color:#f92672">=</span>feed)
</span></span><span style="display:flex;"><span>            word <span style="color:#f92672">=</span> to_word(predict)
</span></span><span style="display:flex;"><span>            poem <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;&#39;</span>
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">while</span> len(poem) <span style="color:#f92672">&lt;</span> poem_len:
</span></span><span style="display:flex;"><span>                poem <span style="color:#f92672">+=</span> word
</span></span><span style="display:flex;"><span>                x <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>zeros((<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>))
</span></span><span style="display:flex;"><span>                x[<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>] <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>poetry<span style="color:#f92672">.</span>word_to_int[word]
</span></span><span style="display:flex;"><span>                feed <span style="color:#f92672">=</span> {inputs: x, initial_state: new_state, keep_prob: <span style="color:#ae81ff">1</span>}
</span></span><span style="display:flex;"><span>                predict, new_state <span style="color:#f92672">=</span> sess<span style="color:#f92672">.</span>run([prediction, final_state], feed_dict<span style="color:#f92672">=</span>feed)
</span></span><span style="display:flex;"><span>                word <span style="color:#f92672">=</span> to_word(predict)
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">return</span> poem
</span></span><span style="display:flex;"><span><span style="color:#75715e">#train&amp;Generate</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> poetry_model <span style="color:#f92672">import</span> PoetryModel
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">if</span> __name__ <span style="color:#f92672">==</span> <span style="color:#e6db74">&#39;__main__&#39;</span>:
</span></span><span style="display:flex;"><span>    poetry <span style="color:#f92672">=</span> PoetryModel()
</span></span><span style="display:flex;"><span>    poetry<span style="color:#f92672">.</span>train(epoch<span style="color:#f92672">=</span><span style="color:#ae81ff">20</span>)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    poetry <span style="color:#f92672">=</span> PoetryModel()
</span></span><span style="display:flex;"><span>    poem <span style="color:#f92672">=</span> poetry<span style="color:#f92672">.</span>gen(poem_len<span style="color:#f92672">=</span><span style="color:#ae81ff">100</span>)
</span></span><span style="display:flex;"><span>    print(poem)
</span></span></code></pre></div><h4 id="2-peoms_generator-kerashttpsgithubcomyouyuge34poems_generator_keras-178">.2. <a href="https://github.com/youyuge34/Poems_generator_Keras"target="_blank" rel="external nofollow noopener noreferrer">Peoms_generator keras<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a> 178*</h4>
<h4 id="3-gpt2-chinesehttpsgithubcommorizeyaogpt2-chineseblobmastergeneratepy-start-4k">.3. <a href="https://github.com/Morizeyao/GPT2-Chinese/blob/master/generate.py"target="_blank" rel="external nofollow noopener noreferrer">GPT2-Chinese<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a> start 4k</h4>
<h3 id="resource">Resource</h3>
<p>[1] <a href="https://link.zhihu.com/?target=https%3A//pdfs.semanticscholar.org/47a8/7c2cbdd928bb081974d308b3d9cf678d257e.pdf"target="_blank" rel="external nofollow noopener noreferrer">Recurrent neural network based language model<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>
[2] <a href="https://link.zhihu.com/?target=http%3A//www.aclweb.org/anthology/D14-1074"target="_blank" rel="external nofollow noopener noreferrer">Chinese Poetry Generation with Recurrent Neural Networks<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>
[3] <a href="https://link.zhihu.com/?target=http%3A//%5B1604.06274%5D%20Chinese%20Song%20Iambics%20Generation%20with%20Neural%20Attention-based%20Model"target="_blank" rel="external nofollow noopener noreferrer">Chinese Song Iambics Generation with Neural Attention-based Model<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>
[4] <a href="https://link.zhihu.com/?target=https%3A//www.ijcai.org/Proceedings/16/Papers/319.pdf"target="_blank" rel="external nofollow noopener noreferrer">i, Poet: Automatic Poetry Composition through Recurrent Neural Networks with Iterative Polishing Schema<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>
[5] <a href="https://link.zhihu.com/?target=http%3A//%5B1610.09889%5D%20Chinese%20Poetry%20Generation%20with%20Planning%20based%20Neural%20Network"target="_blank" rel="external nofollow noopener noreferrer">Chinese Poetry Generation with Planning based Neural Network<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>
[6] <a href="https://link.zhihu.com/?target=http%3A//xingshi.me/data/pdf/EMNLP2016poem-slides.pdf"target="_blank" rel="external nofollow noopener noreferrer">Generating Topical Poetry<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>
[7] <a href="https://link.zhihu.com/?target=http%3A//Sequence%20Generative%20Adversarial%20Nets%20with%20Policy%20Gradient"target="_blank" rel="external nofollow noopener noreferrer">SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
</div>
<div class="post-footer" id="post-footer">
  <div class="post-info">
    <div class="post-info-line">
      <div class="post-info-mod">
        <span title=2023-12-31&#32;15:45:21>更新于 2023-12-31&nbsp;</span>
      </div><div class="post-info-license">
          <span><a rel="license external nofollow noopener noreferrer" href="https://creativecommons.org/licenses/by-nc/4.0/" target="_blank">CC BY-NC 4.0</a></span>
        </div></div>
    <div class="post-info-line">
      <div class="post-info-md"><span><a href="/poetrygen/index.md" title="阅读原始文档" class="link-to-markdown">阅读原始文档</a></span><span><a href="https://liudongdong1.github.io/edit/master/content/posts%5c%e8%87%aa%e7%84%b6%e8%af%ad%e8%a8%80%5cDemo%5cPoetryGen.md" title="编辑此页"target="_blank" rel="external nofollow noopener noreferrer" class="link-to-edit">编辑此页</a></span></div>
      <div class="post-info-share">
        <span><a href="javascript:void(0);" title="分享到 Twitter" data-sharer="twitter" data-url="https://liudongdong1.github.io/poetrygen/" data-title="PoetryGen" data-hashtags="Demo,NLP"><i class="fa-brands fa-twitter fa-fw" aria-hidden="true"></i></a>
  <a href="javascript:void(0);" title="分享到 Facebook" data-sharer="facebook" data-url="https://liudongdong1.github.io/poetrygen/" data-hashtag="Demo"><i class="fa-brands fa-facebook-square fa-fw" aria-hidden="true"></i></a>
  <a href="javascript:void(0);" title="分享到 微博" data-sharer="weibo" data-url="https://liudongdong1.github.io/poetrygen/" data-title="PoetryGen" data-image="https://cdn.pixabay.com/photo/2014/12/22/19/59/macbook-577758__340.jpg"><i class="fa-brands fa-weibo fa-fw" aria-hidden="true"></i></a>
  </span>
      </div>
    </div>
  </div>

  <div class="post-info-more">
    <section class="post-tags"><i class="fa-solid fa-tags fa-fw" aria-hidden="true"></i>&nbsp;<a href="/tags/demo/">Demo</a>,&nbsp;<a href="/tags/nlp/">NLP</a></section>
    <section>
      <span><a href="javascript:void(0);" onclick="window.history.back();">返回</a></span>&nbsp;|&nbsp;<span><a href="/">主页</a></span>
    </section>
  </div>

  <div class="post-nav"><a href="/docker-operaction/" class="prev" rel="prev" title="Docker Operaction"><i class="fa-solid fa-angle-left fa-fw" aria-hidden="true"></i>Docker Operaction</a>
      <a href="/spdk-reactor%E4%BB%8B%E7%BB%8D/" class="next" rel="next" title="SPDK_Reactor 介绍">SPDK_Reactor 介绍<i class="fa-solid fa-angle-right fa-fw" aria-hidden="true"></i></a></div>
</div>
</article></main><footer class="footer">
    <div class="footer-container"><div class="footer-line powered">由 <a href="https://gohugo.io/" target="_blank" rel="external nofollow noopener noreferrer" title="Hugo 0.118.2">Hugo</a> 强力驱动 | 主题 - <a href="https://github.com/hugo-fixit/FixIt" target="_blank" rel="external" title="FixIt v0.2.17-RC"><img class="fixit-icon" src="/fixit.min.svg" alt="FixIt logo" />&nbsp;FixIt</a>
        </div><div class="footer-line copyright" itemscope itemtype="http://schema.org/CreativeWork"><i class="fa-regular fa-copyright fa-fw" aria-hidden="true"></i>
            <span itemprop="copyrightYear">2020 - 2023</span><span class="author" itemprop="copyrightHolder">
              <a href="https://liudongdong1.github.io/"target="_blank" rel="external nofollow noopener noreferrer">LiuDongdong</a></span><span class="license footer-divider"><a rel="license external nofollow noopener noreferrer" href="https://creativecommons.org/licenses/by-nc/4.0/" target="_blank">CC BY-NC 4.0</a></span></div><div class="footer-line statistics"><span class="site-time" title='网站运行中 ...'><i class="fa-solid fa-heartbeat fa-fw animate-icon" aria-hidden="true"></i>&nbsp;<span class="run-times">网站运行中 ...</span></span></div><div class="footer-line ibruce">
          <span id="busuanzi_container_site_uv" title='总访客数'><i class="fa-regular fa-user fa-fw" aria-hidden="true"></i>&nbsp;<span id="busuanzi_value_site_uv"><i class="fa-solid fa-spinner fa-spin fa-fw" aria-hidden="true"></i></span></span><span id="busuanzi_container_site_pv" class="footer-divider" title='总访问量'><i class="fa-regular fa-eye fa-fw" aria-hidden="true"></i>&nbsp;<span id="busuanzi_value_site_pv"><i class="fa-solid fa-spinner fa-spin fa-fw" aria-hidden="true"></i></span></span>
        </div></div>
  </footer></div><div class="widgets"><div class="fixed-buttons animate__faster d-none"><div class="fixed-button back-to-top" role="button" aria-label="回到顶部"><i class="fa-solid fa-arrow-up fa-fw" aria-hidden="true"></i><span class="variant-numeric">0%</span>
        </div></div><a href="https://liudongdong1.github.io/" title="在 GitHub 上查看源代码"target="_blank" rel="external nofollow" class="github-corner right d-none-mobile"><svg viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><div id="mask"></div><div class="reading-progress-bar" style="left: 0;top: 0;--bg-progress: #0076ff;--bg-progress-dark: #fff;"></div><noscript>
    <div class="noscript-warning">FixIt 主题在启用 JavaScript 的情况下效果最佳。</div>
  </noscript>
</div><link rel="stylesheet" href="/lib/katex/katex.min.css"><link rel="stylesheet" href="/lib/cookieconsent/cookieconsent.min.css"><script src="/lib/autocomplete/autocomplete.min.js" defer></script><script src="/lib/algoliasearch/algoliasearch-lite.umd.min.js" defer></script><script src="/lib/lazysizes/lazysizes.min.js" async defer></script><script src="/lib/sharer/sharer.min.js" async defer></script><script src="/lib/typeit/index.umd.js" defer></script><script src="/lib/katex/katex.min.js" defer></script><script src="/lib/katex/auto-render.min.js" defer></script><script src="/lib/katex/copy-tex.min.js" defer></script><script src="/lib/katex/mhchem.min.js" defer></script><script src="/lib/cookieconsent/cookieconsent.min.js" defer></script><script src="/lib/pangu/pangu.min.js" defer></script><script src="/lib/cell-watermark/watermark.min.js" defer></script><script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async defer></script><script>window.config={"code":{"copyTitle":"复制到剪贴板","editLockTitle":"锁定可编辑代码块","editUnLockTitle":"解锁可编辑代码块","editable":true,"maxShownLines":10},"comment":{"enable":false},"cookieconsent":{"content":{"dismiss":"同意","link":"了解更多","message":"本网站使用 Cookies 来改善您的浏览体验。"},"enable":true,"palette":{"button":{"background":"#f0f0f0"},"popup":{"background":"#1aa3ff"}},"theme":"edgeless"},"data":{"typeit-header-subtitle-desktop":"\u003cspan style='font-family: MMT,\"沐目体\";'\u003e吾日三省吾身\u003c/span\u003e","typeit-header-subtitle-mobile":"\u003cspan style='font-family: MMT,\"沐目体\";'\u003e吾日三省吾身\u003c/span\u003e"},"enablePWA":true,"enablePangu":true,"math":{"delimiters":[{"display":true,"left":"$$","right":"$$"},{"display":true,"left":"\\[","right":"\\]"},{"display":true,"left":"\\begin{equation}","right":"\\end{equation}"},{"display":true,"left":"\\begin{equation*}","right":"\\end{equation*}"},{"display":true,"left":"\\begin{align}","right":"\\end{align}"},{"display":true,"left":"\\begin{align*}","right":"\\end{align*}"},{"display":true,"left":"\\begin{alignat}","right":"\\end{alignat}"},{"display":true,"left":"\\begin{alignat*}","right":"\\end{alignat*}"},{"display":true,"left":"\\begin{gather}","right":"\\end{gather}"},{"display":true,"left":"\\begin{CD}","right":"\\end{CD}"},{"display":false,"left":"$","right":"$"},{"display":false,"left":"\\(","right":"\\)"}],"strict":false},"search":{"algoliaAppID":"2R1K9SKLQZ","algoliaIndex":"index.zh-cn","algoliaSearchKey":"4a226aa1c5c98d6859e4d1386adb2bc7","highlightTag":"em","maxResultLength":10,"noResultsFound":"没有找到结果","snippetLength":50,"type":"algolia"},"siteTime":"2020-12-18T16:15:22+08:00","typeit":{"cursorChar":"|","cursorSpeed":1000,"data":{"typeit-header-subtitle-desktop":["typeit-header-subtitle-desktop"],"typeit-header-subtitle-mobile":["typeit-header-subtitle-mobile"]},"duration":-1,"speed":100},"watermark":{"appendto":".wrapper\u003emain","colspacing":30,"content":"\u003cimg class=\"fixit-icon\" src=\"/fixit.min.svg\" alt=\"FixIt logo\" /\u003e FixIt 主题","enable":true,"fontfamily":"inherit","fontsize":0.85,"height":21,"opacity":0.0125,"rotate":15,"rowspacing":60,"width":150}};</script><script src="/js/theme.min.js" defer></script><script src="/js/custom.min.js" defer></script></body>
</html>
