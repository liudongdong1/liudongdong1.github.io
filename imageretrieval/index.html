<!doctype html><html itemscope itemtype=http://schema.org/WebPage lang=zh-CN><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=2"><meta name=robots content="noodp"><title>ImageRetrieval - DAY By DAY</title><meta name=author content="LiuDongdong"><meta name=author-link content="https://liudongdong1.github.io/"><meta name=description content="给定一个包含特定实例(例如特定目标、场景、建筑等)的查询图像，图像检索旨在从数据库图像中找到包含相同实例的图像。但由于不同图像的拍摄视角、光"><meta name=keywords content="Retrieval"><meta itemprop=name content="ImageRetrieval"><meta itemprop=description content="给定一个包含特定实例(例如特定目标、场景、建筑等)的查询图像，图像检索旨在从数据库图像中找到包含相同实例的图像。但由于不同图像的拍摄视角、光"><meta itemprop=dateModified content="2023-09-28T23:08:44+08:00"><meta itemprop=wordCount content="3111"><meta itemprop=image content="/logo.png"><meta itemprop=keywords content="Retrieval,"><meta property="og:title" content="ImageRetrieval"><meta property="og:description" content="给定一个包含特定实例(例如特定目标、场景、建筑等)的查询图像，图像检索旨在从数据库图像中找到包含相同实例的图像。但由于不同图像的拍摄视角、光"><meta property="og:type" content="article"><meta property="og:url" content="liudongdong1.github.io/imageretrieval/"><meta property="og:image" content="/logo.png"><meta property="article:section" content="posts"><meta property="article:modified_time" content="2023-09-28T23:08:44+08:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="/logo.png"><meta name=twitter:title content="ImageRetrieval"><meta name=twitter:description content="给定一个包含特定实例(例如特定目标、场景、建筑等)的查询图像，图像检索旨在从数据库图像中找到包含相同实例的图像。但由于不同图像的拍摄视角、光"><meta name=application-name content="DAY By DAY"><meta name=apple-mobile-web-app-title content="DAY By DAY"><meta name=theme-color data-light=#f8f8f8 data-dark=#252627 content="#f8f8f8"><meta name=msapplication-TileColor content="#da532c"><link rel="shortcut icon" type=image/x-icon href=/favicon.ico><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link rel=manifest href=/site.webmanifest><link rel=canonical href=liudongdong1.github.io/imageretrieval/><link rel=prev href=liudongdong1.github.io/infraredsensor/><link rel=next href=liudongdong1.github.io/ideaplugin/><link rel=stylesheet href=/liudongdong1.github.io/css/style.min.css><link rel=stylesheet href=/liudongdong1.github.io/lib/fontawesome-free/all.min.css><link rel=stylesheet href=/liudongdong1.github.io/lib/animate/animate.min.css><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"ImageRetrieval","inLanguage":"zh-CN","mainEntityOfPage":{"@type":"WebPage","@id":"liudongdong1.github.io\/imageretrieval\/"},"genre":"posts","keywords":"Retrieval","wordcount":3111,"url":"liudongdong1.github.io\/imageretrieval\/","dateModified":"2023-09-28T23:08:44+08:00","license":"This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.","publisher":{"@type":"Organization","name":"LiuDongdong","logo":"\/images\/person.png"},"author":{"@type":"Person","name":"liudongdong1"},"description":""}</script></head><body data-header-desktop=auto data-header-mobile=auto><script>(window.localStorage?.getItem("theme")?localStorage.getItem("theme")==="dark":"auto"==="auto"?window.matchMedia("(prefers-color-scheme: dark)").matches:"auto"==="dark")&&document.body.setAttribute("data-theme","dark")</script><div class=wrapper><header class="desktop animate__faster" id=header-desktop><div class=header-wrapper data-github-corner=right><div class=header-title><a href=liudongdong1.github.io/ title="DAY By DAY"><img class="lazyload logo" src=/liudongdong1.github.io/svg/loading.min.svg data-src=/fixit.min.svg data-srcset="/fixit.min.svg, /fixit.min.svg 1.5x, /fixit.min.svg 2x" data-sizes=auto alt="DAY By DAY" title="DAY By DAY"><span class=header-title-text></span></a><span id=typeit-header-subtitle-desktop class="typeit header-subtitle"></span></div><nav><ul class=menu><li class=menu-item><a class=menu-link href=/liudongdong1.github.io/posts/><i class="fa-solid fa-archive fa-fw fa-sm" aria-hidden=true></i> 所有文章</a></li><li class=menu-item><a class=menu-link href=/liudongdong1.github.io/categories/><i class="fa-solid fa-th fa-fw fa-sm" aria-hidden=true></i> 分类</a></li><li class=menu-item><a class=menu-link href=/liudongdong1.github.io/tags/><i class="fa-solid fa-tags fa-fw fa-sm" aria-hidden=true></i> 标签</a></li><li class=menu-item><a class=menu-link href=/liudongdong1.github.io/friends/ title=友情链接><i class="fa-solid fa-users fa-fw fa-sm" aria-hidden=true></i> 友链</a></li><li class=menu-item><a class=menu-link href=/liudongdong1.github.io/about/><i class="fa-solid fa-info-circle fa-fw fa-sm" aria-hidden=true></i> 关于</a></li><li class="menu-item delimiter"></li><li class="menu-item language"><span role=button aria-label=选择语言 title=选择语言>简体中文<i class="dropdown-icon fa-solid fa-chevron-down" aria-hidden=true></i></span><ul class=sub-menu><li class=menu-item>没有更多翻译</li></ul></li><li class="menu-item search" id=search-desktop><input type=text placeholder="搜索文章标题或内容 ..." id=search-input-desktop>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-desktop title=搜索><i class="fa-solid fa-search fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); class="search-button search-clear" id=search-clear-desktop title=清空><i class="fa-solid fa-times-circle fa-fw" aria-hidden=true></i></a>
<span class="search-button search-loading" id=search-loading-desktop><i class="fa-solid fa-spinner fa-fw fa-spin" aria-hidden=true></i></span></li><li class="menu-item theme-switch" title=切换主题><i class="fa-solid fa-adjust fa-fw" aria-hidden=true></i></li></ul></nav></div></header><header class="mobile animate__faster" id=header-mobile><div class=header-container><div class=header-wrapper><div class=header-title><a href=liudongdong1.github.io/ title="DAY By DAY"><img class="lazyload logo" src=/liudongdong1.github.io/svg/loading.min.svg data-src=/fixit.min.svg data-srcset="/fixit.min.svg, /fixit.min.svg 1.5x, /fixit.min.svg 2x" data-sizes=auto alt=/fixit.min.svg title=/fixit.min.svg><span class=header-title-text></span></a><span id=typeit-header-subtitle-mobile class="typeit header-subtitle"></span></div><div class=menu-toggle id=menu-toggle-mobile><span></span><span></span><span></span></div></div><nav><ul class=menu id=menu-mobile><li class=search-wrapper><div class="search mobile" id=search-mobile><input type=text placeholder="搜索文章标题或内容 ..." id=search-input-mobile>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-mobile title=搜索><i class="fa-solid fa-search fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); class="search-button search-clear" id=search-clear-mobile title=清空><i class="fa-solid fa-times-circle fa-fw" aria-hidden=true></i></a>
<span class="search-button search-loading" id=search-loading-mobile><i class="fa-solid fa-spinner fa-fw fa-spin" aria-hidden=true></i></span></div><a href=javascript:void(0); class=search-cancel id=search-cancel-mobile>取消</a></li><li class=menu-item><a class=menu-link href=/liudongdong1.github.io/posts/><i class="fa-solid fa-archive fa-fw fa-sm" aria-hidden=true></i> 所有文章</a></li><li class=menu-item><a class=menu-link href=/liudongdong1.github.io/categories/><i class="fa-solid fa-th fa-fw fa-sm" aria-hidden=true></i> 分类</a></li><li class=menu-item><a class=menu-link href=/liudongdong1.github.io/tags/><i class="fa-solid fa-tags fa-fw fa-sm" aria-hidden=true></i> 标签</a></li><li class=menu-item><a class=menu-link href=/liudongdong1.github.io/friends/ title=友情链接><i class="fa-solid fa-users fa-fw fa-sm" aria-hidden=true></i> 友链</a></li><li class=menu-item><a class=menu-link href=/liudongdong1.github.io/about/><i class="fa-solid fa-info-circle fa-fw fa-sm" aria-hidden=true></i> 关于</a></li><li class="menu-item text-center"><a class=menu-link href=https://liudongdong1.github.io/ title=GitHub rel="noopener noreferrer" target=_blank><i class='fa-brands fa-github fa-fw' aria-hidden=true></i></a></li><li class="menu-item theme-switch" title=切换主题><i class="fa-solid fa-adjust fa-fw" aria-hidden=true></i></li><li class="menu-item language"><span role=button aria-label=选择语言 title=选择语言>简体中文<i class="dropdown-icon fa-solid fa-chevron-down" aria-hidden=true></i></span>
<select class=language-select onchange="location=this.value"><option disabled>没有更多翻译</option></select></li></ul></nav></div></header><div class="search-dropdown desktop"><div id=search-dropdown-desktop></div></div><div class="search-dropdown mobile"><div id=search-dropdown-mobile></div></div><main class=container data-page-style=normal><aside class=toc id=toc-auto><h2 class=toc-title>目录 <i class="toc-icon fa-solid fa-angle-down fa-fw"></i></h2><div class=toc-content id=toc-content-auto></div></aside><aside class=aside-custom id=aside-sakana><div class=sakana-widget><div class=sakana-item id=takina-widget></div><div class=sakana-item id=chisato-widget></div></div><script>function initSakanaWidget(){const e=SakanaWidget.getCharacter("takina");SakanaWidget.registerCharacter("takina-slow",e),new SakanaWidget({character:"takina-slow",controls:!1,autoFit:!0,stroke:{color:"#b4b4b4",width:2}}).mount("#takina-widget");const t=SakanaWidget.getCharacter("chisato");SakanaWidget.registerCharacter("chisato-slow",t),new SakanaWidget({character:"chisato-slow",controls:!1,autoFit:!0,stroke:{color:"#b4b4b4",width:2}}).mount("#chisato-widget")}</script><script async onload=initSakanaWidget() src=https://cdn.jsdelivr.net/npm/sakana-widget@2.3.0/lib/sakana.min.js></script></aside><article class="page single"><div class=header><h1 class="single-title animate__animated animate__flipInX"><span>ImageRetrieval</span></h1></div><div class=post-meta><div class=post-meta-line><span class=post-author><span class=author><i class="fa-solid fa-user-circle" aria-hidden=true></i>
liudongdong1</span></span>
<span class=post-category>收录于 <a href=liudongdong1.github.io/categories/><i class="fa-regular fa-folder fa-fw"></i>&nbsp;Categories</a>&ensp;<a href=liudongdong1.github.io/categories/%E6%97%B6%E7%A9%BA%E6%95%B0%E6%8D%AE/><i class="fa-regular fa-folder fa-fw"></i>&nbsp;时空数据</a></span></div><div class=post-meta-line><span title="0001-01-01 00:00:00"><i class="fa-regular fa-calendar-alt fa-fw"></i>&nbsp;<time datetime=0001-01-01>0001-01-01</time>
</span>&nbsp;<i class="fa-solid fa-pencil-alt fa-fw"></i>&nbsp;约 3111 字&nbsp;
<i class="fa-regular fa-clock fa-fw"></i>&nbsp;预计阅读 7 分钟&nbsp;<span id=busuanzi_container_page_pv class="busuanzi_visitors comment-visitors" data-flag-title=ImageRetrieval>
<i class="fa-regular fa-eye fa-fw"></i>&nbsp;<span id=busuanzi_value_page_pv>-</span>&nbsp;次阅读
</span>&nbsp;</div></div><div class=featured-image><img class=lazyload src=/liudongdong1.github.io/svg/loading.min.svg data-src=https://cdn.stocksnap.io/img-thumbs/280h/VTRBWVOXY6.jpg data-srcset="https://cdn.stocksnap.io/img-thumbs/280h/VTRBWVOXY6.jpg, https://cdn.stocksnap.io/img-thumbs/280h/VTRBWVOXY6.jpg 1.5x, https://cdn.stocksnap.io/img-thumbs/280h/VTRBWVOXY6.jpg 2x" data-sizes=auto alt=https://cdn.stocksnap.io/img-thumbs/280h/VTRBWVOXY6.jpg title=https://cdn.stocksnap.io/img-thumbs/280h/VTRBWVOXY6.jpg></div><div class="details toc" id=toc-static kept=true><div class="details-summary toc-title"><span>目录</span>
<span><i class="details-icon fa-solid fa-angle-right"></i></span></div><div class="details-content toc-content" id=toc-content-static><nav id=TableOfContents><ul><li><a href=#paper-scene-graphs>Paper: Scene Graphs</a><ul><li></li></ul></li><li><a href=#paper-detect-to-retrieve>Paper: Detect-to-retrieve</a><ul><li></li></ul></li><li><a href=#paper-google-landmarks-dataset-v2>Paper: Google Landmarks Dataset v2</a><ul><li></li></ul></li></ul><ul><li><ul><li></li></ul></li></ul><ul><li><ul><li><a href=#1-image-matchhttpsgithubcomprovenancelabsimage-match>.1. <a href=https://github.com/ProvenanceLabs/image-match>image-match</a></a></li><li><a href=#2-eagleeyehttpsgithubcomthoughtfuldeveagleeye>.2. <strong><a href=https://github.com/ThoughtfulDev/EagleEye>EagleEye</a></strong></a></li><li><a href=#3-pyretrihttpsgithubcompyretripyretri>.3. <strong><a href=https://github.com/PyRetri/PyRetri><code>PyRetri</code></a></strong></a></li><li><a href=#4-natural-language-image-searchhttpsgithubcomhaltakovnatural-language-image-search>4. <a href=https://github.com/haltakov/natural-language-image-search>natural-language-image-search</a></a></li><li><a href=#5-sishttpsgithubcommatsui528sis>5. <a href=https://github.com/matsui528/sis>sis</a></a></li></ul></li></ul></nav></div></div><div class=content id=content><blockquote><p>给定一个包含特定实例(例如特定目标、场景、建筑等)的查询图像，图像检索旨在从<code>数据库图像</code>中找到包含<code>相同实例的图像</code>。但由于<code>不同图像的拍摄视角、光照、或遮挡情况不同</code>，如何设计出能应对这些类内差异的有效且高效的图像检索算法仍是一项研究难题。</p></blockquote><h1 id=1-survey>1. Survey</h1><blockquote><p>Chen W, Liu Y, Wang W, et al. Deep image retrieval: A survey[J]. arXiv preprint arXiv:2101.11282, 2021. <a href="chrome-extension://ikhdkkncnoglghljlkmcimlnlhkeamad/pdf-viewer/web/viewer.html?file=https%3A%2F%2Farxiv.org%2Fpdf%2F2101.11282.pdf" target=_blank rel="external nofollow noopener noreferrer">url<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden=true></i></a></p></blockquote><ul><li><strong>content based image retrieval(CBIR)</strong> is the problem of searching for semantically matched or similar image in a large image gallery by analyzing their visua, content. <code>campact yet rich feature representations</code><ul><li><code>Application era:</code> Person re-identification, remote sensing, medical image search, shopping recommendation in onlene markets;</li><li><code>instance level</code>: a query image of a particular object or scene is given and the goal is to find images containing the same object or scene that ma be captured under different conditions;</li><li><code>cactegory level:</code> find images of the same class as the query;</li><li><code>feature engineering era:</code> hand-engineered feature descriptors SIFT;</li><li><code>feature learning era: </code>AlexNet, ImageNet;</li></ul></li><li><strong>Challenges & Goal:</strong><ul><li>reducing the <code>semantic gap</code></li><li>improving retrival scalability: <code>domain shift</code></li><li>balancing<code> retrieval accuracy and efficiency</code>: high dimensional anc contain more semantic-aware information</li></ul></li></ul><p><img class=lazyload src=/liudongdong1.github.io/svg/loading.min.svg data-src=https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211006104414562.png data-srcset="https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211006104414562.png, https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211006104414562.png 1.5x, https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211006104414562.png 2x" data-sizes=auto alt=https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211006104414562.png title=https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211006104414562.png></p><p><img class=lazyload src=/liudongdong1.github.io/svg/loading.min.svg data-src=https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211006104630732.png data-srcset="https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211006104630732.png, https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211006104630732.png 1.5x, https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211006104630732.png 2x" data-sizes=auto alt=https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211006104630732.png title=https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211006104630732.png></p><p><img class=lazyload src=/liudongdong1.github.io/svg/loading.min.svg data-src=https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211006105237367.png data-srcset="https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211006105237367.png, https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211006105237367.png 1.5x, https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211006105237367.png 2x" data-sizes=auto alt=https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211006105237367.png title=https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211006105237367.png></p><ul><li><strong>single feedforward pass methods:</strong> take the whole image and feed it into model to extract features. lacks geometric invariance and spatial information.</li><li><strong>multiple feedforward pass methods:</strong> using sliding windows or spatial pyramid model to create multi-scale image patches, and each patch is fed into the model before being encodeded as a final global feature. instead of generating multi-scale image patches randomly and densely, <code>region proposal methods</code> are introduced like RPNs, CKNs.</li></ul><blockquote><p>(a)-(b) <code>Non-parametric mechanisms:</code> The attention is based on convolutional feature maps x with size H ×W ×C. <code>Channel-wise attention in (a)</code> produces a C-dimensional importance vector α1 [10], [30]. <code>Spatial-wise attention in (b) </code>computes a 2-dimensional attention map α2 [10], [28], [59], [79]. (c)-(d) <code>Parametric mechanisms</code>: The attention weights β are provided by a sub-network with trainable parameters (e.g. θ in (c)) [97], [98]. Likewise, some off-the-shelf models [91], [99] can predict the attention maps from the input image directly.</p></blockquote><p><img class=lazyload src=/liudongdong1.github.io/svg/loading.min.svg data-src=https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211006122356475.png data-srcset="https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211006122356475.png, https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211006122356475.png 1.5x, https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211006122356475.png 2x" data-sizes=auto alt="Attention mechanisms" title="Attention mechanisms"></p><blockquote><p>Representative methods in single feedforward frameworks, focusing on convolutional feature maps x with
size H ×W ×C: MAC [47], R-MAC [27], GeM pooling [41], SPoC with the Gaussian weighting scheme [7], CroW [10], and CAM+CroW [28]. Note that g1(·) and g2(·) represent spatialwise and channel-wise weighting functions, respectively.</p></blockquote><p><img class=lazyload src=/liudongdong1.github.io/svg/loading.min.svg data-src=https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211006110245921.png data-srcset="https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211006110245921.png, https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211006110245921.png 1.5x, https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211006110245921.png 2x" data-sizes=auto alt=https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211006110245921.png title=https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211006110245921.png></p><p><img class=lazyload src=/liudongdong1.github.io/svg/loading.min.svg data-src=https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211006111137946.png data-srcset="https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211006111137946.png, https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211006111137946.png 1.5x, https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211006111137946.png 2x" data-sizes=auto alt="Image patch generation" title="Image patch generation"></p><ul><li><strong>Deep feature selection:</strong><ul><li>a fully-connected layer has a global receptive field, but<code> lack of spatial information (using multiple feedforward passes)</code>; <code>lack of local geometric invariance (leverage intermediate convolutional layers)</code></li><li>a convolutional layer arranges the spatial information well and produces location-adaptive features. sum-pooling convolutional features(SPoC) to compact descriptors pre-processed with Gaussian center prior. use BoW model to embed convolutional featuers separately, use VLAD to encode local features into VLAD features.</li></ul></li><li><strong>Feature fustion strategy</strong><ul><li><code>a layer-level fusion</code>: fusing feature from different layers with different balancing weights aims at combining different feature properties within a feature extractor. <code>features from fully-connected layers(global features) and features from convolutional layers(local features) can complement each other when measuring semantic similarity and guarantee retrival performance</code>.</li><li><code>model-level fusion</code>: combine features on different models to achieve improved performance, categorized into intra-model and inter-model.<ul><li>intra-model: multiple deep models having similar or highly compatible structures.</li><li>inter-model: involves models with more differin structures.</li><li><code>early fusion</code>: straightforward to fuse all types of features from the candidate models and then learning a metric based on the concatenated feature; <code>late fusion:</code> learn optimal metrics separately for the features from each model, and then to uniformly combine these metrics for final retrieval ranking. <code>What features are the best to combined</code></li></ul></li></ul></li><li><strong>Deep feature Enhancement</strong><ul><li><code>feature aggregation:</code> sum/average pooling is less disciminative, taking into all activated outputs from conv layer, which weakening the effect of highly activated features. <code>max pooling</code>: for sparse features that have a low probability of being active. conv feature maps can be directly aggregated to produce global features by spatial pooling.</li><li><code>feature embedding:</code> embed the conv feature maps into a high dimensional space to obtain compact features, lik BoW, VLAD, FV. And using PCA to reduce embedding demision.</li><li><code>Attention Mechanisms</code>: to highlight th emost relevant features and to avoid the influence of irrelevant activations.</li><li><code>Deep hash Embedding</code>: to tranform deep features into more compact codes, hash functions can be plugged as a layer into deep networks, so that the hasn codes can be trained and optimized with model simultaneously. The hash codes of originally similar images are embedded as close as possible.<ul><li>preserving image similarity: to minimize the inconsistencies between the real-valued features and corresponding hash codes.<ul><li>class label available: loss function are designed to learn hash codes in a Hamming space. like optimize the difference between matrices computed from the binary codes and their supervision lavels. <code>Siamese loss, triplet loss, adversarial learning</code> is used to retain semantic similarity where only dissiilar pairs keep their distance within a margin.</li><li>unsupervised hash learning: using Bayes classifiers, KNN graphs, K-means algorithms, AutoEncoders, Generative adversarial networks</li></ul></li><li>improving hash function quality: aims at making the binary codes uniformly distributed.</li></ul></li></ul></li><li><strong>Supervised Fine-tuning</strong>:<ul><li>classification-based Fine-tuning: improves the model-lebel adaptability for new datasets, but may have some difficulties in learning discriminative intra-class variability to distinguish particular objects.</li><li>verification-based Fine-tuning: learn an optimal metric which minimizes or maximizes the distance of pairs to validate and maintain their similarity. Focus on both inter-class and intra-class samples.<ul><li>a pair-wise constraint, corresponding to a Siamese network, in which input images are paired with either a positive or negative sample.</li><li>a triplet constraint, associated with triplet netwroks, in which anchor images are paired with both similar and dissimilar samples.</li><li>glovally supervised approaces(c,d) learn a metric on gloval features by satisfying all constraints, locally supervised approaches focus on local areas by only satisfying the given local constriants.</li></ul></li></ul></li></ul><p><img class=lazyload src=/liudongdong1.github.io/svg/loading.min.svg data-src=https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211006124924358.png data-srcset="https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211006124924358.png, https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211006124924358.png 1.5x, https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211006124924358.png 2x" data-sizes=auto alt=https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211006124924358.png title=https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211006124924358.png></p><p><img class=lazyload src=/liudongdong1.github.io/svg/loading.min.svg data-src=https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211006125645651.png data-srcset="https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211006125645651.png, https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211006125645651.png 1.5x, https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211006125645651.png 2x" data-sizes=auto alt="sample mining strategies" title="sample mining strategies"></p><ul><li><strong>Unsupervised Fine-tuning</strong>:<ul><li><code>mining samples with Manifold learning:</code> to capture intrinsic correlations on the manifold structure to mine or deduce revelance.</li><li><code>AutoEncoder-based Frameworks</code>: to reconstruct its output as closely as possible to its input.</li></ul></li></ul><blockquote><p>First stage: the affinity matrix is interpreted as a weighted kNN graph, where each vector is represented by a node, and edges are difined by the pairwise affinities of two connected nodes. the pairwise affinities are re-evaluated in the context of all other elements by diffusing the similarity values through the graph. the difference among random walk are lie primarily in three aspects</p><ul><li>similarity initialization: affects the subsequenct KNN graph construction in an affinity matrix;</li><li>transition matrix definition: a row-stochastic matrix, determines the probabilities of transiting from one node to another in the graph.</li><li>iteration scheme: to re-valuate and update the values in affinity matrix by the manifold similarity until some kind of convergence is achieved.</li></ul></blockquote><p><img class=lazyload src=/liudongdong1.github.io/svg/loading.min.svg data-src=https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211006130746382.png data-srcset="https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211006130746382.png, https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211006130746382.png 1.5x, https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211006130746382.png 2x" data-sizes=auto alt=https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211006130746382.png title=https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211006130746382.png></p><h1 id=2-paper-reading>2. Paper Reading</h1><blockquote><p>Yoon, Sangwoong, et al. &ldquo;Image-to-Image Retrieval by Learning Similarity between Scene Graphs.&rdquo; <em>arXiv preprint arXiv:2012.14700</em> (2020).</p></blockquote><hr><h2 id=paper-scene-graphs>Paper: Scene Graphs</h2><h4 id=summary>Summary</h4><ol><li>performing image retrieval with complex images that have multiple objects and various relationships between them remains challenging:<ul><li>overly sensitive to low-level and local visual features;</li><li>no publicly available labeled data to train and evaluate the image retrieval system for complex image.</li></ul></li><li>propose IRSGS, a novel image retrieval framework that utilizes the similarty between scene graphs computed from a graph neural network to retrieve semantically similar images;</li><li>propose to train the proposed retrieval framework with the surrogate relevance measure obtained from image captions and a pre-trained language model;</li></ol><ul><li>the scene graph S={objects, attributes, relations}; the surrogate relevance measure between two images as the similarity between their captions;</li></ul><p><img class=lazyload src=/liudongdong1.github.io/svg/loading.min.svg data-src=https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210109154029944.png data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210109154029944.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210109154029944.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210109154029944.png 2x" data-sizes=auto alt=https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210109154029944.png title=https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210109154029944.png></p><h4 id=relative-work>Relative work:</h4><ul><li><strong>Image Retrieval:</strong> using visual feature representations; object categories, text descriptions;</li><li><strong>Scene Graphs</strong>: image captioning; visual question answering; image-ground dialog;</li><li><strong>Graph Similarity learning:</strong> use the learned graph representations of two graph to calculate similarity;</li></ul><p><img class=lazyload src=/liudongdong1.github.io/svg/loading.min.svg data-src=https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20210109153411093.png data-srcset="https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20210109153411093.png, https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20210109153411093.png 1.5x, https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20210109153411093.png 2x" data-sizes=auto alt=image-20210109153411093 title=image-20210109153411093></p><blockquote><p>Teichmann, Marvin, et al. &ldquo;Detect-to-retrieve: Efficient regional aggregation for image search.&rdquo; <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em>. 2019.</p></blockquote><hr><h2 id=paper-detect-to-retrieve>Paper: Detect-to-retrieve</h2><h4 id=summary-1>Summary</h4><ol><li>improving region selection: introduce a dataset of manually boxed landmark images, with 86k images from 15k unique classes;</li><li>leverage the trained detector and produce more efficient regional search systems, which improves accuracy for small objects with only a modest increase to the databases size;</li><li>propose regional aggregated match kernels to leverage selected image regions and produce a discriminative image representation.</li></ol><p><img class=lazyload src=/liudongdong1.github.io/svg/loading.min.svg data-src=https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20210110083533162.png data-srcset="https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20210110083533162.png, https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20210110083533162.png 1.5x, https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20210110083533162.png 2x" data-sizes=auto alt=https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20210110083533162.png title=https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20210110083533162.png></p><blockquote><p>Deep local features and object regions are extracted from an image. Regional aggregation proceeds in two steps, using a large codebook of visual words: first, per-region <code>VLAD description</code>; second, sum pooling and per-visual word normalization.</p></blockquote><h4 id=relatedwork>RelatedWork:</h4><ul><li>Region search and aggregation:<ul><li>regional search: selected regions are encoded independently in the database; using <code>VLAD</code> or <code>Fisher Vectors</code>;</li><li>regional aggregation: selected regions are used to improve image representations. like leverage the grid structure form to pool pretained CNN features;</li></ul></li></ul><h4 id=features>Features:</h4><ul><li><p>**Regional Search&&amp;Aggregation: ** <code>build on top of deep local features(DELF) and aggregated selective match kernels(ASMK)</code>;</p><ul><li>match kernel framework:<ul><li>Image X with M local descriptors: <img class=lazyload src=/liudongdong1.github.io/svg/loading.min.svg data-src=https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210110084612921.png data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210110084612921.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210110084612921.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210110084612921.png 2x" data-sizes=auto alt=https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210110084612921.png title=https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210110084612921.png></li><li>code book C comprising C visual words, learned using k-means, is used to quantize the descriptors;</li><li><img class=lazyload src=/liudongdong1.github.io/svg/loading.min.svg data-src=https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210110084743327.png data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210110084743327.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210110084743327.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210110084743327.png 2x" data-sizes=auto alt=https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210110084743327.png title=https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210110084743327.png></li><li>encompasses popular local feature aggregation techniques such as Bag-of-Words, VLAD, and ASMK. Similarity: <img class=lazyload src=/liudongdong1.github.io/svg/loading.min.svg data-src=https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210110084818574.png data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210110084818574.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210110084818574.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210110084818574.png 2x" data-sizes=auto alt=https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210110084818574.png title=https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210110084818574.png></li><li>an aggregated vector representation: <img class=lazyload src=/liudongdong1.github.io/svg/loading.min.svg data-src=https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210110085056545.png data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210110085056545.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210110085056545.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210110085056545.png 2x" data-sizes=auto alt=https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210110085056545.png title=https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210110085056545.png></li><li>a scalar selectivity function: $\sigma(.)$;<img class=lazyload src=/liudongdong1.github.io/svg/loading.min.svg data-src=https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210110085307986.png data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210110085307986.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210110085307986.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210110085307986.png 2x" data-sizes=auto alt=https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210110085307986.png title=https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210110085307986.png></li><li>normalization factor: <img class=lazyload src=/liudongdong1.github.io/svg/loading.min.svg data-src=https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210110085732839.png data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210110085732839.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210110085732839.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210110085732839.png 2x" data-sizes=auto alt=https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210110085732839.png title=https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210110085732839.png></li></ul></li></ul></li><li><p><strong>Regional Search:</strong> query image X && database image $Y^{(n)}$;</p><p><img class=lazyload src=/liudongdong1.github.io/svg/loading.min.svg data-src=https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210110090030382.png data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210110090030382.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210110090030382.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210110090030382.png 2x" data-sizes=auto alt=https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210110090030382.png title=https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210110090030382.png></p></li><li><p><strong>Regional Aggregated Match Kernels:</strong></p><ul><li><p>storing descriptors of each region independently in the database incurs additional cost for both memory and search computation.</p></li><li><p>utilizing the detected bounding boxes to instead improve the aggregated representations of database images&ndash;producing discriminative descriptors at no additional cost.</p><p><img class=lazyload src=/liudongdong1.github.io/svg/loading.min.svg data-src=https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210110090415987.png data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210110090415987.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210110090415987.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210110090415987.png 2x" data-sizes=auto alt=https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210110090415987.png title=https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210110090415987.png></p><p><img class=lazyload src=/liudongdong1.github.io/svg/loading.min.svg data-src=https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210110090445547.png data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210110090445547.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210110090445547.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210110090445547.png 2x" data-sizes=auto alt="For VLAD" title="For VLAD"></p><p><img class=lazyload src=/liudongdong1.github.io/svg/loading.min.svg data-src=https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210110090613560.png data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210110090613560.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210110090613560.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210110090613560.png 2x" data-sizes=auto alt=R-ASMK title=R-ASMK></p><p><img class=lazyload src=/liudongdong1.github.io/svg/loading.min.svg data-src=https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210110090643427.png data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210110090643427.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210110090643427.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210110090643427.png 2x" data-sizes=auto alt=R-AMK title=R-AMK></p></li></ul></li></ul><blockquote><p>Weyand, Tobias, et al. &ldquo;Google Landmarks Dataset v2-A Large-Scale Benchmark for Instance-Level Recognition and Retrieval.&rdquo; <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>. 2020.</p></blockquote><hr><h2 id=paper-google-landmarks-dataset-v2>Paper: Google Landmarks Dataset v2</h2><h4 id=summary-2>Summary</h4><ul><li>the new dataset has several long-tailed class distribution, a large fraction of out-of-domain test photos and large intro-class variability.<ul><li>class distribution; intra-class variation; out-of-domain query images;</li></ul></li><li>introduce the Google Landmarks Dataset v2, a new large-scale dataset for instance-level recognition and retrieval, includes over 5M images of over 200k human-made and natural landmarks.</li></ul><p><img class=lazyload src=/liudongdong1.github.io/svg/loading.min.svg data-src=https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210110093703863.png data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210110093703863.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210110093703863.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210110093703863.png 2x" data-sizes=auto alt=https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210110093703863.png title=https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210110093703863.png></p><p><img class=lazyload src=/liudongdong1.github.io/svg/loading.min.svg data-src=https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210110094638916.png data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210110094638916.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210110094638916.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210110094638916.png 2x" data-sizes=auto alt=https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210110094638916.png title=https://gitee.com/github-25970295/blogImage/raw/master/img/image-20210110094638916.png></p><blockquote><p>Caron M, Touvron H, Misra I, et al. Emerging properties in self-supervised vision transformers[J]. arXiv preprint arXiv:2104.14294, 2021.</p></blockquote><hr><h1 id=paper-vit>Paper: ViT</h1><h4 id=summary-3>Summary</h4><ol><li>self-supervised ViT features contain explicit information about the <code>semantic segmentation of an image</code>, which does not emerge as clearly with supervised ViTs, nor with convnets. These features are also excellent k-NN classifiers.</li></ol><h4 id=application-area><strong>Application Area</strong>:</h4><ul><li>Image Retrieval</li><li>Copy detection</li><li>discovering the semantic layout of scenes</li><li>video instance segmentation</li></ul><h4 id=methods>Methods</h4><ul><li><p><strong>Problem Formulation</strong>:</p></li><li><p><strong>system overview</strong>:</p></li></ul><p><img class=lazyload src=/liudongdong1.github.io/svg/loading.min.svg data-src=https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211007135531081.png data-srcset="https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211007135531081.png, https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211007135531081.png 1.5x, https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211007135531081.png 2x" data-sizes=auto alt=https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211007135531081.png title=https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211007135531081.png></p><h1 id=3-project-code>3. project Code</h1><h3 id=1-image-matchhttpsgithubcomprovenancelabsimage-match>.1. <a href=https://github.com/ProvenanceLabs/image-match target=_blank rel="external nofollow noopener noreferrer">image-match<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden=true></i></a></h3><blockquote><p>Wong H C, Bern M, Goldberg D. An image signature for any kind of image[C]//Proceedings. International Conference on Image Processing. IEEE, 2002, 1: I-I. <a href=https://github.com/dsys/match target=_blank rel="external nofollow noopener noreferrer">code<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden=true></i></a></p><ul><li>star 2.6k, 1.6k, tutorial: <a href=https://image-match.readthedocs.io/en/latest/start.html target=_blank rel="external nofollow noopener noreferrer">link<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden=true></i></a>, traditional methods, including <code>python, elesticserach</code></li><li>sensitive enough to allow efficient nearest-neighbor search, to effectively filter a database for possible duplicates, and yet robutst enough to find duplicates that hve been resized, rescanned, or lossily compressed.</li></ul></blockquote><ol><li>if the image is color, first convert it to 8-bit gray scale using the standard color-conversion algorithms, include djpeg and ppmtopgm. 255: pure white; 0: pure black;</li><li>impose a 9*9 grid of points on the image. fro each column of the image, compute teh sum of absolute values of differences between adjacent pixels, compute the total of all columns, and crop the image at 5% and 95% columns. and crop the rows of the image the same way. Then divede the croped image into 10*10 grid of blocks, and setting a 9*9 grid of points on the image.</li><li>at each grid point, compute the average gray level of the P*P square centered at the grid point.</li><li>for each grid point, compute an 8-element array whose elements give a comparision of the average gray level of the grid point square with those of its eight neighbors.</li><li>the signature of an image is simply the concatenation of the 8-element arrays corresponding to the grid points, ordered left-right, top-bottom. 9*9*8=648;</li></ol><p><img class=lazyload src=/liudongdong1.github.io/svg/loading.min.svg data-src=https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211006135153568.png data-srcset="https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211006135153568.png, https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211006135153568.png 1.5x, https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211006135153568.png 2x" data-sizes=auto alt=https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211006135153568.png title=https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211006135153568.png></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> image_match.goldberg <span style=color:#f92672>import</span> ImageSignature
</span></span><span style=display:flex><span>gis <span style=color:#f92672>=</span> ImageSignature()
</span></span><span style=display:flex><span>a <span style=color:#f92672>=</span> gis<span style=color:#f92672>.</span>generate_signature(<span style=color:#e6db74>&#39;https://upload.wikimedia.org/wikipedia/commons/thumb/e/ec/Mona_Lisa,_by_Leonardo_da_Vinci,_from_C2RMF_retouched.jpg/687px-Mona_Lisa,_by_Leonardo_da_Vinci,_from_C2RMF_retouched.jpg&#39;</span>)
</span></span><span style=display:flex><span>b <span style=color:#f92672>=</span> gis<span style=color:#f92672>.</span>generate_signature(<span style=color:#e6db74>&#39;https://pixabay.com/static/uploads/photo/2012/11/28/08/56/mona-lisa-67506_960_720.jpg&#39;</span>)
</span></span><span style=display:flex><span>gis<span style=color:#f92672>.</span>normalized_distance(a, b)  <span style=color:#75715e>#计算俩个图像之间距离</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> elasticsearch <span style=color:#f92672>import</span> Elasticsearch
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> image_match.elasticsearch_driver <span style=color:#f92672>import</span> SignatureES
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>es <span style=color:#f92672>=</span> Elasticsearch()
</span></span><span style=display:flex><span>ses <span style=color:#f92672>=</span> SignatureES(es)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>ses<span style=color:#f92672>.</span>add_image(<span style=color:#e6db74>&#39;https://upload.wikimedia.org/wikipedia/commons/thumb/e/ec/Mona_Lisa,_by_Leonardo_da_Vinci,_from_C2RMF_retouched.jpg/687px-Mona_Lisa,_by_Leonardo_da_Vinci,_from_C2RMF_retouched.jpg&#39;</span>)
</span></span><span style=display:flex><span>ses<span style=color:#f92672>.</span>add_image(<span style=color:#e6db74>&#39;https://pixabay.com/static/uploads/photo/2012/11/28/08/56/mona-lisa-67506_960_720.jpg&#39;</span>)
</span></span><span style=display:flex><span>ses<span style=color:#f92672>.</span>add_image(<span style=color:#e6db74>&#39;https://upload.wikimedia.org/wikipedia/commons/e/e0/Caravaggio_-_Cena_in_Emmaus.jpg&#39;</span>)
</span></span><span style=display:flex><span>ses<span style=color:#f92672>.</span>add_image(<span style=color:#e6db74>&#39;https://c2.staticflickr.com/8/7158/6814444991_08d82de57e_z.jpg&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>ses<span style=color:#f92672>.</span>search_image(<span style=color:#e6db74>&#39;https://pixabay.com/static/uploads/photo/2012/11/28/08/56/mona-lisa-67506_960_720.jpg&#39;</span>)
</span></span></code></pre></div><h3 id=2-eagleeyehttpsgithubcomthoughtfuldeveagleeye>.2. <strong><a href=https://github.com/ThoughtfulDev/EagleEye target=_blank rel="external nofollow noopener noreferrer">EagleEye<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden=true></i></a></strong></h3><blockquote><ul><li>You enter this data into EagleEye and it tries to find Instagram, Youtube, Facebook, and Twitter Profiles of this person.</li><li>using python, dlib, face_reognition, Selenum, find person, star 2.8k</li><li>including facebook, google, imageraider, instagram 代码。</li></ul></blockquote><h3 id=3-pyretrihttpsgithubcompyretripyretri>.3. <strong><a href=https://github.com/PyRetri/PyRetri target=_blank rel="external nofollow noopener noreferrer"><code>PyRetri</code><i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden=true></i></a></strong></h3><blockquote><p>Hu B, Song R J, Wei X S, et al. PyRetri: A PyTorch-based library for unsupervised image retrieval by Deep Convolutional Neural Networks[C]//Proceedings of the 28th ACM International Conference on Multimedia. 2020: 4461-4464. <a href="chrome-extension://ikhdkkncnoglghljlkmcimlnlhkeamad/pdf-viewer/web/viewer.html?file=https%3A%2F%2Farxiv.org%2Fpdf%2F2005.02154.pdf" target=_blank rel="external nofollow noopener noreferrer">pdf<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden=true></i></a> star: 946</p></blockquote><ul><li>an open source library for deep learning based unsupervised image retrival, encapsulating the retrieval process in several stages and provides functionality tha tcovers various prominent methods for each stage.</li><li>propose the first open sourve framework to unify the pipeline of deep learning based unsupervised image retrieval.</li><li>provide high quality implementations of CBIR algorithms to solve retrieval tasks with emphasis on usaility</li></ul><p><img class=lazyload src=/liudongdong1.github.io/svg/loading.min.svg data-src=https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211006140532252.png data-srcset="https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211006140532252.png, https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211006140532252.png 1.5x, https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211006140532252.png 2x" data-sizes=auto alt=https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211006140532252.png title=https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211006140532252.png></p><h4 id=1-pre-processing-methods>.1. Pre-processing methods</h4><ul><li>DirectResize (DR): Scaling the height and width of the image to the target size directly.</li><li>PadResize (PR): Scaling the longer side of the image to the target size and filling the remaining pixels with the meanvalues of ImageNet.</li><li>ShorterResize (SR): Scaling the shorter side of the image to the target size.</li><li>TwoFlip (TF): Returning the original image and the corresponding horizontally flipped image.</li><li>CenterCrop (CC): Cropping the image from its center region according to the given size.</li><li>TenCrop (TC): Cropping the original image and the flipping image from up down left right and center, respectively.</li></ul><h4 id=2-feature-represention-methods>.2. Feature Represention Methods</h4><ul><li>GAP: Global average pooling.</li><li>GMP: Global max pooling.</li><li>R-MAC [14]: Calculating feature vectors based on the regionalmaximum activation of convolutions.</li><li>SPoC [2]: Assigning larger weights to the central descriptorsduring aggregation.</li><li>CroW [7]: A weighted pooling method for both spatial- andchannel-wise.</li><li>SCDA [17]: Keeping useful deep descriptors based on the sum-mation of feature map activations.</li><li>GeM [11]: Exploiting the generalized mean to reserve theinformation of each channel.</li><li>PWA [19]: Aggregating the regional representations weightedby the selected part detectors’ output.</li><li>PCB [13]: Outputting a convolutional descriptor consisting ofseveral part-level features</li></ul><h4 id=3-post-precessing-methods>.3. Post-precessing Methods</h4><ul><li>SVD [6]: Reducing feature dimension through singular valuedecomposition of matrix.</li><li>PCA [18]: Projecting high-dimensional features into fewerinformative dimensions.</li><li>DBA [1]: Every feature in the database is replaced with aweighted sum of the point’s own value and those of its topknearest neighbors (k-NN).</li><li>QE [3]: Combining the retrieved top-knearest neighbors withthe original query and doing another retrieval.</li><li>k-reciprocal [22]: Encodingk-reciprocal nearest neighbors toenhance the accuracy of retrieval</li></ul><h4 id=4-database>.4. Database</h4><ul><li>Oxford5k [9] collects crawling images fromFlickrusing thenames of 11 different landmarks in Oxford, which is a repre-sentative landmark retrieval task.</li><li>CUB-200-2011 [15] contains photos of 200 bird species, whichrepresents fine-grained image retrieval.</li><li>Indoor [10] contains indoor scene images with 67 categories,representing for the scene retrieval/recognition task.</li><li>Caltech101 [4] consists pictures of objects belonging to 101categories, standing for the generic image retrieval task.</li><li>•Market-1501 [20] contains images taken on the Tsinghua cam-pus under six camera viewpoints, which is the benchmarkdataset for person re-identification.</li><li>DukeMTMC-reID [12] contains images captured by eight cam-eras, which is a more challenging person Re-ID dataset.</li></ul><h3 id=4-natural-language-image-searchhttpsgithubcomhaltakovnatural-language-image-search>4. <a href=https://github.com/haltakov/natural-language-image-search target=_blank rel="external nofollow noopener noreferrer">natural-language-image-search<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden=true></i></a></h3><blockquote><p>Radford A, Kim J W, Hallacy C, et al. Learning transferable visual models from natural language supervision[J]. arXiv preprint arXiv:2103.00020, 2021. <code>star 4.9k</code> <a href=https://github.com/openai/CLIP target=_blank rel="external nofollow noopener noreferrer">url<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden=true></i></a> <a href="chrome-extension://ikhdkkncnoglghljlkmcimlnlhkeamad/pdf-viewer/web/viewer.html?file=https%3A%2F%2Farxiv.org%2Fpdf%2F2103.00020.pdf" target=_blank rel="external nofollow noopener noreferrer">pdf<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden=true></i></a></p></blockquote><blockquote><p>OpenAI&rsquo;s <a href=https://openai.com/blog/clip/ target=_blank rel="external nofollow noopener noreferrer">CLIP<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden=true></i></a> neural networs is able to transform both images and text into the same latent space, where they can be compared using a similarity measure. all photos from the full <a href=https://unsplash.com/data target=_blank rel="external nofollow noopener noreferrer">Unsplash Dataset<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden=true></i></a> (almost 2M photos) were downloaded and processed with CLIP. The precomputed feature vectors for all images can then be used to find the best match to a natural language search query.</p></blockquote><blockquote><p>introducing a neural network called <code>CLIP</code> which efficiently learns <code>visual concepts from natural language supervision</code>. CLIP can be applied to any visual classification benchmark by simply providing the names of the visual categories to be recognized, similar to the “zero-shot” capabilities of GPT-2 and GPT-3.</p></blockquote><blockquote><p>CLIP pre-trains an image encoder and a text encoder to predict which images were paired with which texts in our dataset. We then use this behavior to turn CLIP into a zero-shot classifier. We convert all of a dataset’s classes into captions such as “a photo of a <em>dog</em>” and predict the class of the caption CLIP estimates best pairs with a given image.</p></blockquote><p><img class=lazyload src=/liudongdong1.github.io/svg/loading.min.svg data-src=https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211006141807218.png data-srcset="https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211006141807218.png, https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211006141807218.png 1.5x, https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211006141807218.png 2x" data-sizes=auto alt=https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211006141807218.png title=https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211006141807218.png></p><h3 id=5-sishttpsgithubcommatsui528sis>5. <a href=https://github.com/matsui528/sis target=_blank rel="external nofollow noopener noreferrer">sis<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden=true></i></a></h3><p><img class=lazyload src=/liudongdong1.github.io/svg/loading.min.svg data-src=https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211006142200903.png data-srcset="https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211006142200903.png, https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211006142200903.png 1.5x, https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211006142200903.png 2x" data-sizes=auto alt=https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211006142200903.png title=https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20211006142200903.png></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> tensorflow.keras.preprocessing <span style=color:#f92672>import</span> image
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> tensorflow.keras.applications.vgg16 <span style=color:#f92672>import</span> VGG16, preprocess_input
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> tensorflow.keras.models <span style=color:#f92672>import</span> Model
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># See https://keras.io/api/applications/ for details</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>FeatureExtractor</span>:
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __init__(self):
</span></span><span style=display:flex><span>        base_model <span style=color:#f92672>=</span> VGG16(weights<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;imagenet&#39;</span>)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>model <span style=color:#f92672>=</span> Model(inputs<span style=color:#f92672>=</span>base_model<span style=color:#f92672>.</span>input, outputs<span style=color:#f92672>=</span>base_model<span style=color:#f92672>.</span>get_layer(<span style=color:#e6db74>&#39;fc1&#39;</span>)<span style=color:#f92672>.</span>output)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>extract</span>(self, img):
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        Extract a deep feature from an input image
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        Args:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>            img: from PIL.Image.open(path) or tensorflow.keras.preprocessing.image.load_img(path)
</span></span></span><span style=display:flex><span><span style=color:#e6db74>
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        Returns:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>            feature (np.ndarray): deep feature with the shape=(4096, )
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>        img <span style=color:#f92672>=</span> img<span style=color:#f92672>.</span>resize((<span style=color:#ae81ff>224</span>, <span style=color:#ae81ff>224</span>))  <span style=color:#75715e># VGG must take a 224x224 img as an input</span>
</span></span><span style=display:flex><span>        img <span style=color:#f92672>=</span> img<span style=color:#f92672>.</span>convert(<span style=color:#e6db74>&#39;RGB&#39;</span>)  <span style=color:#75715e># Make sure img is color</span>
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> image<span style=color:#f92672>.</span>img_to_array(img)  <span style=color:#75715e># To np.array. Height x Width x Channel. dtype=float32</span>
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>expand_dims(x, axis<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>)  <span style=color:#75715e># (H, W, C)-&gt;(1, H, W, C), where the first elem is the number of img</span>
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> preprocess_input(x)  <span style=color:#75715e># Subtracting avg values for each pixel</span>
</span></span><span style=display:flex><span>        feature <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>model<span style=color:#f92672>.</span>predict(x)[<span style=color:#ae81ff>0</span>]  <span style=color:#75715e># (1, 4096) -&gt; (4096, )</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> feature <span style=color:#f92672>/</span> np<span style=color:#f92672>.</span>linalg<span style=color:#f92672>.</span>norm(feature)  <span style=color:#75715e># Normalize</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>if</span> __name__ <span style=color:#f92672>==</span> <span style=color:#e6db74>&#39;__main__&#39;</span>:
</span></span><span style=display:flex><span>    fe <span style=color:#f92672>=</span> FeatureExtractor()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> img_path <span style=color:#f92672>in</span> sorted(Path(<span style=color:#e6db74>&#34;./static/img&#34;</span>)<span style=color:#f92672>.</span>glob(<span style=color:#e6db74>&#34;*.jpg&#34;</span>)):
</span></span><span style=display:flex><span>        print(img_path)  <span style=color:#75715e># e.g., ./static/img/xxx.jpg</span>
</span></span><span style=display:flex><span>        feature <span style=color:#f92672>=</span> fe<span style=color:#f92672>.</span>extract(img<span style=color:#f92672>=</span>Image<span style=color:#f92672>.</span>open(img_path))
</span></span><span style=display:flex><span>        feature_path <span style=color:#f92672>=</span> Path(<span style=color:#e6db74>&#34;./static/feature&#34;</span>) <span style=color:#f92672>/</span> (img_path<span style=color:#f92672>.</span>stem <span style=color:#f92672>+</span> <span style=color:#e6db74>&#34;.npy&#34;</span>)  <span style=color:#75715e># e.g., ./static/feature/xxx.npy</span>
</span></span><span style=display:flex><span>        np<span style=color:#f92672>.</span>save(feature_path, feature)
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> PIL <span style=color:#f92672>import</span> Image
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> feature_extractor <span style=color:#f92672>import</span> FeatureExtractor
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> datetime <span style=color:#f92672>import</span> datetime
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> flask <span style=color:#f92672>import</span> Flask, request, render_template
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> pathlib <span style=color:#f92672>import</span> Path
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>app <span style=color:#f92672>=</span> Flask(__name__)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Read image features</span>
</span></span><span style=display:flex><span>fe <span style=color:#f92672>=</span> FeatureExtractor()
</span></span><span style=display:flex><span>features <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>img_paths <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> feature_path <span style=color:#f92672>in</span> Path(<span style=color:#e6db74>&#34;./static/feature&#34;</span>)<span style=color:#f92672>.</span>glob(<span style=color:#e6db74>&#34;*.npy&#34;</span>):
</span></span><span style=display:flex><span>    features<span style=color:#f92672>.</span>append(np<span style=color:#f92672>.</span>load(feature_path))
</span></span><span style=display:flex><span>    img_paths<span style=color:#f92672>.</span>append(Path(<span style=color:#e6db74>&#34;./static/img&#34;</span>) <span style=color:#f92672>/</span> (feature_path<span style=color:#f92672>.</span>stem <span style=color:#f92672>+</span> <span style=color:#e6db74>&#34;.jpg&#34;</span>))
</span></span><span style=display:flex><span>features <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>array(features)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>@app.route</span>(<span style=color:#e6db74>&#39;/&#39;</span>, methods<span style=color:#f92672>=</span>[<span style=color:#e6db74>&#39;GET&#39;</span>, <span style=color:#e6db74>&#39;POST&#39;</span>])
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>index</span>():
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> request<span style=color:#f92672>.</span>method <span style=color:#f92672>==</span> <span style=color:#e6db74>&#39;POST&#39;</span>:
</span></span><span style=display:flex><span>        file <span style=color:#f92672>=</span> request<span style=color:#f92672>.</span>files[<span style=color:#e6db74>&#39;query_img&#39;</span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># Save query image</span>
</span></span><span style=display:flex><span>        img <span style=color:#f92672>=</span> Image<span style=color:#f92672>.</span>open(file<span style=color:#f92672>.</span>stream)  <span style=color:#75715e># PIL image</span>
</span></span><span style=display:flex><span>        uploaded_img_path <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;static/uploaded/&#34;</span> <span style=color:#f92672>+</span> datetime<span style=color:#f92672>.</span>now()<span style=color:#f92672>.</span>isoformat()<span style=color:#f92672>.</span>replace(<span style=color:#e6db74>&#34;:&#34;</span>, <span style=color:#e6db74>&#34;.&#34;</span>) <span style=color:#f92672>+</span> <span style=color:#e6db74>&#34;_&#34;</span> <span style=color:#f92672>+</span> file<span style=color:#f92672>.</span>filename
</span></span><span style=display:flex><span>        img<span style=color:#f92672>.</span>save(uploaded_img_path)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># Run search</span>
</span></span><span style=display:flex><span>        query <span style=color:#f92672>=</span> fe<span style=color:#f92672>.</span>extract(img)
</span></span><span style=display:flex><span>        dists <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>linalg<span style=color:#f92672>.</span>norm(features<span style=color:#f92672>-</span>query, axis<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>)  <span style=color:#75715e># L2 distances to features</span>
</span></span><span style=display:flex><span>        ids <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>argsort(dists)[:<span style=color:#ae81ff>30</span>]  <span style=color:#75715e># Top 30 results</span>
</span></span><span style=display:flex><span>        scores <span style=color:#f92672>=</span> [(dists[id], img_paths[id]) <span style=color:#66d9ef>for</span> id <span style=color:#f92672>in</span> ids]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> render_template(<span style=color:#e6db74>&#39;index.html&#39;</span>,
</span></span><span style=display:flex><span>                               query_path<span style=color:#f92672>=</span>uploaded_img_path,
</span></span><span style=display:flex><span>                               scores<span style=color:#f92672>=</span>scores)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>else</span>:
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> render_template(<span style=color:#e6db74>&#39;index.html&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>if</span> __name__<span style=color:#f92672>==</span><span style=color:#e6db74>&#34;__main__&#34;</span>:
</span></span><span style=display:flex><span>    app<span style=color:#f92672>.</span>run(<span style=color:#e6db74>&#34;0.0.0.0&#34;</span>)
</span></span></code></pre></div></div><div class=post-footer id=post-footer><div class=post-info><div class=post-info-line><div class=post-info-mod><span title="2023-09-28 23:08:44">更新于 2023-09-28&nbsp;</span></div><div class=post-info-license><span><a rel="license external nofollow noopener noreferrer" href=https://creativecommons.org/licenses/by-nc/4.0/ target=_blank>CC BY-NC 4.0</a></span></div></div><div class=post-info-line><div class=post-info-md><span><a href=liudongdong1.github.io/imageretrieval/index.md title=阅读原始文档 class=link-to-markdown>阅读原始文档</a></span><span><a href=https://liudongdong1.github.io/edit/master/content/posts%5c%e6%97%b6%e7%a9%ba%e6%95%b0%e6%8d%ae%5cRetrieve%5cImageRetrieval.md title=编辑此页 target=_blank rel="external nofollow noopener noreferrer" class=link-to-edit>编辑此页</a></span></div><div class=post-info-share><span><a href=javascript:void(0); title="分享到 Twitter" data-sharer=twitter data-url=liudongdong1.github.io/imageretrieval/ data-title=ImageRetrieval data-hashtags=Retrieval><i class="fa-brands fa-twitter fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); title="分享到 Facebook" data-sharer=facebook data-url=liudongdong1.github.io/imageretrieval/ data-hashtag=Retrieval><i class="fa-brands fa-facebook-square fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); title="分享到 微博" data-sharer=weibo data-url=liudongdong1.github.io/imageretrieval/ data-title=ImageRetrieval data-image=https://cdn.stocksnap.io/img-thumbs/280h/VTRBWVOXY6.jpg><i class="fa-brands fa-weibo fa-fw" aria-hidden=true></i></a></span></div></div></div><div class=post-info-more><section class=post-tags><i class="fa-solid fa-tags fa-fw" aria-hidden=true></i>&nbsp;<a href=liudongdong1.github.io/tags/retrieval/>Retrieval</a></section><section><span><a href=javascript:void(0); onclick=window.history.back()>返回</a></span>&nbsp;|&nbsp;<span><a href=liudongdong1.github.io/>主页</a></span></section></div><div class=post-nav><a href=liudongdong1.github.io/infraredsensor/ class=prev rel=prev title=infraredsensor><i class="fa-solid fa-angle-left fa-fw" aria-hidden=true></i>infraredsensor</a>
<a href=liudongdong1.github.io/ideaplugin/ class=next rel=next title=IDEAPlugin>IDEAPlugin<i class="fa-solid fa-angle-right fa-fw" aria-hidden=true></i></a></div></div></article></main><footer class=footer><div class=footer-container><div class="footer-line powered">由 <a href=https://gohugo.io/ target=_blank rel="external nofollow noopener noreferrer" title="Hugo 0.118.2">Hugo</a> 强力驱动 | 主题 - <a href=https://github.com/hugo-fixit/FixIt target=_blank rel=external title="FixIt v0.2.17-RC"><img class=fixit-icon src=/liudongdong1.github.io/fixit.min.svg alt="FixIt logo">&nbsp;FixIt</a></div><div class="footer-line copyright" itemscope itemtype=http://schema.org/CreativeWork><i class="fa-regular fa-copyright fa-fw" aria-hidden=true></i>
<span itemprop=copyrightYear>2020 - 2023</span><span class=author itemprop=copyrightHolder>
<a href=https://liudongdong1.github.io/ target=_blank rel="external nofollow noopener noreferrer">LiuDongdong</a></span><span class="license footer-divider"><a rel="license external nofollow noopener noreferrer" href=https://creativecommons.org/licenses/by-nc/4.0/ target=_blank>CC BY-NC 4.0</a></span></div><div class="footer-line statistics"><span class=site-time title='网站运行中 ...'><i class="fa-solid fa-heartbeat fa-fw animate-icon" aria-hidden=true></i>&nbsp;<span class=run-times>网站运行中 ...</span></span></div><div class="footer-line ibruce"><span id=busuanzi_container_site_uv title=总访客数><i class="fa-regular fa-user fa-fw" aria-hidden=true></i>&nbsp;<span id=busuanzi_value_site_uv><i class="fa-solid fa-spinner fa-spin fa-fw" aria-hidden=true></i></span></span><span id=busuanzi_container_site_pv class=footer-divider title=总访问量><i class="fa-regular fa-eye fa-fw" aria-hidden=true></i>&nbsp;<span id=busuanzi_value_site_pv><i class="fa-solid fa-spinner fa-spin fa-fw" aria-hidden=true></i></span></span></div></div></footer></div><div class=widgets><div class="fixed-buttons animate__faster d-none"><div class="fixed-button back-to-top" role=button aria-label=回到顶部><i class="fa-solid fa-arrow-up fa-fw" aria-hidden=true></i><span class=variant-numeric>0%</span></div></div><a href=https://liudongdong1.github.io/ title="在 GitHub 上查看源代码" target=_blank rel="external nofollow" class="github-corner right d-none-mobile"><svg viewBox="0 0 250 250" aria-hidden="true"><path d="M0 0 115 115h15l12 27L250 250V0z"/><path d="M128.3 109C113.8 99.7 119 89.6 119 89.6 122 82.7 120.5 78.6 120.5 78.6 119.2 72 123.4 76.3 123.4 76.3 127.3 80.9 125.5 87.3 125.5 87.3 122.9 97.6 130.6 101.9 134.4 103.2" fill="currentcolor" style="transform-origin:130px 106px" class="octo-arm"/><path d="M115 115C114.9 115.1 118.7 116.5 119.8 115.4l13.9-13.8C136.9 99.2 139.9 98.4 142.2 98.6 133.8 88 127.5 74.4 143.8 58 148.5 53.4 154 51.2 159.7 51 160.3 49.4 163.2 43.6 171.4 40.1 171.4 40.1 176.1 42.5 178.8 56.2 183.1 58.6 187.2 61.8 190.9 65.4 194.5 69 197.7 73.2 200.1 77.6 213.8 80.2 216.3 84.9 216.3 84.9 212.7 93.1 206.9 96 205.4 96.6 205.1 102.4 203 107.8 198.3 112.5 181.9 128.9 168.3 122.5 157.7 114.1 157.9 116.9 156.7 120.9 152.7 124.9L141 136.5C139.8 137.7 141.6 141.9 141.8 141.8z" fill="currentcolor" class="octo-body"/></svg></a><div id=mask></div><div class=reading-progress-bar style=left:0;top:0;--bg-progress:#0076ff;--bg-progress-dark:#fff></div><noscript><div class=noscript-warning>FixIt 主题在启用 JavaScript 的情况下效果最佳。</div></noscript></div><link rel=stylesheet href=/liudongdong1.github.io/lib/katex/katex.min.css><link rel=stylesheet href=/liudongdong1.github.io/lib/cookieconsent/cookieconsent.min.css><script src=/liudongdong1.github.io/lib/autocomplete/autocomplete.min.js defer></script><script src=/liudongdong1.github.io/lib/algoliasearch/algoliasearch-lite.umd.min.js defer></script><script src=/liudongdong1.github.io/lib/lazysizes/lazysizes.min.js async defer></script><script src=/liudongdong1.github.io/lib/sharer/sharer.min.js async defer></script><script src=/liudongdong1.github.io/lib/typeit/index.umd.js defer></script><script src=/liudongdong1.github.io/lib/katex/katex.min.js defer></script><script src=/liudongdong1.github.io/lib/katex/auto-render.min.js defer></script><script src=/liudongdong1.github.io/lib/katex/copy-tex.min.js defer></script><script src=/liudongdong1.github.io/lib/katex/mhchem.min.js defer></script><script src=/liudongdong1.github.io/lib/cookieconsent/cookieconsent.min.js defer></script><script src=/liudongdong1.github.io/lib/pangu/pangu.min.js defer></script><script src=/liudongdong1.github.io/lib/cell-watermark/watermark.min.js defer></script><script src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js async defer></script><script>window.config={code:{copyTitle:"复制到剪贴板",editLockTitle:"锁定可编辑代码块",editUnLockTitle:"解锁可编辑代码块",editable:!0,maxShownLines:10},comment:{enable:!1},cookieconsent:{content:{dismiss:"同意",link:"了解更多",message:"本网站使用 Cookies 来改善您的浏览体验。"},enable:!0,palette:{button:{background:"#f0f0f0"},popup:{background:"#1aa3ff"}},theme:"edgeless"},data:{"typeit-header-subtitle-desktop":`<span style='font-family: MMT,"沐目体";'>吾日三省吾身</span>`,"typeit-header-subtitle-mobile":`<span style='font-family: MMT,"沐目体";'>吾日三省吾身</span>`},enablePWA:!0,enablePangu:!0,math:{delimiters:[{display:!0,left:"$$",right:"$$"},{display:!0,left:"\\[",right:"\\]"},{display:!0,left:"\\begin{equation}",right:"\\end{equation}"},{display:!0,left:"\\begin{equation*}",right:"\\end{equation*}"},{display:!0,left:"\\begin{align}",right:"\\end{align}"},{display:!0,left:"\\begin{align*}",right:"\\end{align*}"},{display:!0,left:"\\begin{alignat}",right:"\\end{alignat}"},{display:!0,left:"\\begin{alignat*}",right:"\\end{alignat*}"},{display:!0,left:"\\begin{gather}",right:"\\end{gather}"},{display:!0,left:"\\begin{CD}",right:"\\end{CD}"},{display:!1,left:"$",right:"$"},{display:!1,left:"\\(",right:"\\)"}],strict:!1},search:{algoliaAppID:"2R1K9SKLQZ",algoliaIndex:"index.zh-cn",algoliaSearchKey:"4a226aa1c5c98d6859e4d1386adb2bc7",highlightTag:"em",maxResultLength:10,noResultsFound:"没有找到结果",snippetLength:50,type:"algolia"},siteTime:"2020-12-18T16:15:22+08:00",typeit:{cursorChar:"|",cursorSpeed:1e3,data:{"typeit-header-subtitle-desktop":["typeit-header-subtitle-desktop"],"typeit-header-subtitle-mobile":["typeit-header-subtitle-mobile"]},duration:-1,speed:100},watermark:{appendto:".wrapper>main",colspacing:30,content:'<img class="fixit-icon" src="/fixit.min.svg" alt="FixIt logo" /> FixIt 主题',enable:!0,fontfamily:"inherit",fontsize:.85,height:21,opacity:.0125,rotate:15,rowspacing:60,width:150}}</script><script src=/liudongdong1.github.io/js/theme.min.js defer></script><script src=/liudongdong1.github.io/js/custom.min.js defer></script></body></html>