<!DOCTYPE html>
<html itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">
  <head>
    
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
    <meta name="robots" content="noodp" />
    <title>SuperResolution - DAY By DAY</title><meta name="author" content="LiuDongdong">
<meta name="author-link" content="https://liudongdong1.github.io/">
<meta name="description" content="Tian Y, Zhang Y, Fu Y, et al. Tdan: Temporally-deformable alignment network for video super-resolution[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020: 3360-3369. Paper: Tdan Summary propose a temporally-deformable alignment network(TDAN) to adaptively align the reference frame and each supporting frame a the feature level without computing optical flow. use features from both the reference frame and each supporting frame to dynamically predict offsets of sampling" /><meta name="keywords" content='VideoAnalyse' /><meta itemprop="name" content="SuperResolution">
<meta itemprop="description" content="Tian Y, Zhang Y, Fu Y, et al. Tdan: Temporally-deformable alignment network for video super-resolution[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020: 3360-3369. Paper: Tdan Summary propose a temporally-deformable alignment network(TDAN) to adaptively align the reference frame and each supporting frame a the feature level without computing optical flow. use features from both the reference frame and each supporting frame to dynamically predict offsets of sampling"><meta itemprop="datePublished" content="2021-09-23T21:45:45+00:00" />
<meta itemprop="dateModified" content="2023-12-31T13:31:55+08:00" />
<meta itemprop="wordCount" content="3416"><meta itemprop="image" content="https://liudongdong1.github.io/logo.png"/>
<meta itemprop="keywords" content="VideoAnalyse," /><meta property="og:title" content="SuperResolution" />
<meta property="og:description" content="Tian Y, Zhang Y, Fu Y, et al. Tdan: Temporally-deformable alignment network for video super-resolution[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020: 3360-3369. Paper: Tdan Summary propose a temporally-deformable alignment network(TDAN) to adaptively align the reference frame and each supporting frame a the feature level without computing optical flow. use features from both the reference frame and each supporting frame to dynamically predict offsets of sampling" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://liudongdong1.github.io/superresolution/" /><meta property="og:image" content="https://liudongdong1.github.io/logo.png"/><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2021-09-23T21:45:45+00:00" />
<meta property="article:modified_time" content="2023-12-31T13:31:55+08:00" />
<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://liudongdong1.github.io/logo.png"/>

<meta name="twitter:title" content="SuperResolution"/>
<meta name="twitter:description" content="Tian Y, Zhang Y, Fu Y, et al. Tdan: Temporally-deformable alignment network for video super-resolution[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020: 3360-3369. Paper: Tdan Summary propose a temporally-deformable alignment network(TDAN) to adaptively align the reference frame and each supporting frame a the feature level without computing optical flow. use features from both the reference frame and each supporting frame to dynamically predict offsets of sampling"/>
<meta name="application-name" content="DAY By DAY">
<meta name="apple-mobile-web-app-title" content="DAY By DAY"><meta name="theme-color" data-light="#f8f8f8" data-dark="#252627" content="#f8f8f8"><meta name="msapplication-TileColor" content="#da532c"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" />
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"><link rel="manifest" href="/site.webmanifest"><link rel="canonical" href="https://liudongdong1.github.io/superresolution/" /><link rel="prev" href="https://liudongdong1.github.io/compreface/" /><link rel="next" href="https://liudongdong1.github.io/robot_grasp/" /><link rel="stylesheet" href="/css/style.min.css"><link rel="stylesheet" href="/lib/fontawesome-free/all.min.css"><link rel="stylesheet" href="/lib/animate/animate.min.css"><script type="application/ld+json">
  {
    "@context": "http://schema.org",
    "@type": "BlogPosting",
    "headline": "SuperResolution",
    "inLanguage": "zh-CN",
    "mainEntityOfPage": {
      "@type": "WebPage",
      "@id": "https:\/\/liudongdong1.github.io\/superresolution\/"
    },"genre": "posts","keywords": "VideoAnalyse","wordcount":  3416 ,
    "url": "https:\/\/liudongdong1.github.io\/superresolution\/","datePublished": "2021-09-23T21:45:45+00:00","dateModified": "2023-12-31T13:31:55+08:00","license": "This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.","publisher": {
      "@type": "Organization",
      "name": "LiuDongdong","logo": "https:\/\/liudongdong1.github.io\/images\/person.png"},"author": {
        "@type": "Person",
        "name": "liudongdong1"
      },"description": ""
  }
  </script></head>
  <body data-header-desktop="auto" data-header-mobile="auto"><script>(window.localStorage?.getItem('theme') ? localStorage.getItem('theme') === 'dark' : ('auto' === 'auto' ? window.matchMedia('(prefers-color-scheme: dark)').matches : 'auto' === 'dark')) && document.body.setAttribute('data-theme', 'dark');</script><div class="wrapper"><script type="text/javascript"
        async
        src="https://cdnjs.cloudflare.com/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$']],
    displayMath: [['$$','$$']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
});

MathJax.Hub.Queue(function() {
    
    
    
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>

<style>
code.has-jax {
    font: inherit;
    font-size: 100%;
    background: inherit;
    border: inherit;
    color: #515151;
}
</style>
<header class="desktop animate__faster" id="header-desktop">
  <div class="header-wrapper" data-github-corner="right">
    <div class="header-title">
      <a href="/" title="DAY By DAY"><img
    class="lazyload logo"
    src="/svg/loading.min.svg"
    data-src="/fixit.min.svg"
    data-srcset="/fixit.min.svg, /fixit.min.svg 1.5x, /fixit.min.svg 2x"
    data-sizes="auto"
    alt="DAY By DAY"
    title="DAY By DAY"/><span class="header-title-text"></span></a><span id="typeit-header-subtitle-desktop" class="typeit header-subtitle"></span></div>
    <nav>
      <ul class="menu"><li class="menu-item">
              <a
                class="menu-link"
                href="/posts/"
                
                
              ><i class="fa-solid fa-archive fa-fw fa-sm" aria-hidden="true"></i> 所有文章</a></li><li class="menu-item">
              <a
                class="menu-link"
                href="/categories/"
                
                
              ><i class="fa-solid fa-th fa-fw fa-sm" aria-hidden="true"></i> 分类</a></li><li class="menu-item">
              <a
                class="menu-link"
                href="/tags/"
                
                
              ><i class="fa-solid fa-tags fa-fw fa-sm" aria-hidden="true"></i> 标签</a></li><li class="menu-item">
              <a
                class="menu-link"
                href="/friends/"
                title="友情链接"
                
              ><i class="fa-solid fa-users fa-fw fa-sm" aria-hidden="true"></i> 友链</a></li><li class="menu-item">
              <a
                class="menu-link"
                href="/about/"
                
                
              ><i class="fa-solid fa-info-circle fa-fw fa-sm" aria-hidden="true"></i> 关于</a></li><li class="menu-item delimiter"></li><li class="menu-item language">
            <span role="button" aria-label="选择语言" title="选择语言">简体中文<i class="dropdown-icon fa-solid fa-chevron-down" aria-hidden="true"></i>
            </span>
            <ul class="sub-menu"><li class="menu-item">没有更多翻译</li></ul>
          </li><li class="menu-item search" id="search-desktop">
            <input type="text" placeholder="搜索文章标题或内容 ..." id="search-input-desktop">
            <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-desktop" title="搜索">
              <i class="fa-solid fa-search fa-fw" aria-hidden="true"></i>
            </a>
            <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-desktop" title="清空">
              <i class="fa-solid fa-times-circle fa-fw" aria-hidden="true"></i>
            </a>
            <span class="search-button search-loading" id="search-loading-desktop">
              <i class="fa-solid fa-spinner fa-fw fa-spin" aria-hidden="true"></i>
            </span>
          </li><li class="menu-item theme-switch" title="切换主题">
          <i class="fa-solid fa-adjust fa-fw" aria-hidden="true"></i>
        </li>
      </ul>
    </nav>
  </div>
</header><header class="mobile animate__faster" id="header-mobile">
  <div class="header-container">
    <div class="header-wrapper">
      <div class="header-title">
        <a href="/" title="DAY By DAY"><img
    class="lazyload logo"
    src="/svg/loading.min.svg"
    data-src="/fixit.min.svg"
    data-srcset="/fixit.min.svg, /fixit.min.svg 1.5x, /fixit.min.svg 2x"
    data-sizes="auto"
    alt="/fixit.min.svg"
    title="/fixit.min.svg"/><span class="header-title-text"></span></a><span id="typeit-header-subtitle-mobile" class="typeit header-subtitle"></span></div>
      <div class="menu-toggle" id="menu-toggle-mobile">
        <span></span><span></span><span></span>
      </div>
    </div>
    <nav>
      <ul class="menu" id="menu-mobile"><li class="search-wrapper">
            <div class="search mobile" id="search-mobile">
              <input type="text" placeholder="搜索文章标题或内容 ..." id="search-input-mobile">
              <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-mobile" title="搜索">
                <i class="fa-solid fa-search fa-fw" aria-hidden="true"></i>
              </a>
              <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-mobile" title="清空">
                <i class="fa-solid fa-times-circle fa-fw" aria-hidden="true"></i>
              </a>
              <span class="search-button search-loading" id="search-loading-mobile">
                <i class="fa-solid fa-spinner fa-fw fa-spin" aria-hidden="true"></i>
              </span>
            </div>
            <a href="javascript:void(0);" class="search-cancel" id="search-cancel-mobile">
              取消
            </a>
          </li><li
              class="menu-item"
            ><a
                  class="menu-link"
                  href="/posts/"
                  
                  
                ><i class="fa-solid fa-archive fa-fw fa-sm" aria-hidden="true"></i> 所有文章</a></li><li
              class="menu-item"
            ><a
                  class="menu-link"
                  href="/categories/"
                  
                  
                ><i class="fa-solid fa-th fa-fw fa-sm" aria-hidden="true"></i> 分类</a></li><li
              class="menu-item"
            ><a
                  class="menu-link"
                  href="/tags/"
                  
                  
                ><i class="fa-solid fa-tags fa-fw fa-sm" aria-hidden="true"></i> 标签</a></li><li
              class="menu-item"
            ><a
                  class="menu-link"
                  href="/friends/"
                  title="友情链接"
                  
                ><i class="fa-solid fa-users fa-fw fa-sm" aria-hidden="true"></i> 友链</a></li><li
              class="menu-item"
            ><a
                  class="menu-link"
                  href="/about/"
                  
                  
                ><i class="fa-solid fa-info-circle fa-fw fa-sm" aria-hidden="true"></i> 关于</a></li><li
              class="menu-item text-center"
            ><a
                  class="menu-link"
                  href="/"
                  title="GitHub"
                  
                ><i class='fa-brands fa-github fa-fw' aria-hidden='true'></i> </a></li><li class="menu-item theme-switch" title="切换主题">
          <i class="fa-solid fa-adjust fa-fw" aria-hidden="true"></i>
        </li><li class="menu-item language">
            <span role="button" aria-label="选择语言" title="选择语言">简体中文<i class="dropdown-icon fa-solid fa-chevron-down" aria-hidden="true"></i>
            </span>
            <select class="language-select" onchange="location = this.value;"><option disabled>没有更多翻译</option></select>
          </li></ul>
    </nav>
  </div>
</header><div class="search-dropdown desktop">
    <div id="search-dropdown-desktop"></div>
  </div>
  <div class="search-dropdown mobile">
    <div id="search-dropdown-mobile"></div>
  </div><main class="container" data-page-style="normal"><aside class="toc" id="toc-auto"><h2 class="toc-title">目录 <i class="toc-icon fa-solid fa-angle-down fa-fw"></i></h2>
      <div class="toc-content" id="toc-content-auto"></div></aside>

  <aside class="aside-custom" id="aside-sakana">
    

<div class="sakana-widget">
  <div class="sakana-item" id="takina-widget"></div>
  <div class="sakana-item" id="chisato-widget"></div>
</div>
<script>
  function initSakanaWidget() {
    const takina = SakanaWidget.getCharacter('takina')
    SakanaWidget.registerCharacter('takina-slow', takina);
    new SakanaWidget({
      character: 'takina-slow',
      controls: false,
      autoFit: true,
      stroke: {
        color: "#b4b4b4",
        width: 2
      }
    }).mount('#takina-widget');

    const chisato = SakanaWidget.getCharacter('chisato')
    SakanaWidget.registerCharacter('chisato-slow', chisato);
    new SakanaWidget({
      character: 'chisato-slow',
      controls: false,
      autoFit: true,
      stroke: {
        color: "#b4b4b4",
        width: 2
      }
    }).mount('#chisato-widget');
  }
</script>
<script async onload="initSakanaWidget()" src="https://cdn.jsdelivr.net/npm/sakana-widget@2.3.0/lib/sakana.min.js">
</script></aside>

  <article class="page single">
    <div class="header"><h1 class="single-title animate__animated animate__flipInX">
        <span>SuperResolution</span>
      </h1></div><div class="post-meta">
      <div class="post-meta-line"><span class="post-author"><span class="author"><i class="fa-solid fa-user-circle" aria-hidden="true"></i>
      liudongdong1</span></span>
          <span class="post-category">收录于 <a href="/categories/"><i class="fa-regular fa-folder fa-fw"></i>&nbsp;Categories</a>&ensp;<a href="/categories/%E8%A7%86%E8%A7%89ai/"><i class="fa-regular fa-folder fa-fw"></i>&nbsp;视觉AI</a></span></div>
      <div class="post-meta-line"><span title=2021-09-23&#32;21:45:45>
            <i class="fa-regular fa-calendar-alt fa-fw"></i>&nbsp;<time datetime="2021-09-23" >2021-09-23</time>
          </span>&nbsp;<i class="fa-solid fa-pencil-alt fa-fw"></i>&nbsp;约 3416 字&nbsp;
        <i class="fa-regular fa-clock fa-fw"></i>&nbsp;预计阅读 7 分钟&nbsp;<span id="busuanzi_container_page_pv" class="busuanzi_visitors comment-visitors" data-flag-title="SuperResolution">
            <i class="fa-regular fa-eye fa-fw"></i>&nbsp;<span id="busuanzi_value_page_pv">-</span>&nbsp;次阅读
          </span>&nbsp;</div>
    </div><div class="featured-image"><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://cdn.stocksnap.io/img-thumbs/280h/QSCRVBUU2G.jpg"
    data-srcset="https://cdn.stocksnap.io/img-thumbs/280h/QSCRVBUU2G.jpg, https://cdn.stocksnap.io/img-thumbs/280h/QSCRVBUU2G.jpg 1.5x, https://cdn.stocksnap.io/img-thumbs/280h/QSCRVBUU2G.jpg 2x"
    data-sizes="auto"
    alt="https://cdn.stocksnap.io/img-thumbs/280h/QSCRVBUU2G.jpg"
    title="https://cdn.stocksnap.io/img-thumbs/280h/QSCRVBUU2G.jpg"/></div><div class="details toc" id="toc-static" kept="true">
        <div class="details-summary toc-title">
          <span>目录</span>
          <span><i class="details-icon fa-solid fa-angle-right"></i></span>
        </div>
        <div class="details-content toc-content" id="toc-content-static"><nav id="TableOfContents">
  <ul>
    <li>
      <ul>
        <li><a href="#paper-tdan">Paper: Tdan</a></li>
        <li><a href="#paper-edsr">Paper: EDSR</a></li>
        <li><a href="#paper-edvr">Paper: Edvr</a></li>
        <li><a href="#paper-seven-ways">Paper: Seven ways</a></li>
        <li><a href="#project">Project</a></li>
        <li><a href="#survey">Survey</a></li>
      </ul>
    </li>
  </ul>
</nav></div>
      </div><div
      class="content"
      id="content"
      
      
    ><blockquote>
<p>Tian Y, Zhang Y, Fu Y, et al. Tdan: Temporally-deformable alignment network for video super-resolution[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020: 3360-3369.</p>
</blockquote>
<h3 id="paper-tdan">Paper: Tdan</h3>
<!-- raw HTML omitted -->
<h4 id="summary">Summary</h4>
<ol>
<li>propose a temporally-deformable alignment network(TDAN) to <code>adaptively align the reference frame and each supporting frame a the feature level without computing optical flow.</code></li>
<li>use features from both the reference frame and each supporting frame to <code>dynamically predict offsets of sampling convolution kernels</code>, to transforms <code>supporting frames to align with the reference frame</code>.</li>
<li>taking aligned frames and the reference frame to predict the HR video frame.</li>
</ol>
<h4 id="research-objective">Research Objective</h4>
<ul>
<li><strong>Application Area</strong>: <code>Video super-resolution</code> aims to restore a photo-realistic high-resolution video frame from both its<code> corresponding low-resolution frame (reference frame)</code> and <code>multiple neighboring frames (supporting frames).</code>
<ul>
<li>varying motion of cameras, or objects, the reference frame and each support frame are not alighned.</li>
</ul>
</li>
<li><strong>Relative work</strong>:
<ul>
<li>optical flow to <code>predict motion fields</code> between the reference frame and supporting frames, then warp the supporting frames using their corresponding motion fields.</li>
</ul>
</li>
</ul>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211016224337688.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211016224337688.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211016224337688.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211016224337688.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211016224337688.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211016224337688.png"/></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211016223010811.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211016223010811.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211016223010811.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211016223010811.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211016223010811.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211016223010811.png"/></p>
<ul>
<li>
<p>TDAN to <code>align each supporting frame with the reference frame</code>, a LR supproting frame, and referencfe frame&ndash;&gt;feeding 2N support frames to get  2N corresponding aligned LR frames.</p>
<ul>
<li>feature extraction: use one convolutional layer amd k1 residual blocksto extracts visual features.</li>
<li>deformable alignment: takes the features mentioned above to predict sampling parameters. (refers to the offsdets o fthe convolution kernels.)  the feature of the reference frame is only used for computing the offset, its information will not propagated into the aligned feature of the supporting frame.  The adaptively-learned offset will implicitly capture motion cues and explore neighboring features within the sma eimage structures for alignment.</li>
<li>aligned frame reconstruction: restore an aligned LR frame and utilize an alignment loss to enforce the deformable alignment module to sample useful features for accurate temporal alignment.</li>
</ul>
</li>
<li>
<p>supre resolution reconstruction network to predict the HR frame:  2N corresponding aligned LR frames+ reference frame &ndash;&gt; reconstruct the HR video frame.</p>
<ul>
<li>Temporal Fusion: concatenate the 2N+1 frames and then feed them into a 3*3 convolutional layer to output fused feature map;</li>
<li>Nonlinear Mapping: take th eshadow fused features as input to predict deep features.</li>
<li>utilize an upscaling layer to increase the resolution of th efeature map with a sub-pixel convolution.</li>
</ul>
</li>
<li>
<p>Loss Function:</p>
<ul>
<li>utilize the reference frame as the label and make the aligned LR frames close to the reference frame.</li>
<li>utilize the final HR video estimated frame with HR video frame.</li>
</ul>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211017123513481.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211017123513481.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211017123513481.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211017123513481.png 2x"
    data-sizes="auto"
    alt="Lalign"
    title="Lalign"/></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211017123619469.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211017123619469.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211017123619469.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211017123619469.png 2x"
    data-sizes="auto"
    alt="Lsr"
    title="Lsr"/></p>
</li>
</ul>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211022212648591.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211022212648591.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211022212648591.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211022212648591.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211022212648591.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211022212648591.png"/></p>
<blockquote>
<p>the aligned frame is reconstructed from features from the reference and supporting frames. Green points in the supporting frame indicate sampling positions for predicting corresponding pixels labeled withred color in the aligned frame.</p>
</blockquote>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211016223232945.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211016223232945.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211016223232945.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211016223232945.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211016223232945.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211016223232945.png"/></p>
<blockquote>
<p>the TDAN can expoit rich image contexts containing similar content (green regions) as target pixels (red points) from the supporting frame to employ accurately temporal alignment.</p>
</blockquote>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211016223752368.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211016223752368.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211016223752368.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211016223752368.png 2x"
    data-sizes="auto"
    alt="Temporal alignment"
    title="Temporal alignment"/></p>
<h4 id="code">Code</h4>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">TDAN_VSR</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self):
</span></span><span style="display:flex;"><span>        super(TDAN_VSR, self)<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>name <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;TDAN&#39;</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>conv_first <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Conv2d(<span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">64</span>, <span style="color:#ae81ff">3</span>, padding<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>residual_layer <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>make_layer(Res_Block, <span style="color:#ae81ff">5</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>relu <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>ReLU(inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># deformable</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>cr <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Conv2d(<span style="color:#ae81ff">128</span>, <span style="color:#ae81ff">64</span>, <span style="color:#ae81ff">3</span>, padding<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>off2d_1 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Conv2d(<span style="color:#ae81ff">64</span>, <span style="color:#ae81ff">18</span> <span style="color:#f92672">*</span> <span style="color:#ae81ff">8</span>, <span style="color:#ae81ff">3</span>, padding<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>dconv_1 <span style="color:#f92672">=</span> ConvOffset2d(<span style="color:#ae81ff">64</span>, <span style="color:#ae81ff">64</span>, <span style="color:#ae81ff">3</span>, padding<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, num_deformable_groups<span style="color:#f92672">=</span><span style="color:#ae81ff">8</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>off2d_2 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Conv2d(<span style="color:#ae81ff">64</span>, <span style="color:#ae81ff">18</span> <span style="color:#f92672">*</span> <span style="color:#ae81ff">8</span>, <span style="color:#ae81ff">3</span>, padding<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>deconv_2 <span style="color:#f92672">=</span> ConvOffset2d(<span style="color:#ae81ff">64</span>, <span style="color:#ae81ff">64</span>, <span style="color:#ae81ff">3</span>, padding<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, num_deformable_groups<span style="color:#f92672">=</span><span style="color:#ae81ff">8</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>off2d_3 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Conv2d(<span style="color:#ae81ff">64</span>, <span style="color:#ae81ff">18</span> <span style="color:#f92672">*</span> <span style="color:#ae81ff">8</span>, <span style="color:#ae81ff">3</span>, padding<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>deconv_3 <span style="color:#f92672">=</span> ConvOffset2d(<span style="color:#ae81ff">64</span>, <span style="color:#ae81ff">64</span>, <span style="color:#ae81ff">3</span>, padding<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, num_deformable_groups<span style="color:#f92672">=</span><span style="color:#ae81ff">8</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>off2d <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Conv2d(<span style="color:#ae81ff">64</span>, <span style="color:#ae81ff">18</span> <span style="color:#f92672">*</span> <span style="color:#ae81ff">8</span>, <span style="color:#ae81ff">3</span>, padding<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>dconv <span style="color:#f92672">=</span> ConvOffset2d(<span style="color:#ae81ff">64</span>, <span style="color:#ae81ff">64</span>, (<span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">3</span>), padding<span style="color:#f92672">=</span>(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>), num_deformable_groups<span style="color:#f92672">=</span><span style="color:#ae81ff">8</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>recon_lr <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Conv2d(<span style="color:#ae81ff">64</span>, <span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">3</span>, padding<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        fea_ex <span style="color:#f92672">=</span> [nn<span style="color:#f92672">.</span>Conv2d(<span style="color:#ae81ff">5</span> <span style="color:#f92672">*</span> <span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">64</span>, <span style="color:#ae81ff">3</span>, padding<span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>),
</span></span><span style="display:flex;"><span>                       nn<span style="color:#f92672">.</span>ReLU()]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>fea_ex <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Sequential(<span style="color:#f92672">*</span>fea_ex)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>recon_layer <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>make_layer(Res_Block, <span style="color:#ae81ff">10</span>)     
</span></span><span style="display:flex;"><span>        upscaling <span style="color:#f92672">=</span> [
</span></span><span style="display:flex;"><span>            Upsampler(default_conv, <span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">64</span>, act<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>),      <span style="color:#75715e">#？？</span>
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Conv2d(<span style="color:#ae81ff">64</span>, <span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">3</span>, padding<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>up <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Sequential(<span style="color:#f92672">*</span>upscaling)  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># xavier initialization</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> m <span style="color:#f92672">in</span> self<span style="color:#f92672">.</span>modules():
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">if</span> isinstance(m, nn<span style="color:#f92672">.</span>Conv2d):
</span></span><span style="display:flex;"><span>                n <span style="color:#f92672">=</span> m<span style="color:#f92672">.</span>kernel_size[<span style="color:#ae81ff">0</span>] <span style="color:#f92672">*</span> m<span style="color:#f92672">.</span>kernel_size[<span style="color:#ae81ff">1</span>] <span style="color:#f92672">*</span> m<span style="color:#f92672">.</span>out_channels
</span></span><span style="display:flex;"><span>                m<span style="color:#f92672">.</span>weight<span style="color:#f92672">.</span>data<span style="color:#f92672">.</span>normal_(<span style="color:#ae81ff">0</span>, math<span style="color:#f92672">.</span>sqrt(<span style="color:#ae81ff">2.</span> <span style="color:#f92672">/</span> n))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">align</span>(self, x, x_center):
</span></span><span style="display:flex;"><span>        y <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>        batch_size, num, ch, w, h <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>size()
</span></span><span style="display:flex;"><span>        center <span style="color:#f92672">=</span> num <span style="color:#f92672">//</span> <span style="color:#ae81ff">2</span>
</span></span><span style="display:flex;"><span>        ref <span style="color:#f92672">=</span> x[:, center, :, :, :]<span style="color:#f92672">.</span>clone()
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(num):
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">if</span> i <span style="color:#f92672">==</span> center:
</span></span><span style="display:flex;"><span>                y<span style="color:#f92672">.</span>append(x_center<span style="color:#f92672">.</span>unsqueeze(<span style="color:#ae81ff">1</span>))
</span></span><span style="display:flex;"><span>                <span style="color:#66d9ef">continue</span>
</span></span><span style="display:flex;"><span>            supp <span style="color:#f92672">=</span> x[:, i, :, :, :]
</span></span><span style="display:flex;"><span>            fea <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>cat([ref, supp], dim<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)  <span style="color:#75715e"># 按dim 维度进行拼接，</span>
</span></span><span style="display:flex;"><span>            fea <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>cr(fea)
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># feature trans</span>
</span></span><span style="display:flex;"><span>            offset1 <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>off2d_1(fea)
</span></span><span style="display:flex;"><span>            fea <span style="color:#f92672">=</span> (self<span style="color:#f92672">.</span>dconv_1(fea, offset1))
</span></span><span style="display:flex;"><span>            offset2 <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>off2d_2(fea)
</span></span><span style="display:flex;"><span>            fea <span style="color:#f92672">=</span> (self<span style="color:#f92672">.</span>deconv_2(fea, offset2))
</span></span><span style="display:flex;"><span>            offset3 <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>off2d_3(fea)
</span></span><span style="display:flex;"><span>            fea <span style="color:#f92672">=</span> (self<span style="color:#f92672">.</span>deconv_3(supp, offset3))
</span></span><span style="display:flex;"><span>            offset4 <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>off2d(fea)
</span></span><span style="display:flex;"><span>            aligned_fea <span style="color:#f92672">=</span> (self<span style="color:#f92672">.</span>dconv(fea, offset4))
</span></span><span style="display:flex;"><span>            im <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>recon_lr(aligned_fea)<span style="color:#f92672">.</span>unsqueeze(<span style="color:#ae81ff">1</span>)  <span style="color:#75715e">#去掉维数为1的的维度，比如是一行或者一列这种</span>
</span></span><span style="display:flex;"><span>            y<span style="color:#f92672">.</span>append(im)
</span></span><span style="display:flex;"><span>        y <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>cat(y, dim<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> y
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">make_layer</span>(self, block, num_of_layer):
</span></span><span style="display:flex;"><span>        layers <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> _ <span style="color:#f92672">in</span> range(num_of_layer):
</span></span><span style="display:flex;"><span>            layers<span style="color:#f92672">.</span>append(block())
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> nn<span style="color:#f92672">.</span>Sequential(<span style="color:#f92672">*</span>layers)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x):
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        batch_size, num, ch, w, h <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>size()  <span style="color:#75715e"># 5 video frames</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># center frame interpolation</span>
</span></span><span style="display:flex;"><span>        center <span style="color:#f92672">=</span> num <span style="color:#f92672">//</span> <span style="color:#ae81ff">2</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># extract features</span>
</span></span><span style="display:flex;"><span>        y <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>view(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, ch, w, h)     <span style="color:#75715e">#这个y作用是什么？原始图像，和recon_lr 特征提取后的数据融合在一起           # batch_size*num, ch, w, h</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># y = y.unsqueeze(1)</span>
</span></span><span style="display:flex;"><span>        out <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>relu(self<span style="color:#f92672">.</span>conv_first(y))
</span></span><span style="display:flex;"><span>        x_center <span style="color:#f92672">=</span> x[:, center, :, :, :]
</span></span><span style="display:flex;"><span>        out <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>residual_layer(out)
</span></span><span style="display:flex;"><span>        out <span style="color:#f92672">=</span> out<span style="color:#f92672">.</span>view(batch_size, num, <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, w, h)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># align supporting frames</span>
</span></span><span style="display:flex;"><span>        lrs <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>align(out, x_center) <span style="color:#75715e"># motion alignments</span>
</span></span><span style="display:flex;"><span>        y <span style="color:#f92672">=</span> lrs<span style="color:#f92672">.</span>view(batch_size, <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, w, h)
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># reconstruction</span>
</span></span><span style="display:flex;"><span>        fea <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>fea_ex(y)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        out <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>recon_layer(fea)
</span></span><span style="display:flex;"><span>        out <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>up(out)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> out, lrs
</span></span></code></pre></div><blockquote>
<p>Lim B, Son S, Kim H, et al. Enhanced deep residual networks for single image super-resolution[C]//Proceedings of the IEEE conference on computer vision and pattern recognition workshops. 2017: 136-144.  cite  <a href="chrome-extension://ikhdkkncnoglghljlkmcimlnlhkeamad/pdf-viewer/web/viewer.html?file=https%3A%2F%2Farxiv.org%2Fpdf%2F1707.02921.pdf"target="_blank" rel="external nofollow noopener noreferrer">pdf<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
</blockquote>
<hr>
<h3 id="paper-edsr">Paper: EDSR</h3>
<!-- raw HTML omitted -->
<ol>
<li>optimize the SRResNet architecture by analyzing and removing unnecessary modules to simplify the network architecture. Train the network with appropricate loss function and careful model modification upon training.</li>
<li>propose a new multi-scale architecture that shares most of the parameters across different scales.</li>
</ol>
<h4 id="previous-work">previous work:</h4>
<ul>
<li>
<h4 id="interpolation-techniques-based-on-sampling-theory-limites-in-predicting-detailed-realistic-textures"><code>interpolation techniques</code> based on sampling theory limites in predicting detailed, realistic textures.</h4>
</li>
<li>learn the mapping functions between $I^{LR}$ to $I^{HR}$​, including neighbor embedding, to sparse coding.</li>
</ul>
<h4 id="methods">Methods</h4>
<ul>
<li><strong>system overview</strong>:</li>
</ul>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211023172415860.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211023172415860.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211023172415860.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211023172415860.png 2x"
    data-sizes="auto"
    alt="Single-scale model"
    title="Single-scale model"/></p>
<blockquote>
<p>building a multi-scale architecture that takes the advantage of inter-scale correlation as VDSR, and introduce scale specific processing modules to handle the super-resolution at multiple scales.</p>
<ul>
<li><code>pre-processing modules</code> are located at the head of networks to reduce the variance from input images of different scales. each consists of two residual blocks with 5*5 kernels to keep the scale-specific part shallow while the larger receptive field is covered in early stages of networks.</li>
<li>at the end of the multi-scale model, <code>scale-specific upsampling modules</code> are located in parallel to handle multi-scale reconstruction.</li>
</ul>
</blockquote>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211024085357018.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211024085357018.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211024085357018.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211024085357018.png 2x"
    data-sizes="auto"
    alt="MDSR"
    title="MDSR"/></p>
<blockquote>
<p>batch normalization layers normalize the features, <code>they get rid of range flexibility</code> from networks by normalizing the features. GPU memory usage sufficiently reduced.</p>
</blockquote>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211023172200022.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211023172200022.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211023172200022.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211023172200022.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211023172200022.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211023172200022.png"/></p>
<h4 id="evaluation">Evaluation</h4>
<ul>
<li>Dataset:
<ul>
<li>DIV2K dataset is a newly proposed high-quality(2K resolution) image dataset for image restoration tasks, consisting 800 training images, 100 valication images, and 100 test images.</li>
</ul>
</li>
<li>use the RGB input patches of size 48*48 from LR image with the corresponding HR patches.</li>
</ul>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211024090426859.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211024090426859.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211024090426859.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211024090426859.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211024090426859.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211024090426859.png"/></p>
<blockquote>
<p>public benchmark test results and DIV2K validation results( PSNR(db)/SSIM), red indicates the best performance and the blue indicates the second best.</p>
</blockquote>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211024090558584.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211024090558584.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211024090558584.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211024090558584.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211024090558584.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211024090558584.png"/></p>
<h4 id="code-1">Code</h4>
<ul>
<li>single-scale EDSR network</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#a6e22e">@ARCH_REGISTRY.register</span>()
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">EDSR</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;&#34;&#34;EDSR network structure.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    Paper: Enhanced Deep Residual Networks for Single Image Super-Resolution.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    Ref git repo: https://github.com/thstkdgus35/EDSR-PyTorch
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    Args:
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        num_in_ch (int): Channel number of inputs.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        num_out_ch (int): Channel number of outputs.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        num_feat (int): Channel number of intermediate features.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            Default: 64.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        num_block (int): Block number in the trunk network. Default: 16.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        upscale (int): Upsampling factor. Support 2^n and 3.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            Default: 4.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        res_scale (float): Used to scale the residual in residual block.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            Default: 1.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        img_range (float): Image range. Default: 255.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        rgb_mean (tuple[float]): Image mean in RGB orders.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            Default: (0.4488, 0.4371, 0.4040), calculated from DIV2K dataset.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self,
</span></span><span style="display:flex;"><span>                 num_in_ch,
</span></span><span style="display:flex;"><span>                 num_out_ch,
</span></span><span style="display:flex;"><span>                 num_feat<span style="color:#f92672">=</span><span style="color:#ae81ff">64</span>,
</span></span><span style="display:flex;"><span>                 num_block<span style="color:#f92672">=</span><span style="color:#ae81ff">16</span>,
</span></span><span style="display:flex;"><span>                 upscale<span style="color:#f92672">=</span><span style="color:#ae81ff">4</span>,
</span></span><span style="display:flex;"><span>                 res_scale<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>,
</span></span><span style="display:flex;"><span>                 img_range<span style="color:#f92672">=</span><span style="color:#ae81ff">255.</span>,
</span></span><span style="display:flex;"><span>                 rgb_mean<span style="color:#f92672">=</span>(<span style="color:#ae81ff">0.4488</span>, <span style="color:#ae81ff">0.4371</span>, <span style="color:#ae81ff">0.4040</span>)):
</span></span><span style="display:flex;"><span>        super(EDSR, self)<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>img_range <span style="color:#f92672">=</span> img_range
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>mean <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>Tensor(rgb_mean)<span style="color:#f92672">.</span>view(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>conv_first <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Conv2d(num_in_ch, num_feat, <span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>body <span style="color:#f92672">=</span> make_layer(ResidualBlockNoBN, num_block, num_feat<span style="color:#f92672">=</span>num_feat, res_scale<span style="color:#f92672">=</span>res_scale, pytorch_init<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>conv_after_body <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Conv2d(num_feat, num_feat, <span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>upsample <span style="color:#f92672">=</span> Upsample(upscale, num_feat)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>conv_last <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Conv2d(num_feat, num_out_ch, <span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x):
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>mean <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>mean<span style="color:#f92672">.</span>type_as(x)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> (x <span style="color:#f92672">-</span> self<span style="color:#f92672">.</span>mean) <span style="color:#f92672">*</span> self<span style="color:#f92672">.</span>img_range
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>conv_first(x)
</span></span><span style="display:flex;"><span>        res <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>conv_after_body(self<span style="color:#f92672">.</span>body(x))
</span></span><span style="display:flex;"><span>        res <span style="color:#f92672">+=</span> x
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>conv_last(self<span style="color:#f92672">.</span>upsample(res))
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> x <span style="color:#f92672">/</span> self<span style="color:#f92672">.</span>img_range <span style="color:#f92672">+</span> self<span style="color:#f92672">.</span>mean
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> x
</span></span></code></pre></div><blockquote>
<p>Wang X, Chan K C K, Yu K, et al. Edvr: Video restoration with enhanced deformable convolutional networks[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops. 2019: 0-0.  <a href="chrome-extension://ikhdkkncnoglghljlkmcimlnlhkeamad/pdf-viewer/web/viewer.html?file=https%3A%2F%2Farxiv.org%2Fpdf%2F1905.02716.pdf"target="_blank" rel="external nofollow noopener noreferrer">pdf<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
</blockquote>
<hr>
<h3 id="paper-edvr">Paper: Edvr</h3>
<!-- raw HTML omitted -->
<h4 id="summary-1">Summary</h4>
<ol>
<li>devise a Pyramid, Cascading and Deformable alignment module, in which frame alignment is done at the feature level using deformable convolutions in a coarse-to-fine manner, to handle large motions.</li>
<li>propose a Temporal and Spatial Attention fusion module, in which attention is applied both temporally and spatially, so as to emphasize important features for subsequenct restoration.</li>
</ol>
<h4 id="proble-statement">Proble Statement</h4>
<ul>
<li>how to align multiple frames given large motions?</li>
<li>how to effectively fuse different frames with diverse motion and blur?</li>
</ul>
<h4 id="relative-work">Relative Work</h4>
<ul>
<li><strong>Video Super-Resolution</strong>: RCAN, DeepSR, BayesSR, VESPCN, SPMC, TOFlow, FRVSR, DUF, RBPN on three testing datasets, Vid4, Vimeo-90K-T, REDS4.</li>
<li><strong>Video Deblurring:</strong> DeepDeblur, DeblurGAN, SRNDEblur, DBN on the REDS4 dataset.</li>
</ul>
<h4 id="methods-1">Methods</h4>
<ul>
<li><strong>system overview</strong>:</li>
</ul>
<blockquote>
<p>Given 2N+1 consecutive low-quality frames $I_{t-N:t+N}$ , denote the middle frame $I_t$ as the reference frame and the other frames as neighboring frames, to estimate a high-quality reference frame $Q_t$​.</p>
<ul>
<li>each neighboring frame is aligned to the reference one by the PCD alignment module at the feature level.</li>
<li>TSA fusion module fuses image information of different frames.</li>
</ul>
</blockquote>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211025201730909.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211025201730909.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211025201730909.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211025201730909.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211025201730909.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211025201730909.png"/></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#a6e22e">@ARCH_REGISTRY.register</span>()
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">EDVR</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;&#34;&#34;EDVR network structure for video super-resolution.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    Now only support X4 upsampling factor.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    Paper:
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        EDVR: Video Restoration with Enhanced Deformable Convolutional Networks
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    Args:
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        num_in_ch (int): Channel number of input image. Default: 3.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        num_out_ch (int): Channel number of output image. Default: 3.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        num_feat (int): Channel number of intermediate features. Default: 64.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        num_frame (int): Number of input frames. Default: 5.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        deformable_groups (int): Deformable groups. Defaults: 8.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        num_extract_block (int): Number of blocks for feature extraction.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            Default: 5.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        num_reconstruct_block (int): Number of blocks for reconstruction.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            Default: 10.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        center_frame_idx (int): The index of center frame. Frame counting from
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            0. Default: Middle of input frames.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        hr_in (bool): Whether the input has high resolution. Default: False.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        with_predeblur (bool): Whether has predeblur module.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            Default: False.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        with_tsa (bool): Whether has TSA module. Default: True.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self,
</span></span><span style="display:flex;"><span>                 num_in_ch<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>,
</span></span><span style="display:flex;"><span>                 num_out_ch<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>,
</span></span><span style="display:flex;"><span>                 num_feat<span style="color:#f92672">=</span><span style="color:#ae81ff">64</span>,
</span></span><span style="display:flex;"><span>                 num_frame<span style="color:#f92672">=</span><span style="color:#ae81ff">5</span>,
</span></span><span style="display:flex;"><span>                 deformable_groups<span style="color:#f92672">=</span><span style="color:#ae81ff">8</span>,
</span></span><span style="display:flex;"><span>                 num_extract_block<span style="color:#f92672">=</span><span style="color:#ae81ff">5</span>,
</span></span><span style="display:flex;"><span>                 num_reconstruct_block<span style="color:#f92672">=</span><span style="color:#ae81ff">10</span>,
</span></span><span style="display:flex;"><span>                 center_frame_idx<span style="color:#f92672">=</span><span style="color:#66d9ef">None</span>,
</span></span><span style="display:flex;"><span>                 hr_in<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>,
</span></span><span style="display:flex;"><span>                 with_predeblur<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>,
</span></span><span style="display:flex;"><span>                 with_tsa<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>):
</span></span><span style="display:flex;"><span>        super(EDVR, self)<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> center_frame_idx <span style="color:#f92672">is</span> <span style="color:#66d9ef">None</span>:
</span></span><span style="display:flex;"><span>            self<span style="color:#f92672">.</span>center_frame_idx <span style="color:#f92672">=</span> num_frame <span style="color:#f92672">//</span> <span style="color:#ae81ff">2</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>            self<span style="color:#f92672">.</span>center_frame_idx <span style="color:#f92672">=</span> center_frame_idx
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>hr_in <span style="color:#f92672">=</span> hr_in
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>with_predeblur <span style="color:#f92672">=</span> with_predeblur
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>with_tsa <span style="color:#f92672">=</span> with_tsa
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># extract features for each frame</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> self<span style="color:#f92672">.</span>with_predeblur:
</span></span><span style="display:flex;"><span>            self<span style="color:#f92672">.</span>predeblur <span style="color:#f92672">=</span> PredeblurModule(num_feat<span style="color:#f92672">=</span>num_feat, hr_in<span style="color:#f92672">=</span>self<span style="color:#f92672">.</span>hr_in)
</span></span><span style="display:flex;"><span>            self<span style="color:#f92672">.</span>conv_1x1 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Conv2d(num_feat, num_feat, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>            self<span style="color:#f92672">.</span>conv_first <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Conv2d(num_in_ch, num_feat, <span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># extract pyramid features</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>feature_extraction <span style="color:#f92672">=</span> make_layer(ResidualBlockNoBN, num_extract_block, num_feat<span style="color:#f92672">=</span>num_feat)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>conv_l2_1 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Conv2d(num_feat, num_feat, <span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>conv_l2_2 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Conv2d(num_feat, num_feat, <span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>conv_l3_1 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Conv2d(num_feat, num_feat, <span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>conv_l3_2 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Conv2d(num_feat, num_feat, <span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># pcd and tsa module</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>pcd_align <span style="color:#f92672">=</span> PCDAlignment(num_feat<span style="color:#f92672">=</span>num_feat, deformable_groups<span style="color:#f92672">=</span>deformable_groups)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> self<span style="color:#f92672">.</span>with_tsa:
</span></span><span style="display:flex;"><span>            self<span style="color:#f92672">.</span>fusion <span style="color:#f92672">=</span> TSAFusion(num_feat<span style="color:#f92672">=</span>num_feat, num_frame<span style="color:#f92672">=</span>num_frame, center_frame_idx<span style="color:#f92672">=</span>self<span style="color:#f92672">.</span>center_frame_idx)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>            self<span style="color:#f92672">.</span>fusion <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Conv2d(num_frame <span style="color:#f92672">*</span> num_feat, num_feat, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># reconstruction</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>reconstruction <span style="color:#f92672">=</span> make_layer(ResidualBlockNoBN, num_reconstruct_block, num_feat<span style="color:#f92672">=</span>num_feat)
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># upsample</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>upconv1 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Conv2d(num_feat, num_feat <span style="color:#f92672">*</span> <span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>upconv2 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Conv2d(num_feat, <span style="color:#ae81ff">64</span> <span style="color:#f92672">*</span> <span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>pixel_shuffle <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>PixelShuffle(<span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>conv_hr <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Conv2d(<span style="color:#ae81ff">64</span>, <span style="color:#ae81ff">64</span>, <span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>conv_last <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Conv2d(<span style="color:#ae81ff">64</span>, <span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># activation function</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>lrelu <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>LeakyReLU(negative_slope<span style="color:#f92672">=</span><span style="color:#ae81ff">0.1</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x):
</span></span><span style="display:flex;"><span>        b, t, c, h, w <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>size()
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> self<span style="color:#f92672">.</span>hr_in:
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">assert</span> h <span style="color:#f92672">%</span> <span style="color:#ae81ff">16</span> <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span> <span style="color:#f92672">and</span> w <span style="color:#f92672">%</span> <span style="color:#ae81ff">16</span> <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span>, (<span style="color:#e6db74">&#39;The height and width must be multiple of 16.&#39;</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">assert</span> h <span style="color:#f92672">%</span> <span style="color:#ae81ff">4</span> <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span> <span style="color:#f92672">and</span> w <span style="color:#f92672">%</span> <span style="color:#ae81ff">4</span> <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span>, (<span style="color:#e6db74">&#39;The height and width must be multiple of 4.&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        x_center <span style="color:#f92672">=</span> x[:, self<span style="color:#f92672">.</span>center_frame_idx, :, :, :]<span style="color:#f92672">.</span>contiguous()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># extract features for each frame</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># L1</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> self<span style="color:#f92672">.</span>with_predeblur:
</span></span><span style="display:flex;"><span>            feat_l1 <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>conv_1x1(self<span style="color:#f92672">.</span>predeblur(x<span style="color:#f92672">.</span>view(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, c, h, w)))
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">if</span> self<span style="color:#f92672">.</span>hr_in:
</span></span><span style="display:flex;"><span>                h, w <span style="color:#f92672">=</span> h <span style="color:#f92672">//</span> <span style="color:#ae81ff">4</span>, w <span style="color:#f92672">//</span> <span style="color:#ae81ff">4</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>            feat_l1 <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>lrelu(self<span style="color:#f92672">.</span>conv_first(x<span style="color:#f92672">.</span>view(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, c, h, w)))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        feat_l1 <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>feature_extraction(feat_l1)
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># L2</span>
</span></span><span style="display:flex;"><span>        feat_l2 <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>lrelu(self<span style="color:#f92672">.</span>conv_l2_1(feat_l1))
</span></span><span style="display:flex;"><span>        feat_l2 <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>lrelu(self<span style="color:#f92672">.</span>conv_l2_2(feat_l2))
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># L3</span>
</span></span><span style="display:flex;"><span>        feat_l3 <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>lrelu(self<span style="color:#f92672">.</span>conv_l3_1(feat_l2))
</span></span><span style="display:flex;"><span>        feat_l3 <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>lrelu(self<span style="color:#f92672">.</span>conv_l3_2(feat_l3))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        feat_l1 <span style="color:#f92672">=</span> feat_l1<span style="color:#f92672">.</span>view(b, t, <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, h, w)
</span></span><span style="display:flex;"><span>        feat_l2 <span style="color:#f92672">=</span> feat_l2<span style="color:#f92672">.</span>view(b, t, <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, h <span style="color:#f92672">//</span> <span style="color:#ae81ff">2</span>, w <span style="color:#f92672">//</span> <span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>        feat_l3 <span style="color:#f92672">=</span> feat_l3<span style="color:#f92672">.</span>view(b, t, <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, h <span style="color:#f92672">//</span> <span style="color:#ae81ff">4</span>, w <span style="color:#f92672">//</span> <span style="color:#ae81ff">4</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># PCD alignment</span>
</span></span><span style="display:flex;"><span>        ref_feat_l <span style="color:#f92672">=</span> [  <span style="color:#75715e"># reference feature list</span>
</span></span><span style="display:flex;"><span>            feat_l1[:, self<span style="color:#f92672">.</span>center_frame_idx, :, :, :]<span style="color:#f92672">.</span>clone(), feat_l2[:, self<span style="color:#f92672">.</span>center_frame_idx, :, :, :]<span style="color:#f92672">.</span>clone(),
</span></span><span style="display:flex;"><span>            feat_l3[:, self<span style="color:#f92672">.</span>center_frame_idx, :, :, :]<span style="color:#f92672">.</span>clone()
</span></span><span style="display:flex;"><span>        ]
</span></span><span style="display:flex;"><span>        aligned_feat <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(t):
</span></span><span style="display:flex;"><span>            nbr_feat_l <span style="color:#f92672">=</span> [  <span style="color:#75715e"># neighboring feature list</span>
</span></span><span style="display:flex;"><span>                feat_l1[:, i, :, :, :]<span style="color:#f92672">.</span>clone(), feat_l2[:, i, :, :, :]<span style="color:#f92672">.</span>clone(), feat_l3[:, i, :, :, :]<span style="color:#f92672">.</span>clone()
</span></span><span style="display:flex;"><span>            ]
</span></span><span style="display:flex;"><span>            aligned_feat<span style="color:#f92672">.</span>append(self<span style="color:#f92672">.</span>pcd_align(nbr_feat_l, ref_feat_l))
</span></span><span style="display:flex;"><span>        aligned_feat <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>stack(aligned_feat, dim<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)  <span style="color:#75715e"># (b, t, c, h, w)  # 到这里每一张图片和reference都有一个对应关系</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> <span style="color:#f92672">not</span> self<span style="color:#f92672">.</span>with_tsa:
</span></span><span style="display:flex;"><span>            aligned_feat <span style="color:#f92672">=</span> aligned_feat<span style="color:#f92672">.</span>view(b, <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, h, w)
</span></span><span style="display:flex;"><span>        feat <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>fusion(aligned_feat)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        out <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>reconstruction(feat)
</span></span><span style="display:flex;"><span>        out <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>lrelu(self<span style="color:#f92672">.</span>pixel_shuffle(self<span style="color:#f92672">.</span>upconv1(out)))
</span></span><span style="display:flex;"><span>        out <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>lrelu(self<span style="color:#f92672">.</span>pixel_shuffle(self<span style="color:#f92672">.</span>upconv2(out)))
</span></span><span style="display:flex;"><span>        out <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>lrelu(self<span style="color:#f92672">.</span>conv_hr(out))
</span></span><span style="display:flex;"><span>        out <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>conv_last(out)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> self<span style="color:#f92672">.</span>hr_in:
</span></span><span style="display:flex;"><span>            base <span style="color:#f92672">=</span> x_center
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>            base <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>interpolate(x_center, scale_factor<span style="color:#f92672">=</span><span style="color:#ae81ff">4</span>, mode<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;bilinear&#39;</span>, align_corners<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>        out <span style="color:#f92672">+=</span> base
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> out
</span></span></code></pre></div><ul>
<li><strong>Alignment with Pyramid, Cascading and Deformable convolution:</strong></li>
</ul>
<blockquote>
<p>To generate feature $F^l_{t+i}$​​ at the l-th level, use strided convolution filters to downsample the features at the (l-1)-th pyramid level by a factor of 2, obtaining L-level pyramids of feature representation. At the l-th level, offsets and aligned features are predicted also with the *2 upsampled offsets and aligned features from the upper (l+1)-th level.  (<code>下面这个公式没有看懂</code>)</p>
</blockquote>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211025210506574.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211025210506574.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211025210506574.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211025210506574.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211025210506574.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211025210506574.png"/></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211025221603992.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211025221603992.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211025221603992.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211025221603992.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211025221603992.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211025221603992.png"/></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">PCDAlignment</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;&#34;&#34;Alignment module using Pyramid, Cascading and Deformable convolution
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    (PCD). It is used in EDVR.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    Ref:
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        EDVR: Video Restoration with Enhanced Deformable Convolutional Networks
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    Args:
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        num_feat (int): Channel number of middle features. Default: 64.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        deformable_groups (int): Deformable groups. Defaults: 8.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, num_feat<span style="color:#f92672">=</span><span style="color:#ae81ff">64</span>, deformable_groups<span style="color:#f92672">=</span><span style="color:#ae81ff">8</span>):
</span></span><span style="display:flex;"><span>        super(PCDAlignment, self)<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Pyramid has three levels:</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># L3: level 3, 1/4 spatial size</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># L2: level 2, 1/2 spatial size</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># L1: level 1, original spatial size</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>offset_conv1 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>ModuleDict()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>offset_conv2 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>ModuleDict()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>offset_conv3 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>ModuleDict()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>dcn_pack <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>ModuleDict()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>feat_conv <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>ModuleDict()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Pyramids</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">0</span>, <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>):
</span></span><span style="display:flex;"><span>            level <span style="color:#f92672">=</span> <span style="color:#e6db74">f</span><span style="color:#e6db74">&#39;l</span><span style="color:#e6db74">{</span>i<span style="color:#e6db74">}</span><span style="color:#e6db74">&#39;</span>
</span></span><span style="display:flex;"><span>            self<span style="color:#f92672">.</span>offset_conv1[level] <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Conv2d(num_feat <span style="color:#f92672">*</span> <span style="color:#ae81ff">2</span>, num_feat, <span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">if</span> i <span style="color:#f92672">==</span> <span style="color:#ae81ff">3</span>:
</span></span><span style="display:flex;"><span>                self<span style="color:#f92672">.</span>offset_conv2[level] <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Conv2d(num_feat, num_feat, <span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>                self<span style="color:#f92672">.</span>offset_conv2[level] <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Conv2d(num_feat <span style="color:#f92672">*</span> <span style="color:#ae81ff">2</span>, num_feat, <span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>                self<span style="color:#f92672">.</span>offset_conv3[level] <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Conv2d(num_feat, num_feat, <span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>            self<span style="color:#f92672">.</span>dcn_pack[level] <span style="color:#f92672">=</span> DCNv2Pack(num_feat, num_feat, <span style="color:#ae81ff">3</span>, padding<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, deformable_groups<span style="color:#f92672">=</span>deformable_groups)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">if</span> i <span style="color:#f92672">&lt;</span> <span style="color:#ae81ff">3</span>:
</span></span><span style="display:flex;"><span>                self<span style="color:#f92672">.</span>feat_conv[level] <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Conv2d(num_feat <span style="color:#f92672">*</span> <span style="color:#ae81ff">2</span>, num_feat, <span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Cascading dcn</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>cas_offset_conv1 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Conv2d(num_feat <span style="color:#f92672">*</span> <span style="color:#ae81ff">2</span>, num_feat, <span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>cas_offset_conv2 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Conv2d(num_feat, num_feat, <span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>cas_dcnpack <span style="color:#f92672">=</span> DCNv2Pack(num_feat, num_feat, <span style="color:#ae81ff">3</span>, padding<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, deformable_groups<span style="color:#f92672">=</span>deformable_groups)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>upsample <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Upsample(scale_factor<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>, mode<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;bilinear&#39;</span>, align_corners<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>lrelu <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>LeakyReLU(negative_slope<span style="color:#f92672">=</span><span style="color:#ae81ff">0.1</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, nbr_feat_l, ref_feat_l):
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;&#34;&#34;Align neighboring frame features to the reference frame features.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        Args:
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            nbr_feat_l (list[Tensor]): Neighboring feature list. It
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">                contains three pyramid levels (L1, L2, L3),
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">                each with shape (b, c, h, w).
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            ref_feat_l (list[Tensor]): Reference feature list. It
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">                contains three pyramid levels (L1, L2, L3),
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">                each with shape (b, c, h, w).
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        Returns:
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            Tensor: Aligned features.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Pyramids</span>
</span></span><span style="display:flex;"><span>        upsampled_offset, upsampled_feat <span style="color:#f92672">=</span> <span style="color:#66d9ef">None</span>, <span style="color:#66d9ef">None</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">0</span>, <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>):
</span></span><span style="display:flex;"><span>            level <span style="color:#f92672">=</span> <span style="color:#e6db74">f</span><span style="color:#e6db74">&#39;l</span><span style="color:#e6db74">{</span>i<span style="color:#e6db74">}</span><span style="color:#e6db74">&#39;</span>
</span></span><span style="display:flex;"><span>            offset <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>cat([nbr_feat_l[i <span style="color:#f92672">-</span> <span style="color:#ae81ff">1</span>], ref_feat_l[i <span style="color:#f92672">-</span> <span style="color:#ae81ff">1</span>]], dim<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>            offset <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>lrelu(self<span style="color:#f92672">.</span>offset_conv1[level](offset))
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">if</span> i <span style="color:#f92672">==</span> <span style="color:#ae81ff">3</span>:
</span></span><span style="display:flex;"><span>                offset <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>lrelu(self<span style="color:#f92672">.</span>offset_conv2[level](offset))
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>                offset <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>lrelu(self<span style="color:#f92672">.</span>offset_conv2[level](torch<span style="color:#f92672">.</span>cat([offset, upsampled_offset], dim<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)))
</span></span><span style="display:flex;"><span>                offset <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>lrelu(self<span style="color:#f92672">.</span>offset_conv3[level](offset))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            feat <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>dcn_pack[level](nbr_feat_l[i <span style="color:#f92672">-</span> <span style="color:#ae81ff">1</span>], offset)
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">if</span> i <span style="color:#f92672">&lt;</span> <span style="color:#ae81ff">3</span>:
</span></span><span style="display:flex;"><span>                feat <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>feat_conv[level](torch<span style="color:#f92672">.</span>cat([feat, upsampled_feat], dim<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>))
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">if</span> i <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">1</span>:
</span></span><span style="display:flex;"><span>                feat <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>lrelu(feat)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">if</span> i <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">1</span>:  <span style="color:#75715e"># upsample offset and features</span>
</span></span><span style="display:flex;"><span>                <span style="color:#75715e"># x2: when we upsample the offset, we should also enlarge</span>
</span></span><span style="display:flex;"><span>                <span style="color:#75715e"># the magnitude.</span>
</span></span><span style="display:flex;"><span>                upsampled_offset <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>upsample(offset) <span style="color:#f92672">*</span> <span style="color:#ae81ff">2</span>
</span></span><span style="display:flex;"><span>                upsampled_feat <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>upsample(feat)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Cascading</span>
</span></span><span style="display:flex;"><span>        offset <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>cat([feat, ref_feat_l[<span style="color:#ae81ff">0</span>]], dim<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        offset <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>lrelu(self<span style="color:#f92672">.</span>cas_offset_conv2(self<span style="color:#f92672">.</span>lrelu(self<span style="color:#f92672">.</span>cas_offset_conv1(offset))))
</span></span><span style="display:flex;"><span>        feat <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>lrelu(self<span style="color:#f92672">.</span>cas_dcnpack(feat, offset))
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> feat
</span></span></code></pre></div><ul>
<li><strong>Fusion with Temporal and Spatial Attention</strong></li>
</ul>
<blockquote>
<p>Inter-frame temporal relation and intra-frame spatial relation are critical in fusion:</p>
<ul>
<li><code>different neighboring frames are not equally informative</code> due to occlusion, blurry regions and parallax problems.</li>
<li><code>misalignment and unalignment</code> arising from the preceding alignment stage adversely affect the subsequent reconstruction performance.</li>
</ul>
<p>propose TSA fusion module to assign pixel-level aggregation weights on each frame, adopt temporal and spatial attentions during the fusion process.</p>
<ul>
<li>temporal attention is to compute frame similarity in an embedding space. In an embedding space, a neighboring frame that is more similar to the reference one, should be paid more attention.</li>
</ul>
</blockquote>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211025212147206.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211025212147206.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211025212147206.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211025212147206.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211025212147206.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211025212147206.png"/></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211025211618576.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211025211618576.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211025211618576.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211025211618576.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211025211618576.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211025211618576.png"/></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">TSAFusion</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;&#34;&#34;Temporal Spatial Attention (TSA) fusion module.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    Temporal: Calculate the correlation between center frame and
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        neighboring frames;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    Spatial: It has 3 pyramid levels, the attention is similar to SFT.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        (SFT: Recovering realistic texture in image super-resolution by deep
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            spatial feature transform.)
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    Args:
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        num_feat (int): Channel number of middle features. Default: 64.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        num_frame (int): Number of frames. Default: 5.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        center_frame_idx (int): The index of center frame. Default: 2.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, num_feat<span style="color:#f92672">=</span><span style="color:#ae81ff">64</span>, num_frame<span style="color:#f92672">=</span><span style="color:#ae81ff">5</span>, center_frame_idx<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>):
</span></span><span style="display:flex;"><span>        super(TSAFusion, self)<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>center_frame_idx <span style="color:#f92672">=</span> center_frame_idx
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># temporal attention (before fusion conv)</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>temporal_attn1 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Conv2d(num_feat, num_feat, <span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>temporal_attn2 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Conv2d(num_feat, num_feat, <span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>feat_fusion <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Conv2d(num_frame <span style="color:#f92672">*</span> num_feat, num_feat, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># spatial attention (after fusion conv)</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>max_pool <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>MaxPool2d(<span style="color:#ae81ff">3</span>, stride<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>, padding<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>avg_pool <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>AvgPool2d(<span style="color:#ae81ff">3</span>, stride<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>, padding<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>spatial_attn1 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Conv2d(num_frame <span style="color:#f92672">*</span> num_feat, num_feat, <span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>spatial_attn2 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Conv2d(num_feat <span style="color:#f92672">*</span> <span style="color:#ae81ff">2</span>, num_feat, <span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>spatial_attn3 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Conv2d(num_feat, num_feat, <span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>spatial_attn4 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Conv2d(num_feat, num_feat, <span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>spatial_attn5 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Conv2d(num_feat, num_feat, <span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>spatial_attn_l1 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Conv2d(num_feat, num_feat, <span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>spatial_attn_l2 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Conv2d(num_feat <span style="color:#f92672">*</span> <span style="color:#ae81ff">2</span>, num_feat, <span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>spatial_attn_l3 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Conv2d(num_feat, num_feat, <span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>spatial_attn_add1 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Conv2d(num_feat, num_feat, <span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>spatial_attn_add2 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Conv2d(num_feat, num_feat, <span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>lrelu <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>LeakyReLU(negative_slope<span style="color:#f92672">=</span><span style="color:#ae81ff">0.1</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>upsample <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Upsample(scale_factor<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>, mode<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;bilinear&#39;</span>, align_corners<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, aligned_feat):
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        Args:
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            aligned_feat (Tensor): Aligned features with shape (b, t, c, h, w).
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        Returns:
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            Tensor: Features after TSA with the shape (b, c, h, w).
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>        b, t, c, h, w <span style="color:#f92672">=</span> aligned_feat<span style="color:#f92672">.</span>size()
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># temporal attention</span>
</span></span><span style="display:flex;"><span>        embedding_ref <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>temporal_attn1(aligned_feat[:, self<span style="color:#f92672">.</span>center_frame_idx, :, :, :]<span style="color:#f92672">.</span>clone())
</span></span><span style="display:flex;"><span>        embedding <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>temporal_attn2(aligned_feat<span style="color:#f92672">.</span>view(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, c, h, w))
</span></span><span style="display:flex;"><span>        embedding <span style="color:#f92672">=</span> embedding<span style="color:#f92672">.</span>view(b, t, <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, h, w)  <span style="color:#75715e"># (b, t, c, h, w)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        corr_l <span style="color:#f92672">=</span> []  <span style="color:#75715e"># correlation list</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(t):
</span></span><span style="display:flex;"><span>            emb_neighbor <span style="color:#f92672">=</span> embedding[:, i, :, :, :]
</span></span><span style="display:flex;"><span>            corr <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>sum(emb_neighbor <span style="color:#f92672">*</span> embedding_ref, <span style="color:#ae81ff">1</span>)  <span style="color:#75715e"># (b, h, w)</span>
</span></span><span style="display:flex;"><span>            corr_l<span style="color:#f92672">.</span>append(corr<span style="color:#f92672">.</span>unsqueeze(<span style="color:#ae81ff">1</span>))  <span style="color:#75715e"># (b, 1, h, w)</span>
</span></span><span style="display:flex;"><span>        corr_prob <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>sigmoid(torch<span style="color:#f92672">.</span>cat(corr_l, dim<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>))  <span style="color:#75715e"># (b, t, h, w)</span>
</span></span><span style="display:flex;"><span>        corr_prob <span style="color:#f92672">=</span> corr_prob<span style="color:#f92672">.</span>unsqueeze(<span style="color:#ae81ff">2</span>)<span style="color:#f92672">.</span>expand(b, t, c, h, w)
</span></span><span style="display:flex;"><span>        corr_prob <span style="color:#f92672">=</span> corr_prob<span style="color:#f92672">.</span>contiguous()<span style="color:#f92672">.</span>view(b, <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, h, w)  <span style="color:#75715e"># (b, t*c, h, w)</span>
</span></span><span style="display:flex;"><span>        aligned_feat <span style="color:#f92672">=</span> aligned_feat<span style="color:#f92672">.</span>view(b, <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, h, w) <span style="color:#f92672">*</span> corr_prob
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># fusion</span>
</span></span><span style="display:flex;"><span>        feat <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>lrelu(self<span style="color:#f92672">.</span>feat_fusion(aligned_feat))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># spatial attention</span>
</span></span><span style="display:flex;"><span>        attn <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>lrelu(self<span style="color:#f92672">.</span>spatial_attn1(aligned_feat))
</span></span><span style="display:flex;"><span>        attn_max <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>max_pool(attn)
</span></span><span style="display:flex;"><span>        attn_avg <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>avg_pool(attn)
</span></span><span style="display:flex;"><span>        attn <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>lrelu(self<span style="color:#f92672">.</span>spatial_attn2(torch<span style="color:#f92672">.</span>cat([attn_max, attn_avg], dim<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)))
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># pyramid levels</span>
</span></span><span style="display:flex;"><span>        attn_level <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>lrelu(self<span style="color:#f92672">.</span>spatial_attn_l1(attn))
</span></span><span style="display:flex;"><span>        attn_max <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>max_pool(attn_level)
</span></span><span style="display:flex;"><span>        attn_avg <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>avg_pool(attn_level)
</span></span><span style="display:flex;"><span>        attn_level <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>lrelu(self<span style="color:#f92672">.</span>spatial_attn_l2(torch<span style="color:#f92672">.</span>cat([attn_max, attn_avg], dim<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)))
</span></span><span style="display:flex;"><span>        attn_level <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>lrelu(self<span style="color:#f92672">.</span>spatial_attn_l3(attn_level))
</span></span><span style="display:flex;"><span>        attn_level <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>upsample(attn_level)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        attn <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>lrelu(self<span style="color:#f92672">.</span>spatial_attn3(attn)) <span style="color:#f92672">+</span> attn_level
</span></span><span style="display:flex;"><span>        attn <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>lrelu(self<span style="color:#f92672">.</span>spatial_attn4(attn))
</span></span><span style="display:flex;"><span>        attn <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>upsample(attn)
</span></span><span style="display:flex;"><span>        attn <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>spatial_attn5(attn)
</span></span><span style="display:flex;"><span>        attn_add <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>spatial_attn_add2(self<span style="color:#f92672">.</span>lrelu(self<span style="color:#f92672">.</span>spatial_attn_add1(attn)))
</span></span><span style="display:flex;"><span>        attn <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>sigmoid(attn)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># after initialization, * 2 makes (attn * 2) to be close to 1.</span>
</span></span><span style="display:flex;"><span>        feat <span style="color:#f92672">=</span> feat <span style="color:#f92672">*</span> attn <span style="color:#f92672">*</span> <span style="color:#ae81ff">2</span> <span style="color:#f92672">+</span> attn_add
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> feat
</span></span></code></pre></div><h4 id="evaluation-1">Evaluation</h4>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211025213641309.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211025213641309.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211025213641309.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211025213641309.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211025213641309.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211025213641309.png"/></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211025213721167.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211025213721167.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211025213721167.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211025213721167.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211025213721167.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211025213721167.png"/></p>
<blockquote>
<p>Timofte R, Rothe R, Van Gool L. Seven ways to improve example-based single image super resolution[C] //Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2016: 1865-1873.  <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=7780575&amp;tag=1"target="_blank" rel="external nofollow noopener noreferrer">pdf<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
</blockquote>
<hr>
<h3 id="paper-seven-ways">Paper: Seven ways</h3>
<!-- raw HTML omitted -->
<h4 id="summary-2">Summary</h4>
<ol>
<li>present seven techniques that everybody should know to improve example-based single image supre resolution.</li>
</ol>
<h4 id="ways">Ways</h4>
<ul>
<li>Augmentation of training data</li>
</ul>
<blockquote>
<p>If we rotate the original images by 90,180,270, and flip them upside-down, we get images without altered content. Using an interpolation for other rotation angles can corrupt edges and impact the performance.</p>
</blockquote>
<ul>
<li>large dictionary and hierarchical search</li>
</ul>
<blockquote>
<p>if the dictionary size(basis of samples/anchoring points) is increased, the performance for sparse coding or anchoed methods improves, as the learned model generalizes better.</p>
</blockquote>
<h3 id="project">Project</h3>
<h4 id="1-image-super-resolution-3khttpsgithubcomidealoimage-super-resolution">1. <strong><a href="https://github.com/idealo/image-super-resolution"target="_blank" rel="external nofollow noopener noreferrer">image-super-resolution 3k<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></strong></h4>
<p>This project contains <code>Keras implementations of different Residual Dense Networks for Single Image Super-Resolution (ISR)</code> as well as scripts to train these networks using content and adversarial loss components.</p>
<p>The implemented networks include:</p>
<ul>
<li>The super-scaling Residual Dense Network described in <a href="https://arxiv.org/abs/1802.08797"target="_blank" rel="external nofollow noopener noreferrer">Residual Dense Network for Image Super-Resolution<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a> (Zhang et al. 2018)</li>
<li>The super-scaling Residual in Residual Dense Network described in <a href="https://arxiv.org/abs/1809.00219"target="_blank" rel="external nofollow noopener noreferrer">ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a> (Wang et al. 2018)</li>
<li>A multi-output version of the Keras VGG19 network for deep features extraction used in the perceptual loss</li>
<li>A custom discriminator network based on the one described in <a href="https://arxiv.org/abs/1609.04802"target="_blank" rel="external nofollow noopener noreferrer">Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a> (SRGANS, Ledig et al. 2017)</li>
</ul>
<p>Read the full documentation at: <a href="https://idealo.github.io/image-super-resolution/"target="_blank" rel="external nofollow noopener noreferrer">https://idealo.github.io/image-super-resolution/<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>.</p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211017171103641.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211017171103641.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211017171103641.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211017171103641.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211017171103641.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211017171103641.png"/></p>
<h4 id="2-waifu2x-extension-guihttpsgithubcomaaronfeng753waifu2x-extension-gui-5k">2. <a href="https://github.com/AaronFeng753/Waifu2x-Extension-GUI"target="_blank" rel="external nofollow noopener noreferrer">Waifu2x-Extension-GUI<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a> 5k</h4>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211017171647835.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211017171647835.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211017171647835.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211017171647835.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211017171647835.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211017171647835.png"/></p>
<h3 id="survey">Survey</h3>
<blockquote>
<p>Anwar S, Khan S, Barnes N. A deep journey into super-resolution: A survey[J]. ACM Computing Surveys (CSUR), 2020, 53(3): 1-34. cite 107 <a href="chrome-extension://ikhdkkncnoglghljlkmcimlnlhkeamad/pdf-viewer/web/viewer.html?file=https%3A%2F%2Farxiv.org%2Fpdf%2F1904.07523.pdf"target="_blank" rel="external nofollow noopener noreferrer">pdf<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
</blockquote>
<blockquote>
<p>Liu A, Liu Y, Gu J, et al. Blind image super-resolution: A survey and beyond[J]. arXiv preprint arXiv:2107.03055, 2021. cite 2, <a href="chrome-extension://ikhdkkncnoglghljlkmcimlnlhkeamad/pdf-viewer/web/viewer.html?file=https%3A%2F%2Farxiv.org%2Fpdf%2F2107.03055.pdf"target="_blank" rel="external nofollow noopener noreferrer">pdf<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
</blockquote>
</div>
<div class="post-footer" id="post-footer">
  <div class="post-info">
    <div class="post-info-line">
      <div class="post-info-mod">
        <span title=2023-12-31&#32;13:31:55>更新于 2023-12-31&nbsp;</span>
      </div><div class="post-info-license">
          <span><a rel="license external nofollow noopener noreferrer" href="https://creativecommons.org/licenses/by-nc/4.0/" target="_blank">CC BY-NC 4.0</a></span>
        </div></div>
    <div class="post-info-line">
      <div class="post-info-md"><span><a href="/superresolution/index.md" title="阅读原始文档" class="link-to-markdown">阅读原始文档</a></span><span><a href="https://liudongdong1.github.io/edit/master/content/posts%5c%e8%a7%86%e8%a7%89%e8%bf%90%e5%8a%a8%5cvideo_understand%5cSuperResolution.md" title="编辑此页"target="_blank" rel="external nofollow noopener noreferrer" class="link-to-edit">编辑此页</a></span></div>
      <div class="post-info-share">
        <span><a href="javascript:void(0);" title="分享到 Twitter" data-sharer="twitter" data-url="https://liudongdong1.github.io/superresolution/" data-title="SuperResolution" data-hashtags="VideoAnalyse"><i class="fa-brands fa-twitter fa-fw" aria-hidden="true"></i></a>
  <a href="javascript:void(0);" title="分享到 Facebook" data-sharer="facebook" data-url="https://liudongdong1.github.io/superresolution/" data-hashtag="VideoAnalyse"><i class="fa-brands fa-facebook-square fa-fw" aria-hidden="true"></i></a>
  <a href="javascript:void(0);" title="分享到 微博" data-sharer="weibo" data-url="https://liudongdong1.github.io/superresolution/" data-title="SuperResolution" data-image="https://cdn.stocksnap.io/img-thumbs/280h/QSCRVBUU2G.jpg"><i class="fa-brands fa-weibo fa-fw" aria-hidden="true"></i></a>
  </span>
      </div>
    </div>
  </div>

  <div class="post-info-more">
    <section class="post-tags"><i class="fa-solid fa-tags fa-fw" aria-hidden="true"></i>&nbsp;<a href="/tags/videoanalyse/">VideoAnalyse</a></section>
    <section>
      <span><a href="javascript:void(0);" onclick="window.history.back();">返回</a></span>&nbsp;|&nbsp;<span><a href="/">主页</a></span>
    </section>
  </div>

  <div class="post-nav"><a href="/compreface/" class="prev" rel="prev" title="Compreface"><i class="fa-solid fa-angle-left fa-fw" aria-hidden="true"></i>Compreface</a>
      <a href="/robot_grasp/" class="next" rel="next" title="robot_grasp">robot_grasp<i class="fa-solid fa-angle-right fa-fw" aria-hidden="true"></i></a></div>
</div>
</article></main><footer class="footer">
    <div class="footer-container"><div class="footer-line powered">由 <a href="https://gohugo.io/" target="_blank" rel="external nofollow noopener noreferrer" title="Hugo 0.118.2">Hugo</a> 强力驱动 | 主题 - <a href="https://github.com/hugo-fixit/FixIt" target="_blank" rel="external" title="FixIt v0.2.17-RC"><img class="fixit-icon" src="/fixit.min.svg" alt="FixIt logo" />&nbsp;FixIt</a>
        </div><div class="footer-line copyright" itemscope itemtype="http://schema.org/CreativeWork"><i class="fa-regular fa-copyright fa-fw" aria-hidden="true"></i>
            <span itemprop="copyrightYear">2020 - 2024</span><span class="author" itemprop="copyrightHolder">
              <a href="https://liudongdong1.github.io/"target="_blank" rel="external nofollow noopener noreferrer">LiuDongdong</a></span><span class="license footer-divider"><a rel="license external nofollow noopener noreferrer" href="https://creativecommons.org/licenses/by-nc/4.0/" target="_blank">CC BY-NC 4.0</a></span></div><div class="footer-line statistics"><span class="site-time" title='网站运行中 ...'><i class="fa-solid fa-heartbeat fa-fw animate-icon" aria-hidden="true"></i>&nbsp;<span class="run-times">网站运行中 ...</span></span></div><div class="footer-line ibruce">
          <span id="busuanzi_container_site_uv" title='总访客数'><i class="fa-regular fa-user fa-fw" aria-hidden="true"></i>&nbsp;<span id="busuanzi_value_site_uv"><i class="fa-solid fa-spinner fa-spin fa-fw" aria-hidden="true"></i></span></span><span id="busuanzi_container_site_pv" class="footer-divider" title='总访问量'><i class="fa-regular fa-eye fa-fw" aria-hidden="true"></i>&nbsp;<span id="busuanzi_value_site_pv"><i class="fa-solid fa-spinner fa-spin fa-fw" aria-hidden="true"></i></span></span>
        </div></div>
  </footer></div><div class="widgets"><div class="fixed-buttons animate__faster d-none"><div class="fixed-button back-to-top" role="button" aria-label="回到顶部"><i class="fa-solid fa-arrow-up fa-fw" aria-hidden="true"></i><span class="variant-numeric">0%</span>
        </div></div><a href="https://liudongdong1.github.io/" title="在 GitHub 上查看源代码"target="_blank" rel="external nofollow" class="github-corner right d-none-mobile"><svg viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><div id="mask"></div><div class="reading-progress-bar" style="left: 0;top: 0;--bg-progress: #0076ff;--bg-progress-dark: #fff;"></div><noscript>
    <div class="noscript-warning">FixIt 主题在启用 JavaScript 的情况下效果最佳。</div>
  </noscript>
</div><link rel="stylesheet" href="/lib/katex/katex.min.css"><link rel="stylesheet" href="/lib/cookieconsent/cookieconsent.min.css"><script src="/lib/autocomplete/autocomplete.min.js" defer></script><script src="/lib/algoliasearch/algoliasearch-lite.umd.min.js" defer></script><script src="/lib/lazysizes/lazysizes.min.js" async defer></script><script src="/lib/sharer/sharer.min.js" async defer></script><script src="/lib/typeit/index.umd.js" defer></script><script src="/lib/katex/katex.min.js" defer></script><script src="/lib/katex/auto-render.min.js" defer></script><script src="/lib/katex/copy-tex.min.js" defer></script><script src="/lib/katex/mhchem.min.js" defer></script><script src="/lib/cookieconsent/cookieconsent.min.js" defer></script><script src="/lib/pangu/pangu.min.js" defer></script><script src="/lib/cell-watermark/watermark.min.js" defer></script><script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async defer></script><script>window.config={"code":{"copyTitle":"复制到剪贴板","editLockTitle":"锁定可编辑代码块","editUnLockTitle":"解锁可编辑代码块","editable":true,"maxShownLines":10},"comment":{"enable":false},"cookieconsent":{"content":{"dismiss":"同意","link":"了解更多","message":"本网站使用 Cookies 来改善您的浏览体验。"},"enable":true,"palette":{"button":{"background":"#f0f0f0"},"popup":{"background":"#1aa3ff"}},"theme":"edgeless"},"data":{"typeit-header-subtitle-desktop":"\u003cspan style='font-family: MMT,\"沐目体\";'\u003e吾日三省吾身\u003c/span\u003e","typeit-header-subtitle-mobile":"\u003cspan style='font-family: MMT,\"沐目体\";'\u003e吾日三省吾身\u003c/span\u003e"},"enablePWA":true,"enablePangu":true,"math":{"delimiters":[{"display":true,"left":"$$","right":"$$"},{"display":true,"left":"\\[","right":"\\]"},{"display":true,"left":"\\begin{equation}","right":"\\end{equation}"},{"display":true,"left":"\\begin{equation*}","right":"\\end{equation*}"},{"display":true,"left":"\\begin{align}","right":"\\end{align}"},{"display":true,"left":"\\begin{align*}","right":"\\end{align*}"},{"display":true,"left":"\\begin{alignat}","right":"\\end{alignat}"},{"display":true,"left":"\\begin{alignat*}","right":"\\end{alignat*}"},{"display":true,"left":"\\begin{gather}","right":"\\end{gather}"},{"display":true,"left":"\\begin{CD}","right":"\\end{CD}"},{"display":false,"left":"$","right":"$"},{"display":false,"left":"\\(","right":"\\)"}],"strict":false},"search":{"algoliaAppID":"2R1K9SKLQZ","algoliaIndex":"index.zh-cn","algoliaSearchKey":"4a226aa1c5c98d6859e4d1386adb2bc7","highlightTag":"em","maxResultLength":10,"noResultsFound":"没有找到结果","snippetLength":50,"type":"algolia"},"siteTime":"2020-12-18T16:15:22+08:00","typeit":{"cursorChar":"|","cursorSpeed":1000,"data":{"typeit-header-subtitle-desktop":["typeit-header-subtitle-desktop"],"typeit-header-subtitle-mobile":["typeit-header-subtitle-mobile"]},"duration":-1,"speed":100},"watermark":{"appendto":".wrapper\u003emain","colspacing":30,"content":"\u003cimg class=\"fixit-icon\" src=\"/fixit.min.svg\" alt=\"FixIt logo\" /\u003e FixIt 主题","enable":true,"fontfamily":"inherit","fontsize":0.85,"height":21,"opacity":0.0125,"rotate":15,"rowspacing":60,"width":150}};</script><script src="/js/theme.min.js" defer></script><script src="/js/custom.min.js" defer></script></body>
</html>
