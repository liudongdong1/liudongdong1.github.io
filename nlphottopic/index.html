<!DOCTYPE html>
<html itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">
  <head>
    
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
    <meta name="robots" content="noodp" />
    <title>NLPHotTopic - DAY By DAY</title><meta name="author" content="LiuDongdong">
<meta name="author-link" content="https://liudongdong1.github.io/">
<meta name="description" content="This week i get a summary knowledge of NLP, and learn some direction for further learning. And in this blog, i will record what i learned this weak by searching some information on Internet, the content is organized as follows: the Preparatory knowledge which need to be master in the following years, and some direction in NLP areas from model sides, application sides and the scene task, and some paper" /><meta name="keywords" content='Semantic Parsing' /><meta itemprop="name" content="NLPHotTopic">
<meta itemprop="description" content="This week i get a summary knowledge of NLP, and learn some direction for further learning. And in this blog, i will record what i learned this weak by searching some information on Internet, the content is organized as follows: the Preparatory knowledge which need to be master in the following years, and some direction in NLP areas from model sides, application sides and the scene task, and some paper"><meta itemprop="datePublished" content="2020-08-15T07:56:09+00:00" />
<meta itemprop="dateModified" content="2023-09-28T22:16:16+08:00" />
<meta itemprop="wordCount" content="3253"><meta itemprop="image" content="/logo.png"/>
<meta itemprop="keywords" content="Semantic Parsing," /><meta property="og:title" content="NLPHotTopic" />
<meta property="og:description" content="This week i get a summary knowledge of NLP, and learn some direction for further learning. And in this blog, i will record what i learned this weak by searching some information on Internet, the content is organized as follows: the Preparatory knowledge which need to be master in the following years, and some direction in NLP areas from model sides, application sides and the scene task, and some paper" />
<meta property="og:type" content="article" />
<meta property="og:url" content="liudongdong1.github.io/nlphottopic/" /><meta property="og:image" content="/logo.png"/><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2020-08-15T07:56:09+00:00" />
<meta property="article:modified_time" content="2023-09-28T22:16:16+08:00" />
<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="/logo.png"/>

<meta name="twitter:title" content="NLPHotTopic"/>
<meta name="twitter:description" content="This week i get a summary knowledge of NLP, and learn some direction for further learning. And in this blog, i will record what i learned this weak by searching some information on Internet, the content is organized as follows: the Preparatory knowledge which need to be master in the following years, and some direction in NLP areas from model sides, application sides and the scene task, and some paper"/>
<meta name="application-name" content="DAY By DAY">
<meta name="apple-mobile-web-app-title" content="DAY By DAY"><meta name="theme-color" data-light="#f8f8f8" data-dark="#252627" content="#f8f8f8"><meta name="msapplication-TileColor" content="#da532c"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" />
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"><link rel="manifest" href="/site.webmanifest"><link rel="canonical" href="liudongdong1.github.io/nlphottopic/" /><link rel="prev" href="liudongdong1.github.io/nlprelative/" /><link rel="next" href="liudongdong1.github.io/jdk_tomcat/" /><link rel="stylesheet" href="/liudongdong1.github.io/css/style.min.css"><link rel="stylesheet" href="/liudongdong1.github.io/lib/fontawesome-free/all.min.css"><link rel="stylesheet" href="/liudongdong1.github.io/lib/animate/animate.min.css"><script type="application/ld+json">
  {
    "@context": "http://schema.org",
    "@type": "BlogPosting",
    "headline": "NLPHotTopic",
    "inLanguage": "zh-CN",
    "mainEntityOfPage": {
      "@type": "WebPage",
      "@id": "liudongdong1.github.io\/nlphottopic\/"
    },"genre": "posts","keywords": "Semantic Parsing","wordcount":  3253 ,
    "url": "liudongdong1.github.io\/nlphottopic\/","datePublished": "2020-08-15T07:56:09+00:00","dateModified": "2023-09-28T22:16:16+08:00","license": "This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.","publisher": {
      "@type": "Organization",
      "name": "LiuDongdong","logo": "\/images\/person.png"},"author": {
        "@type": "Person",
        "name": "liudongdong1"
      },"description": ""
  }
  </script></head>
  <body data-header-desktop="auto" data-header-mobile="auto"><script>(window.localStorage?.getItem('theme') ? localStorage.getItem('theme') === 'dark' : ('auto' === 'auto' ? window.matchMedia('(prefers-color-scheme: dark)').matches : 'auto' === 'dark')) && document.body.setAttribute('data-theme', 'dark');</script><div class="wrapper"><header class="desktop animate__faster" id="header-desktop">
  <div class="header-wrapper" data-github-corner="right">
    <div class="header-title">
      <a href="liudongdong1.github.io/" title="DAY By DAY"><img
    class="lazyload logo"
    src="/liudongdong1.github.io/svg/loading.min.svg"
    data-src="/fixit.min.svg"
    data-srcset="/fixit.min.svg, /fixit.min.svg 1.5x, /fixit.min.svg 2x"
    data-sizes="auto"
    alt="DAY By DAY"
    title="DAY By DAY"/><span class="header-title-text"></span></a><span id="typeit-header-subtitle-desktop" class="typeit header-subtitle"></span></div>
    <nav>
      <ul class="menu"><li class="menu-item">
              <a
                class="menu-link"
                href="/liudongdong1.github.io/posts/"
                
                
              ><i class="fa-solid fa-archive fa-fw fa-sm" aria-hidden="true"></i> 所有文章</a></li><li class="menu-item">
              <a
                class="menu-link"
                href="/liudongdong1.github.io/categories/"
                
                
              ><i class="fa-solid fa-th fa-fw fa-sm" aria-hidden="true"></i> 分类</a></li><li class="menu-item">
              <a
                class="menu-link"
                href="/liudongdong1.github.io/tags/"
                
                
              ><i class="fa-solid fa-tags fa-fw fa-sm" aria-hidden="true"></i> 标签</a></li><li class="menu-item">
              <a
                class="menu-link"
                href="/liudongdong1.github.io/friends/"
                title="友情链接"
                
              ><i class="fa-solid fa-users fa-fw fa-sm" aria-hidden="true"></i> 友链</a></li><li class="menu-item">
              <a
                class="menu-link"
                href="/liudongdong1.github.io/about/"
                
                
              ><i class="fa-solid fa-info-circle fa-fw fa-sm" aria-hidden="true"></i> 关于</a></li><li class="menu-item delimiter"></li><li class="menu-item language">
            <span role="button" aria-label="选择语言" title="选择语言">简体中文<i class="dropdown-icon fa-solid fa-chevron-down" aria-hidden="true"></i>
            </span>
            <ul class="sub-menu"><li class="menu-item">没有更多翻译</li></ul>
          </li><li class="menu-item search" id="search-desktop">
            <input type="text" placeholder="搜索文章标题或内容 ..." id="search-input-desktop">
            <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-desktop" title="搜索">
              <i class="fa-solid fa-search fa-fw" aria-hidden="true"></i>
            </a>
            <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-desktop" title="清空">
              <i class="fa-solid fa-times-circle fa-fw" aria-hidden="true"></i>
            </a>
            <span class="search-button search-loading" id="search-loading-desktop">
              <i class="fa-solid fa-spinner fa-fw fa-spin" aria-hidden="true"></i>
            </span>
          </li><li class="menu-item theme-switch" title="切换主题">
          <i class="fa-solid fa-adjust fa-fw" aria-hidden="true"></i>
        </li>
      </ul>
    </nav>
  </div>
</header><header class="mobile animate__faster" id="header-mobile">
  <div class="header-container">
    <div class="header-wrapper">
      <div class="header-title">
        <a href="liudongdong1.github.io/" title="DAY By DAY"><img
    class="lazyload logo"
    src="/liudongdong1.github.io/svg/loading.min.svg"
    data-src="/fixit.min.svg"
    data-srcset="/fixit.min.svg, /fixit.min.svg 1.5x, /fixit.min.svg 2x"
    data-sizes="auto"
    alt="/fixit.min.svg"
    title="/fixit.min.svg"/><span class="header-title-text"></span></a><span id="typeit-header-subtitle-mobile" class="typeit header-subtitle"></span></div>
      <div class="menu-toggle" id="menu-toggle-mobile">
        <span></span><span></span><span></span>
      </div>
    </div>
    <nav>
      <ul class="menu" id="menu-mobile"><li class="search-wrapper">
            <div class="search mobile" id="search-mobile">
              <input type="text" placeholder="搜索文章标题或内容 ..." id="search-input-mobile">
              <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-mobile" title="搜索">
                <i class="fa-solid fa-search fa-fw" aria-hidden="true"></i>
              </a>
              <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-mobile" title="清空">
                <i class="fa-solid fa-times-circle fa-fw" aria-hidden="true"></i>
              </a>
              <span class="search-button search-loading" id="search-loading-mobile">
                <i class="fa-solid fa-spinner fa-fw fa-spin" aria-hidden="true"></i>
              </span>
            </div>
            <a href="javascript:void(0);" class="search-cancel" id="search-cancel-mobile">
              取消
            </a>
          </li><li
              class="menu-item"
            ><a
                  class="menu-link"
                  href="/liudongdong1.github.io/posts/"
                  
                  
                ><i class="fa-solid fa-archive fa-fw fa-sm" aria-hidden="true"></i> 所有文章</a></li><li
              class="menu-item"
            ><a
                  class="menu-link"
                  href="/liudongdong1.github.io/categories/"
                  
                  
                ><i class="fa-solid fa-th fa-fw fa-sm" aria-hidden="true"></i> 分类</a></li><li
              class="menu-item"
            ><a
                  class="menu-link"
                  href="/liudongdong1.github.io/tags/"
                  
                  
                ><i class="fa-solid fa-tags fa-fw fa-sm" aria-hidden="true"></i> 标签</a></li><li
              class="menu-item"
            ><a
                  class="menu-link"
                  href="/liudongdong1.github.io/friends/"
                  title="友情链接"
                  
                ><i class="fa-solid fa-users fa-fw fa-sm" aria-hidden="true"></i> 友链</a></li><li
              class="menu-item"
            ><a
                  class="menu-link"
                  href="/liudongdong1.github.io/about/"
                  
                  
                ><i class="fa-solid fa-info-circle fa-fw fa-sm" aria-hidden="true"></i> 关于</a></li><li
              class="menu-item text-center"
            ><a
                  class="menu-link"
                  href="https://liudongdong1.github.io/"
                  title="GitHub"
                  rel="noopener noreferrer" target="_blank"
                ><i class='fa-brands fa-github fa-fw' aria-hidden='true'></i> </a></li><li class="menu-item theme-switch" title="切换主题">
          <i class="fa-solid fa-adjust fa-fw" aria-hidden="true"></i>
        </li><li class="menu-item language">
            <span role="button" aria-label="选择语言" title="选择语言">简体中文<i class="dropdown-icon fa-solid fa-chevron-down" aria-hidden="true"></i>
            </span>
            <select class="language-select" onchange="location = this.value;"><option disabled>没有更多翻译</option></select>
          </li></ul>
    </nav>
  </div>
</header><div class="search-dropdown desktop">
    <div id="search-dropdown-desktop"></div>
  </div>
  <div class="search-dropdown mobile">
    <div id="search-dropdown-mobile"></div>
  </div><main class="container" data-page-style="normal"><aside class="toc" id="toc-auto"><h2 class="toc-title">目录 <i class="toc-icon fa-solid fa-angle-down fa-fw"></i></h2>
      <div class="toc-content" id="toc-content-auto"></div></aside>

  <aside class="aside-custom" id="aside-sakana">
    

<div class="sakana-widget">
  <div class="sakana-item" id="takina-widget"></div>
  <div class="sakana-item" id="chisato-widget"></div>
</div>
<script>
  function initSakanaWidget() {
    const takina = SakanaWidget.getCharacter('takina')
    SakanaWidget.registerCharacter('takina-slow', takina);
    new SakanaWidget({
      character: 'takina-slow',
      controls: false,
      autoFit: true,
      stroke: {
        color: "#b4b4b4",
        width: 2
      }
    }).mount('#takina-widget');

    const chisato = SakanaWidget.getCharacter('chisato')
    SakanaWidget.registerCharacter('chisato-slow', chisato);
    new SakanaWidget({
      character: 'chisato-slow',
      controls: false,
      autoFit: true,
      stroke: {
        color: "#b4b4b4",
        width: 2
      }
    }).mount('#chisato-widget');
  }
</script>
<script async onload="initSakanaWidget()" src="https://cdn.jsdelivr.net/npm/sakana-widget@2.3.0/lib/sakana.min.js">
</script></aside>

  <article class="page single">
    <div class="header"><h1 class="single-title animate__animated animate__flipInX">
        <span>NLPHotTopic</span>
      </h1></div><div class="post-meta">
      <div class="post-meta-line"><span class="post-author"><span class="author"><i class="fa-solid fa-user-circle" aria-hidden="true"></i>
      liudongdong1</span></span>
          <span class="post-category">收录于 <a href="liudongdong1.github.io/categories/ai/"><i class="fa-regular fa-folder fa-fw"></i>&nbsp;AI</a>&ensp;<a href="liudongdong1.github.io/categories/nlp/"><i class="fa-regular fa-folder fa-fw"></i>&nbsp;NLP</a></span></div>
      <div class="post-meta-line"><span title=2020-08-15&#32;07:56:09>
            <i class="fa-regular fa-calendar-alt fa-fw"></i>&nbsp;<time datetime="2020-08-15" >2020-08-15</time>
          </span>&nbsp;<i class="fa-solid fa-pencil-alt fa-fw"></i>&nbsp;约 3253 字&nbsp;
        <i class="fa-regular fa-clock fa-fw"></i>&nbsp;预计阅读 7 分钟&nbsp;<span id="busuanzi_container_page_pv" class="busuanzi_visitors comment-visitors" data-flag-title="NLPHotTopic">
            <i class="fa-regular fa-eye fa-fw"></i>&nbsp;<span id="busuanzi_value_page_pv">-</span>&nbsp;次阅读
          </span>&nbsp;</div>
    </div><div class="featured-image"><img
    class="lazyload"
    src="/liudongdong1.github.io/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/20210501113138.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/20210501113138.png, https://gitee.com/github-25970295/blogImage/raw/master/img/20210501113138.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/20210501113138.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/20210501113138.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/20210501113138.png"/></div><div class="details toc" id="toc-static" kept="true">
        <div class="details-summary toc-title">
          <span>目录</span>
          <span><i class="details-icon fa-solid fa-angle-right"></i></span>
        </div>
        <div class="details-content toc-content" id="toc-content-static"><nav id="TableOfContents">
  <ul>
    <li>
      <ul>
        <li><a href="#0--preparatory-knowledge">0.  Preparatory knowledge</a></li>
        <li><a href="#1-model-sides">1. Model sides</a></li>
        <li><a href="#2-application-sides">2. Application sides</a></li>
        <li><a href="#3-scene-task">3. Scene task</a></li>
        <li><a href="#4-paper--relative-article">4. Paper &amp; Relative Article</a></li>
        <li><a href="#5-referencelearning-resource">5. Reference&amp;Learning Resource</a></li>
      </ul>
    </li>
  </ul>
</nav></div>
      </div><div
      class="content"
      id="content"
      
      
    ><blockquote>
<p>This week i get a summary knowledge of NLP, and learn some direction for further learning. And in this blog, i will record what i learned this weak by searching some information on Internet, the content is organized as follows: the Preparatory knowledge which need to be master in the following years, and some direction in NLP areas from model sides, application sides and the scene task, and some paper and learning resource recording.</p>
</blockquote>
<h3 id="0--preparatory-knowledge">0.  Preparatory knowledge</h3>
<ul>
<li>Probability&amp; Statistics</li>
</ul>
<p><img
    class="lazyload"
    src="/liudongdong1.github.io/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201008165645446.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201008165645446.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201008165645446.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201008165645446.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201008165645446.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201008165645446.png"/></p>
<ul>
<li>
<p><strong>Machine Learning</strong><img
    class="lazyload"
    src="/liudongdong1.github.io/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/ml.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/ml.png, https://gitee.com/github-25970295/blogImage/raw/master/img/ml.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/ml.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/ml.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/ml.png"/></p>
</li>
<li>
<p><strong>Text Mining</strong></p>
</li>
</ul>
<p><img
    class="lazyload"
    src="/liudongdong1.github.io/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/textmining.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/textmining.png, https://gitee.com/github-25970295/blogImage/raw/master/img/textmining.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/textmining.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/textmining.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/textmining.png"/></p>
<ul>
<li><strong>NLP</strong></li>
</ul>
<p><img
    class="lazyload"
    src="/liudongdong1.github.io/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/prob.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/prob.png, https://gitee.com/github-25970295/blogImage/raw/master/img/prob.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/prob.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/prob.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/prob.png"/></p>
<h3 id="1-model-sides">1. Model sides</h3>
<h4 id="11-transformers-and-pre-trained-language-models">1.1. Transformers and pre-trained language models</h4>
<ul>
<li>“Attention is all you need” (<a href="https://arxiv.org/abs/1706.03762"target="_blank" rel="external nofollow noopener noreferrer">Vaswani et al., 2017<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>)</li>
<li>“BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding” (<a href="https://arxiv.org/abs/1810.04805"target="_blank" rel="external nofollow noopener noreferrer">Devlin et al., 2018<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>)</li>
</ul>
<blockquote>
<p><strong>Theory-proving side:</strong>  (<a href="https://arxiv.org/abs/2002.06622"target="_blank" rel="external nofollow noopener noreferrer">Shi et al., 2020<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>; <a href="https://arxiv.org/abs/1908.04211"target="_blank" rel="external nofollow noopener noreferrer">Brunner et al., 2020<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>; <a href="https://arxiv.org/abs/1912.10077"target="_blank" rel="external nofollow noopener noreferrer">Yun et al., 2019<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>; <a href="https://arxiv.org/abs/1911.03584"target="_blank" rel="external nofollow noopener noreferrer">Cordonnier et al., 2019<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>).</p>
</blockquote>
<blockquote>
<p><strong>improving the task performances of Transformers and pre-trained language models:</strong>(<a href="https://arxiv.org/abs/1908.04577"target="_blank" rel="external nofollow noopener noreferrer">Wang et al. 2019<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>; <a href="https://arxiv.org/abs/1909.11299"target="_blank" rel="external nofollow noopener noreferrer">Lee et al., 2019<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>).</p>
</blockquote>
<blockquote>
<p><strong>Reducing the size of models or the time of training:</strong>(<a href="https://arxiv.org/abs/2004.11886"target="_blank" rel="external nofollow noopener noreferrer">Wu et al., 2020<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>; <a href="https://arxiv.org/abs/1909.11942"target="_blank" rel="external nofollow noopener noreferrer">Lan et al., 2019<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>; <a href="https://arxiv.org/abs/2001.04451"target="_blank" rel="external nofollow noopener noreferrer">Kitaev et al., 2020<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>; <a href="https://arxiv.org/abs/2003.10555"target="_blank" rel="external nofollow noopener noreferrer">Clark et al., 2020<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>; <a href="https://arxiv.org/abs/1911.05507"target="_blank" rel="external nofollow noopener noreferrer">Rae et al., 2019<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>; <a href="https://arxiv.org/abs/1909.11556"target="_blank" rel="external nofollow noopener noreferrer">Fan et al., 2019<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>; <a href="https://arxiv.org/abs/1904.00962"target="_blank" rel="external nofollow noopener noreferrer">You et al., 2019<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>).</p>
<ul>
<li>Model Compression/Pruning (<a href="https://arxiv.org/abs/1912.00120"target="_blank" rel="external nofollow noopener noreferrer">Zhang et al., 2019<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>)</li>
</ul>
</blockquote>
<h4 id="12-multilingualcross-lingual-tasks">1.2. Multilingual/Cross-lingual tasks:</h4>
<ul>
<li>(<a href="https://arxiv.org/abs/1912.07840"target="_blank" rel="external nofollow noopener noreferrer">Karthikeyan et al., 2019<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>; <a href="https://openreview.net/forum?id=HyeYTgrFPB"target="_blank" rel="external nofollow noopener noreferrer">Berend 2020<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>; <a href="https://arxiv.org/abs/2002.03518"target="_blank" rel="external nofollow noopener noreferrer">Cao et al., 2020<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>; <a href="https://arxiv.org/abs/1910.04708"target="_blank" rel="external nofollow noopener noreferrer">Wang et al., 2019<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>)</li>
<li>Multimodal models(<a href="https://arxiv.org/abs/1908.08530"target="_blank" rel="external nofollow noopener noreferrer">Su et al., 2019<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>)</li>
<li><a href="https://openreview.net/forum?id=HJeT3yrtDr"target="_blank" rel="external nofollow noopener noreferrer"><strong>Cross-Lingual Ability of Multilingual BERT: An Empirical Study</strong><i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></li>
<li><a href="https://openreview.net/forum?id=HJlnC1rKPB"target="_blank" rel="external nofollow noopener noreferrer"><strong>On the Relationship between Self-Attention and Convolutional Layers</strong><i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></li>
</ul>
<h4 id="13-reinforcement-learning-and-nlp">1.3. Reinforcement learning and NLP</h4>
<ul>
<li>(<a href="https://arxiv.org/abs/1906.02768"target="_blank" rel="external nofollow noopener noreferrer">Yu et al., 2019<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>; <a href="https://arxiv.org/abs/1909.00668"target="_blank" rel="external nofollow noopener noreferrer">Clift et al., 2019<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>)</li>
</ul>
<blockquote>
<p><strong>Session 4：The Machine Learning in NLP</strong></p>
<ul>
<li>
<p>Learning Sparse Sharing Architectures for Multiple Tasks</p>
</li>
<li>
<p>Reinforcement Learning from Imperfect Demonstrations under Soft Expert Guidance</p>
</li>
<li>
<p>Shapley Q-value: A Local Reward Approach to Solve Global Reward Games</p>
</li>
<li>
<p>Measuring and relieving the over-smoothing problem in graph neural networks from the topological view</p>
</li>
<li>
<p>Neighborhood Cognition Consistent Multi-Agent Reinforcement Learning</p>
</li>
<li>
<p>Neural Snowball for Few-Shot Relation Learning</p>
</li>
<li>
<p>Multi-Task Self-Supervised Learning for Disfluency Detection</p>
</li>
<li>
<p>Constructing Multiple Tasks for Augmentation: Improving Neural Image Classification With K-means Features</p>
</li>
<li>
<p>Graph-propagation based correlation learning for fine-grained image classification</p>
</li>
<li>
<p>End-to-End Bootstrapping Neural Network for Entity Set Expansion</p>
</li>
</ul>
</blockquote>
<h3 id="2-application-sides">2. Application sides</h3>
<h4 id="21-natural-language-generation">2.1. Natural language generation</h4>
<ul>
<li>Generation of realistic, rhymed and theme based poetry (creative writing)</li>
<li>Generation of theme based short stories (creative writing)</li>
<li>Generation of theme based novels (creative writing)</li>
<li>Generation of news / short articles based on numerical / audio / video data</li>
<li>Generation of research papers based on a topic.</li>
</ul>
<h4 id="22-natural-language-understanding">2.2. Natural language understanding</h4>
<ul>
<li><strong>Sentiment Analysis</strong></li>
</ul>
<blockquote>
<p>Deriving sentiments in sentences (positive, negative, neutral), and also in articles (though that will be more appropriate like bag of sentence sentiments). The future is to include emotions (attributes) in that, like the attributes now on Facebook posts - Love, Like, Angry, Surprised, Sad, Hilarious. These attributes make a lot more sense for sentiments going forward.</p>
</blockquote>
<ul>
<li><strong>Text Summarization（汇总）</strong></li>
</ul>
<blockquote>
<p>Summarizing a single or many articles according to a particular theme.</p>
</blockquote>
<ul>
<li><strong>Textual entailment（语篇蕴涵）</strong></li>
</ul>
<blockquote>
<p>Inferring directional causal relationships between textual fragments. This can be challenging in a long article.</p>
<ul>
<li>Towards Building a Multilingual Sememe Knowledge Base: Predicting Sememes for BabelNet Synsets</li>
<li>Multi-Scale Self-Attention for Text Classification</li>
<li>Learning Multi-level Dependencies for Robust Word Recognition</li>
</ul>
</blockquote>
<ul>
<li><strong>Information Extraction</strong> or <strong>Relationship Extraction</strong> or <strong>Knowledge Graph</strong></li>
</ul>
<blockquote>
<p>Find structured information from unstructured data, like entities, relationships, co-reference resolution. This at a basic level is very useful for algorithmic trading. An extension of this is a global form of extracting logic structures (first order and higher order).</p>
</blockquote>
<ul>
<li><strong>Topic Segmentation</strong></li>
</ul>
<blockquote>
<p>Topic Extraction (with regions). Normally, there will be overlapping regions.</p>
</blockquote>
<ul>
<li><!-- raw HTML omitted --><strong>Question Answering</strong> or <strong>NLP-based voice assistant</strong><!-- raw HTML omitted --></li>
</ul>
<blockquote>
<p>Answer the questions to both closed (specific) and open questions (subjective). Answers to subjective questions is the main challenge for the likes of realistic Virtual Assistants.</p>
<ul>
<li>Modeling Fluency and Faithfulness for Diverse Neural Machine Translation</li>
<li>Minimizing the Bag-of-Ngrams Difference for Non-Autoregressive Neural Machine Translation</li>
<li>Neural Machine Translation with Joint Representation</li>
<li>Task-Oriented Dialog Systems that Consider Multiple Appropriate Responses under the Same Context</li>
<li>A pre-training based personalized dialogue generation model with persona-sparse data</li>
<li>Knowledge Graph Grounded Goal Planning for Open-Domain Conversation Generation</li>
</ul>
</blockquote>
<ul>
<li><strong>Parsing</strong></li>
</ul>
<blockquote>
<p>Parsing natural language generally in the form a tree. This involves hierarchical segmentation of the language involving the grammar rules.</p>
</blockquote>
<ul>
<li><strong>Prediction</strong></li>
</ul>
<blockquote>
<p>Given a short text, predict what happens next. The prediction problem is beginning to be targeted in vision, but it has never ever gained paths for realistic products. For closed and deterministic prediction (not innovative else that would fall under the paradigm of creative writing), this can be a useful task for prediction of future events based on past evidences and analysis. This can be then very useful for finance sectors.</p>
</blockquote>
<ul>
<li><strong>Part of Speech Tagging(词性标注)</strong></li>
</ul>
<blockquote>
<p>Tagging words whether they are nouns, verbs or adjectives.</p>
</blockquote>
<ul>
<li><strong>Translation</strong></li>
</ul>
<blockquote>
<p>Translate one language to another. This can be very challenging given the nature of the language, and the grammar. Normally, under probabilistic models, this assumes that the underlying grammar is mostly the same, and thus, models normally fail for Sanskrit.</p>
</blockquote>
<ul>
<li><strong>Query Expansion</strong></li>
</ul>
<blockquote>
<p>Expand query in possible ways for making the search results more meaningful. This is normally an issue with search engines, where people do not know what all keywords (or query sentences) to include to cover the entire gamut of relevancy.</p>
</blockquote>
<ul>
<li><strong>Argumentation Mining(论证分析挖掘）</strong></li>
</ul>
<blockquote>
<p>Evolving field of NLP, where one wants to analyse discussions and arguments.</p>
</blockquote>
<ul>
<li><strong>Interestingness(趣味性挖掘）</strong></li>
</ul>
<h4 id="2-3-nlp-and-cv">2. 3. NLP and CV</h4>
<ul>
<li><strong>Visual Question Answering</strong></li>
<li><strong>Automated Image Captioning（自动图像字幕）</strong></li>
<li><strong>OCR</strong></li>
</ul>
<blockquote>
<ul>
<li>DualVD: An Adaptive Dual Encoding Model for Deep Visual  Understanding  in Visual Dialogue</li>
<li>Storytelling from an Image Stream Using Scene Graphs</li>
</ul>
</blockquote>
<h4 id="24-voice-and-nlp">2.4. Voice and NLP</h4>
<ul>
<li><strong>speech to text</strong></li>
</ul>
<blockquote>
<p>Analysts predict speech recognition technologies will be substantially improved in the near future thanks to natural language processing. This will involve minimization of errors, recognition of what several individuals are saying despite different accents and a noisy environment.</p>
</blockquote>
<h3 id="3-scene-task">3. Scene task</h3>
<ul>
<li>
<p>Integrated Chatbot</p>
</li>
<li>
<p>Human-to-machine Interaction</p>
</li>
</ul>
<blockquote>
<p>conversing with a machine is as simple as conversing with a human.</p>
</blockquote>
<ul>
<li>Company monitoring</li>
</ul>
<blockquote>
<p>Banks and other monetary organizations can utilize NLP to find and parse client sentiment by checking social media and analyzing discussions about their services and strategies. With the capacity to get to significant, separated data, financial services analysts can compose increasingly definite reports and give better advice to customers and internal decision makers.</p>
</blockquote>
<ul>
<li>Business intelligence</li>
</ul>
<blockquote>
<p>getting business intelligence from raw business information, including product information, marketing and sales information, customer service, brand notoriety and the present talent pool of a company. This implies NLP will be the way to moving numerous legacy organizations from data-driven to intelligence-driven platforms, helping humankind rapidly get the insights to make decisions.</p>
</blockquote>
<blockquote>
<ul>
<li>
<p>搜索是NLP技术最早得到大规模应用的技术，例如百度搜索、知乎话题搜索以及各大互联网公司的query搜索技术，都涉及到语义匹配或文本分类技术。此外，大型的搜索引擎，知识图谱的搭建是必须的。</p>
</li>
<li>
<p>推荐系统在一定层面来说是跟搜索场景相反的。搜索是基于用户的意图，在文本库中寻找匹配项；推荐则相反，通常基于积累的用户信息，给用户推荐可能感兴趣的内容。推荐系统常常涉及用户画像、标签定义等过程，需要一定程度的依赖NLP技术。</p>
</li>
<li>
<p>聊天机器人是目前NLP技术应用最多的场景，基于NLP技术构建一个能够替代客服、销售、办公文员是这一任务的终极目标。目前，聊天机器人已经以各种形态出现在人们面前，有站在银行门口迎接顾客的迎宾机器人，有放在卧室床头的智能音箱，有呆在各个APP首页的助手机器人等等。在聊天机器人中，运用了文本分类、语义匹配、对话管理、实体识别等大量的NLP技术。要做好是一件难度大、超复杂的任务。</p>
</li>
<li>
<p>知识图谱是AI时代一个非常重要基础设施，大规模结构化的知识网络的搭建，能够重塑很多的智能场景。</p>
</li>
</ul>
</blockquote>
<h3 id="4-paper--relative-article">4. Paper &amp; Relative Article</h3>
<p><strong>4.1. Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context</strong></p>
<blockquote>
<p><strong>About:</strong> In this paper, researchers from Carnegie Mellon University and Google Brain proposed a novel neural architecture known as Transformer-XL that enables learning dependency beyond a fixed-length without disrupting temporal coherence. According to the researchers, TransformerXL learns dependency that is 80% longer than RNNs and 450% longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation.</p>
</blockquote>
<p><strong>4.2. Bridging The Gap Between Training &amp; Inference For Neural Machine Translation</strong></p>
<blockquote>
<p><strong>About:</strong> This paper is one of the top <a href="https://analyticsindiamag.com/6-top-nlp-papers-from-acl-2019-you-should-read/"target="_blank" rel="external nofollow noopener noreferrer">NLP papers<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a> from the premier conference, Association for Computational Linguistics (ACL). This paper talks about the error accumulation during Neural Machine Translation. The researchers addressed such problems by sampling context words, not only from the ground truth sequence but also from the predicted sequence by the model during training, where the predicted sequence is selected with a sentence-level optimum. According to the researchers, this approach can achieve significant improvements in multiple datasets.</p>
</blockquote>
<p><strong>4.3. BERT: Pre-training Of Deep Bidirectional Transformers For Language Understanding</strong></p>
<blockquote>
<p>BERT by Google AI is one of the most popular language representation models. Several organisations, including Facebook as well as academia, have been researching NLP using this transformer model. BERT stands for Bidirectional Encoder Representations from Transformers and is designed to pre-train deep bidirectional representations from the unlabeled text by jointly conditioning on both left and right context in all layers. The model obtained new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5%, MultiNLI accuracy to 86.7%, and much more.</p>
</blockquote>
<p><strong>4.4. Emotion-Cause Pair Extraction: A New Task To Emotion Analysis In Texts</strong></p>
<blockquote>
<p>Emotion cause extraction (ECE) is a task that is aimed at extracting the potential causes behind certain emotions in text. In this paper, researchers from China proposed a new task known as emotion-cause pair extraction (ECPE), which aims to extract the potential pairs of emotions and corresponding causes in a document. The experimental results on a benchmark emotion cause corpus that prove the feasibility of the ECPE task as well as the effectiveness of this approach.</p>
</blockquote>
<p><strong>4.5. Improving Language Understanding By Generative Pre-Training</strong></p>
<blockquote>
<p>This paper is published by OpenAI, where the researchers talked about natural language understanding and how it can be challenging for discriminatively trained models to perform adequately. The researchers demonstrated the effectiveness of the approach on a wide range of benchmarks for natural language understanding. They proposed a general task-agnostic model, which outperformed discriminatively trained models that use architectures specifically crafted for each task, significantly improving upon state-of-the art in 9 out of the 12 tasks studied.</p>
</blockquote>
<p><strong>4.6. Neural Approaches To Conversational AI</strong></p>
<blockquote>
<p>This research paper by Microsoft Research surveys neural approaches to conversational AI that have been developed in the last few years. In this paper, the researchers grouped conversational systems into three categories, which are question answering agents, task-oriented dialogue agents, and chatbots. For each category, a review of state-of-the-art neural approaches is presented, drawing the connection between them and traditional approaches, as well as discussing the progress that has been made and challenges still being faced, using specific systems and models as case studies.</p>
</blockquote>
<p><strong>Session 1：翻译、对话与文本生成</strong></p>
<p>(1) Modeling Fluency and Faithfulness for Diverse Neural Machine Translation</p>
<p>(2) Minimizing the Bag-of-Ngrams Difference for Non-Autoregressive Neural Machine Translation</p>
<p>(3) Task-Oriented Dialog Systems that Consider Multiple Appropriate Responses under the Same Context</p>
<p>(4) A pre-training based personalized dialogue generation model with persona-sparse data</p>
<p>(5) Synchronous Speech Recognition and Speech-to-Text Translation with Interactive Decoding</p>
<p>(6) SPARQA: Skeleton-based Semantic Parsing for Complex Questions over Knowledge Bases</p>
<p>(7) Knowledge Graph Grounded Goal Planning for Open-Domain Conversation Generation</p>
<p>(8) Neural Machine Translation with Joint Representation</p>
<p><strong>Session 2：文本分析与内容挖掘</strong></p>
<p>(9) Multi-Scale Self-Attention for Text Classification</p>
<p>(10) Learning Multi-level Dependencies for Robust Word Recognition</p>
<p>(11) Towards Building a Multilingual Sememe Knowledge Base: Predicting Sememes for BabelNet Synsets</p>
<p>(12) Cross-Lingual Low-Resource Set-to-Description Retrieval for Global E-Commerce</p>
<p>(13) Integrating Relation Constraints with Neural Relation Extractors</p>
<p>(14) Capturing Sentence Relations for Answer Sentence Selection with Multi-Perspective Graph Encoding</p>
<p>(15) Replicate, Walk, and Stop on Syntax: an Effective Neural Network Model for Aspect-Level Sentiment Classification</p>
<p>(16) Cross-Lingual Natural Language Generation via Pre-Training</p>
<p><strong>Session 3：知识理解与NLP应用</strong></p>
<p>(17) Hyperbolic Interaction Model For Hierarchical Multi-Label Classification</p>
<p>(18) Multi-channel Reverse Dictionary Model</p>
<p>(19) Discovering New Intents via Constrained Deep Adaptive Clustering with Cluster Refinement</p>
<p>(20) Logo-2K+: A Large-Scale Logo Dataset for Scalable Logo Classification</p>
<p>(21) DMRM: A Dual-channel Multi-hop Reasoning Model for Visual Dialog</p>
<p>(22) DualVD: An Adaptive Dual Encoding Model for Deep Visual  Understanding  in Visual Dialogue</p>
<p>(23) Storytelling from an Image Stream Using Scene Graphs</p>
<p>(24) Draft and Edit: Automatic Storytelling Through Multi-Pass Hierarchical Conditional Variational Autoencoder</p>
<p><a href="https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649035757%26idx%3D1%26sn%3Dcaaf1d3f78e65a4df46fcffa0720f931%26chksm%3D8712ad90b065248603f8db9fbdc18a19ee2af5900bcc55ddc381833467ff1cb7e15120d504c1%26scene%3D21%23wechat_redirect"target="_blank" rel="external nofollow noopener noreferrer">【NLP-词向量】词向量的由来及本质<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<p><a href="https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649035906%26idx%3D1%26sn%3D24df0e979ad2761a763c4f073ea92ac2%26chksm%3D8712aaffb06523e933148d4146cc343e12275ae91ea81710b4e2bab3bb6435e8d35b13223445%26scene%3D21%23wechat_redirect"target="_blank" rel="external nofollow noopener noreferrer">【NLP-词向量】从模型结构到损失函数详解word2vec<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<p><a href="https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649034734%26idx%3D1%26sn%3D78b209c04b3f69387240efa1a904278e%26chksm%3D8712b193b0653885b808090c5c8e96ba4c7dac75fa013b1e4f72ef0027b6035155baae41c397%26scene%3D21%23wechat_redirect"target="_blank" rel="external nofollow noopener noreferrer">【NLP】 聊聊NLP中的attention机制<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<p><a href="https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649034901%26idx%3D2%26sn%3D5a12aff786df3f305a5a05595fb6b8b8%26chksm%3D8712aee8b06527fee9a62c070313c47067e2bc00cb1a39b19401b4bf8d0e364eb88e28826667%26scene%3D21%23wechat_redirect"target="_blank" rel="external nofollow noopener noreferrer">【NLP】 理解NLP中网红特征抽取器Tranformer<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<p><a href="https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649035055%26idx%3D1%26sn%3Dc49f6919ec8d0fef269f751680819edf%26chksm%3D8712af52b06526443ed01d2ec3bb9d8621ec4ef714b132dfa88020bbda268fdc22ab2e598f78%26scene%3D21%23wechat_redirect"target="_blank" rel="external nofollow noopener noreferrer">【NLP】 深入浅出解析BERT原理及其表征的内容<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<p><a href="https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649035554%26idx%3D2%26sn%3D61cbd0046aa055b16dd2e74f6a625a4d%26chksm%3D8712ad5fb06524495d663310836fd222c9e89c002ff778cba7996c90c27ca4f41b85a050cd6b%26scene%3D21%23wechat_redirect"target="_blank" rel="external nofollow noopener noreferrer">【NLP】GPT：第一个引入Transformer的预训练模型<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<p><a href="https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649035407%26idx%3D2%26sn%3De84f0f9f2c7458658514bf9a4e934324%26chksm%3D8712acf2b06525e41d82fbc5a9b60efeca91a0eec1a1f4c96f44800b30d2fe08fb0bf5f67917%26scene%3D21%23wechat_redirect"target="_blank" rel="external nofollow noopener noreferrer">【NLP】XLnet：GPT和BERT的合体，博采众长，所以更强<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<p><a href="https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649036099%26idx%3D1%26sn%3D7671dfd7c4f748c3aa0d12f57956fabf%26chksm%3D8712ab3eb065222862a03a0f18ec62cce6a6a8166656a3477c7c2f492c9749b7b68b75679693%26scene%3D21%23wechat_redirect"target="_blank" rel="external nofollow noopener noreferrer">【NLP-NER】什么是命名实体识别？<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<p><a href="https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649036142%26idx%3D1%26sn%3D00b0a2588b0e4eb1f67f4e0997562c53%26chksm%3D8712ab13b0652205517ecf622982410ab81dd22ff7bc23f0eed2c89525ce95d8cf851000efff%26scene%3D21%23wechat_redirect"target="_blank" rel="external nofollow noopener noreferrer">【NLP-NER】命名实体识别中最常用的两种深度学习模型<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<p><a href="https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649036272%26idx%3D1%26sn%3D3ce6800462c6ea8d911909489bef4ed0%26chksm%3D8712ab8db065229bc8ea68f94332be9e03a06eb54b84c42d7e1ce8dc3d366a7e03035d0e3ae1%26scene%3D21%23wechat_redirect"target="_blank" rel="external nofollow noopener noreferrer">【NLP-NER】如何使用BERT来做命名实体识别<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<p><a href="https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649036559%26idx%3D1%26sn%3D97bd5d699ceffd7f5f98b831cc26ec1b%26chksm%3D8712a972b065206481a853852939ba4c7f4e8a713197ca4f2c3b55e98fbeb405af339ea3644c%26scene%3D21%23wechat_redirect"target="_blank" rel="external nofollow noopener noreferrer">【NLP实战系列】Tensorflow命名实体识别实战<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<p><a href="https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649035102%26idx%3D2%26sn%3D75957ee0aec259c1ada9b9015fc93828%26chksm%3D8712af23b0652635c255bd3e1c58d998e6b01cbd1513d8ce4e40e0a4dc0841560b1710699c54%26scene%3D21%23wechat_redirect"target="_blank" rel="external nofollow noopener noreferrer">【每周NLP论文推荐】 NLP中命名实体识别从机器学习到深度学习的代表性研究<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<p><a href="https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649036470%26idx%3D1%26sn%3Dcc44bc3babdb25b959fb644975382156%26chksm%3D8712a8cbb06521dd17e2848a567b69b91b42baa4d0527186a64e0cbd059576bd99ee40e7ac86%26scene%3D21%23wechat_redirect"target="_blank" rel="external nofollow noopener noreferrer">【NLP实战系列】朴素贝叶斯文本分类实战<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<p><a href="https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649036836%26idx%3D1%26sn%3Da4e0b73a4ed227b53c305494b848e094%26chksm%3D8712a659b0652f4fab5613d0c2a85da6ee898159d1648e13d5be4fc8484692334bced37d0dd7%26scene%3D21%23wechat_redirect"target="_blank" rel="external nofollow noopener noreferrer">【NLP实战】基于ALBERT的文本相似度计算<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<p><a href="https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649037438%26idx%3D1%26sn%3Dc68e8734c19bade085f7a5a23a5401a7%26chksm%3D8712a403b0652d15b3c5d8a721c6a3a838e100118116a53c5d6a443daf05c9d285f950ce956c%26scene%3D21%23wechat_redirect"target="_blank" rel="external nofollow noopener noreferrer">【文本信息抽取与结构化】目前NLP领域最有应用价值的子任务之一<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<p><a href="https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649037522%26idx%3D2%26sn%3D4c3b77627fd6a879d34781476bfd194f%26chksm%3D8712a4afb0652db964a5f7e1de3c927eb99ec0e9f5fad6588d4d1243f356392b4232ac3fc001%26scene%3D21%23wechat_redirect"target="_blank" rel="external nofollow noopener noreferrer">【文本信息抽取与结构化】详聊文本的结构化【上】<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<p><a href="https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649037655%26idx%3D2%26sn%3Dd83d62d87227e34d9324faeb6d536cd1%26chksm%3D8712a52ab0652c3c6f28e08456ef9f740f2fbd42b4f46ca5f71914103a20b28a8bc88ad2497e%26scene%3D21%23wechat_redirect"target="_blank" rel="external nofollow noopener noreferrer">【文本信息抽取与结构化】详聊文本的结构化【下】<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<p><a href="https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649037990%26idx%3D2%26sn%3D76b2b4c32f72aaddfec60a3a04dac90a%26chksm%3D8712a2dbb0652bcd9ae280267ca62bbe1f2c62430fca96534eb853db298efa1dc43f73e8a7bd%26scene%3D21%23wechat_redirect"target="_blank" rel="external nofollow noopener noreferrer">【文本信息抽取与结构化】详聊如何用BERT实现关系抽取<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<p><a href="https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649035654%26idx%3D2%26sn%3Df9f8020da1faa66390c424c5faec3260%26chksm%3D8712adfbb06524ed14db1a5e35a62cb1db9ed7cf1b3298771969cf95731b1410ac08740891e2%26scene%3D21%23wechat_redirect"target="_blank" rel="external nofollow noopener noreferrer">【每周NLP论文推荐】 掌握实体关系抽取必读的文章<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<p><a href="https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649036303%26idx%3D1%26sn%3D15d4cd20640fae64535ef5bff08ca1fa%26chksm%3D8712a872b065216466b0be4f44567c8470746c673f4113da0269a14f5342044f93a9b07d3e76%26scene%3D21%23wechat_redirect"target="_blank" rel="external nofollow noopener noreferrer">【NLP-ChatBot】我们熟悉的聊天机器人都有哪几类？<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<p><a href="https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649036396%26idx%3D1%26sn%3D55370a63f225ae9d734fdc31dad5869f%26chksm%3D8712a811b0652107ef7152df9ee001cc90aafb4db332b88bcb29a5f3ca2c0da04b30250ecbb7%26scene%3D21%23wechat_redirect"target="_blank" rel="external nofollow noopener noreferrer">【NLP-ChatBot】搜索引擎的最终形态之问答系统（FAQ）详述<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<p><a href="https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649036508%26idx%3D1%26sn%3Dddd9a454497b7a766ca7246448fe2eb2%26chksm%3D8712a8a1b06521b789ca66143391efef8ba88e243d306ce6a45040213ff31fb2493635f93994%26scene%3D21%23wechat_redirect"target="_blank" rel="external nofollow noopener noreferrer">【NLP-ChatBot】能干活的聊天机器人-对话系统概述<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<p><a href="https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649036142%26idx%3D2%26sn%3Df3ce4c20b0827b9f08babfd225a93aa9%26chksm%3D8712ab13b065220512ebf7339b58395e04634081904e6e6fe3a20237051ef6c2a846972f1fb5%26scene%3D21%23wechat_redirect"target="_blank" rel="external nofollow noopener noreferrer">【每周NLP论文推荐】 对话管理中的标志性论文介绍<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<p><a href="https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649035491%26idx%3D2%26sn%3D4ec519ce322949e9d5117bbfc7bd074e%26chksm%3D8712ac9eb0652588b7218a396140563553dc399e00136b6cd5403d675f852ef5a56616da3535%26scene%3D21%23wechat_redirect"target="_blank" rel="external nofollow noopener noreferrer">【每周NLP论文推荐】 开发聊天机器人必读的重要论文<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<p><a href="https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649036786%26idx%3D1%26sn%3Dbf010d6a8c561b80d163f5c51598030f%26chksm%3D8712a98fb06520991297e3ac2b710643ce91d881d38d01cd9ba8ed80055826a1fcc9c14c0383%26scene%3D21%23wechat_redirect"target="_blank" rel="external nofollow noopener noreferrer">【知识图谱】人工智能技术最重要基础设施之一，知识图谱你该学习的东西<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<p><a href="https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649036864%26idx%3D1%26sn%3D78c14394b20f80d481004cca5156a776%26chksm%3D8712a63db0652f2b513810ce6190c5bf1a7c98746f7d1188710cfa44d6249272a66aef893451%26scene%3D21%23wechat_redirect"target="_blank" rel="external nofollow noopener noreferrer">【知识图谱】知识表示：知识图谱如何表示结构化的知识？<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<p><a href="https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649036959%26idx%3D1%26sn%3Ddce0df28080545324e40cabcb5c9e1e6%26chksm%3D8712a6e2b0652ff488e691c1db605570d6f4d888ac516548e843c9324e95e2ad42cb8fc7fe34%26scene%3D21%23wechat_redirect"target="_blank" rel="external nofollow noopener noreferrer">【知识图谱】如何构建知识体系：知识图谱搭建的第一步<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<p><a href="https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649037188%26idx%3D1%26sn%3D2ac2099c02c1fadd2455a71601d3921f%26chksm%3D8712a7f9b0652eefaa14d21f186bfd0609ae220ed1a30dfc36a9dcc1fea26d4791ef47776553%26scene%3D21%23wechat_redirect"target="_blank" rel="external nofollow noopener noreferrer">【知识图谱】获取到知识后，如何进行存储和便捷的检索？<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<p><a href="https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649037252%26idx%3D1%26sn%3D668affc58c11e731ad2f2488311c3df4%26chksm%3D8712a7b9b0652eaf980bc40311cd20370e93f49904e08d788a20f51ab2878220a69bd84b3627%26scene%3D21%23wechat_redirect"target="_blank" rel="external nofollow noopener noreferrer">【知识图谱】知识推理，知识图谱里最“人工智能”的一段<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<h3 id="5-referencelearning-resource">5. Reference&amp;Learning Resource</h3>
<ul>
<li>
<p><a href="https://github.com/graykode/nlp-roadmap"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/graykode/nlp-roadmap<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
</li>
<li>
<p><!-- raw HTML omitted --><a href="https://github.com/ivan-bilan/The-NLP-Pandect"target="_blank" rel="external nofollow noopener noreferrer">A comprehensive reference for all topics related to Natural Language Processing<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a><!-- raw HTML omitted --></p>
</li>
<li>
<p><a href="https://github.com/keon/awesome-nlp"target="_blank" rel="external nofollow noopener noreferrer">A curated list of resources dedicated to Natural Language Processing<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
</li>
<li>
<p><a href="https://github.com/yandexdataschool/nlp_course"target="_blank" rel="external nofollow noopener noreferrer">YSDA course in Natural Language Processing<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
</li>
<li>
<p><a href="https://github.com/makcedward/nlp"target="_blank" rel="external nofollow noopener noreferrer">NLP Learning journey.<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
</li>
<li>
<p><a href="https://github.com/lyeoni/nlp-tutorial"target="_blank" rel="external nofollow noopener noreferrer">NLP(Natural Language Processing) tutorials Pytorch example<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
</li>
</ul>
</div>
<div class="post-footer" id="post-footer">
  <div class="post-info">
    <div class="post-info-line">
      <div class="post-info-mod">
        <span title=2023-09-28&#32;22:16:16>更新于 2023-09-28&nbsp;</span>
      </div><div class="post-info-license">
          <span><a rel="license external nofollow noopener noreferrer" href="https://creativecommons.org/licenses/by-nc/4.0/" target="_blank">CC BY-NC 4.0</a></span>
        </div></div>
    <div class="post-info-line">
      <div class="post-info-md"><span><a href="liudongdong1.github.io/nlphottopic/index.md" title="阅读原始文档" class="link-to-markdown">阅读原始文档</a></span><span><a href="https://liudongdong1.github.io/edit/master/content/posts%5c%e8%87%aa%e7%84%b6%e8%af%ad%e8%a8%80%5cNLPHotTopic.md" title="编辑此页"target="_blank" rel="external nofollow noopener noreferrer" class="link-to-edit">编辑此页</a></span></div>
      <div class="post-info-share">
        <span><a href="javascript:void(0);" title="分享到 Twitter" data-sharer="twitter" data-url="liudongdong1.github.io/nlphottopic/" data-title="NLPHotTopic" data-hashtags="Semantic Parsing"><i class="fa-brands fa-twitter fa-fw" aria-hidden="true"></i></a>
  <a href="javascript:void(0);" title="分享到 Facebook" data-sharer="facebook" data-url="liudongdong1.github.io/nlphottopic/" data-hashtag="Semantic Parsing"><i class="fa-brands fa-facebook-square fa-fw" aria-hidden="true"></i></a>
  <a href="javascript:void(0);" title="分享到 微博" data-sharer="weibo" data-url="liudongdong1.github.io/nlphottopic/" data-title="NLPHotTopic" data-image="https://gitee.com/github-25970295/blogImage/raw/master/img/20210501113138.png"><i class="fa-brands fa-weibo fa-fw" aria-hidden="true"></i></a>
  </span>
      </div>
    </div>
  </div>

  <div class="post-info-more">
    <section class="post-tags"><i class="fa-solid fa-tags fa-fw" aria-hidden="true"></i>&nbsp;<a href="liudongdong1.github.io/tags/semantic-parsing/">Semantic Parsing</a></section>
    <section>
      <span><a href="javascript:void(0);" onclick="window.history.back();">返回</a></span>&nbsp;|&nbsp;<span><a href="liudongdong1.github.io/">主页</a></span>
    </section>
  </div>

  <div class="post-nav"><a href="liudongdong1.github.io/nlprelative/" class="prev" rel="prev" title="NLPRelative"><i class="fa-solid fa-angle-left fa-fw" aria-hidden="true"></i>NLPRelative</a>
      <a href="liudongdong1.github.io/jdk_tomcat/" class="next" rel="next" title="jdk_tomcat">jdk_tomcat<i class="fa-solid fa-angle-right fa-fw" aria-hidden="true"></i></a></div>
</div>
</article></main><footer class="footer">
    <div class="footer-container"><div class="footer-line powered">由 <a href="https://gohugo.io/" target="_blank" rel="external nofollow noopener noreferrer" title="Hugo 0.118.2">Hugo</a> 强力驱动 | 主题 - <a href="https://github.com/hugo-fixit/FixIt" target="_blank" rel="external" title="FixIt v0.2.17-RC"><img class="fixit-icon" src="/liudongdong1.github.io/fixit.min.svg" alt="FixIt logo" />&nbsp;FixIt</a>
        </div><div class="footer-line copyright" itemscope itemtype="http://schema.org/CreativeWork"><i class="fa-regular fa-copyright fa-fw" aria-hidden="true"></i>
            <span itemprop="copyrightYear">2020 - 2023</span><span class="author" itemprop="copyrightHolder">
              <a href="https://liudongdong1.github.io/"target="_blank" rel="external nofollow noopener noreferrer">LiuDongdong</a></span><span class="license footer-divider"><a rel="license external nofollow noopener noreferrer" href="https://creativecommons.org/licenses/by-nc/4.0/" target="_blank">CC BY-NC 4.0</a></span></div><div class="footer-line statistics"><span class="site-time" title='网站运行中 ...'><i class="fa-solid fa-heartbeat fa-fw animate-icon" aria-hidden="true"></i>&nbsp;<span class="run-times">网站运行中 ...</span></span></div><div class="footer-line ibruce">
          <span id="busuanzi_container_site_uv" title='总访客数'><i class="fa-regular fa-user fa-fw" aria-hidden="true"></i>&nbsp;<span id="busuanzi_value_site_uv"><i class="fa-solid fa-spinner fa-spin fa-fw" aria-hidden="true"></i></span></span><span id="busuanzi_container_site_pv" class="footer-divider" title='总访问量'><i class="fa-regular fa-eye fa-fw" aria-hidden="true"></i>&nbsp;<span id="busuanzi_value_site_pv"><i class="fa-solid fa-spinner fa-spin fa-fw" aria-hidden="true"></i></span></span>
        </div></div>
  </footer></div><div class="widgets"><div class="fixed-buttons animate__faster d-none"><div class="fixed-button back-to-top" role="button" aria-label="回到顶部"><i class="fa-solid fa-arrow-up fa-fw" aria-hidden="true"></i><span class="variant-numeric">0%</span>
        </div></div><a href="https://liudongdong1.github.io/" title="在 GitHub 上查看源代码"target="_blank" rel="external nofollow" class="github-corner right d-none-mobile"><svg viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><div id="mask"></div><div class="reading-progress-bar" style="left: 0;top: 0;--bg-progress: #0076ff;--bg-progress-dark: #fff;"></div><noscript>
    <div class="noscript-warning">FixIt 主题在启用 JavaScript 的情况下效果最佳。</div>
  </noscript>
</div><link rel="stylesheet" href="/liudongdong1.github.io/lib/katex/katex.min.css"><link rel="stylesheet" href="/liudongdong1.github.io/lib/cookieconsent/cookieconsent.min.css"><script src="/liudongdong1.github.io/lib/autocomplete/autocomplete.min.js" defer></script><script src="/liudongdong1.github.io/lib/algoliasearch/algoliasearch-lite.umd.min.js" defer></script><script src="/liudongdong1.github.io/lib/lazysizes/lazysizes.min.js" async defer></script><script src="/liudongdong1.github.io/lib/sharer/sharer.min.js" async defer></script><script src="/liudongdong1.github.io/lib/typeit/index.umd.js" defer></script><script src="/liudongdong1.github.io/lib/katex/katex.min.js" defer></script><script src="/liudongdong1.github.io/lib/katex/auto-render.min.js" defer></script><script src="/liudongdong1.github.io/lib/katex/copy-tex.min.js" defer></script><script src="/liudongdong1.github.io/lib/katex/mhchem.min.js" defer></script><script src="/liudongdong1.github.io/lib/cookieconsent/cookieconsent.min.js" defer></script><script src="/liudongdong1.github.io/lib/pangu/pangu.min.js" defer></script><script src="/liudongdong1.github.io/lib/cell-watermark/watermark.min.js" defer></script><script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async defer></script><script>window.config={"code":{"copyTitle":"复制到剪贴板","editLockTitle":"锁定可编辑代码块","editUnLockTitle":"解锁可编辑代码块","editable":true,"maxShownLines":10},"comment":{"enable":false},"cookieconsent":{"content":{"dismiss":"同意","link":"了解更多","message":"本网站使用 Cookies 来改善您的浏览体验。"},"enable":true,"palette":{"button":{"background":"#f0f0f0"},"popup":{"background":"#1aa3ff"}},"theme":"edgeless"},"data":{"typeit-header-subtitle-desktop":"\u003cspan style='font-family: MMT,\"沐目体\";'\u003e吾日三省吾身\u003c/span\u003e","typeit-header-subtitle-mobile":"\u003cspan style='font-family: MMT,\"沐目体\";'\u003e吾日三省吾身\u003c/span\u003e"},"enablePWA":true,"enablePangu":true,"math":{"delimiters":[{"display":true,"left":"$$","right":"$$"},{"display":true,"left":"\\[","right":"\\]"},{"display":true,"left":"\\begin{equation}","right":"\\end{equation}"},{"display":true,"left":"\\begin{equation*}","right":"\\end{equation*}"},{"display":true,"left":"\\begin{align}","right":"\\end{align}"},{"display":true,"left":"\\begin{align*}","right":"\\end{align*}"},{"display":true,"left":"\\begin{alignat}","right":"\\end{alignat}"},{"display":true,"left":"\\begin{alignat*}","right":"\\end{alignat*}"},{"display":true,"left":"\\begin{gather}","right":"\\end{gather}"},{"display":true,"left":"\\begin{CD}","right":"\\end{CD}"},{"display":false,"left":"$","right":"$"},{"display":false,"left":"\\(","right":"\\)"}],"strict":false},"search":{"algoliaAppID":"2R1K9SKLQZ","algoliaIndex":"index.zh-cn","algoliaSearchKey":"4a226aa1c5c98d6859e4d1386adb2bc7","highlightTag":"em","maxResultLength":10,"noResultsFound":"没有找到结果","snippetLength":50,"type":"algolia"},"siteTime":"2020-12-18T16:15:22+08:00","typeit":{"cursorChar":"|","cursorSpeed":1000,"data":{"typeit-header-subtitle-desktop":["typeit-header-subtitle-desktop"],"typeit-header-subtitle-mobile":["typeit-header-subtitle-mobile"]},"duration":-1,"speed":100},"watermark":{"appendto":".wrapper\u003emain","colspacing":30,"content":"\u003cimg class=\"fixit-icon\" src=\"/fixit.min.svg\" alt=\"FixIt logo\" /\u003e FixIt 主题","enable":true,"fontfamily":"inherit","fontsize":0.85,"height":21,"opacity":0.0125,"rotate":15,"rowspacing":60,"width":150}};</script><script src="/liudongdong1.github.io/js/theme.min.js" defer></script><script src="/liudongdong1.github.io/js/custom.min.js" defer></script></body>
</html>
