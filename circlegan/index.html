<!DOCTYPE html>
<html itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">
  <head>
    
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
    <meta name="robots" content="noodp" />
    <title>CircleGan - DAY By DAY</title><meta name="author" content="LiuDongdong">
<meta name="author-link" content="https://liudongdong1.github.io/">
<meta name="description" content="Zhu, Jun-Yan, et al. &ldquo;Unpaired image-to-image translation using cycle-consistent adversarial networks.&rdquo; Proceedings of the IEEE international conference on computer vision. 2017. cite 10600 [pdf] [code] Paper: CircleGAN Summary present a method that can learn to do the same, capturing special characteristics of one image collection and figureing out how these characteristics could be translated into the other image collection. Research Objective Application Area: Collection style transfer: learns to mimic the" /><meta name="keywords" content='GAN' /><meta itemprop="name" content="CircleGan">
<meta itemprop="description" content="Zhu, Jun-Yan, et al. &ldquo;Unpaired image-to-image translation using cycle-consistent adversarial networks.&rdquo; Proceedings of the IEEE international conference on computer vision. 2017. cite 10600 [pdf] [code] Paper: CircleGAN Summary present a method that can learn to do the same, capturing special characteristics of one image collection and figureing out how these characteristics could be translated into the other image collection. Research Objective Application Area: Collection style transfer: learns to mimic the"><meta itemprop="datePublished" content="2021-11-20T22:45:45+00:00" />
<meta itemprop="dateModified" content="2023-12-31T15:30:45+08:00" />
<meta itemprop="wordCount" content="2745"><meta itemprop="image" content="https://liudongdong1.github.io/logo.png"/>
<meta itemprop="keywords" content="GAN," /><meta property="og:title" content="CircleGan" />
<meta property="og:description" content="Zhu, Jun-Yan, et al. &ldquo;Unpaired image-to-image translation using cycle-consistent adversarial networks.&rdquo; Proceedings of the IEEE international conference on computer vision. 2017. cite 10600 [pdf] [code] Paper: CircleGAN Summary present a method that can learn to do the same, capturing special characteristics of one image collection and figureing out how these characteristics could be translated into the other image collection. Research Objective Application Area: Collection style transfer: learns to mimic the" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://liudongdong1.github.io/circlegan/" /><meta property="og:image" content="https://liudongdong1.github.io/logo.png"/><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2021-11-20T22:45:45+00:00" />
<meta property="article:modified_time" content="2023-12-31T15:30:45+08:00" />
<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://liudongdong1.github.io/logo.png"/>

<meta name="twitter:title" content="CircleGan"/>
<meta name="twitter:description" content="Zhu, Jun-Yan, et al. &ldquo;Unpaired image-to-image translation using cycle-consistent adversarial networks.&rdquo; Proceedings of the IEEE international conference on computer vision. 2017. cite 10600 [pdf] [code] Paper: CircleGAN Summary present a method that can learn to do the same, capturing special characteristics of one image collection and figureing out how these characteristics could be translated into the other image collection. Research Objective Application Area: Collection style transfer: learns to mimic the"/>
<meta name="application-name" content="DAY By DAY">
<meta name="apple-mobile-web-app-title" content="DAY By DAY"><meta name="theme-color" data-light="#f8f8f8" data-dark="#252627" content="#f8f8f8"><meta name="msapplication-TileColor" content="#da532c"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" />
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"><link rel="manifest" href="/site.webmanifest"><link rel="canonical" href="https://liudongdong1.github.io/circlegan/" /><link rel="prev" href="https://liudongdong1.github.io/eventbus/" /><link rel="next" href="https://liudongdong1.github.io/mvp-arch/" /><link rel="stylesheet" href="/css/style.min.css"><link rel="stylesheet" href="/lib/fontawesome-free/all.min.css"><link rel="stylesheet" href="/lib/animate/animate.min.css"><script type="application/ld+json">
  {
    "@context": "http://schema.org",
    "@type": "BlogPosting",
    "headline": "CircleGan",
    "inLanguage": "zh-CN",
    "mainEntityOfPage": {
      "@type": "WebPage",
      "@id": "https:\/\/liudongdong1.github.io\/circlegan\/"
    },"genre": "posts","keywords": "GAN","wordcount":  2745 ,
    "url": "https:\/\/liudongdong1.github.io\/circlegan\/","datePublished": "2021-11-20T22:45:45+00:00","dateModified": "2023-12-31T15:30:45+08:00","license": "This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.","publisher": {
      "@type": "Organization",
      "name": "LiuDongdong","logo": "https:\/\/liudongdong1.github.io\/images\/person.png"},"author": {
        "@type": "Person",
        "name": "liudongdong1"
      },"description": ""
  }
  </script></head>
  <body data-header-desktop="auto" data-header-mobile="auto"><script>(window.localStorage?.getItem('theme') ? localStorage.getItem('theme') === 'dark' : ('auto' === 'auto' ? window.matchMedia('(prefers-color-scheme: dark)').matches : 'auto' === 'dark')) && document.body.setAttribute('data-theme', 'dark');</script><div class="wrapper"><script type="text/javascript"
        async
        src="https://cdnjs.cloudflare.com/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$']],
    displayMath: [['$$','$$']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
});

MathJax.Hub.Queue(function() {
    
    
    
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>

<style>
code.has-jax {
    font: inherit;
    font-size: 100%;
    background: inherit;
    border: inherit;
    color: #515151;
}
</style>
<header class="desktop animate__faster" id="header-desktop">
  <div class="header-wrapper" data-github-corner="right">
    <div class="header-title">
      <a href="/" title="DAY By DAY"><img
    class="lazyload logo"
    src="/svg/loading.min.svg"
    data-src="/fixit.min.svg"
    data-srcset="/fixit.min.svg, /fixit.min.svg 1.5x, /fixit.min.svg 2x"
    data-sizes="auto"
    alt="DAY By DAY"
    title="DAY By DAY"/><span class="header-title-text"></span></a><span id="typeit-header-subtitle-desktop" class="typeit header-subtitle"></span></div>
    <nav>
      <ul class="menu"><li class="menu-item">
              <a
                class="menu-link"
                href="/posts/"
                
                
              ><i class="fa-solid fa-archive fa-fw fa-sm" aria-hidden="true"></i> 所有文章</a></li><li class="menu-item">
              <a
                class="menu-link"
                href="/categories/"
                
                
              ><i class="fa-solid fa-th fa-fw fa-sm" aria-hidden="true"></i> 分类</a></li><li class="menu-item">
              <a
                class="menu-link"
                href="/tags/"
                
                
              ><i class="fa-solid fa-tags fa-fw fa-sm" aria-hidden="true"></i> 标签</a></li><li class="menu-item">
              <a
                class="menu-link"
                href="/friends/"
                title="友情链接"
                
              ><i class="fa-solid fa-users fa-fw fa-sm" aria-hidden="true"></i> 友链</a></li><li class="menu-item">
              <a
                class="menu-link"
                href="/about/"
                
                
              ><i class="fa-solid fa-info-circle fa-fw fa-sm" aria-hidden="true"></i> 关于</a></li><li class="menu-item delimiter"></li><li class="menu-item language">
            <span role="button" aria-label="选择语言" title="选择语言">简体中文<i class="dropdown-icon fa-solid fa-chevron-down" aria-hidden="true"></i>
            </span>
            <ul class="sub-menu"><li class="menu-item">没有更多翻译</li></ul>
          </li><li class="menu-item search" id="search-desktop">
            <input type="text" placeholder="搜索文章标题或内容 ..." id="search-input-desktop">
            <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-desktop" title="搜索">
              <i class="fa-solid fa-search fa-fw" aria-hidden="true"></i>
            </a>
            <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-desktop" title="清空">
              <i class="fa-solid fa-times-circle fa-fw" aria-hidden="true"></i>
            </a>
            <span class="search-button search-loading" id="search-loading-desktop">
              <i class="fa-solid fa-spinner fa-fw fa-spin" aria-hidden="true"></i>
            </span>
          </li><li class="menu-item theme-switch" title="切换主题">
          <i class="fa-solid fa-adjust fa-fw" aria-hidden="true"></i>
        </li>
      </ul>
    </nav>
  </div>
</header><header class="mobile animate__faster" id="header-mobile">
  <div class="header-container">
    <div class="header-wrapper">
      <div class="header-title">
        <a href="/" title="DAY By DAY"><img
    class="lazyload logo"
    src="/svg/loading.min.svg"
    data-src="/fixit.min.svg"
    data-srcset="/fixit.min.svg, /fixit.min.svg 1.5x, /fixit.min.svg 2x"
    data-sizes="auto"
    alt="/fixit.min.svg"
    title="/fixit.min.svg"/><span class="header-title-text"></span></a><span id="typeit-header-subtitle-mobile" class="typeit header-subtitle"></span></div>
      <div class="menu-toggle" id="menu-toggle-mobile">
        <span></span><span></span><span></span>
      </div>
    </div>
    <nav>
      <ul class="menu" id="menu-mobile"><li class="search-wrapper">
            <div class="search mobile" id="search-mobile">
              <input type="text" placeholder="搜索文章标题或内容 ..." id="search-input-mobile">
              <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-mobile" title="搜索">
                <i class="fa-solid fa-search fa-fw" aria-hidden="true"></i>
              </a>
              <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-mobile" title="清空">
                <i class="fa-solid fa-times-circle fa-fw" aria-hidden="true"></i>
              </a>
              <span class="search-button search-loading" id="search-loading-mobile">
                <i class="fa-solid fa-spinner fa-fw fa-spin" aria-hidden="true"></i>
              </span>
            </div>
            <a href="javascript:void(0);" class="search-cancel" id="search-cancel-mobile">
              取消
            </a>
          </li><li
              class="menu-item"
            ><a
                  class="menu-link"
                  href="/posts/"
                  
                  
                ><i class="fa-solid fa-archive fa-fw fa-sm" aria-hidden="true"></i> 所有文章</a></li><li
              class="menu-item"
            ><a
                  class="menu-link"
                  href="/categories/"
                  
                  
                ><i class="fa-solid fa-th fa-fw fa-sm" aria-hidden="true"></i> 分类</a></li><li
              class="menu-item"
            ><a
                  class="menu-link"
                  href="/tags/"
                  
                  
                ><i class="fa-solid fa-tags fa-fw fa-sm" aria-hidden="true"></i> 标签</a></li><li
              class="menu-item"
            ><a
                  class="menu-link"
                  href="/friends/"
                  title="友情链接"
                  
                ><i class="fa-solid fa-users fa-fw fa-sm" aria-hidden="true"></i> 友链</a></li><li
              class="menu-item"
            ><a
                  class="menu-link"
                  href="/about/"
                  
                  
                ><i class="fa-solid fa-info-circle fa-fw fa-sm" aria-hidden="true"></i> 关于</a></li><li
              class="menu-item text-center"
            ><a
                  class="menu-link"
                  href="/"
                  title="GitHub"
                  
                ><i class='fa-brands fa-github fa-fw' aria-hidden='true'></i> </a></li><li class="menu-item theme-switch" title="切换主题">
          <i class="fa-solid fa-adjust fa-fw" aria-hidden="true"></i>
        </li><li class="menu-item language">
            <span role="button" aria-label="选择语言" title="选择语言">简体中文<i class="dropdown-icon fa-solid fa-chevron-down" aria-hidden="true"></i>
            </span>
            <select class="language-select" onchange="location = this.value;"><option disabled>没有更多翻译</option></select>
          </li></ul>
    </nav>
  </div>
</header><div class="search-dropdown desktop">
    <div id="search-dropdown-desktop"></div>
  </div>
  <div class="search-dropdown mobile">
    <div id="search-dropdown-mobile"></div>
  </div><main class="container" data-page-style="normal"><aside class="toc" id="toc-auto"><h2 class="toc-title">目录 <i class="toc-icon fa-solid fa-angle-down fa-fw"></i></h2>
      <div class="toc-content" id="toc-content-auto"></div></aside>

  <aside class="aside-custom" id="aside-sakana">
    

<div class="sakana-widget">
  <div class="sakana-item" id="takina-widget"></div>
  <div class="sakana-item" id="chisato-widget"></div>
</div>
<script>
  function initSakanaWidget() {
    const takina = SakanaWidget.getCharacter('takina')
    SakanaWidget.registerCharacter('takina-slow', takina);
    new SakanaWidget({
      character: 'takina-slow',
      controls: false,
      autoFit: true,
      stroke: {
        color: "#b4b4b4",
        width: 2
      }
    }).mount('#takina-widget');

    const chisato = SakanaWidget.getCharacter('chisato')
    SakanaWidget.registerCharacter('chisato-slow', chisato);
    new SakanaWidget({
      character: 'chisato-slow',
      controls: false,
      autoFit: true,
      stroke: {
        color: "#b4b4b4",
        width: 2
      }
    }).mount('#chisato-widget');
  }
</script>
<script async onload="initSakanaWidget()" src="https://cdn.jsdelivr.net/npm/sakana-widget@2.3.0/lib/sakana.min.js">
</script></aside>

  <article class="page single">
    <div class="header"><h1 class="single-title animate__animated animate__flipInX">
        <span>CircleGan</span>
      </h1></div><div class="post-meta">
      <div class="post-meta-line"><span class="post-author"><span class="author"><i class="fa-solid fa-user-circle" aria-hidden="true"></i>
      liudongdong1</span></span>
          <span class="post-category">收录于 <a href="/categories/"><i class="fa-regular fa-folder fa-fw"></i>&nbsp;Categories</a>&ensp;<a href="/categories/%E8%A7%86%E8%A7%89ai/"><i class="fa-regular fa-folder fa-fw"></i>&nbsp;视觉AI</a></span></div>
      <div class="post-meta-line"><span title=2021-11-20&#32;22:45:45>
            <i class="fa-regular fa-calendar-alt fa-fw"></i>&nbsp;<time datetime="2021-11-20" >2021-11-20</time>
          </span>&nbsp;<i class="fa-solid fa-pencil-alt fa-fw"></i>&nbsp;约 2745 字&nbsp;
        <i class="fa-regular fa-clock fa-fw"></i>&nbsp;预计阅读 6 分钟&nbsp;<span id="busuanzi_container_page_pv" class="busuanzi_visitors comment-visitors" data-flag-title="CircleGan">
            <i class="fa-regular fa-eye fa-fw"></i>&nbsp;<span id="busuanzi_value_page_pv">-</span>&nbsp;次阅读
          </span>&nbsp;</div>
    </div><div class="featured-image"><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://cdn.stocksnap.io/img-thumbs/280h/TDAU1ERCD4.jpg"
    data-srcset="https://cdn.stocksnap.io/img-thumbs/280h/TDAU1ERCD4.jpg, https://cdn.stocksnap.io/img-thumbs/280h/TDAU1ERCD4.jpg 1.5x, https://cdn.stocksnap.io/img-thumbs/280h/TDAU1ERCD4.jpg 2x"
    data-sizes="auto"
    alt="https://cdn.stocksnap.io/img-thumbs/280h/TDAU1ERCD4.jpg"
    title="https://cdn.stocksnap.io/img-thumbs/280h/TDAU1ERCD4.jpg"/></div><div class="details toc" id="toc-static" kept="true">
        <div class="details-summary toc-title">
          <span>目录</span>
          <span><i class="details-icon fa-solid fa-angle-right"></i></span>
        </div>
        <div class="details-content toc-content" id="toc-content-static"><nav id="TableOfContents">
  <ul>
    <li>
      <ul>
        <li></li>
      </ul>
    </li>
  </ul>

  <ul>
    <li>
      <ul>
        <li></li>
      </ul>
    </li>
  </ul>

  <ul>
    <li>
      <ul>
        <li></li>
      </ul>
    </li>
  </ul>
</nav></div>
      </div><div
      class="content"
      id="content"
      
      
    ><blockquote>
<p>Zhu, Jun-Yan, et al. &ldquo;Unpaired image-to-image translation using cycle-consistent adversarial networks.&rdquo; <em>Proceedings of the IEEE international conference on computer vision</em>. 2017.  cite 10600  [<a href="chrome-extension://ikhdkkncnoglghljlkmcimlnlhkeamad/pdf-viewer/web/viewer.html?file=https%3A%2F%2Farxiv.org%2Fpdf%2F1703.10593.pdf"target="_blank" rel="external nofollow noopener noreferrer">pdf<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>] [<a href="https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix"target="_blank" rel="external nofollow noopener noreferrer">code<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>]</p>
</blockquote>
<hr>
<h1 id="paper-circlegan">Paper: CircleGAN</h1>
<!-- raw HTML omitted -->
<h4 id="summary">Summary</h4>
<ol>
<li>present a method that can learn to do the same, <code>capturing special characteristics </code>of one image collection and figureing out how these characteristics could be <code>translated into the other image collection</code>.</li>
</ol>
<h4 id="research-objective">Research Objective</h4>
<ul>
<li><strong>Application Area</strong>:
<ul>
<li><code>Collection style transfer</code>: learns to mimic the style of an entire collection of artworks, rather than transferring the style of a single selseted piece of art.</li>
<li><code>Object transfiguration:</code>  trained to tranlate one object class form ImageNet to another, or translate one object into another object of the same category, or translate between two visually similar categories.</li>
<li><code>Season transfer:</code></li>
<li><code>Photo generation from paintings:</code></li>
<li><code>Photo enhancement:</code> generate photos with shallower depth of field.</li>
</ul>
</li>
<li><strong>Purpose</strong>:
<ul>
<li>to lear the mapping between an input image and an output image using a training set of aligned image pairs.</li>
<li>to translate an image from a source domain X to a target domain Y in the absence of paired examples.</li>
</ul>
</li>
</ul>
<h4 id="relative-work">Relative work:</h4>
<ul>
<li><strong>Generative Adversarial Networks(GANs):</strong>   like image generation, image editing, representation learing, text2image, image inpainting, future prediction.   <code>the adversarial loss that forces the generated images to be</code></li>
<li><strong>Image-to-Image Translation</strong>: use a dataset of input-output examples to learn a parametric translation function using CNNs,  using pix2pix framework, a conditional generative adversarial network to learn a mapping from input to output images, like <code>generating photographs from sketches or from attribute and semantic layouts</code></li>
<li><strong>Unpaired Image-to-Image Translation:</strong> use a weight-sharing strategy to learn a common representation across domains.</li>
<li><strong>Cycle Consistency:</strong> using transitivity as a way to regularize structured data.</li>
<li><strong>Neural Style Transfer</strong>: synthesizes a novel image by combining the<code> content of one image with the style of another image</code> based on matching the <code>Gram matrix statistics of pre-trained deep features</code>. <code>to capture correspondences between higher-level appearance structures</code></li>
</ul>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211120123435526.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211120123435526.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211120123435526.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211120123435526.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211120123435526.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211120123435526.png"/></p>
<h4 id="methods">Methods</h4>
<ul>
<li><strong>Problem Formulation</strong>:</li>
</ul>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211120161816552.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211120161816552.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211120161816552.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211120161816552.png 2x"
    data-sizes="auto"
    alt="image-20211120161816552"
    title="image-20211120161816552"/></p>
<ul>
<li><strong>system overview</strong>:
<ul>
<li>given any two unordered image collections X, Y, our algorithm learns to automatically translate an image from one into the other and vice versa.</li>
</ul>
</li>
</ul>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211120160938235.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211120160938235.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211120160938235.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211120160938235.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211120160938235.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211120160938235.png"/></p>
<ul>
<li>idt loss的定义在论文的application之中，防止input 与out put之间的color compostion过多</li>
<li><strong>Adversarial loss</strong>:  尽可能让生成器生成的数据分布接近于真实的数据分布</li>
</ul>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211120163349319.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211120163349319.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211120163349319.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211120163349319.png 2x"
    data-sizes="auto"
    alt="adversarial loss"
    title="adversarial loss"/></p>
<ul>
<li><strong>Cycle Consistency Loss:</strong>  due to the factor that the adversarial losses alone cannot guarantee that the learned function can map an individual input x to a desired output y.  <code>for each image x from domain x, the image translation cycle should be able to bring x back to the original image.</code></li>
</ul>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211120164015832.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211120164015832.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211120164015832.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211120164015832.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211120164015832.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211120164015832.png"/></p>
<ul>
<li>生成器G的loss： self.loss_G_A + self.loss_G_B + self.loss_cycle_A + self.loss_cycle_B + self.loss_idt_A + self.loss_idt_B</li>
<li>判别器D的loss：<img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211121125715426.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211121125715426.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211121125715426.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211121125715426.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211121125715426.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211121125715426.png"/></li>
</ul>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211120163705404.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211120163705404.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211120163705404.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211120163705404.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211120163705404.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211120163705404.png"/></p>
<ul>
<li><strong>Full Objective</strong></li>
</ul>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211120164258463.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211120164258463.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211120164258463.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211120164258463.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211120164258463.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211120164258463.png"/></p>
<h4 id="evaluation">Evaluation</h4>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211120122855683.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211120122855683.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211120122855683.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211120122855683.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211120122855683.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211120122855683.png"/></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211120164439195.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211120164439195.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211120164439195.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211120164439195.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211120164439195.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211120164439195.png"/></p>
<h4 id="code">Code</h4>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211121124249044.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211121124249044.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211121124249044.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211121124249044.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211121124249044.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211121124249044.png"/></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> torch
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> itertools
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> util.image_pool <span style="color:#f92672">import</span> ImagePool
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> .base_model <span style="color:#f92672">import</span> BaseModel
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> . <span style="color:#f92672">import</span> networks
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">CycleGANModel</span>(BaseModel):
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    This class implements the CycleGAN model, for learning image-to-image translation without paired data.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    The model training requires &#39;--dataset_mode unaligned&#39; dataset.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    By default, it uses a &#39;--netG resnet_9blocks&#39; ResNet generator,
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    a &#39;--netD basic&#39; discriminator (PatchGAN introduced by pix2pix),
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    and a least-square GANs objective (&#39;--gan_mode lsgan&#39;).
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    CycleGAN paper: https://arxiv.org/pdf/1703.10593.pdf
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#a6e22e">@staticmethod</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">modify_commandline_options</span>(parser, is_train<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>):
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;&#34;&#34;Add new dataset-specific options, and rewrite default values for existing options.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        Parameters:
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            parser          -- original option parser
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            is_train (bool) -- whether training phase or test phase. You can use this flag to add training-specific or test-specific options.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        Returns:
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            the modified parser.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        For CycleGAN, in addition to GAN losses, we introduce lambda_A, lambda_B, and lambda_identity for the following losses.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        A (source domain), B (target domain).
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        Generators: G_A: A -&gt; B; G_B: B -&gt; A.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        Discriminators: D_A: G_A(A) vs. B; D_B: G_B(B) vs. A.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        Forward cycle loss:  lambda_A * ||G_B(G_A(A)) - A|| (Eqn. (2) in the paper)
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        Backward cycle loss: lambda_B * ||G_A(G_B(B)) - B|| (Eqn. (2) in the paper)
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        Identity loss (optional): lambda_identity * (||G_A(B) - B|| * lambda_B + ||G_B(A) - A|| * lambda_A) (Sec 5.2 &#34;Photo generation from paintings&#34; in the paper)
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        Dropout is not used in the original CycleGAN paper.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>        parser<span style="color:#f92672">.</span>set_defaults(no_dropout<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)  <span style="color:#75715e"># default CycleGAN did not use dropout</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> is_train:
</span></span><span style="display:flex;"><span>            parser<span style="color:#f92672">.</span>add_argument(<span style="color:#e6db74">&#39;--lambda_A&#39;</span>, type<span style="color:#f92672">=</span>float, default<span style="color:#f92672">=</span><span style="color:#ae81ff">10.0</span>, help<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;weight for cycle loss (A -&gt; B -&gt; A)&#39;</span>)
</span></span><span style="display:flex;"><span>            parser<span style="color:#f92672">.</span>add_argument(<span style="color:#e6db74">&#39;--lambda_B&#39;</span>, type<span style="color:#f92672">=</span>float, default<span style="color:#f92672">=</span><span style="color:#ae81ff">10.0</span>, help<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;weight for cycle loss (B -&gt; A -&gt; B)&#39;</span>)
</span></span><span style="display:flex;"><span>            parser<span style="color:#f92672">.</span>add_argument(<span style="color:#e6db74">&#39;--lambda_identity&#39;</span>, type<span style="color:#f92672">=</span>float, default<span style="color:#f92672">=</span><span style="color:#ae81ff">0.5</span>, help<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;use identity mapping. Setting lambda_identity other than 0 has an effect of scaling the weight of the identity mapping loss. For example, if the weight of the identity loss should be 10 times smaller than the weight of the reconstruction loss, please set lambda_identity = 0.1&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> parser
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, opt):
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;&#34;&#34;Initialize the CycleGAN class.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        Parameters:
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            opt (Option class)-- stores all the experiment flags; needs to be a subclass of BaseOptions
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        BaseModel<span style="color:#f92672">.</span>__init__(self, opt)
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># specify the training losses you want to print out. The training/test scripts will call &lt;BaseModel.get_current_losses&gt;</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>loss_names <span style="color:#f92672">=</span> [<span style="color:#e6db74">&#39;D_A&#39;</span>, <span style="color:#e6db74">&#39;G_A&#39;</span>, <span style="color:#e6db74">&#39;cycle_A&#39;</span>, <span style="color:#e6db74">&#39;idt_A&#39;</span>, <span style="color:#e6db74">&#39;D_B&#39;</span>, <span style="color:#e6db74">&#39;G_B&#39;</span>, <span style="color:#e6db74">&#39;cycle_B&#39;</span>, <span style="color:#e6db74">&#39;idt_B&#39;</span>]
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># specify the images you want to save/display. The training/test scripts will call &lt;BaseModel.get_current_visuals&gt;</span>
</span></span><span style="display:flex;"><span>        visual_names_A <span style="color:#f92672">=</span> [<span style="color:#e6db74">&#39;real_A&#39;</span>, <span style="color:#e6db74">&#39;fake_B&#39;</span>, <span style="color:#e6db74">&#39;rec_A&#39;</span>]
</span></span><span style="display:flex;"><span>        visual_names_B <span style="color:#f92672">=</span> [<span style="color:#e6db74">&#39;real_B&#39;</span>, <span style="color:#e6db74">&#39;fake_A&#39;</span>, <span style="color:#e6db74">&#39;rec_B&#39;</span>]
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> self<span style="color:#f92672">.</span>isTrain <span style="color:#f92672">and</span> self<span style="color:#f92672">.</span>opt<span style="color:#f92672">.</span>lambda_identity <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">0.0</span>:  <span style="color:#75715e"># if identity loss is used, we also visualize idt_B=G_A(B) ad idt_A=G_A(B)</span>
</span></span><span style="display:flex;"><span>            visual_names_A<span style="color:#f92672">.</span>append(<span style="color:#e6db74">&#39;idt_B&#39;</span>)
</span></span><span style="display:flex;"><span>            visual_names_B<span style="color:#f92672">.</span>append(<span style="color:#e6db74">&#39;idt_A&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>visual_names <span style="color:#f92672">=</span> visual_names_A <span style="color:#f92672">+</span> visual_names_B  <span style="color:#75715e"># combine visualizations for A and B</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># specify the models you want to save to the disk. The training/test scripts will call &lt;BaseModel.save_networks&gt; and &lt;BaseModel.load_networks&gt;.</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> self<span style="color:#f92672">.</span>isTrain:
</span></span><span style="display:flex;"><span>            self<span style="color:#f92672">.</span>model_names <span style="color:#f92672">=</span> [<span style="color:#e6db74">&#39;G_A&#39;</span>, <span style="color:#e6db74">&#39;G_B&#39;</span>, <span style="color:#e6db74">&#39;D_A&#39;</span>, <span style="color:#e6db74">&#39;D_B&#39;</span>]
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">else</span>:  <span style="color:#75715e"># during test time, only load Gs</span>
</span></span><span style="display:flex;"><span>            self<span style="color:#f92672">.</span>model_names <span style="color:#f92672">=</span> [<span style="color:#e6db74">&#39;G_A&#39;</span>, <span style="color:#e6db74">&#39;G_B&#39;</span>]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># define networks (both Generators and discriminators)</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># The naming is different from those used in the paper.</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Code (vs. paper): G_A (G), G_B (F), D_A (D_Y), D_B (D_X)</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>netG_A <span style="color:#f92672">=</span> networks<span style="color:#f92672">.</span>define_G(opt<span style="color:#f92672">.</span>input_nc, opt<span style="color:#f92672">.</span>output_nc, opt<span style="color:#f92672">.</span>ngf, opt<span style="color:#f92672">.</span>netG, opt<span style="color:#f92672">.</span>norm,
</span></span><span style="display:flex;"><span>                                        <span style="color:#f92672">not</span> opt<span style="color:#f92672">.</span>no_dropout, opt<span style="color:#f92672">.</span>init_type, opt<span style="color:#f92672">.</span>init_gain, self<span style="color:#f92672">.</span>gpu_ids)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>netG_B <span style="color:#f92672">=</span> networks<span style="color:#f92672">.</span>define_G(opt<span style="color:#f92672">.</span>output_nc, opt<span style="color:#f92672">.</span>input_nc, opt<span style="color:#f92672">.</span>ngf, opt<span style="color:#f92672">.</span>netG, opt<span style="color:#f92672">.</span>norm,
</span></span><span style="display:flex;"><span>                                        <span style="color:#f92672">not</span> opt<span style="color:#f92672">.</span>no_dropout, opt<span style="color:#f92672">.</span>init_type, opt<span style="color:#f92672">.</span>init_gain, self<span style="color:#f92672">.</span>gpu_ids)
</span></span><span style="display:flex;"><span>        <span style="color:#75715e">#self.Alignment_net = Alignment()</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> self<span style="color:#f92672">.</span>isTrain:  <span style="color:#75715e"># define discriminators</span>
</span></span><span style="display:flex;"><span>            self<span style="color:#f92672">.</span>netD_A <span style="color:#f92672">=</span> networks<span style="color:#f92672">.</span>define_D(opt<span style="color:#f92672">.</span>output_nc, opt<span style="color:#f92672">.</span>ndf, opt<span style="color:#f92672">.</span>netD,
</span></span><span style="display:flex;"><span>                                            opt<span style="color:#f92672">.</span>n_layers_D, opt<span style="color:#f92672">.</span>norm, opt<span style="color:#f92672">.</span>init_type, opt<span style="color:#f92672">.</span>init_gain, self<span style="color:#f92672">.</span>gpu_ids)
</span></span><span style="display:flex;"><span>            self<span style="color:#f92672">.</span>netD_B <span style="color:#f92672">=</span> networks<span style="color:#f92672">.</span>define_D(opt<span style="color:#f92672">.</span>input_nc, opt<span style="color:#f92672">.</span>ndf, opt<span style="color:#f92672">.</span>netD,
</span></span><span style="display:flex;"><span>                                            opt<span style="color:#f92672">.</span>n_layers_D, opt<span style="color:#f92672">.</span>norm, opt<span style="color:#f92672">.</span>init_type, opt<span style="color:#f92672">.</span>init_gain, self<span style="color:#f92672">.</span>gpu_ids)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> self<span style="color:#f92672">.</span>isTrain:
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">if</span> opt<span style="color:#f92672">.</span>lambda_identity <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">0.0</span>:  <span style="color:#75715e"># only works when input and output images have the same number of channels</span>
</span></span><span style="display:flex;"><span>                <span style="color:#66d9ef">assert</span>(opt<span style="color:#f92672">.</span>input_nc <span style="color:#f92672">==</span> opt<span style="color:#f92672">.</span>output_nc)
</span></span><span style="display:flex;"><span>            self<span style="color:#f92672">.</span>fake_A_pool <span style="color:#f92672">=</span> ImagePool(opt<span style="color:#f92672">.</span>pool_size)  <span style="color:#75715e"># create image buffer to store previously generated images</span>
</span></span><span style="display:flex;"><span>            self<span style="color:#f92672">.</span>fake_B_pool <span style="color:#f92672">=</span> ImagePool(opt<span style="color:#f92672">.</span>pool_size)  <span style="color:#75715e"># create image buffer to store previously generated images</span>
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># define loss functions</span>
</span></span><span style="display:flex;"><span>            self<span style="color:#f92672">.</span>criterionGAN <span style="color:#f92672">=</span> networks<span style="color:#f92672">.</span>GANLoss(opt<span style="color:#f92672">.</span>gan_mode)<span style="color:#f92672">.</span>to(self<span style="color:#f92672">.</span>device)  <span style="color:#75715e"># define GAN loss.</span>
</span></span><span style="display:flex;"><span>            self<span style="color:#f92672">.</span>criterionCycle <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>L1Loss()
</span></span><span style="display:flex;"><span>            self<span style="color:#f92672">.</span>criterionIdt <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>L1Loss()
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># initialize optimizers; schedulers will be automatically created by function &lt;BaseModel.setup&gt;.</span>
</span></span><span style="display:flex;"><span>            self<span style="color:#f92672">.</span>optimizer_G <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>optim<span style="color:#f92672">.</span>Adam(itertools<span style="color:#f92672">.</span>chain(self<span style="color:#f92672">.</span>netG_A<span style="color:#f92672">.</span>parameters(), self<span style="color:#f92672">.</span>netG_B<span style="color:#f92672">.</span>parameters()), lr<span style="color:#f92672">=</span>opt<span style="color:#f92672">.</span>lr, betas<span style="color:#f92672">=</span>(opt<span style="color:#f92672">.</span>beta1, <span style="color:#ae81ff">0.999</span>))
</span></span><span style="display:flex;"><span>            self<span style="color:#f92672">.</span>optimizer_D <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>optim<span style="color:#f92672">.</span>Adam(itertools<span style="color:#f92672">.</span>chain(self<span style="color:#f92672">.</span>netD_A<span style="color:#f92672">.</span>parameters(), self<span style="color:#f92672">.</span>netD_B<span style="color:#f92672">.</span>parameters()), lr<span style="color:#f92672">=</span>opt<span style="color:#f92672">.</span>lr, betas<span style="color:#f92672">=</span>(opt<span style="color:#f92672">.</span>beta1, <span style="color:#ae81ff">0.999</span>))
</span></span><span style="display:flex;"><span>            self<span style="color:#f92672">.</span>optimizers<span style="color:#f92672">.</span>append(self<span style="color:#f92672">.</span>optimizer_G)
</span></span><span style="display:flex;"><span>            self<span style="color:#f92672">.</span>optimizers<span style="color:#f92672">.</span>append(self<span style="color:#f92672">.</span>optimizer_D)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">set_input</span>(self, input):
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;&#34;&#34;Unpack input data from the dataloader and perform necessary pre-processing steps.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        Parameters:
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            input (dict): include the data itself and its metadata information.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        The option &#39;direction&#39; can be used to swap domain A and domain B.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>        AtoB <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>opt<span style="color:#f92672">.</span>direction <span style="color:#f92672">==</span> <span style="color:#e6db74">&#39;AtoB&#39;</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>real_A <span style="color:#f92672">=</span> input[<span style="color:#e6db74">&#39;A&#39;</span> <span style="color:#66d9ef">if</span> AtoB <span style="color:#66d9ef">else</span> <span style="color:#e6db74">&#39;B&#39;</span>]<span style="color:#f92672">.</span>to(self<span style="color:#f92672">.</span>device)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>real_B <span style="color:#f92672">=</span> input[<span style="color:#e6db74">&#39;B&#39;</span> <span style="color:#66d9ef">if</span> AtoB <span style="color:#66d9ef">else</span> <span style="color:#e6db74">&#39;A&#39;</span>]<span style="color:#f92672">.</span>to(self<span style="color:#f92672">.</span>device)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>image_paths <span style="color:#f92672">=</span> input[<span style="color:#e6db74">&#39;A_paths&#39;</span> <span style="color:#66d9ef">if</span> AtoB <span style="color:#66d9ef">else</span> <span style="color:#e6db74">&#39;B_paths&#39;</span>]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self):
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;&#34;&#34;Run forward pass; called by both functions &lt;optimize_parameters&gt; and &lt;test&gt;.&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>fake_B <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>netG_A(self<span style="color:#f92672">.</span>real_A)  <span style="color:#75715e"># G_A(A)</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>rec_A <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>netG_B(self<span style="color:#f92672">.</span>fake_B)   <span style="color:#75715e"># G_B(G_A(A))</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>fake_A <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>netG_B(self<span style="color:#f92672">.</span>real_B)  <span style="color:#75715e"># G_B(B)</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>rec_B <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>netG_A(self<span style="color:#f92672">.</span>fake_A)   <span style="color:#75715e"># G_A(G_B(B))</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e">#inter,final = self.Alignment_net(self.real_A,self.rec_B)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">backward_D_basic</span>(self, netD, real, fake):
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;&#34;&#34;Calculate GAN loss for the discriminator
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        Parameters:
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            netD (network)      -- the discriminator D
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            real (tensor array) -- real images
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            fake (tensor array) -- images generated by a generator
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        Return the discriminator loss.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        We also call loss_D.backward() to calculate the gradients.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Real</span>
</span></span><span style="display:flex;"><span>        pred_real <span style="color:#f92672">=</span> netD(real)
</span></span><span style="display:flex;"><span>        loss_D_real <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>criterionGAN(pred_real, <span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Fake</span>
</span></span><span style="display:flex;"><span>        pred_fake <span style="color:#f92672">=</span> netD(fake<span style="color:#f92672">.</span>detach())
</span></span><span style="display:flex;"><span>        loss_D_fake <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>criterionGAN(pred_fake, <span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Combined loss and calculate gradients</span>
</span></span><span style="display:flex;"><span>        loss_D <span style="color:#f92672">=</span> (loss_D_real <span style="color:#f92672">+</span> loss_D_fake) <span style="color:#f92672">*</span> <span style="color:#ae81ff">0.5</span>
</span></span><span style="display:flex;"><span>        loss_D<span style="color:#f92672">.</span>backward()
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> loss_D
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">backward_D_A</span>(self):
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;&#34;&#34;Calculate GAN loss for discriminator D_A&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>        fake_B <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>fake_B_pool<span style="color:#f92672">.</span>query(self<span style="color:#f92672">.</span>fake_B)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>loss_D_A <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>backward_D_basic(self<span style="color:#f92672">.</span>netD_A, self<span style="color:#f92672">.</span>real_B, fake_B)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">backward_D_B</span>(self):
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;&#34;&#34;Calculate GAN loss for discriminator D_B&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>        fake_A <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>fake_A_pool<span style="color:#f92672">.</span>query(self<span style="color:#f92672">.</span>fake_A)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>loss_D_B <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>backward_D_basic(self<span style="color:#f92672">.</span>netD_B, self<span style="color:#f92672">.</span>real_A, fake_A)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">backward_G</span>(self):
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;&#34;&#34;Calculate the loss for generators G_A and G_B&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>        lambda_idt <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>opt<span style="color:#f92672">.</span>lambda_identity
</span></span><span style="display:flex;"><span>        lambda_A <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>opt<span style="color:#f92672">.</span>lambda_A
</span></span><span style="display:flex;"><span>        lambda_B <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>opt<span style="color:#f92672">.</span>lambda_B
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Identity loss</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> lambda_idt <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">0</span>:
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># G_A should be identity if real_B is fed: ||G_A(B) - B||</span>
</span></span><span style="display:flex;"><span>            self<span style="color:#f92672">.</span>idt_A <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>netG_A(self<span style="color:#f92672">.</span>real_B)
</span></span><span style="display:flex;"><span>            self<span style="color:#f92672">.</span>loss_idt_A <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>criterionIdt(self<span style="color:#f92672">.</span>idt_A, self<span style="color:#f92672">.</span>real_B) <span style="color:#f92672">*</span> lambda_B <span style="color:#f92672">*</span> lambda_idt
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># G_B should be identity if real_A is fed: ||G_B(A) - A||</span>
</span></span><span style="display:flex;"><span>            self<span style="color:#f92672">.</span>idt_B <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>netG_B(self<span style="color:#f92672">.</span>real_A)
</span></span><span style="display:flex;"><span>            self<span style="color:#f92672">.</span>loss_idt_B <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>criterionIdt(self<span style="color:#f92672">.</span>idt_B, self<span style="color:#f92672">.</span>real_A) <span style="color:#f92672">*</span> lambda_A <span style="color:#f92672">*</span> lambda_idt
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>            self<span style="color:#f92672">.</span>loss_idt_A <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>            self<span style="color:#f92672">.</span>loss_idt_B <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># GAN loss D_A(G_A(A))</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>loss_G_A <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>criterionGAN(self<span style="color:#f92672">.</span>netD_A(self<span style="color:#f92672">.</span>fake_B), <span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># GAN loss D_B(G_B(B))</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>loss_G_B <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>criterionGAN(self<span style="color:#f92672">.</span>netD_B(self<span style="color:#f92672">.</span>fake_A), <span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Forward cycle loss || G_B(G_A(A)) - A||</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>loss_cycle_A <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>criterionCycle(self<span style="color:#f92672">.</span>rec_A, self<span style="color:#f92672">.</span>real_A) <span style="color:#f92672">*</span> lambda_A
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Backward cycle loss || G_A(G_B(B)) - B||</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>loss_cycle_B <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>criterionCycle(self<span style="color:#f92672">.</span>rec_B, self<span style="color:#f92672">.</span>real_B) <span style="color:#f92672">*</span> lambda_B
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># combined loss and calculate gradients</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>loss_G <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>loss_G_A <span style="color:#f92672">+</span> self<span style="color:#f92672">.</span>loss_G_B <span style="color:#f92672">+</span> self<span style="color:#f92672">.</span>loss_cycle_A <span style="color:#f92672">+</span> self<span style="color:#f92672">.</span>loss_cycle_B <span style="color:#f92672">+</span> self<span style="color:#f92672">.</span>loss_idt_A <span style="color:#f92672">+</span> self<span style="color:#f92672">.</span>loss_idt_B
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>loss_G<span style="color:#f92672">.</span>backward()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">optimize_parameters</span>(self):
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;&#34;&#34;Calculate losses, gradients, and update network weights; called in every training iteration&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># forward</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>forward()      <span style="color:#75715e"># compute fake images and reconstruction images.</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># G_A and G_B</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>set_requires_grad([self<span style="color:#f92672">.</span>netD_A, self<span style="color:#f92672">.</span>netD_B], <span style="color:#66d9ef">False</span>)  <span style="color:#75715e"># Ds require no gradients when optimizing Gs</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>optimizer_G<span style="color:#f92672">.</span>zero_grad()  <span style="color:#75715e"># set G_A and G_B&#39;s gradients to zero</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>backward_G()             <span style="color:#75715e"># calculate gradients for G_A and G_B</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>optimizer_G<span style="color:#f92672">.</span>step()       <span style="color:#75715e"># update G_A and G_B&#39;s weights</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># D_A and D_B</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>set_requires_grad([self<span style="color:#f92672">.</span>netD_A, self<span style="color:#f92672">.</span>netD_B], <span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>optimizer_D<span style="color:#f92672">.</span>zero_grad()   <span style="color:#75715e"># set D_A and D_B&#39;s gradients to zero</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>backward_D_A()      <span style="color:#75715e"># calculate gradients for D_A</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>backward_D_B()      <span style="color:#75715e"># calculate graidents for D_B</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>optimizer_D<span style="color:#f92672">.</span>step()  <span style="color:#75715e"># update D_A and D_B&#39;s weights</span>
</span></span></code></pre></div><p><strong>level</strong>: CCF_A
<strong>author</strong>: Phillip Isola;  Jun-Yan Zhu; Tinghui Zhou; Alexei A. Efros
<strong>date</strong>: 2018
<strong>keyword</strong>:</p>
<ul>
<li>GANs,</li>
</ul>
<hr>
<h1 id="paper-pix2pix">Paper: Pix2pix</h1>
<!-- raw HTML omitted -->
<h4 id="summary-1">Summary</h4>
<ol>
<li>investigate conditional adversarial networks as general-purpose solution to image-to-image translation problem, that the net learn the mapping from input image to output image, and learn a loss function to train the mapping;</li>
</ol>
<h4 id="research-objective-1">Research Objective</h4>
<ul>
<li><strong>Application Area</strong>:</li>
</ul>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201018123156008.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201018123156008.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201018123156008.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201018123156008.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201018123156008.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201018123156008.png"/></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201018142258797.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201018142258797.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201018142258797.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201018142258797.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201018142258797.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201018142258797.png"/></p>
<h4 id="previous-work">Previous work:</h4>
<ul>
<li><strong>Structured losses for image modeling:</strong>  image -to-image translation problems as per-pixel classification or regression that each output pixel is considered conditionally independent from all others given the input image; conditional GANs instead learn a structured loss which penalize the joint configuration of the output.<img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211121115016608.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211121115016608.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211121115016608.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211121115016608.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211121115016608.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211121115016608.png"/></li>
<li><strong>Conditional GANs</strong>: used for discrete labels, text, image prediction from a normal map, future frame prediction, product photo generation, image generation from sparse annotations, inpainting, future state prediction, image manipulation guided by user constraints, style transfer, superresolution.</li>
</ul>
<h4 id="methods-1">Methods</h4>
<ul>
<li>
<p><strong>Challenge:</strong></p>
<ul>
<li>output is high-dimensional, structured object;
<ul>
<li>Use a deep net, D, to analyze output!</li>
</ul>
</li>
<li>uncertainty in mapping; many plausible outputs;
<ul>
<li>D only cares about “plausibility”, doesn’t hedge; like <strong>MAD-GAN; BiCycleGAN</strong></li>
</ul>
</li>
</ul>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201018131631479.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201018131631479.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201018131631479.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201018131631479.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201018131631479.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201018131631479.png"/></p>
</li>
<li>
<p><strong>Problem Formulation</strong>:</p>
</li>
</ul>
<blockquote>
<p>For GANs, learning a mapping from random noise vector z to output image  $y$, $G: z-&gt;y$;</p>
<p>For conditional GANs, learning a mapping from observed image x and random noise vector z to $y$, $G:{x,z}&ndash;&gt;y$;</p>
</blockquote>
<ul>
<li><strong>cGAN vs Pix2pix</strong></li>
</ul>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211121115129571.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211121115129571.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211121115129571.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211121115129571.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211121115129571.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20211121115129571.png"/></p>
<ul>
<li><strong>Loss Function Object</strong>:</li>
</ul>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201018121240333.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201018121240333.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201018121240333.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201018121240333.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201018121240333.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201018121240333.png"/></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201018140505080.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201018140505080.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201018140505080.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201018140505080.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201018140505080.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201018140505080.png"/></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>   <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self):
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;&#34;&#34;Run forward pass; called by both functions &lt;optimize_parameters&gt; and &lt;test&gt;.&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>fake_B <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>netG(self<span style="color:#f92672">.</span>real_A)  <span style="color:#75715e"># G(A)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">backward_D</span>(self):
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;&#34;&#34;Calculate GAN loss for the discriminator&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Fake; stop backprop to the generator by detaching fake_B</span>
</span></span><span style="display:flex;"><span>        fake_AB <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>cat((self<span style="color:#f92672">.</span>real_A, self<span style="color:#f92672">.</span>fake_B), <span style="color:#ae81ff">1</span>)  <span style="color:#75715e"># we use conditional GANs; we need to feed both input and output to the discriminator</span>
</span></span><span style="display:flex;"><span>        pred_fake <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>netD(fake_AB<span style="color:#f92672">.</span>detach())
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>loss_D_fake <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>criterionGAN(pred_fake, <span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Real</span>
</span></span><span style="display:flex;"><span>        real_AB <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>cat((self<span style="color:#f92672">.</span>real_A, self<span style="color:#f92672">.</span>real_B), <span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        pred_real <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>netD(real_AB)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>loss_D_real <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>criterionGAN(pred_real, <span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># combine loss and calculate gradients</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>loss_D <span style="color:#f92672">=</span> (self<span style="color:#f92672">.</span>loss_D_fake <span style="color:#f92672">+</span> self<span style="color:#f92672">.</span>loss_D_real) <span style="color:#f92672">*</span> <span style="color:#ae81ff">0.5</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>loss_D<span style="color:#f92672">.</span>backward()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">backward_G</span>(self):
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;&#34;&#34;Calculate GAN and L1 loss for the generator&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># First, G(A) should fake the discriminator</span>
</span></span><span style="display:flex;"><span>        fake_AB <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>cat((self<span style="color:#f92672">.</span>real_A, self<span style="color:#f92672">.</span>fake_B), <span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        pred_fake <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>netD(fake_AB)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>loss_G_GAN <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>criterionGAN(pred_fake, <span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Second, G(A) = B</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>loss_G_L1 <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>criterionL1(self<span style="color:#f92672">.</span>fake_B, self<span style="color:#f92672">.</span>real_B) <span style="color:#f92672">*</span> self<span style="color:#f92672">.</span>opt<span style="color:#f92672">.</span>lambda_L1
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># combine loss and calculate gradients</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>loss_G <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>loss_G_GAN <span style="color:#f92672">+</span> self<span style="color:#f92672">.</span>loss_G_L1
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>loss_G<span style="color:#f92672">.</span>backward()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">optimize_parameters</span>(self):
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>forward()                   <span style="color:#75715e"># compute fake images: G(A)</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># update D</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>set_requires_grad(self<span style="color:#f92672">.</span>netD, <span style="color:#66d9ef">True</span>)  <span style="color:#75715e"># enable backprop for D</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>optimizer_D<span style="color:#f92672">.</span>zero_grad()     <span style="color:#75715e"># set D&#39;s gradients to zero</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>backward_D()                <span style="color:#75715e"># calculate gradients for D</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>optimizer_D<span style="color:#f92672">.</span>step()          <span style="color:#75715e"># update D&#39;s weights</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># update G</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>set_requires_grad(self<span style="color:#f92672">.</span>netD, <span style="color:#66d9ef">False</span>)  <span style="color:#75715e"># D requires no gradients when optimizing G</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>optimizer_G<span style="color:#f92672">.</span>zero_grad()        <span style="color:#75715e"># set G&#39;s gradients to zero</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>backward_G()                   <span style="color:#75715e"># calculate graidents for G</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>optimizer_G<span style="color:#f92672">.</span>step()             <span style="color:#75715e"># udpate G&#39;s weights</span>
</span></span></code></pre></div><blockquote>
<p>restricting the GAN discriminator to only model high-frequency structure, relying on an L1 term to force low-frequency correctness, term a PatchGAN that only penalizes structure at the scale of patches, like tries to classify if each N*N patch in an image is real or fake.</p>
</blockquote>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201018121401145.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201018121401145.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201018121401145.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201018121401145.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201018121401145.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201018121401145.png"/></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201018131416705.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201018131416705.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201018131416705.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201018131416705.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201018131416705.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201018131416705.png"/></p>
<h4 id="evaluation-1">Evaluation</h4>
<ul>
<li>
<p><strong>Environment</strong>:</p>
<ul>
<li>Semantic labels↔photo, trained on the Cityscapes dataset [12].</li>
<li>Architectural labels→photo, trained on CMP Facades [45].</li>
<li>Map↔aerial photo, trained on data scraped from Google Maps.</li>
<li>BW→color photos, trained on [51].</li>
<li>Edges→photo, trained on data from [65] and [60]; binary edges generated using the HED edge detector [58] plus postprocessing.</li>
<li>Sketch→photo: tests edges→photo models on humandrawn sketches from [19].</li>
<li>Day→night, trained on [33].</li>
<li>Thermal→color photos, trained on data from [27].</li>
<li>Photo with missing pixels→inpainted photo, trained on Paris StreetView from [14].</li>
</ul>
</li>
<li>
<p><strong>Evaluation metrics:</strong></p>
<ul>
<li>run “real vs. fake” perceptual studies on Amazon Mechanical Turk (AMT).</li>
<li>measure whether or not our synthesized cityscapes are realistic enough that off-the-shelf recognition system can recognize the objects in them.</li>
</ul>
</li>
<li>
<p><strong>Results</strong></p>
<ul>
<li>
<p>Different Loss <img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201018122611607.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201018122611607.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201018122611607.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201018122611607.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201018122611607.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201018122611607.png"/><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201018122709256.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201018122709256.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201018122709256.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201018122709256.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201018122709256.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201018122709256.png"/></p>
</li>
<li>
<p>Different Generator architectures:<img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201018122741654.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201018122741654.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201018122741654.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201018122741654.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201018122741654.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201018122741654.png"/></p>
</li>
<li>
<p>Different receptive field:<img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201018122825191.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201018122825191.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201018122825191.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201018122825191.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201018122825191.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201018122825191.png"/></p>
</li>
</ul>
</li>
</ul>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201018122931674.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201018122931674.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201018122931674.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201018122931674.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201018122931674.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201018122931674.png"/></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201018123033271.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201018123033271.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201018123033271.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201018123033271.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201018123033271.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201018123033271.png"/></p>
<h4 id="notes-font-colororange去加强了解font">Notes <!-- raw HTML omitted -->去加强了解<!-- raw HTML omitted --></h4>
<ul>
<li>运行论文代码</li>
</ul>
<p><strong>level</strong>:  2021 ICRL
<strong>author</strong>: Richardson, E., Alaluf, Y., Patashnik, O., Nitzan, Y., Azar, Y., Shapiro, S., &amp; Cohen-Or, D.
<strong>date</strong>: 2020
<strong>keyword</strong>:</p>
<ul>
<li>GAN, styletransfer; latent space encoding</li>
</ul>
<blockquote>
<p>Richardson, E., Alaluf, Y., Patashnik, O., Nitzan, Y., Azar, Y., Shapiro, S., &amp; Cohen-Or, D. (2020). Encoding in Style: a StyleGAN Encoder for Image-to-Image Translation. <em>arXiv preprint arXiv:2008.00951</em>.</p>
</blockquote>
<hr>
<h1 id="paper-styleganpsp">Paper: StyleGAN(pSp)</h1>
<!-- raw HTML omitted -->
<h4 id="summary-2">Summary</h4>
<ol>
<li>pSp framework is based on an encoder network that directly generates a series of style vectors which are fed into a pretrained style-GAN generator, forming the extended W+ latent space;</li>
<li>introduce a dedicated identity loss which is shown to achieve improved performance in the reconstruction of an input image.</li>
<li>a novel styleGan encoder able to directly encode real face images into the W+ latent domain.</li>
<li>makes our model operate globally instead of locally, without requiring pixel-to-pixel correspondence.</li>
</ol>
<h4 id="research-objective-2">Research Objective</h4>
<ul>
<li>
<p><strong>Application Area</strong>: stylegan inversion; frontalization; inpainting; face generation from segmentation; super resolution; face interpolation for real images;<img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201019191118769.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201019191118769.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201019191118769.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201019191118769.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201019191118769.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201019191118769.png"/></p>
</li>
<li>
<p><strong>Purpose</strong>:   retrieval of the latent vector that generates a desired, not necessarily known image;</p>
</li>
</ul>
<h4 id="proble-statement">Proble Statement</h4>
<ul>
<li>how to controlling stylegan&rsquo;s latent space and performing meaningful manipulations in W;</li>
<li>how to accelerate the encoding process?</li>
</ul>
<p>previous work:</p>
<ul>
<li>StyleGAN 23,24:   a disentangled latent space W, obtained from the initial space Z via a multi-layer perceptron mapping network, which offer control and editing capabilities.</li>
<li><strong>Latent Space Embedding:</strong> inversion methods
<ul>
<li>directly optimize the latent vector to minimize the error for the given image;</li>
<li>train an encoder to map the given image to the latent space;</li>
<li>use a hybrid approach combining both;</li>
</ul>
</li>
<li><strong>Latent-Space Manipulation:</strong>  first, inverting  a given image into the latent space, then editing the inverted latent code <strong>in a semantically meaningful manner</strong> to obtain a new code used by the unconditional GAN to generate the output image;
<ul>
<li>finding linear directions that correspond to changes in a given binary labeled attribute;</li>
<li>utilize a pretrained 3DMM to learn semantic face edits in the latent space;</li>
<li>finding useful paths in a completely unsupervised manner by using PCA;</li>
</ul>
</li>
<li>the input image must be invertible, like exits a latent code that reconstructs the image;</li>
<li>the input image domain must typically be the same domain the GAN was trained on.</li>
<li>the latent space does not contain rich semantics for an unknown data damain;</li>
<li>GAN inversion remains difficult.</li>
</ul>
<h4 id="methods-2">Methods</h4>
<ul>
<li>
<p><strong>Problem Formulation</strong>:</p>
</li>
<li>
<p><strong>system overview</strong>:</p>
</li>
</ul>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201019200512316.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201019200512316.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201019200512316.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201019200512316.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201019200512316.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201019200512316.png"/></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201019200645869.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201019200645869.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201019200645869.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201019200645869.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201019200645869.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201019200645869.png"/></p>
<p>【Loss Function】</p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201019200940361.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201019200940361.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201019200940361.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201019200940361.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201019200940361.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201019200940361.png"/></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201019201025121.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201019201025121.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201019201025121.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201019201025121.png 2x"
    data-sizes="auto"
    alt="image-20201019201025121"
    title="image-20201019201025121"/></p>
<ul>
<li>
<p>preserve identity between the input and output images: <img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201019201156223.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201019201156223.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201019201156223.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201019201156223.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201019201156223.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201019201156223.png"/></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201019201248655.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201019201248655.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201019201248655.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201019201248655.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201019201248655.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201019201248655.png"/></p>
</li>
</ul>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201019201651904.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201019201651904.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201019201651904.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201019201651904.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201019201651904.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201019201651904.png"/></p>
<h4 id="evaluation-2">Evaluation</h4>
<ul>
<li><strong>Environment</strong>:
<ul>
<li>Dataset: CelebA-HQ dataset [20], which contains 30,000 high quality images</li>
</ul>
</li>
</ul>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201019201837985.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201019201837985.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201019201837985.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201019201837985.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201019201837985.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201019201837985.png"/></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201019201918457.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201019201918457.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201019201918457.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201019201918457.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201019201918457.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20201019201918457.png"/></p>
<h4 id="notes-font-colororange去加强了解font-1">Notes <!-- raw HTML omitted -->去加强了解<!-- raw HTML omitted --></h4>
<ul>
<li>学习一下相关代码</li>
</ul>
</div>
<div class="post-footer" id="post-footer">
  <div class="post-info">
    <div class="post-info-line">
      <div class="post-info-mod">
        <span title=2023-12-31&#32;15:30:45>更新于 2023-12-31&nbsp;</span>
      </div><div class="post-info-license">
          <span><a rel="license external nofollow noopener noreferrer" href="https://creativecommons.org/licenses/by-nc/4.0/" target="_blank">CC BY-NC 4.0</a></span>
        </div></div>
    <div class="post-info-line">
      <div class="post-info-md"><span><a href="/circlegan/index.md" title="阅读原始文档" class="link-to-markdown">阅读原始文档</a></span><span><a href="https://liudongdong1.github.io/edit/master/content/posts%5c%e8%a7%86%e8%a7%89%e8%bf%90%e5%8a%a8%5cvideo_understand%5cCircleGan.md" title="编辑此页"target="_blank" rel="external nofollow noopener noreferrer" class="link-to-edit">编辑此页</a></span></div>
      <div class="post-info-share">
        <span><a href="javascript:void(0);" title="分享到 Twitter" data-sharer="twitter" data-url="https://liudongdong1.github.io/circlegan/" data-title="CircleGan" data-hashtags="GAN"><i class="fa-brands fa-twitter fa-fw" aria-hidden="true"></i></a>
  <a href="javascript:void(0);" title="分享到 Facebook" data-sharer="facebook" data-url="https://liudongdong1.github.io/circlegan/" data-hashtag="GAN"><i class="fa-brands fa-facebook-square fa-fw" aria-hidden="true"></i></a>
  <a href="javascript:void(0);" title="分享到 微博" data-sharer="weibo" data-url="https://liudongdong1.github.io/circlegan/" data-title="CircleGan" data-image="https://cdn.stocksnap.io/img-thumbs/280h/TDAU1ERCD4.jpg"><i class="fa-brands fa-weibo fa-fw" aria-hidden="true"></i></a>
  </span>
      </div>
    </div>
  </div>

  <div class="post-info-more">
    <section class="post-tags"><i class="fa-solid fa-tags fa-fw" aria-hidden="true"></i>&nbsp;<a href="/tags/gan/">GAN</a></section>
    <section>
      <span><a href="javascript:void(0);" onclick="window.history.back();">返回</a></span>&nbsp;|&nbsp;<span><a href="/">主页</a></span>
    </section>
  </div>

  <div class="post-nav"><a href="/eventbus/" class="prev" rel="prev" title="EventBus"><i class="fa-solid fa-angle-left fa-fw" aria-hidden="true"></i>EventBus</a>
      <a href="/mvp-arch/" class="next" rel="next" title="Mvp-arch">Mvp-arch<i class="fa-solid fa-angle-right fa-fw" aria-hidden="true"></i></a></div>
</div>
</article></main><footer class="footer">
    <div class="footer-container"><div class="footer-line powered">由 <a href="https://gohugo.io/" target="_blank" rel="external nofollow noopener noreferrer" title="Hugo 0.118.2">Hugo</a> 强力驱动 | 主题 - <a href="https://github.com/hugo-fixit/FixIt" target="_blank" rel="external" title="FixIt v0.2.17-RC"><img class="fixit-icon" src="/fixit.min.svg" alt="FixIt logo" />&nbsp;FixIt</a>
        </div><div class="footer-line copyright" itemscope itemtype="http://schema.org/CreativeWork"><i class="fa-regular fa-copyright fa-fw" aria-hidden="true"></i>
            <span itemprop="copyrightYear">2020 - 2024</span><span class="author" itemprop="copyrightHolder">
              <a href="https://liudongdong1.github.io/"target="_blank" rel="external nofollow noopener noreferrer">LiuDongdong</a></span><span class="license footer-divider"><a rel="license external nofollow noopener noreferrer" href="https://creativecommons.org/licenses/by-nc/4.0/" target="_blank">CC BY-NC 4.0</a></span></div><div class="footer-line statistics"><span class="site-time" title='网站运行中 ...'><i class="fa-solid fa-heartbeat fa-fw animate-icon" aria-hidden="true"></i>&nbsp;<span class="run-times">网站运行中 ...</span></span></div><div class="footer-line ibruce">
          <span id="busuanzi_container_site_uv" title='总访客数'><i class="fa-regular fa-user fa-fw" aria-hidden="true"></i>&nbsp;<span id="busuanzi_value_site_uv"><i class="fa-solid fa-spinner fa-spin fa-fw" aria-hidden="true"></i></span></span><span id="busuanzi_container_site_pv" class="footer-divider" title='总访问量'><i class="fa-regular fa-eye fa-fw" aria-hidden="true"></i>&nbsp;<span id="busuanzi_value_site_pv"><i class="fa-solid fa-spinner fa-spin fa-fw" aria-hidden="true"></i></span></span>
        </div></div>
  </footer></div><div class="widgets"><div class="fixed-buttons animate__faster d-none"><div class="fixed-button back-to-top" role="button" aria-label="回到顶部"><i class="fa-solid fa-arrow-up fa-fw" aria-hidden="true"></i><span class="variant-numeric">0%</span>
        </div></div><a href="https://liudongdong1.github.io/" title="在 GitHub 上查看源代码"target="_blank" rel="external nofollow" class="github-corner right d-none-mobile"><svg viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><div id="mask"></div><div class="reading-progress-bar" style="left: 0;top: 0;--bg-progress: #0076ff;--bg-progress-dark: #fff;"></div><noscript>
    <div class="noscript-warning">FixIt 主题在启用 JavaScript 的情况下效果最佳。</div>
  </noscript>
</div><link rel="stylesheet" href="/lib/katex/katex.min.css"><link rel="stylesheet" href="/lib/cookieconsent/cookieconsent.min.css"><script src="/lib/autocomplete/autocomplete.min.js" defer></script><script src="/lib/algoliasearch/algoliasearch-lite.umd.min.js" defer></script><script src="/lib/lazysizes/lazysizes.min.js" async defer></script><script src="/lib/sharer/sharer.min.js" async defer></script><script src="/lib/typeit/index.umd.js" defer></script><script src="/lib/katex/katex.min.js" defer></script><script src="/lib/katex/auto-render.min.js" defer></script><script src="/lib/katex/copy-tex.min.js" defer></script><script src="/lib/katex/mhchem.min.js" defer></script><script src="/lib/cookieconsent/cookieconsent.min.js" defer></script><script src="/lib/pangu/pangu.min.js" defer></script><script src="/lib/cell-watermark/watermark.min.js" defer></script><script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async defer></script><script>window.config={"code":{"copyTitle":"复制到剪贴板","editLockTitle":"锁定可编辑代码块","editUnLockTitle":"解锁可编辑代码块","editable":true,"maxShownLines":10},"comment":{"enable":false},"cookieconsent":{"content":{"dismiss":"同意","link":"了解更多","message":"本网站使用 Cookies 来改善您的浏览体验。"},"enable":true,"palette":{"button":{"background":"#f0f0f0"},"popup":{"background":"#1aa3ff"}},"theme":"edgeless"},"data":{"typeit-header-subtitle-desktop":"\u003cspan style='font-family: MMT,\"沐目体\";'\u003e吾日三省吾身\u003c/span\u003e","typeit-header-subtitle-mobile":"\u003cspan style='font-family: MMT,\"沐目体\";'\u003e吾日三省吾身\u003c/span\u003e"},"enablePWA":true,"enablePangu":true,"math":{"delimiters":[{"display":true,"left":"$$","right":"$$"},{"display":true,"left":"\\[","right":"\\]"},{"display":true,"left":"\\begin{equation}","right":"\\end{equation}"},{"display":true,"left":"\\begin{equation*}","right":"\\end{equation*}"},{"display":true,"left":"\\begin{align}","right":"\\end{align}"},{"display":true,"left":"\\begin{align*}","right":"\\end{align*}"},{"display":true,"left":"\\begin{alignat}","right":"\\end{alignat}"},{"display":true,"left":"\\begin{alignat*}","right":"\\end{alignat*}"},{"display":true,"left":"\\begin{gather}","right":"\\end{gather}"},{"display":true,"left":"\\begin{CD}","right":"\\end{CD}"},{"display":false,"left":"$","right":"$"},{"display":false,"left":"\\(","right":"\\)"}],"strict":false},"search":{"algoliaAppID":"2R1K9SKLQZ","algoliaIndex":"index.zh-cn","algoliaSearchKey":"4a226aa1c5c98d6859e4d1386adb2bc7","highlightTag":"em","maxResultLength":10,"noResultsFound":"没有找到结果","snippetLength":50,"type":"algolia"},"siteTime":"2020-12-18T16:15:22+08:00","typeit":{"cursorChar":"|","cursorSpeed":1000,"data":{"typeit-header-subtitle-desktop":["typeit-header-subtitle-desktop"],"typeit-header-subtitle-mobile":["typeit-header-subtitle-mobile"]},"duration":-1,"speed":100},"watermark":{"appendto":".wrapper\u003emain","colspacing":30,"content":"\u003cimg class=\"fixit-icon\" src=\"/fixit.min.svg\" alt=\"FixIt logo\" /\u003e FixIt 主题","enable":true,"fontfamily":"inherit","fontsize":0.85,"height":21,"opacity":0.0125,"rotate":15,"rowspacing":60,"width":150}};</script><script src="/js/theme.min.js" defer></script><script src="/js/custom.min.js" defer></script></body>
</html>
