<!DOCTYPE html>
<html itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">
  <head>
    
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
    <meta name="robots" content="noodp" />
    <title>HumanPoseProject - DAY By DAY</title><meta name="author" content="LiuDongdong">
<meta name="author-link" content="https://liudongdong1.github.io/">
<meta name="description" content="1. residual_pose Hourglass model for multi-person 2D pose estimation from depth images. Our regressor NN architecture for 3D human pose estimation. 3D pose prior for recovering from 2D missed detections. Tranined models for 2D and 3D pose estimation. Code for obtaining 2D and 3D pose from a depth image. 11 months ago. star12 2. depth_human_synthesis We have created a collection of 24 human characters, 12 men and 12 women, with" /><meta name="keywords" content='CV' /><meta itemprop="name" content="HumanPoseProject">
<meta itemprop="description" content="1. residual_pose Hourglass model for multi-person 2D pose estimation from depth images. Our regressor NN architecture for 3D human pose estimation. 3D pose prior for recovering from 2D missed detections. Tranined models for 2D and 3D pose estimation. Code for obtaining 2D and 3D pose from a depth image. 11 months ago. star12 2. depth_human_synthesis We have created a collection of 24 human characters, 12 men and 12 women, with"><meta itemprop="datePublished" content="2021-08-13T10:30:29+00:00" />
<meta itemprop="dateModified" content="2023-09-28T23:07:56+08:00" />
<meta itemprop="wordCount" content="951"><meta itemprop="image" content="/logo.png"/>
<meta itemprop="keywords" content="CV," /><meta property="og:title" content="HumanPoseProject" />
<meta property="og:description" content="1. residual_pose Hourglass model for multi-person 2D pose estimation from depth images. Our regressor NN architecture for 3D human pose estimation. 3D pose prior for recovering from 2D missed detections. Tranined models for 2D and 3D pose estimation. Code for obtaining 2D and 3D pose from a depth image. 11 months ago. star12 2. depth_human_synthesis We have created a collection of 24 human characters, 12 men and 12 women, with" />
<meta property="og:type" content="article" />
<meta property="og:url" content="liudongdong1.github.io/humanposeproject/" /><meta property="og:image" content="/logo.png"/><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2021-08-13T10:30:29+00:00" />
<meta property="article:modified_time" content="2023-09-28T23:07:56+08:00" />
<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="/logo.png"/>

<meta name="twitter:title" content="HumanPoseProject"/>
<meta name="twitter:description" content="1. residual_pose Hourglass model for multi-person 2D pose estimation from depth images. Our regressor NN architecture for 3D human pose estimation. 3D pose prior for recovering from 2D missed detections. Tranined models for 2D and 3D pose estimation. Code for obtaining 2D and 3D pose from a depth image. 11 months ago. star12 2. depth_human_synthesis We have created a collection of 24 human characters, 12 men and 12 women, with"/>
<meta name="application-name" content="DAY By DAY">
<meta name="apple-mobile-web-app-title" content="DAY By DAY"><meta name="theme-color" data-light="#f8f8f8" data-dark="#252627" content="#f8f8f8"><meta name="msapplication-TileColor" content="#da532c"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" />
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"><link rel="manifest" href="/site.webmanifest"><link rel="canonical" href="liudongdong1.github.io/humanposeproject/" /><link rel="prev" href="liudongdong1.github.io/streampipline/" /><link rel="next" href="liudongdong1.github.io/v2ray/" /><link rel="stylesheet" href="/liudongdong1.github.io/css/style.min.css"><link rel="stylesheet" href="/liudongdong1.github.io/lib/fontawesome-free/all.min.css"><link rel="stylesheet" href="/liudongdong1.github.io/lib/animate/animate.min.css"><script type="application/ld+json">
  {
    "@context": "http://schema.org",
    "@type": "BlogPosting",
    "headline": "HumanPoseProject",
    "inLanguage": "zh-CN",
    "mainEntityOfPage": {
      "@type": "WebPage",
      "@id": "liudongdong1.github.io\/humanposeproject\/"
    },"genre": "posts","keywords": "CV","wordcount":  951 ,
    "url": "liudongdong1.github.io\/humanposeproject\/","datePublished": "2021-08-13T10:30:29+00:00","dateModified": "2023-09-28T23:07:56+08:00","license": "This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.","publisher": {
      "@type": "Organization",
      "name": "LiuDongdong","logo": "\/images\/person.png"},"author": {
        "@type": "Person",
        "name": "liudongdong1"
      },"description": ""
  }
  </script></head>
  <body data-header-desktop="auto" data-header-mobile="auto"><script>(window.localStorage?.getItem('theme') ? localStorage.getItem('theme') === 'dark' : ('auto' === 'auto' ? window.matchMedia('(prefers-color-scheme: dark)').matches : 'auto' === 'dark')) && document.body.setAttribute('data-theme', 'dark');</script><div class="wrapper"><header class="desktop animate__faster" id="header-desktop">
  <div class="header-wrapper" data-github-corner="right">
    <div class="header-title">
      <a href="liudongdong1.github.io/" title="DAY By DAY"><img
    class="lazyload logo"
    src="/liudongdong1.github.io/svg/loading.min.svg"
    data-src="/fixit.min.svg"
    data-srcset="/fixit.min.svg, /fixit.min.svg 1.5x, /fixit.min.svg 2x"
    data-sizes="auto"
    alt="DAY By DAY"
    title="DAY By DAY"/><span class="header-title-text"></span></a><span id="typeit-header-subtitle-desktop" class="typeit header-subtitle"></span></div>
    <nav>
      <ul class="menu"><li class="menu-item">
              <a
                class="menu-link"
                href="/liudongdong1.github.io/posts/"
                
                
              ><i class="fa-solid fa-archive fa-fw fa-sm" aria-hidden="true"></i> 所有文章</a></li><li class="menu-item">
              <a
                class="menu-link"
                href="/liudongdong1.github.io/categories/"
                
                
              ><i class="fa-solid fa-th fa-fw fa-sm" aria-hidden="true"></i> 分类</a></li><li class="menu-item">
              <a
                class="menu-link"
                href="/liudongdong1.github.io/tags/"
                
                
              ><i class="fa-solid fa-tags fa-fw fa-sm" aria-hidden="true"></i> 标签</a></li><li class="menu-item">
              <a
                class="menu-link"
                href="/liudongdong1.github.io/friends/"
                title="友情链接"
                
              ><i class="fa-solid fa-users fa-fw fa-sm" aria-hidden="true"></i> 友链</a></li><li class="menu-item">
              <a
                class="menu-link"
                href="/liudongdong1.github.io/about/"
                
                
              ><i class="fa-solid fa-info-circle fa-fw fa-sm" aria-hidden="true"></i> 关于</a></li><li class="menu-item delimiter"></li><li class="menu-item language">
            <span role="button" aria-label="选择语言" title="选择语言">简体中文<i class="dropdown-icon fa-solid fa-chevron-down" aria-hidden="true"></i>
            </span>
            <ul class="sub-menu"><li class="menu-item">没有更多翻译</li></ul>
          </li><li class="menu-item search" id="search-desktop">
            <input type="text" placeholder="搜索文章标题或内容 ..." id="search-input-desktop">
            <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-desktop" title="搜索">
              <i class="fa-solid fa-search fa-fw" aria-hidden="true"></i>
            </a>
            <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-desktop" title="清空">
              <i class="fa-solid fa-times-circle fa-fw" aria-hidden="true"></i>
            </a>
            <span class="search-button search-loading" id="search-loading-desktop">
              <i class="fa-solid fa-spinner fa-fw fa-spin" aria-hidden="true"></i>
            </span>
          </li><li class="menu-item theme-switch" title="切换主题">
          <i class="fa-solid fa-adjust fa-fw" aria-hidden="true"></i>
        </li>
      </ul>
    </nav>
  </div>
</header><header class="mobile animate__faster" id="header-mobile">
  <div class="header-container">
    <div class="header-wrapper">
      <div class="header-title">
        <a href="liudongdong1.github.io/" title="DAY By DAY"><img
    class="lazyload logo"
    src="/liudongdong1.github.io/svg/loading.min.svg"
    data-src="/fixit.min.svg"
    data-srcset="/fixit.min.svg, /fixit.min.svg 1.5x, /fixit.min.svg 2x"
    data-sizes="auto"
    alt="/fixit.min.svg"
    title="/fixit.min.svg"/><span class="header-title-text"></span></a><span id="typeit-header-subtitle-mobile" class="typeit header-subtitle"></span></div>
      <div class="menu-toggle" id="menu-toggle-mobile">
        <span></span><span></span><span></span>
      </div>
    </div>
    <nav>
      <ul class="menu" id="menu-mobile"><li class="search-wrapper">
            <div class="search mobile" id="search-mobile">
              <input type="text" placeholder="搜索文章标题或内容 ..." id="search-input-mobile">
              <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-mobile" title="搜索">
                <i class="fa-solid fa-search fa-fw" aria-hidden="true"></i>
              </a>
              <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-mobile" title="清空">
                <i class="fa-solid fa-times-circle fa-fw" aria-hidden="true"></i>
              </a>
              <span class="search-button search-loading" id="search-loading-mobile">
                <i class="fa-solid fa-spinner fa-fw fa-spin" aria-hidden="true"></i>
              </span>
            </div>
            <a href="javascript:void(0);" class="search-cancel" id="search-cancel-mobile">
              取消
            </a>
          </li><li
              class="menu-item"
            ><a
                  class="menu-link"
                  href="/liudongdong1.github.io/posts/"
                  
                  
                ><i class="fa-solid fa-archive fa-fw fa-sm" aria-hidden="true"></i> 所有文章</a></li><li
              class="menu-item"
            ><a
                  class="menu-link"
                  href="/liudongdong1.github.io/categories/"
                  
                  
                ><i class="fa-solid fa-th fa-fw fa-sm" aria-hidden="true"></i> 分类</a></li><li
              class="menu-item"
            ><a
                  class="menu-link"
                  href="/liudongdong1.github.io/tags/"
                  
                  
                ><i class="fa-solid fa-tags fa-fw fa-sm" aria-hidden="true"></i> 标签</a></li><li
              class="menu-item"
            ><a
                  class="menu-link"
                  href="/liudongdong1.github.io/friends/"
                  title="友情链接"
                  
                ><i class="fa-solid fa-users fa-fw fa-sm" aria-hidden="true"></i> 友链</a></li><li
              class="menu-item"
            ><a
                  class="menu-link"
                  href="/liudongdong1.github.io/about/"
                  
                  
                ><i class="fa-solid fa-info-circle fa-fw fa-sm" aria-hidden="true"></i> 关于</a></li><li
              class="menu-item text-center"
            ><a
                  class="menu-link"
                  href="https://liudongdong1.github.io/"
                  title="GitHub"
                  rel="noopener noreferrer" target="_blank"
                ><i class='fa-brands fa-github fa-fw' aria-hidden='true'></i> </a></li><li class="menu-item theme-switch" title="切换主题">
          <i class="fa-solid fa-adjust fa-fw" aria-hidden="true"></i>
        </li><li class="menu-item language">
            <span role="button" aria-label="选择语言" title="选择语言">简体中文<i class="dropdown-icon fa-solid fa-chevron-down" aria-hidden="true"></i>
            </span>
            <select class="language-select" onchange="location = this.value;"><option disabled>没有更多翻译</option></select>
          </li></ul>
    </nav>
  </div>
</header><div class="search-dropdown desktop">
    <div id="search-dropdown-desktop"></div>
  </div>
  <div class="search-dropdown mobile">
    <div id="search-dropdown-mobile"></div>
  </div><main class="container" data-page-style="normal"><aside class="toc" id="toc-auto"><h2 class="toc-title">目录 <i class="toc-icon fa-solid fa-angle-down fa-fw"></i></h2>
      <div class="toc-content" id="toc-content-auto"></div></aside>

  <aside class="aside-custom" id="aside-sakana">
    

<div class="sakana-widget">
  <div class="sakana-item" id="takina-widget"></div>
  <div class="sakana-item" id="chisato-widget"></div>
</div>
<script>
  function initSakanaWidget() {
    const takina = SakanaWidget.getCharacter('takina')
    SakanaWidget.registerCharacter('takina-slow', takina);
    new SakanaWidget({
      character: 'takina-slow',
      controls: false,
      autoFit: true,
      stroke: {
        color: "#b4b4b4",
        width: 2
      }
    }).mount('#takina-widget');

    const chisato = SakanaWidget.getCharacter('chisato')
    SakanaWidget.registerCharacter('chisato-slow', chisato);
    new SakanaWidget({
      character: 'chisato-slow',
      controls: false,
      autoFit: true,
      stroke: {
        color: "#b4b4b4",
        width: 2
      }
    }).mount('#chisato-widget');
  }
</script>
<script async onload="initSakanaWidget()" src="https://cdn.jsdelivr.net/npm/sakana-widget@2.3.0/lib/sakana.min.js">
</script></aside>

  <article class="page single">
    <div class="header"><h1 class="single-title animate__animated animate__flipInX">
        <span>HumanPoseProject</span>
      </h1></div><div class="post-meta">
      <div class="post-meta-line"><span class="post-author"><span class="author"><i class="fa-solid fa-user-circle" aria-hidden="true"></i>
      liudongdong1</span></span>
          <span class="post-category">收录于 <a href="liudongdong1.github.io/categories/"><i class="fa-regular fa-folder fa-fw"></i>&nbsp;Categories</a>&ensp;<a href="liudongdong1.github.io/categories/ai/"><i class="fa-regular fa-folder fa-fw"></i>&nbsp;AI</a></span></div>
      <div class="post-meta-line"><span title=2021-08-13&#32;10:30:29>
            <i class="fa-regular fa-calendar-alt fa-fw"></i>&nbsp;<time datetime="2021-08-13" >2021-08-13</time>
          </span>&nbsp;<i class="fa-solid fa-pencil-alt fa-fw"></i>&nbsp;约 951 字&nbsp;
        <i class="fa-regular fa-clock fa-fw"></i>&nbsp;预计阅读 2 分钟&nbsp;<span id="busuanzi_container_page_pv" class="busuanzi_visitors comment-visitors" data-flag-title="HumanPoseProject">
            <i class="fa-regular fa-eye fa-fw"></i>&nbsp;<span id="busuanzi_value_page_pv">-</span>&nbsp;次阅读
          </span>&nbsp;</div>
    </div><div class="featured-image"><img
    class="lazyload"
    src="/liudongdong1.github.io/svg/loading.min.svg"
    data-src="https://images.unsplash.com/photo-1624788998865-126ccbb55e40?ixid=MnwxMjA3fDB8MHx0b3BpYy1mZWVkfDJ8NnNNVmpUTFNrZVF8fGVufDB8fHx8&amp;ixlib=rb-1.2.1&amp;auto=format&amp;fit=crop&amp;w=500&amp;q=60"
    data-srcset="https://images.unsplash.com/photo-1624788998865-126ccbb55e40?ixid=MnwxMjA3fDB8MHx0b3BpYy1mZWVkfDJ8NnNNVmpUTFNrZVF8fGVufDB8fHx8&amp;ixlib=rb-1.2.1&amp;auto=format&amp;fit=crop&amp;w=500&amp;q=60, https://images.unsplash.com/photo-1624788998865-126ccbb55e40?ixid=MnwxMjA3fDB8MHx0b3BpYy1mZWVkfDJ8NnNNVmpUTFNrZVF8fGVufDB8fHx8&amp;ixlib=rb-1.2.1&amp;auto=format&amp;fit=crop&amp;w=500&amp;q=60 1.5x, https://images.unsplash.com/photo-1624788998865-126ccbb55e40?ixid=MnwxMjA3fDB8MHx0b3BpYy1mZWVkfDJ8NnNNVmpUTFNrZVF8fGVufDB8fHx8&amp;ixlib=rb-1.2.1&amp;auto=format&amp;fit=crop&amp;w=500&amp;q=60 2x"
    data-sizes="auto"
    alt="https://images.unsplash.com/photo-1624788998865-126ccbb55e40?ixid=MnwxMjA3fDB8MHx0b3BpYy1mZWVkfDJ8NnNNVmpUTFNrZVF8fGVufDB8fHx8&amp;ixlib=rb-1.2.1&amp;auto=format&amp;fit=crop&amp;w=500&amp;q=60"
    title="https://images.unsplash.com/photo-1624788998865-126ccbb55e40?ixid=MnwxMjA3fDB8MHx0b3BpYy1mZWVkfDJ8NnNNVmpUTFNrZVF8fGVufDB8fHx8&amp;ixlib=rb-1.2.1&amp;auto=format&amp;fit=crop&amp;w=500&amp;q=60"/></div><div class="details toc" id="toc-static" kept="true">
        <div class="details-summary toc-title">
          <span>目录</span>
          <span><i class="details-icon fa-solid fa-angle-right"></i></span>
        </div>
        <div class="details-content toc-content" id="toc-content-static"><nav id="TableOfContents">
  <ul>
    <li>
      <ul>
        <li></li>
      </ul>
    </li>
  </ul>
</nav></div>
      </div><div
      class="content"
      id="content"
      
      
    ><h4 id="1--residual_posehttpsgithubcomidiapresidual_pose">1. <strong><a href="https://github.com/idiap/residual_pose"target="_blank" rel="external nofollow noopener noreferrer"> residual_pose<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></strong></h4>
<ul>
<li>Hourglass model for multi-person <code>2D pose estimation from depth images</code>.</li>
<li>Our regressor NN architecture for <code>3D human pose estimation</code>.</li>
<li>3D pose prior for <code>recovering from 2D missed detections</code>.</li>
<li>Tranined models for <code>2D and 3D pose estimation</code>.</li>
<li>Code for obtaining <code>2D and 3D pose from a depth image</code>.</li>
<li>11 months ago.    star12</li>
</ul>
<p><img
    class="lazyload"
    src="/liudongdong1.github.io/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20210819141418882.png"
    data-srcset="https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20210819141418882.png, https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20210819141418882.png 1.5x, https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20210819141418882.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20210819141418882.png"
    title="https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20210819141418882.png"/></p>
<h4 id="2-depth_human_synthesishttpsgithubcomidiapdepth_human_synthesis">2. <a href="https://github.com/idiap/depth_human_synthesis"target="_blank" rel="external nofollow noopener noreferrer">depth_human_synthesis<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></h4>
<blockquote>
<ul>
<li>We have created a collection of 24 human characters, 12 men and 12 women, with Makehuman. The characters exhibit different body features such as height and weight.</li>
<li>By<code> synthesizing images with people</code> we have the benefit that 2D and 3D body landmark annotations are extracted automatically during the rendering process.</li>
</ul>
</blockquote>
<p><img
    class="lazyload"
    src="/liudongdong1.github.io/svg/loading.min.svg"
    data-src="https://github.com/idiap/depth_human_synthesis/raw/main/imgs/depth_generation.gif"
    data-srcset="https://github.com/idiap/depth_human_synthesis/raw/main/imgs/depth_generation.gif, https://github.com/idiap/depth_human_synthesis/raw/main/imgs/depth_generation.gif 1.5x, https://github.com/idiap/depth_human_synthesis/raw/main/imgs/depth_generation.gif 2x"
    data-sizes="auto"
    alt="https://github.com/idiap/depth_human_synthesis/raw/main/imgs/depth_generation.gif"
    title="https://github.com/idiap/depth_human_synthesis/raw/main/imgs/depth_generation.gif"/></p>
<h4 id="3-a2jhttpsgithubcomzhangboshena2j">3. <a href="https://github.com/zhangboshen/A2J"target="_blank" rel="external nofollow noopener noreferrer">A2J<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></h4>
<blockquote>
<p>Xiong F, Zhang B, Xiao Y, et al. A2j: Anchor-to-joint regression network for 3d articulated pose estimation from a single depth image[C]//Proceedings of the IEEE/CVF International Conference on Computer Vision. 2019: 793-802. [<a href="https://arxiv.org/abs/1908.09999"target="_blank" rel="external nofollow noopener noreferrer">pdf<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>]</p>
<ul>
<li>propose a simple and effective approach termed A2J, for<code> 3D hand and human pose estimation from a single depth image</code>. Wide-range evaluations on 5 datasets demonstrate A2J&rsquo;s superiority.</li>
<li>anchor points able to capture global-local spatial context information are densely set on depth image as local regressors for the joints. They contribute to predict the positions of the joints in ensemble way to enhance generalization ability.</li>
<li>star 208, 12 month ago;</li>
</ul>
</blockquote>
<p><img
    class="lazyload"
    src="/liudongdong1.github.io/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogpictureV2/raw/master/A2Jpipeline.png"
    data-srcset="https://gitee.com/github-25970295/blogpictureV2/raw/master/A2Jpipeline.png, https://gitee.com/github-25970295/blogpictureV2/raw/master/A2Jpipeline.png 1.5x, https://gitee.com/github-25970295/blogpictureV2/raw/master/A2Jpipeline.png 2x"
    data-sizes="auto"
    alt="pipeline"
    title="pipeline"/></p>
<h5 id="1--nyuhttpsjonathantompsongithubionyu_hand_pose_datasethtm-hand-pose-dataset">.1.  <a href="https://jonathantompson.github.io/NYU_Hand_Pose_Dataset.htm"target="_blank" rel="external nofollow noopener noreferrer">NYU<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a> hand pose dataset:</h5>
<p><img
    class="lazyload"
    src="/liudongdong1.github.io/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogpictureV2/raw/master/NYU_1.png"
    data-srcset="https://gitee.com/github-25970295/blogpictureV2/raw/master/NYU_1.png, https://gitee.com/github-25970295/blogpictureV2/raw/master/NYU_1.png 1.5x, https://gitee.com/github-25970295/blogpictureV2/raw/master/NYU_1.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogpictureV2/raw/master/NYU_1.png"
    title="https://gitee.com/github-25970295/blogpictureV2/raw/master/NYU_1.png"/></p>
<h5 id="2-itophttpswwwalberthaquecomprojectsviewpoint_3d_pose-body-pose-dataset">.2. <a href="https://www.alberthaque.com/projects/viewpoint_3d_pose/"target="_blank" rel="external nofollow noopener noreferrer">ITOP<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a> body pose dataset:</h5>
<p><img
    class="lazyload"
    src="/liudongdong1.github.io/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogpictureV2/raw/master/ITOP_1.png"
    data-srcset="https://gitee.com/github-25970295/blogpictureV2/raw/master/ITOP_1.png, https://gitee.com/github-25970295/blogpictureV2/raw/master/ITOP_1.png 1.5x, https://gitee.com/github-25970295/blogpictureV2/raw/master/ITOP_1.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogpictureV2/raw/master/ITOP_1.png"
    title="https://gitee.com/github-25970295/blogpictureV2/raw/master/ITOP_1.png"/></p>
<h4 id="4-depth-pose-estimationhttpsgithubcommostro-complexitydepth-pose-estimation">4. <a href="https://github.com/Mostro-Complexity/depth-pose-estimation"target="_blank" rel="external nofollow noopener noreferrer">depth-pose-estimation<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></h4>
<blockquote>
<ul>
<li>
<p>Train models to predict body parts or joints. Using depth images to recognise the human pose;</p>
</li>
<li>
<p>3 years ago; star 7;</p>
</li>
</ul>
</blockquote>
<p><img
    class="lazyload"
    src="/liudongdong1.github.io/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogpictureV2/raw/master/header.png"
    data-srcset="https://gitee.com/github-25970295/blogpictureV2/raw/master/header.png, https://gitee.com/github-25970295/blogpictureV2/raw/master/header.png 1.5x, https://gitee.com/github-25970295/blogpictureV2/raw/master/header.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogpictureV2/raw/master/header.png"
    title="https://gitee.com/github-25970295/blogpictureV2/raw/master/header.png"/></p>
<h4 id="5-b_depth_estimatorhttpsgithubcomechen4628b_depth_estimator">5. <a href="https://github.com/echen4628/b_depth_estimator"target="_blank" rel="external nofollow noopener noreferrer">b_depth_estimator<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></h4>
<blockquote>
<ul>
<li>baseline depth estimator using single image and bounding box</li>
<li>通过相机数学相似模型，在已知物体实际大小的时候，估计深度信息。</li>
</ul>
</blockquote>
<h4 id="6-monodepth2httpsgithubcomnianticlabsmonodepth2">6. <a href="https://github.com/nianticlabs/monodepth2"target="_blank" rel="external nofollow noopener noreferrer">monodepth2<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></h4>
<blockquote>
<p>Godard C, Mac Aodha O, Firman M, et al. Digging into self-supervised monocular depth estimation[C]//Proceedings of the IEEE/CVF International Conference on Computer Vision. 2019: 3828-3838.</p>
<ul>
<li>(i) a minimum reprojection loss, designed to robustly handle occlusions, (ii) a full-resolution multi-scale sampling method that reduces visual artifacts, and (iii) an auto-masking loss to ignore training pixels that violate camera motion assumptions.</li>
<li>Per-pixel ground-truth depth data is challenging to acquire at scale</li>
<li>stars 2.5k, Fork: 638;</li>
</ul>
</blockquote>
<!-- raw HTML omitted -->
<h4 id="7-s2r-depthnethttpsgithubcommicrosofts2r-depthnet">7. <a href="https://github.com/microsoft/S2R-DepthNet"target="_blank" rel="external nofollow noopener noreferrer">S2R-DepthNet<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></h4>
<blockquote>
<p>Chen X, Wang Y, Chen X, et al. S2R-DepthNet: Learning a Generalizable Depth-specific Structural Representation[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021: 3034-3043.</p>
<ul>
<li>the first to explore the learning of a <code>depth-specific structural representation</code>, which captures the essential feature for depth estimation and ignores irrelevant style information.</li>
<li>Our S2R-DepthNet (Synthetic to Real DepthNet) can be well generalized to unseen real-world data directly even though it is only trained on synthetic data. S2R-DepthNet consists of: a) a<code> Structure Extraction (STE) module which extracts a domaininvariant structural representation</code> from an image by disentangling the image into domain-invariant structure and domain-specific style components, b) <code>a Depth-specific Attention (DSA) module, which learns task-specific knowledge to suppress depth-irrelevant structures for better depth estimation and generalization</code>, and c) a depth prediction module (DP) to<code> predict depth from the depth-specific representation.</code></li>
<li>star 79, 3 months ago;</li>
</ul>
</blockquote>
<p><img
    class="lazyload"
    src="/liudongdong1.github.io/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogpictureV2/raw/master/overview.png"
    data-srcset="https://gitee.com/github-25970295/blogpictureV2/raw/master/overview.png, https://gitee.com/github-25970295/blogpictureV2/raw/master/overview.png 1.5x, https://gitee.com/github-25970295/blogpictureV2/raw/master/overview.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogpictureV2/raw/master/overview.png"
    title="https://gitee.com/github-25970295/blogpictureV2/raw/master/overview.png"/></p>
<p><img
    class="lazyload"
    src="/liudongdong1.github.io/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogpictureV2/raw/master/intro.PNG"
    data-srcset="https://gitee.com/github-25970295/blogpictureV2/raw/master/intro.PNG, https://gitee.com/github-25970295/blogpictureV2/raw/master/intro.PNG 1.5x, https://gitee.com/github-25970295/blogpictureV2/raw/master/intro.PNG 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogpictureV2/raw/master/intro.PNG"
    title="https://gitee.com/github-25970295/blogpictureV2/raw/master/intro.PNG"/></p>
<h4 id="8-penet_icra2021httpsgithubcomjugghmpenet_icra2021">8. <a href="https://github.com/JUGGHM/PENet_ICRA2021"target="_blank" rel="external nofollow noopener noreferrer">PENet_ICRA2021<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></h4>
<blockquote>
<p>Hu, M., Wang, S., Li, B., Ning, S., Fan, L., &amp; Gong, X. (2021). Towards Precise and Efficient Image Guided Depth Completion. <em>arXiv e-prints</em>, arXiv-2103.  ICRA 2021.</p>
<ul>
<li>6 months ago; Star 93;</li>
<li><code>Image guided depth completion</code> is the task of generating a<code> dense depth map from a sparse depth map and a high quality image</code></li>
<li>This paper proposes a two-branch backbone that consists of a color-dominant branch and a depth-dominant branch to exploit and fuse two modalities thoroughly. More specifically, one branch<code> inputs a color image and a sparse depth map to predict a dense depth map</code>. The other branch takes as <code>inputs the sparse depth map and the previously predicted depth map, and outputs a dense depth map as well</code>. The depth maps predicted from two branches are complimentary to each other and therefore they are adaptively fused.</li>
<li>a simple geometric convolutional layer to encode 3D geometric cues. The geometric encoded backbone conducts the fusion of different modalities at multiple stages, leading to good depth completion results.</li>
</ul>
</blockquote>
<p><img
    class="lazyload"
    src="/liudongdong1.github.io/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20210819142532697.png"
    data-srcset="https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20210819142532697.png, https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20210819142532697.png 1.5x, https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20210819142532697.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20210819142532697.png"
    title="https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20210819142532697.png"/></p>
<h4 id="9-depthai_movenethttpsgithubcomgeaxgxdepthai_movenet">9. <a href="https://github.com/geaxgx/depthai_movenet"target="_blank" rel="external nofollow noopener noreferrer">depthai_movenet<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></h4>
<blockquote>
<ul>
<li>A convolutional neural network model that runs on<code> RGB images</code> and predicts <a href="https://github.com/tensorflow/tfjs-models/tree/master/pose-detection#coco-keypoints-used-in-movenet-and-posenet"target="_blank" rel="external nofollow noopener noreferrer">human joint locations<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a> of a single person. Two variant:<code> Lightning and Thunder,</code> the latter being slower but more accurate.</li>
<li>The cropping algorithm determines from the body detected <code>in frame N, on which region of frame N+1 the inference will run.</code> The mode (Host or Edge) describes where this algorithm runs :
<ul>
<li>in Host mode, the cropping algorithm runs on the host cpu. Only this mode allows images or video files as input. The flow of information between the host and the device is bi-directional: in particular, the host sends frames or cropping instructions to the device;</li>
<li>in Edge mode, the cropping algorithm runs on the MyriadX. So, in this mode, all the functional bricks of MoveNet (inference, determination of the cropping region for next frame, cropping) are executed on the device. The only information exchanged are the body keypoints and optionally the camera video frame.</li>
</ul>
</li>
</ul>
</blockquote>
<p><img
    class="lazyload"
    src="/liudongdong1.github.io/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogpictureV2/raw/master/movenet_nodes.png"
    data-srcset="https://gitee.com/github-25970295/blogpictureV2/raw/master/movenet_nodes.png, https://gitee.com/github-25970295/blogpictureV2/raw/master/movenet_nodes.png 1.5x, https://gitee.com/github-25970295/blogpictureV2/raw/master/movenet_nodes.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogpictureV2/raw/master/movenet_nodes.png"
    title="https://gitee.com/github-25970295/blogpictureV2/raw/master/movenet_nodes.png"/></p>
<ul>
<li>demo: codehttps://github.com/geaxgx/openvino_movenet</li>
</ul>
<!-- raw HTML omitted -->
<h5 id="1-aphabet-classification">.1. Aphabet classification</h5>
<ul>
<li><a href="https://github.com/geaxgx/depthai_movenet/tree/main/examples/semaphore_alphabet"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/geaxgx/depthai_movenet/tree/main/examples/semaphore_alphabet<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></li>
</ul>
<!-- raw HTML omitted -->
<h5 id="2-yoga--classification">.2. Yoga  classification</h5>
<ul>
<li><a href="https://github.com/geaxgx/depthai_movenet/raw/main/examples/yoga_pose_recognition/medias/yoga_pose.gif"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/geaxgx/depthai_movenet/raw/main/examples/yoga_pose_recognition/medias/yoga_pose.gif<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></li>
<li><strong><a href="https://github.com/felixchenfy/Realtime-Action-Recognition"target="_blank" rel="external nofollow noopener noreferrer"> Realtime-Action-Recognition<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></strong> 该项目也是通过利用人体谷歌点进行粗粒度动作分类的。</li>
</ul>
<!-- raw HTML omitted -->
<h4 id="10-arcore-depth-labhttpsgithubcomgooglesamplesarcore-depth-lab">10. <a href="https://github.com/googlesamples/arcore-depth-lab"target="_blank" rel="external nofollow noopener noreferrer">arcore-depth-lab<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></h4>
<blockquote>
<p><strong>Depth Lab</strong> is a set of ARCore Depth API samples that <code>provides assets using depth for advanced geometry-aware features in AR interaction and rendering.  </code><a href="https://www.youtube.com/watch?v=VOVhCTb-1io"target="_blank" rel="external nofollow noopener noreferrer">Depth API overview<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a> <a href="https://developers.google.com/ar/develop/unity/depth/overview"target="_blank" rel="external nofollow noopener noreferrer"><strong>ARCore Depth API</strong><i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a> is enabled on a subset of ARCore-certified Android devices.</p>
</blockquote>
<!-- raw HTML omitted -->
<blockquote>
<p>Lin X, Chen Y, Bao L, et al. High-fidelity 3d digital human creation from rgb-d selfies[J]. arXiv preprint arXiv:2010.05562, 2020.</p>
<ul>
<li>使用RGBD数据，得到粗糙的静态面部模型</li>
</ul>
</blockquote>
<p><img
    class="lazyload"
    src="/liudongdong1.github.io/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20210822163814188.png"
    data-srcset="https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20210822163814188.png, https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20210822163814188.png 1.5x, https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20210822163814188.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20210822163814188.png"
    title="https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20210822163814188.png"/></p>
<p><img
    class="lazyload"
    src="/liudongdong1.github.io/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20210822163832544.png"
    data-srcset="https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20210822163832544.png, https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20210822163832544.png 1.5x, https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20210822163832544.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20210822163832544.png"
    title="https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20210822163832544.png"/></p>
</div>
<div class="post-footer" id="post-footer">
  <div class="post-info">
    <div class="post-info-line">
      <div class="post-info-mod">
        <span title=2023-09-28&#32;23:07:56>更新于 2023-09-28&nbsp;</span>
      </div><div class="post-info-license">
          <span><a rel="license external nofollow noopener noreferrer" href="https://creativecommons.org/licenses/by-nc/4.0/" target="_blank">CC BY-NC 4.0</a></span>
        </div></div>
    <div class="post-info-line">
      <div class="post-info-md"><span><a href="liudongdong1.github.io/humanposeproject/index.md" title="阅读原始文档" class="link-to-markdown">阅读原始文档</a></span><span><a href="https://liudongdong1.github.io/edit/master/content/posts%5c%e8%a7%86%e8%a7%89%e8%bf%90%e5%8a%a8%5cPose%5cHumanPoseProject.md" title="编辑此页"target="_blank" rel="external nofollow noopener noreferrer" class="link-to-edit">编辑此页</a></span></div>
      <div class="post-info-share">
        <span><a href="javascript:void(0);" title="分享到 Twitter" data-sharer="twitter" data-url="liudongdong1.github.io/humanposeproject/" data-title="HumanPoseProject" data-hashtags="CV"><i class="fa-brands fa-twitter fa-fw" aria-hidden="true"></i></a>
  <a href="javascript:void(0);" title="分享到 Facebook" data-sharer="facebook" data-url="liudongdong1.github.io/humanposeproject/" data-hashtag="CV"><i class="fa-brands fa-facebook-square fa-fw" aria-hidden="true"></i></a>
  <a href="javascript:void(0);" title="分享到 微博" data-sharer="weibo" data-url="liudongdong1.github.io/humanposeproject/" data-title="HumanPoseProject" data-image="https://images.unsplash.com/photo-1624788998865-126ccbb55e40?ixid=MnwxMjA3fDB8MHx0b3BpYy1mZWVkfDJ8NnNNVmpUTFNrZVF8fGVufDB8fHx8&amp;ixlib=rb-1.2.1&amp;auto=format&amp;fit=crop&amp;w=500&amp;q=60"><i class="fa-brands fa-weibo fa-fw" aria-hidden="true"></i></a>
  </span>
      </div>
    </div>
  </div>

  <div class="post-info-more">
    <section class="post-tags"><i class="fa-solid fa-tags fa-fw" aria-hidden="true"></i>&nbsp;<a href="liudongdong1.github.io/tags/cv/">CV</a></section>
    <section>
      <span><a href="javascript:void(0);" onclick="window.history.back();">返回</a></span>&nbsp;|&nbsp;<span><a href="liudongdong1.github.io/">主页</a></span>
    </section>
  </div>

  <div class="post-nav"><a href="liudongdong1.github.io/streampipline/" class="prev" rel="prev" title="StreamPipeline"><i class="fa-solid fa-angle-left fa-fw" aria-hidden="true"></i>StreamPipeline</a>
      <a href="liudongdong1.github.io/v2ray/" class="next" rel="next" title="v2ray">v2ray<i class="fa-solid fa-angle-right fa-fw" aria-hidden="true"></i></a></div>
</div>
</article></main><footer class="footer">
    <div class="footer-container"><div class="footer-line powered">由 <a href="https://gohugo.io/" target="_blank" rel="external nofollow noopener noreferrer" title="Hugo 0.118.2">Hugo</a> 强力驱动 | 主题 - <a href="https://github.com/hugo-fixit/FixIt" target="_blank" rel="external" title="FixIt v0.2.17-RC"><img class="fixit-icon" src="/liudongdong1.github.io/fixit.min.svg" alt="FixIt logo" />&nbsp;FixIt</a>
        </div><div class="footer-line copyright" itemscope itemtype="http://schema.org/CreativeWork"><i class="fa-regular fa-copyright fa-fw" aria-hidden="true"></i>
            <span itemprop="copyrightYear">2020 - 2023</span><span class="author" itemprop="copyrightHolder">
              <a href="https://liudongdong1.github.io/"target="_blank" rel="external nofollow noopener noreferrer">LiuDongdong</a></span><span class="license footer-divider"><a rel="license external nofollow noopener noreferrer" href="https://creativecommons.org/licenses/by-nc/4.0/" target="_blank">CC BY-NC 4.0</a></span></div><div class="footer-line statistics"><span class="site-time" title='网站运行中 ...'><i class="fa-solid fa-heartbeat fa-fw animate-icon" aria-hidden="true"></i>&nbsp;<span class="run-times">网站运行中 ...</span></span></div><div class="footer-line ibruce">
          <span id="busuanzi_container_site_uv" title='总访客数'><i class="fa-regular fa-user fa-fw" aria-hidden="true"></i>&nbsp;<span id="busuanzi_value_site_uv"><i class="fa-solid fa-spinner fa-spin fa-fw" aria-hidden="true"></i></span></span><span id="busuanzi_container_site_pv" class="footer-divider" title='总访问量'><i class="fa-regular fa-eye fa-fw" aria-hidden="true"></i>&nbsp;<span id="busuanzi_value_site_pv"><i class="fa-solid fa-spinner fa-spin fa-fw" aria-hidden="true"></i></span></span>
        </div></div>
  </footer></div><div class="widgets"><div class="fixed-buttons animate__faster d-none"><div class="fixed-button back-to-top" role="button" aria-label="回到顶部"><i class="fa-solid fa-arrow-up fa-fw" aria-hidden="true"></i><span class="variant-numeric">0%</span>
        </div></div><a href="https://liudongdong1.github.io/" title="在 GitHub 上查看源代码"target="_blank" rel="external nofollow" class="github-corner right d-none-mobile"><svg viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><div id="mask"></div><div class="reading-progress-bar" style="left: 0;top: 0;--bg-progress: #0076ff;--bg-progress-dark: #fff;"></div><noscript>
    <div class="noscript-warning">FixIt 主题在启用 JavaScript 的情况下效果最佳。</div>
  </noscript>
</div><link rel="stylesheet" href="/liudongdong1.github.io/lib/katex/katex.min.css"><link rel="stylesheet" href="/liudongdong1.github.io/lib/cookieconsent/cookieconsent.min.css"><script src="/liudongdong1.github.io/lib/autocomplete/autocomplete.min.js" defer></script><script src="/liudongdong1.github.io/lib/algoliasearch/algoliasearch-lite.umd.min.js" defer></script><script src="/liudongdong1.github.io/lib/lazysizes/lazysizes.min.js" async defer></script><script src="/liudongdong1.github.io/lib/sharer/sharer.min.js" async defer></script><script src="/liudongdong1.github.io/lib/typeit/index.umd.js" defer></script><script src="/liudongdong1.github.io/lib/katex/katex.min.js" defer></script><script src="/liudongdong1.github.io/lib/katex/auto-render.min.js" defer></script><script src="/liudongdong1.github.io/lib/katex/copy-tex.min.js" defer></script><script src="/liudongdong1.github.io/lib/katex/mhchem.min.js" defer></script><script src="/liudongdong1.github.io/lib/cookieconsent/cookieconsent.min.js" defer></script><script src="/liudongdong1.github.io/lib/pangu/pangu.min.js" defer></script><script src="/liudongdong1.github.io/lib/cell-watermark/watermark.min.js" defer></script><script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async defer></script><script>window.config={"code":{"copyTitle":"复制到剪贴板","editLockTitle":"锁定可编辑代码块","editUnLockTitle":"解锁可编辑代码块","editable":true,"maxShownLines":10},"comment":{"enable":false},"cookieconsent":{"content":{"dismiss":"同意","link":"了解更多","message":"本网站使用 Cookies 来改善您的浏览体验。"},"enable":true,"palette":{"button":{"background":"#f0f0f0"},"popup":{"background":"#1aa3ff"}},"theme":"edgeless"},"data":{"typeit-header-subtitle-desktop":"\u003cspan style='font-family: MMT,\"沐目体\";'\u003e吾日三省吾身\u003c/span\u003e","typeit-header-subtitle-mobile":"\u003cspan style='font-family: MMT,\"沐目体\";'\u003e吾日三省吾身\u003c/span\u003e"},"enablePWA":true,"enablePangu":true,"math":{"delimiters":[{"display":true,"left":"$$","right":"$$"},{"display":true,"left":"\\[","right":"\\]"},{"display":true,"left":"\\begin{equation}","right":"\\end{equation}"},{"display":true,"left":"\\begin{equation*}","right":"\\end{equation*}"},{"display":true,"left":"\\begin{align}","right":"\\end{align}"},{"display":true,"left":"\\begin{align*}","right":"\\end{align*}"},{"display":true,"left":"\\begin{alignat}","right":"\\end{alignat}"},{"display":true,"left":"\\begin{alignat*}","right":"\\end{alignat*}"},{"display":true,"left":"\\begin{gather}","right":"\\end{gather}"},{"display":true,"left":"\\begin{CD}","right":"\\end{CD}"},{"display":false,"left":"$","right":"$"},{"display":false,"left":"\\(","right":"\\)"}],"strict":false},"search":{"algoliaAppID":"2R1K9SKLQZ","algoliaIndex":"index.zh-cn","algoliaSearchKey":"4a226aa1c5c98d6859e4d1386adb2bc7","highlightTag":"em","maxResultLength":10,"noResultsFound":"没有找到结果","snippetLength":50,"type":"algolia"},"siteTime":"2020-12-18T16:15:22+08:00","typeit":{"cursorChar":"|","cursorSpeed":1000,"data":{"typeit-header-subtitle-desktop":["typeit-header-subtitle-desktop"],"typeit-header-subtitle-mobile":["typeit-header-subtitle-mobile"]},"duration":-1,"speed":100},"watermark":{"appendto":".wrapper\u003emain","colspacing":30,"content":"\u003cimg class=\"fixit-icon\" src=\"/fixit.min.svg\" alt=\"FixIt logo\" /\u003e FixIt 主题","enable":true,"fontfamily":"inherit","fontsize":0.85,"height":21,"opacity":0.0125,"rotate":15,"rowspacing":60,"width":150}};</script><script src="/liudongdong1.github.io/js/theme.min.js" defer></script><script src="/liudongdong1.github.io/js/custom.min.js" defer></script></body>
</html>
