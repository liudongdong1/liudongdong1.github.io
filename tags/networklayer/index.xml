<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  <channel>
    <title>networkLayer - 标签 - DAY By DAY</title>
    <link>https://liudongdong1.github.io/tags/networklayer/</link>
    <description>networkLayer - 标签 - DAY By DAY</description>
    <generator>Hugo -- gohugo.io</generator><language>zh-CN</language><managingEditor>3463264078@qq.cn (LiuDongdong)</managingEditor>
      <webMaster>3463264078@qq.cn (LiuDongdong)</webMaster><copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright><lastBuildDate>Mon, 31 Aug 2020 17:23:49 &#43;0000</lastBuildDate><atom:link href="https://liudongdong1.github.io/tags/networklayer/" rel="self" type="application/rss+xml" /><item>
  <title>BatchNormalization</title>
  <link>https://liudongdong1.github.io/batchnormalization/</link>
  <pubDate>Mon, 31 Aug 2020 17:23:49 &#43;0000</pubDate>
  <author>liudongdong1</author>
  <guid>https://liudongdong1.github.io/batchnormalization/</guid>
  <description><![CDATA[<div class="featured-image">
        <img src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/artificial-intelligence-2167835__340.webp" referrerpolicy="no-referrer">
      </div>1. Internal Covariate Shift 统计学习中的一个很重要的假设就是输入的分布是相对稳定的。如果这个假设不满足，则模型的收敛会很慢，甚至无法收敛。所以，对于一般的统计学]]></description>
</item>
</channel>
</rss>
