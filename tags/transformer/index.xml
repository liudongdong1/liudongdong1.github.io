<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  <channel>
    <title>Transformer - 标签 - DAY By DAY</title>
    <link>https://liudongdong1.github.io/tags/transformer/</link>
    <description>Transformer - 标签 - DAY By DAY</description>
    <generator>Hugo -- gohugo.io</generator><language>zh-CN</language><managingEditor>3463264078@qq.cn (LiuDongdong)</managingEditor>
      <webMaster>3463264078@qq.cn (LiuDongdong)</webMaster><copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright><lastBuildDate>Mon, 13 Jul 2020 09:47:19 &#43;0000</lastBuildDate><atom:link href="https://liudongdong1.github.io/tags/transformer/" rel="self" type="application/rss+xml" /><item>
  <title>Transformer_Introduce</title>
  <link>https://liudongdong1.github.io/transformer_introduce/</link>
  <pubDate>Mon, 13 Jul 2020 09:47:19 &#43;0000</pubDate>
  <author>liudongdong1</author>
  <guid>https://liudongdong1.github.io/transformer_introduce/</guid>
  <description><![CDATA[<div class="featured-image">
        <img src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20200715201109572.png" referrerpolicy="no-referrer">
      </div>The Transformer starts by generating initial representations, or embeddings, for each word. These are represented by the unfilled circles. Then, using self-attention, it aggregates information from all of the other words, generating a new representation per word informed by the entire context, represented by the filled balls. This step is then repeated multiple times in parallel for all words, successively generating new representations. 代码讲解地址：]]></description>
</item>
</channel>
</rss>
