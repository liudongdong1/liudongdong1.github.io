<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  <channel>
    <title>AI - 标签 - DAY By DAY</title>
    <link>liudongdong1.github.io/tags/ai/</link>
    <description>AI - 标签 - DAY By DAY</description>
    <generator>Hugo -- gohugo.io</generator><language>zh-CN</language><managingEditor>3463264078@qq.cn (LiuDongdong)</managingEditor>
      <webMaster>3463264078@qq.cn (LiuDongdong)</webMaster><copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright><lastBuildDate>Mon, 14 Jun 2021 16:00:04 &#43;0000</lastBuildDate><atom:link href="liudongdong1.github.io/tags/ai/" rel="self" type="application/rss+xml" /><item>
  <title>WordGenTools</title>
  <link>liudongdong1.github.io/wordgentools/</link>
  <pubDate>Mon, 14 Jun 2021 16:00:04 &#43;0000</pubDate>
  <author>liudongdong1</author>
  <guid>liudongdong1.github.io/wordgentools/</guid>
  <description><![CDATA[<div class="featured-image">
        <img src="https://cdn.stocksnap.io/img-thumbs/280h/J2EZQDXGOT.jpg" referrerpolicy="no-referrer">
      </div>0. GoogleTransBasedFastArticle 查了几个网页的版本，但是使用次数有限，基于google翻译，在Google Trans API 和 FastArtcle , 和文章伪原创工具，基于google翻译软件，在多个语]]></description>
</item>
<item>
  <title>炼丹优化</title>
  <link>liudongdong1.github.io/%E7%82%BC%E4%B8%B9%E4%BC%98%E5%8C%96/</link>
  <pubDate>Mon, 26 Apr 2021 21:31:56 &#43;0000</pubDate>
  <author>liudongdong1</author>
  <guid>liudongdong1.github.io/%E7%82%BC%E4%B8%B9%E4%BC%98%E5%8C%96/</guid>
  <description><![CDATA[<div class="featured-image">
        <img src="https://cdn.stocksnap.io/img-thumbs/280h/cauliflower-food_RESOL80ESF.jpg" referrerpolicy="no-referrer">
      </div>17种最新方法提高炼丹速度https://mp.weixin.qq.com/s?__biz=MzI5MDUyMDIxNA==&amp;mid]]></description>
</item>
<item>
  <title>ObjectDetection</title>
  <link>liudongdong1.github.io/objectdetection/</link>
  <pubDate>Wed, 25 Nov 2020 16:00:04 &#43;0000</pubDate>
  <author>liudongdong1</author>
  <guid>liudongdong1.github.io/objectdetection/</guid>
  <description><![CDATA[<div class="featured-image">
        <img src="https://gitee.com/github-25970295/blogImage/raw/master/img/30.jpeg" referrerpolicy="no-referrer">
      </div>1. NanoDet NanoDet 是一个速度超快和轻量级的移动端 Anchor-free 目标检测模型。该模型具备以下优势： 超轻量级：模型文件大小仅 1.8m； 速度超快：在移动 ARM CPU 上的速度达到 9]]></description>
</item>
<item>
  <title>WordEmbedding</title>
  <link>liudongdong1.github.io/wordembedding/</link>
  <pubDate>Tue, 20 Oct 2020 07:56:09 &#43;0000</pubDate>
  <author>liudongdong1</author>
  <guid>liudongdong1.github.io/wordembedding/</guid>
  <description><![CDATA[<div class="featured-image">
        <img src="https://gitee.com/github-25970295/blogImage/raw/master/img/view-of-coffee-beans.jpg" referrerpolicy="no-referrer">
      </div>TEXT processing deals with humongous amount of text to perform different range of tasks like clustering in the g oogle search example, classification in the second and Machine Translation. How to create a representation for words that capture their meanings, semantic relationships and the different types of contexts they are used in. 作为 Embedding 层嵌入到深度模型中，实现将高维]]></description>
</item>
<item>
  <title>OpenPose_Usage</title>
  <link>liudongdong1.github.io/openpose_usage/</link>
  <pubDate>Tue, 15 Sep 2020 16:00:04 &#43;0000</pubDate>
  <author>liudongdong1</author>
  <guid>liudongdong1.github.io/openpose_usage/</guid>
  <description><![CDATA[<div class="featured-image">
        <img src="https://gitee.com/github-25970295/blogImage/raw/master/img/30.jpeg" referrerpolicy="no-referrer">
      </div>1. KeyPoint Exact 1.1. VideoHandle #with face and hands bin\OpenPoseDemo.exe --video examples\media\video.avi --face --hand # Only body ./build/examples/openpose/openpose.bin --video examples/media/video.avi --write_json output/ --display 0 --render_pose 0 # Body + face + hands ./build/examples/openpose/openpose.bin --video examples/media/video.avi --write_json output/ --display 0 --render_pose 0 --face --hand #save to json and video ./build/examples/openpose/openpose.bin --video examples/media/video.avi --write_video output/result.avi --write_json output/ 1.2. Webcam_Handle :: With face and hands bin\OpenPoseDemo.exe --face --hand 1.3. Images_Handle :: With face]]></description>
</item>
<item>
  <title>AnchorIntroduce</title>
  <link>liudongdong1.github.io/anchorintroduce/</link>
  <pubDate>Tue, 25 Aug 2020 16:00:04 &#43;0000</pubDate>
  <author>liudongdong1</author>
  <guid>liudongdong1.github.io/anchorintroduce/</guid>
  <description><![CDATA[<div class="featured-image">
        <img src="https://gitee.com/github-25970295/blogImage/raw/master/img/laptop-disposable-cup-and-yellow-flowers-in-vase-on-table.jpg" referrerpolicy="no-referrer">
      </div>在网络最后的输出中，对于每个grid cell产生3个bounding box，每个bounding box的输出有三类参数：一个是对象的box参]]></description>
</item>
<item>
  <title>Network</title>
  <link>liudongdong1.github.io/network/</link>
  <pubDate>Thu, 16 Jul 2020 23:28:40 &#43;0000</pubDate>
  <author>liudongdong1</author>
  <guid>liudongdong1.github.io/network/</guid>
  <description><![CDATA[<div class="featured-image">
        <img src="https://gitee.com/github-25970295/blogImage/raw/master/img/desk-designer-typography-brochure.jpg" referrerpolicy="no-referrer">
      </div>level: CVPR CCF A author: Eddy IIg date: 2017 keyword: Optical Flow Paper: FlowNET2.0不理解 Summary 文章以实验主导,数据的顺序, 模型的叠加,各种算法结果 Note 去加强 https://lmb.informatik.uni-freiburg.de/ Proble Statement previous work: End-to-End optical flow estimation with CNNs was first]]></description>
</item>
<item>
  <title>OpenMMLab</title>
  <link>liudongdong1.github.io/openmmlab/</link>
  <pubDate>Mon, 13 Jul 2020 21:59:57 &#43;0000</pubDate>
  <author>liudongdong1</author>
  <guid>liudongdong1.github.io/openmmlab/</guid>
  <description><![CDATA[<div class="featured-image">
        <img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201129135527221.png" referrerpolicy="no-referrer">
      </div>OpenMMLab 在Github上不是一个单独项目，除了大家所熟知的 Github 上万 star 目标检测库 MMDetection，还有其他方向的代码库和数据集,目前Github]]></description>
</item>
<item>
  <title>Seq2seq_Introduce</title>
  <link>liudongdong1.github.io/seq2seq_introduce/</link>
  <pubDate>Tue, 30 Jun 2020 16:00:04 &#43;0000</pubDate>
  <author>liudongdong1</author>
  <guid>liudongdong1.github.io/seq2seq_introduce/</guid>
  <description><![CDATA[<div class="featured-image">
        <img src="https://gitee.com/github-25970295/blogImage/raw/master/img/sunset-over-lake-1.jpg" referrerpolicy="no-referrer">
      </div>A sequence-to-sequence model is a model that takes a sequence of items (words, letters, features of an images…etc) and outputs another sequence of items. The encoder processes each item in the input sequence, it compiles the information it captures into a vector (called the context). After processing the entire input sequence, the encoder sends the context over to the decoder,]]></description>
</item>
<item>
  <title>Template</title>
  <link>liudongdong1.github.io/template/</link>
  <pubDate>Tue, 30 Jun 2020 16:00:04 &#43;0000</pubDate>
  <author>liudongdong1</author>
  <guid>liudongdong1.github.io/template/</guid>
  <description><![CDATA[<div class="featured-image">
        <img src="https://gitee.com/github-25970295/blogImage/raw/master/img/30.jpeg" referrerpolicy="no-referrer">
      </div>level: author: date: keyword: Paper: Summary Research Objective Application Area: Purpose: Proble Statement previous work: Methods Problem Formulation: system overview: 【Qustion 1】 Evaluation Environment: Dataset: Conclusion Notes 去加强了解 Paper: LIRe Summary 现象，辅助支撑材料，目前按解决方案不足, 介于此，我们想]]></description>
</item>
</channel>
</rss>
