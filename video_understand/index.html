<!DOCTYPE html>
<html itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">
  <head>
    
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
    <meta name="robots" content="noodp" />
    <title>Video_Undertand - DAY By DAY</title><meta name="author" content="LiuDongdong">
<meta name="author-link" content="https://liudongdong1.github.io/">
<meta name="description" content="安防监控领域，包括人脸识别、行为识别、运动跟踪、人群分析等等，利用卡口精准位置布控视频监测，实现了监控区域内异常的自动识别，例如动态视频中的" /><meta name="keywords" content='CV' /><meta itemprop="name" content="Video_Undertand">
<meta itemprop="description" content="安防监控领域，包括人脸识别、行为识别、运动跟踪、人群分析等等，利用卡口精准位置布控视频监测，实现了监控区域内异常的自动识别，例如动态视频中的"><meta itemprop="datePublished" content="2020-06-06T21:45:45+00:00" />
<meta itemprop="dateModified" content="2023-09-28T23:52:41+08:00" />
<meta itemprop="wordCount" content="13137"><meta itemprop="image" content="https://liudongdong1.github.io/logo.png"/>
<meta itemprop="keywords" content="CV," /><meta property="og:title" content="Video_Undertand" />
<meta property="og:description" content="安防监控领域，包括人脸识别、行为识别、运动跟踪、人群分析等等，利用卡口精准位置布控视频监测，实现了监控区域内异常的自动识别，例如动态视频中的" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://liudongdong1.github.io/video_understand/" /><meta property="og:image" content="https://liudongdong1.github.io/logo.png"/><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2020-06-06T21:45:45+00:00" />
<meta property="article:modified_time" content="2023-09-28T23:52:41+08:00" />
<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://liudongdong1.github.io/logo.png"/>

<meta name="twitter:title" content="Video_Undertand"/>
<meta name="twitter:description" content="安防监控领域，包括人脸识别、行为识别、运动跟踪、人群分析等等，利用卡口精准位置布控视频监测，实现了监控区域内异常的自动识别，例如动态视频中的"/>
<meta name="application-name" content="DAY By DAY">
<meta name="apple-mobile-web-app-title" content="DAY By DAY"><meta name="theme-color" data-light="#f8f8f8" data-dark="#252627" content="#f8f8f8"><meta name="msapplication-TileColor" content="#da532c"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" />
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"><link rel="manifest" href="/site.webmanifest"><link rel="canonical" href="https://liudongdong1.github.io/video_understand/" /><link rel="prev" href="https://liudongdong1.github.io/data-glove-record/" /><link rel="next" href="https://liudongdong1.github.io/object-tracking/" /><link rel="stylesheet" href="/css/style.min.css"><link rel="stylesheet" href="/lib/fontawesome-free/all.min.css"><link rel="stylesheet" href="/lib/animate/animate.min.css"><script type="application/ld+json">
  {
    "@context": "http://schema.org",
    "@type": "BlogPosting",
    "headline": "Video_Undertand",
    "inLanguage": "zh-CN",
    "mainEntityOfPage": {
      "@type": "WebPage",
      "@id": "https:\/\/liudongdong1.github.io\/video_understand\/"
    },"genre": "posts","keywords": "CV","wordcount":  13137 ,
    "url": "https:\/\/liudongdong1.github.io\/video_understand\/","datePublished": "2020-06-06T21:45:45+00:00","dateModified": "2023-09-28T23:52:41+08:00","license": "This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.","publisher": {
      "@type": "Organization",
      "name": "LiuDongdong","logo": "https:\/\/liudongdong1.github.io\/images\/person.png"},"author": {
        "@type": "Person",
        "name": "liudongdong1"
      },"description": ""
  }
  </script></head>
  <body data-header-desktop="auto" data-header-mobile="auto"><script>(window.localStorage?.getItem('theme') ? localStorage.getItem('theme') === 'dark' : ('auto' === 'auto' ? window.matchMedia('(prefers-color-scheme: dark)').matches : 'auto' === 'dark')) && document.body.setAttribute('data-theme', 'dark');</script><div class="wrapper"><header class="desktop animate__faster" id="header-desktop">
  <div class="header-wrapper" data-github-corner="right">
    <div class="header-title">
      <a href="/" title="DAY By DAY"><img
    class="lazyload logo"
    src="/svg/loading.min.svg"
    data-src="/fixit.min.svg"
    data-srcset="/fixit.min.svg, /fixit.min.svg 1.5x, /fixit.min.svg 2x"
    data-sizes="auto"
    alt="DAY By DAY"
    title="DAY By DAY"/><span class="header-title-text"></span></a><span id="typeit-header-subtitle-desktop" class="typeit header-subtitle"></span></div>
    <nav>
      <ul class="menu"><li class="menu-item">
              <a
                class="menu-link"
                href="/posts/"
                
                
              ><i class="fa-solid fa-archive fa-fw fa-sm" aria-hidden="true"></i> 所有文章</a></li><li class="menu-item">
              <a
                class="menu-link"
                href="/categories/"
                
                
              ><i class="fa-solid fa-th fa-fw fa-sm" aria-hidden="true"></i> 分类</a></li><li class="menu-item">
              <a
                class="menu-link"
                href="/tags/"
                
                
              ><i class="fa-solid fa-tags fa-fw fa-sm" aria-hidden="true"></i> 标签</a></li><li class="menu-item">
              <a
                class="menu-link"
                href="/friends/"
                title="友情链接"
                
              ><i class="fa-solid fa-users fa-fw fa-sm" aria-hidden="true"></i> 友链</a></li><li class="menu-item">
              <a
                class="menu-link"
                href="/about/"
                
                
              ><i class="fa-solid fa-info-circle fa-fw fa-sm" aria-hidden="true"></i> 关于</a></li><li class="menu-item delimiter"></li><li class="menu-item language">
            <span role="button" aria-label="选择语言" title="选择语言">简体中文<i class="dropdown-icon fa-solid fa-chevron-down" aria-hidden="true"></i>
            </span>
            <ul class="sub-menu"><li class="menu-item">没有更多翻译</li></ul>
          </li><li class="menu-item search" id="search-desktop">
            <input type="text" placeholder="搜索文章标题或内容 ..." id="search-input-desktop">
            <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-desktop" title="搜索">
              <i class="fa-solid fa-search fa-fw" aria-hidden="true"></i>
            </a>
            <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-desktop" title="清空">
              <i class="fa-solid fa-times-circle fa-fw" aria-hidden="true"></i>
            </a>
            <span class="search-button search-loading" id="search-loading-desktop">
              <i class="fa-solid fa-spinner fa-fw fa-spin" aria-hidden="true"></i>
            </span>
          </li><li class="menu-item theme-switch" title="切换主题">
          <i class="fa-solid fa-adjust fa-fw" aria-hidden="true"></i>
        </li>
      </ul>
    </nav>
  </div>
</header><header class="mobile animate__faster" id="header-mobile">
  <div class="header-container">
    <div class="header-wrapper">
      <div class="header-title">
        <a href="/" title="DAY By DAY"><img
    class="lazyload logo"
    src="/svg/loading.min.svg"
    data-src="/fixit.min.svg"
    data-srcset="/fixit.min.svg, /fixit.min.svg 1.5x, /fixit.min.svg 2x"
    data-sizes="auto"
    alt="/fixit.min.svg"
    title="/fixit.min.svg"/><span class="header-title-text"></span></a><span id="typeit-header-subtitle-mobile" class="typeit header-subtitle"></span></div>
      <div class="menu-toggle" id="menu-toggle-mobile">
        <span></span><span></span><span></span>
      </div>
    </div>
    <nav>
      <ul class="menu" id="menu-mobile"><li class="search-wrapper">
            <div class="search mobile" id="search-mobile">
              <input type="text" placeholder="搜索文章标题或内容 ..." id="search-input-mobile">
              <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-mobile" title="搜索">
                <i class="fa-solid fa-search fa-fw" aria-hidden="true"></i>
              </a>
              <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-mobile" title="清空">
                <i class="fa-solid fa-times-circle fa-fw" aria-hidden="true"></i>
              </a>
              <span class="search-button search-loading" id="search-loading-mobile">
                <i class="fa-solid fa-spinner fa-fw fa-spin" aria-hidden="true"></i>
              </span>
            </div>
            <a href="javascript:void(0);" class="search-cancel" id="search-cancel-mobile">
              取消
            </a>
          </li><li
              class="menu-item"
            ><a
                  class="menu-link"
                  href="/posts/"
                  
                  
                ><i class="fa-solid fa-archive fa-fw fa-sm" aria-hidden="true"></i> 所有文章</a></li><li
              class="menu-item"
            ><a
                  class="menu-link"
                  href="/categories/"
                  
                  
                ><i class="fa-solid fa-th fa-fw fa-sm" aria-hidden="true"></i> 分类</a></li><li
              class="menu-item"
            ><a
                  class="menu-link"
                  href="/tags/"
                  
                  
                ><i class="fa-solid fa-tags fa-fw fa-sm" aria-hidden="true"></i> 标签</a></li><li
              class="menu-item"
            ><a
                  class="menu-link"
                  href="/friends/"
                  title="友情链接"
                  
                ><i class="fa-solid fa-users fa-fw fa-sm" aria-hidden="true"></i> 友链</a></li><li
              class="menu-item"
            ><a
                  class="menu-link"
                  href="/about/"
                  
                  
                ><i class="fa-solid fa-info-circle fa-fw fa-sm" aria-hidden="true"></i> 关于</a></li><li
              class="menu-item text-center"
            ><a
                  class="menu-link"
                  href="/"
                  title="GitHub"
                  
                ><i class='fa-brands fa-github fa-fw' aria-hidden='true'></i> </a></li><li class="menu-item theme-switch" title="切换主题">
          <i class="fa-solid fa-adjust fa-fw" aria-hidden="true"></i>
        </li><li class="menu-item language">
            <span role="button" aria-label="选择语言" title="选择语言">简体中文<i class="dropdown-icon fa-solid fa-chevron-down" aria-hidden="true"></i>
            </span>
            <select class="language-select" onchange="location = this.value;"><option disabled>没有更多翻译</option></select>
          </li></ul>
    </nav>
  </div>
</header><div class="search-dropdown desktop">
    <div id="search-dropdown-desktop"></div>
  </div>
  <div class="search-dropdown mobile">
    <div id="search-dropdown-mobile"></div>
  </div><main class="container" data-page-style="normal"><aside class="toc" id="toc-auto"><h2 class="toc-title">目录 <i class="toc-icon fa-solid fa-angle-down fa-fw"></i></h2>
      <div class="toc-content" id="toc-content-auto"></div></aside>

  <aside class="aside-custom" id="aside-sakana">
    

<div class="sakana-widget">
  <div class="sakana-item" id="takina-widget"></div>
  <div class="sakana-item" id="chisato-widget"></div>
</div>
<script>
  function initSakanaWidget() {
    const takina = SakanaWidget.getCharacter('takina')
    SakanaWidget.registerCharacter('takina-slow', takina);
    new SakanaWidget({
      character: 'takina-slow',
      controls: false,
      autoFit: true,
      stroke: {
        color: "#b4b4b4",
        width: 2
      }
    }).mount('#takina-widget');

    const chisato = SakanaWidget.getCharacter('chisato')
    SakanaWidget.registerCharacter('chisato-slow', chisato);
    new SakanaWidget({
      character: 'chisato-slow',
      controls: false,
      autoFit: true,
      stroke: {
        color: "#b4b4b4",
        width: 2
      }
    }).mount('#chisato-widget');
  }
</script>
<script async onload="initSakanaWidget()" src="https://cdn.jsdelivr.net/npm/sakana-widget@2.3.0/lib/sakana.min.js">
</script></aside>

  <article class="page single">
    <div class="header"><h1 class="single-title animate__animated animate__flipInX">
        <span>Video_Undertand</span>
      </h1></div><div class="post-meta">
      <div class="post-meta-line"><span class="post-author"><span class="author"><i class="fa-solid fa-user-circle" aria-hidden="true"></i>
      liudongdong1</span></span>
          <span class="post-category">收录于 <a href="/categories/"><i class="fa-regular fa-folder fa-fw"></i>&nbsp;Categories</a>&ensp;<a href="/categories/ai/"><i class="fa-regular fa-folder fa-fw"></i>&nbsp;AI</a></span></div>
      <div class="post-meta-line"><span title=2020-06-06&#32;21:45:45>
            <i class="fa-regular fa-calendar-alt fa-fw"></i>&nbsp;<time datetime="2020-06-06" >2020-06-06</time>
          </span>&nbsp;<i class="fa-solid fa-pencil-alt fa-fw"></i>&nbsp;约 13137 字&nbsp;
        <i class="fa-regular fa-clock fa-fw"></i>&nbsp;预计阅读 27 分钟&nbsp;<span id="busuanzi_container_page_pv" class="busuanzi_visitors comment-visitors" data-flag-title="Video_Undertand">
            <i class="fa-regular fa-eye fa-fw"></i>&nbsp;<span id="busuanzi_value_page_pv">-</span>&nbsp;次阅读
          </span>&nbsp;</div>
    </div><div class="featured-image"><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/voice-recognition-speech-detect-deep-260nw-694633963.webp"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/voice-recognition-speech-detect-deep-260nw-694633963.webp, https://gitee.com/github-25970295/blogImage/raw/master/img/voice-recognition-speech-detect-deep-260nw-694633963.webp 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/voice-recognition-speech-detect-deep-260nw-694633963.webp 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/voice-recognition-speech-detect-deep-260nw-694633963.webp"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/voice-recognition-speech-detect-deep-260nw-694633963.webp"/></div><div class="details toc" id="toc-static" kept="true">
        <div class="details-summary toc-title">
          <span>目录</span>
          <span><i class="details-icon fa-solid fa-angle-right"></i></span>
        </div>
        <div class="details-content toc-content" id="toc-content-static"><nav id="TableOfContents">
  <ul>
    <li>
      <ul>
        <li><a href="#工业界">工业界：</a></li>
      </ul>
    </li>
    <li><a href="#paper-iterative-alignment-network">Paper: Iterative Alignment Network</a>
      <ul>
        <li></li>
      </ul>
    </li>
    <li><a href="#paper-deepasl">Paper: DeepASL</a>
      <ul>
        <li></li>
      </ul>
    </li>
  </ul>

  <ul>
    <li><a href="#paper-anomaly-detection">Paper: Anomaly Detection</a>
      <ul>
        <li></li>
      </ul>
    </li>
  </ul>

  <ul>
    <li>
      <ul>
        <li></li>
      </ul>
    </li>
    <li><a href="#paper-action-knowledge-transfer">Paper: Action Knowledge Transfer</a>
      <ul>
        <li></li>
      </ul>
    </li>
    <li><a href="#paper-hierarchical-representation">Paper: Hierarchical Representation</a>
      <ul>
        <li></li>
      </ul>
    </li>
    <li><a href="#paper-personlab">Paper: PersonLab</a>
      <ul>
        <li></li>
      </ul>
    </li>
  </ul>

  <ul>
    <li><a href="#paper-social-gan">Paper: Social GAN</a>
      <ul>
        <li></li>
      </ul>
    </li>
    <li><a href="#paper-ts-lstm-and-temporal-inception">Paper: TS-LSTM and Temporal-Inception</a>
      <ul>
        <li></li>
      </ul>
    </li>
    <li><a href="#paper-futruepose">Paper: FutruePose</a>
      <ul>
        <li></li>
      </ul>
    </li>
    <li><a href="#paper-computational-foresight">Paper: Computational Foresight</a>
      <ul>
        <li></li>
      </ul>
    </li>
    <li><a href="#paper-peeking-into-the-future">Paper: Peeking into the future</a>
      <ul>
        <li></li>
      </ul>
    </li>
    <li><a href="#paper-social-lstm">Paper: Social LSTM</a>
      <ul>
        <li></li>
      </ul>
    </li>
    <li><a href="#paper-multi-task-pedestrian">Paper: Multi-Task Pedestrian</a>
      <ul>
        <li></li>
      </ul>
    </li>
  </ul>

  <ul>
    <li><a href="#paper-edusense">Paper: EduSense</a>
      <ul>
        <li></li>
      </ul>
    </li>
  </ul>

  <ul>
    <li><a href="#51-40个骨骼提取开源项目httpsmpweixinqqcoms__bizmzi5mduymdixnamid2247507854idx2sne02294c31e6867bf2e4270178c2a75e8chksmec1c3277db6bbb612de6d5b3edaa2a4fb8da19e7b6e62b10feb6c21d8fddc748e570ef05c050scene126sessionid1600264884key3542bed875d644de951ff14ae71a83001ab1e1812ce7aa66a998b68fd92197d07eb6ed465593d3aac71554ab7ccfb47f11533fe51eec433dba65046ba6244a25e8050051445366bb86635b10cd4bcf9f1c9220c9515042c7795e056f147b995b4688cde692c65c611ca97ef9d78c191310ee8ebb1c30e3cd4a12b77aa5d24fe7ascene1uinmze0odmxotqzmq3d3ddevicetypewindows10x64version62090529langzh_cnexportkeyax1vpaqmptdpfiw823ezgry3dpass_tickettfc86xzy4b6esrk2fasnypqs4p0qrnxfr4rzkdh4co2fpl3pb2ehbommndjmdtvipdwx_header0">5.1. <a href="https://mp.weixin.qq.com/s?__biz=MzI5MDUyMDIxNA==&amp;mid=2247507854&amp;idx=2&amp;sn=e02294c31e6867bf2e4270178c2a75e8&amp;chksm=ec1c3277db6bbb612de6d5b3edaa2a4fb8da19e7b6e62b10feb6c21d8fddc748e570ef05c050&amp;scene=126&amp;sessionid=1600264884&amp;key=3542bed875d644de951ff14ae71a83001ab1e1812ce7aa66a998b68fd92197d07eb6ed465593d3aac71554ab7ccfb47f11533fe51eec433dba65046ba6244a25e8050051445366bb86635b10cd4bcf9f1c9220c9515042c7795e056f147b995b4688cde692c65c611ca97ef9d78c191310ee8ebb1c30e3cd4a12b77aa5d24fe7&amp;ascene=1&amp;uin=MzE0ODMxOTQzMQ%3D%3D&amp;devicetype=Windows+10+x64&amp;version=62090529&amp;lang=zh_CN&amp;exportkey=Ax1vPAqMPtdpFIw823EzgRY%3D&amp;pass_ticket=TfC86Xzy4b6ESRk%2FasnYpQs4p0qrNXFR4RzKdh4co%2FPl3pb2EHboMmNDJmdTviPd&amp;wx_header=0">40个骨骼提取开源项目</a></a></li>
    <li><a href="#paper-openpose">Paper: OpenPose</a>
      <ul>
        <li></li>
      </ul>
    </li>
    <li><a href="#paper-co-occurence-feature">Paper: Co-occurence Feature</a>
      <ul>
        <li></li>
      </ul>
    </li>
  </ul>

  <ul>
    <li>
      <ul>
        <li></li>
      </ul>
    </li>
    <li><a href="#paper-cpm">Paper: CPM</a>
      <ul>
        <li></li>
      </ul>
    </li>
    <li><a href="#paper-frustum-pointnets">Paper: Frustum PointNets</a>
      <ul>
        <li></li>
      </ul>
    </li>
    <li><a href="#paper-3d-resnet">Paper: 3D-Resnet</a>
      <ul>
        <li></li>
      </ul>
    </li>
    <li><a href="#paper-vnect">Paper: VNect</a>
      <ul>
        <li></li>
      </ul>
    </li>
  </ul>
</nav></div>
      </div><div
      class="content"
      id="content"
      
      
    ><blockquote>
<p>安防监控领域，包括人脸识别、行为识别、运动跟踪、人群分析等等，利用卡口精准位置布控视频监测，实现了监控区域内异常的自动识别，例如动态视频中的人脸与黑名单库实时比对检测，多视点视频协同分析运行轨迹，视频数据结构化后对关键目标的检索等等；</p>
<p>互联网娱乐场景，包括拍照优化、视频优化、实时人像美颜、AR特效、自定义背景等等，丰富了直播、短视频等互联网娱乐应用；</p>
<p>金融身份认证场景，包括各种刷脸的金融应用，如远程开户、支付取款等等；</p>
<p>无人商场与广告营销，包括线下零售、商品识别、广告AR赋能等等；</p>
<p>工业机器的视觉系统，包括物品分拣、缺陷检验等等，通常是自动图像分析与光学成像等其他方法技术相结合；</p>
<p>无人机无人车控制，包括视觉导航、行人分析、障碍物检测等等，通常作为一种传感器和激光雷达、毫米波雷达、红外探头与惯性测量单元融合生成供自主决策的信息；</p>
</blockquote>
<h1 id="0-视频理解方向">0. 视频理解方向</h1>
<blockquote>
<ul>
<li>Task1：未修剪视频分类(Untrimmed Video Classification)。这个有点类似于图像的分类，未修剪的视频中通常含有多个动作，而且视频很长。有许多动作或许都不是我们所关注的。所以这里提出的Task就是希望通过对输入的长视频进行全局分析，然后软分类到多个类别。</li>
<li>Task2：修剪视频识别(Trimmed Action Recognition)。这个在计算机视觉领域已经研究多年，给出一段只包含一个动作的修剪视频，要求给视频分类。</li>
<li>Task3：时序行为提名(Temporal Action Proposal)。这个同样类似于图像目标检测任务中的候选框提取。在一段长视频中通常含有很多动作，这个任务就是从视频中找出可能含有动作的视频段。</li>
<li>Task4：时序行为定位(Temporal Action Localization)。相比于上面的时序行为提名而言，时序行为定位于我们常说的目标检测一致。要求从视频中找到可能存在行为的视频段，并且给视频段分类。</li>
<li>Task5：密集行为描述(Dense-Captioning Events)。之所以称为密集行为描述，主要是因为该任务要求在时序行为定位(检测)的基础上进行视频行为描述。也就是说，该任务需要将一段未修剪的视频进行时序行为定位得到许多包含行为的视频段后，对该视频段进行行为描述。比如：man playing a piano</li>
</ul>
</blockquote>
<h1 id="1-手语论文">1. 手语论文</h1>
<blockquote>
<h3 id="工业界">工业界：</h3>
<p>腾讯优图实验室AI手语识别 <a href="https://www.jiqizhixin.com/articles/2019-05-16-16"target="_blank" rel="external nofollow noopener noreferrer">https://www.jiqizhixin.com/articles/2019-05-16-16<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></p>
<p>中科大和微软推出了基于Kinect的手语翻译系统，加州大学曾经推出过的手语识别手套</p>
<h5 id="潜在需求分析"><strong>潜在需求分析</strong>：</h5>
<p>​	1. <strong>听障人士数量数量多</strong> 世界卫生组织最新数据显示[1]，目前全球约有4.66亿人患有残疾性听力损失，超过全世界人口的5%，估计到2050年将有9亿多人（约十分之一）出现残疾性听力损失。据北京听力协会2017年公开数据，估计中国残疾性听力障碍人士已达7200万[2]，</p>
<ol start="2">
<li>
<p><strong>无障碍普及率有待提升，听障人群需求被忽视</strong></p>
</li>
<li>
<p>提供一套兼容全球手语的双向翻译器/或是简单的识别器</p>
</li>
</ol>
<ul>
<li>立即可以为上千万聋哑人获得更多的电脑控制权</li>
<li>结合 IFTTT 以及 Home 类似智能家庭控制器</li>
<li>完全可以形成一个嵌入专用硬件的产业了</li>
</ul>
<h5 id="问题">问题</h5>
<p>​	1.  自动区分手语表达中的各类手势、动作以及这些手势和动作之间的切换，最后将表达的手语翻译成文字。传统的方法通常会针对特定的数据集设计合理的特征，再利用这些特征进行动作和手势的分类。受限于人工的特征设计和数据量大小，这些方法在适应性、泛化性和鲁棒性上都非常有限。</p>
<p>使用Kinect摄像机的多种传感器来提前获取手语表达者的肢体关节点信息： 传感器手套、或配备EMG、IMU传感器的手环来获取手臂和手掌的活动信息</p>
</blockquote>
<p><strong>level</strong>: CVPR  CCF_A
<strong>author</strong>:Junfu Pu    CAS Key Laboratory of GIPAS, University of Science and Technology of China
<strong>date</strong>: 2019
<strong>keyword</strong>:</p>
<ul>
<li>ASL , CTC</li>
</ul>
<hr>
<h2 id="paper-iterative-alignment-network">Paper: Iterative Alignment Network</h2>
<!-- raw HTML omitted -->
<h4 id="summary">Summary</h4>
<ol>
<li></li>
</ol>
<h4 id="research-objective">Research Objective</h4>
<ul>
<li><strong>Application Area</strong>:
<ul>
<li>sign language (SL) is used by millions of people with hearing or spoken damage in their daily life</li>
<li>lack of systematic study for sign language, it becomes very difficult for many people to communicate with the deaf-mute</li>
</ul>
</li>
<li><strong>Purpose</strong>:  propose an alignment network with iterative optimization for weakly supervised continuous signlanguage recognition</li>
</ul>
<h4 id="proble-statement">Proble Statement</h4>
<p>previous work:</p>
<ul>
<li>isolated SLR  recognition [16, 22, 42, 43]</li>
<li>video representation： 3D-CNN  ResNet  P3D</li>
<li>sequence modeling:
<ul>
<li>attention-based encoder-decoder network
<ul>
<li>Bahdanau et al. [1] introduce attention mechanism into encoder-decoder network to learn the correspondence between source sequence and target sequence</li>
</ul>
</li>
<li>connectionist temporal classification(CTC) based network
<ul>
<li>CTC is able to deal withunsegmented input data, and learn the correspondence between the input sequence and output sequence.</li>
</ul>
</li>
</ul>
</li>
<li>continuous SLR
<ul>
<li>hand-crafted feature based
<ul>
<li>Hidden Markov Model (HMM) or Hidden Conditional Random Fields (HCRF)</li>
<li>[35] two real-time HMM-based systems for recognizing
sentence-level continuous American Sign Language (ASL).</li>
<li>[40]a discriminative sequence model with Hidden Conditional Random Field (HCRF) for gesture recognition</li>
</ul>
</li>
<li>deep learning based  [9, 23, 25] datasets 了解一下
<ul>
<li>video represntations by redidual network ResNet[18], 3D-CNN [33, 37]</li>
<li>[23] with hierarchical attention in latent space</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="methods">Methods</h4>
<ul>
<li><strong>Problem Formulation</strong>:</li>
</ul>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191223092845392.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191223092845392.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191223092845392.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191223092845392.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191223092845392.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191223092845392.png"/></p>
<ul>
<li><strong>system overview</strong>:</li>
</ul>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191223092906125.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191223092906125.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191223092906125.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191223092906125.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191223092906125.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191223092906125.png"/></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191223093357368.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191223093357368.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191223093357368.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191223093357368.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191223093357368.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191223093357368.png"/></p>
<p><strong>CTC_Loss</strong>:</p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191223093649877.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191223093649877.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191223093649877.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191223093649877.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191223093649877.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191223093649877.png"/></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191223093518165.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191223093518165.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191223093518165.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191223093518165.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191223093518165.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191223093518165.png"/></p>
<p><strong>LSTM_Loss</strong>:</p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191223093740963.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191223093740963.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191223093740963.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191223093740963.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191223093740963.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191223093740963.png"/></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="../../../../MEGA/MEGAsync/actionPrediction/ActionPrediction.assets/image-20191223093756890.png"
    data-srcset="../../../../MEGA/MEGAsync/actionPrediction/ActionPrediction.assets/image-20191223093756890.png, ../../../../MEGA/MEGAsync/actionPrediction/ActionPrediction.assets/image-20191223093756890.png 1.5x, ../../../../MEGA/MEGAsync/actionPrediction/ActionPrediction.assets/image-20191223093756890.png 2x"
    data-sizes="auto"
    alt="image-20191223093756890"
    title="image-20191223093756890"/></p>
<p><strong>The Whole NetworkLoss</strong>:</p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191223093851361.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191223093851361.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191223093851361.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191223093851361.png 2x"
    data-sizes="auto"
    alt="image-20191223093851361"
    title="image-20191223093851361"/></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="../../../../MEGA/MEGAsync/actionPrediction/ActionPrediction.assets/image-20191223093859610.png"
    data-srcset="../../../../MEGA/MEGAsync/actionPrediction/ActionPrediction.assets/image-20191223093859610.png, ../../../../MEGA/MEGAsync/actionPrediction/ActionPrediction.assets/image-20191223093859610.png 1.5x, ../../../../MEGA/MEGAsync/actionPrediction/ActionPrediction.assets/image-20191223093859610.png 2x"
    data-sizes="auto"
    alt="image-20191223093859610"
    title="image-20191223093859610"/></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191223093025875.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191223093025875.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191223093025875.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191223093025875.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191223093025875.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191223093025875.png"/></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191223093947114.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191223093947114.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191223093947114.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191223093947114.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191223093947114.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191223093947114.png"/></p>
<h4 id="evaluation">Evaluation</h4>
<ul>
<li><strong>Environment</strong>:
<ul>
<li>Dataset:
<ul>
<li>RWTH-PHOENIX-Weather multi-signer [25] for German SLR</li>
<li>CSL [23] for Chinese SLR</li>
</ul>
</li>
</ul>
</li>
<li><strong>Evaluate Methods</strong>: <img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191223094117184.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191223094117184.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191223094117184.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191223094117184.png 2x"
    data-sizes="auto"
    alt="image-20191223094117184"
    title="image-20191223094117184"/></li>
<li>The window size is set to be 8 with a stride of 4,the 3D-ResNet is pre-trained on an isolated sign language recognition dataset released in [43]</li>
<li><strong>Performance</strong>:</li>
</ul>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191223094423096.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191223094423096.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191223094423096.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191223094423096.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191223094423096.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191223094423096.png"/></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191223094431353.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191223094431353.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191223094431353.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191223094431353.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191223094431353.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191223094431353.png"/></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191223094443353.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191223094443353.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191223094443353.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191223094443353.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191223094443353.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191223094443353.png"/></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191223094507331.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191223094507331.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191223094507331.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191223094507331.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191223094507331.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191223094507331.png"/></p>
<h4 id="conclusion">Conclusion</h4>
<ul>
<li>A unified deep learning architecture integrating encoderdecoder network and connectionist temporal classification (CTC) for continuous sign language recognition.</li>
<li>A soft dynamic time warping (soft-DTW) alignment constraint between the LSTM and CTC decoders, which indicates the temporal segmentation in sign videos</li>
<li>Iterative optimization strategy to train feature extractor and encoder-decoder network alternately with alignment proposals by warping path</li>
</ul>
<h4 id="notes-font-colororange去加强了解font">Notes <!-- raw HTML omitted -->去加强了解<!-- raw HTML omitted --></h4>
<ul>
<li><i class="fa-regular fa-square fa-fw" aria-hidden="true"></i> 论文23: Video-based sign language recognition without
temporal segmentation   China</li>
<li><i class="fa-regular fa-square fa-fw" aria-hidden="true"></i> paper25 数据集 German Continuous
sign language recognition: Towards large vocabulary statistical recognition systems handling multiple signers</li>
<li><i class="fa-regular fa-square fa-fw" aria-hidden="true"></i> SubUNets: End-to-end hand shape and continuous sign language recognition</li>
<li><i class="fa-regular fa-square fa-fw" aria-hidden="true"></i> Online early-late fusion based on adaptive HMM for sign language recognition</li>
<li><i class="fa-regular fa-square fa-fw" aria-hidden="true"></i> Can spatiotemporal 3D CNNs retrace the history of 2D CNNs and imagenet</li>
<li><i class="fa-regular fa-square fa-fw" aria-hidden="true"></i> Attention based 3D-CNNs for large-vocabulary sign language recognition</li>
<li><i class="fa-regular fa-square fa-fw" aria-hidden="true"></i> Video-based sign language recognition without temporal segmentation</li>
<li><i class="fa-regular fa-square fa-fw" aria-hidden="true"></i> Dilated convolutional network with iterative optimization for continuous
sign language recognition</li>
<li><i class="fa-regular fa-square fa-fw" aria-hidden="true"></i> Continuous sign language recognition: Towards large vocabulary statistical recognition systems handling multiple signers</li>
<li><i class="fa-regular fa-square fa-fw" aria-hidden="true"></i> Online early-late fusion based on adaptive HMM for sign
language recognition</li>
<li><i class="fa-regular fa-square fa-fw" aria-hidden="true"></i> Joint CTC/attention decoding for end-to-end speech recognition</li>
<li><i class="fa-regular fa-square fa-fw" aria-hidden="true"></i> Attention based 3D-CNNs for large-vocabulary sign language recognition</li>
<li><i class="fa-regular fa-square fa-fw" aria-hidden="true"></i> Video-based sign language recognition without
temporal segmentation</li>
<li><i class="fa-regular fa-square fa-fw" aria-hidden="true"></i> Deep sign: hybrid CNN-HMM for continuous sign language recognition</li>
<li><i class="fa-regular fa-square fa-fw" aria-hidden="true"></i> Re-sign: Re-aligned end-to-end sequence modelling with deep recurrent CNN-HMMs.</li>
<li><i class="fa-regular fa-square fa-fw" aria-hidden="true"></i> Online detection and classification of dynamic hand gestures with recurrent 3D
convolutional neural networks</li>
<li><i class="fa-regular fa-square fa-fw" aria-hidden="true"></i> Dilated convolutional network with iterative optimization for continuous sign language recognition</li>
</ul>
<p><strong>level</strong>: Sensys     CCF_B
<strong>author</strong>:Biyi Fang  Michigan State University
<strong>date</strong>: 2017
<strong>keyword</strong>:</p>
<ul>
<li>ASL, Leep Motion(an infrared light-based sensing device)</li>
</ul>
<hr>
<h2 id="paper-deepasl">Paper: DeepASL</h2>
<!-- raw HTML omitted -->
<ol>
<li>performance at both word level and sentence level (unseen ASL sentences ,unseen users)</li>
<li>robustness under various real-world settings (various ambient lighting conditions, body postures,and interference sources )</li>
<li>system performance test in terms of runtime , memory usage and energy consumption.</li>
</ol>
<h4 id="research-objective-1">Research Objective</h4>
<ul>
<li><strong>Application Area</strong>:seeking help from a sign language interpreter, writing on paper, or typing on a mobile phone,each of these methods has its own key limitations in terms of cost,
availability, or convenience</li>
<li><strong>Purpose</strong>:</li>
</ul>
<h4 id="proble-statement-1">Proble Statement</h4>
<ul>
<li>ASL : hand shape, hand movement, relative location of two hands, body movement, face emotions</li>
<li>Electromyography (EMG) sensors, RGB cameras, Kinect sensors intrusive where
sensors have to be attached to !ngers and palms of users, lack of resolutions to capture the key characteristics of signs, or significantly constrained by ambient lighting conditions or backgrounds
in real-world settings</li>
<li>existing sign language translation systems can only translate a single sign at a time, thus
requiring users to pause between adjacent signs.</li>
</ul>
<p>previous work:</p>
<ul>
<li>wearable sensor-based :motion sensors(accelerometers, gyroscopes), EMG sensors, bending of fingers to infer the performed fingers. <!-- raw HTML omitted -->intrusive and impractical for daily usage<!-- raw HTML omitted --></li>
<li>Radio Frequency-based: <!-- raw HTML omitted -->wire-less signals have very limited resolutions to see the hands<!-- raw HTML omitted --></li>
<li>RGB camera-based: <!-- raw HTML omitted --> poor lighting conditions or generally uncontrolled backgrounds, privacy <!-- raw HTML omitted --></li>
<li>Kinect-based: hard to capture the hand shape information</li>
</ul>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191224090129594.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191224090129594.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191224090129594.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191224090129594.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191224090129594.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191224090129594.png"/></p>
<ul>
<li>Leap Motion is able to extract skeleton joints of the fingers, palms and forearms from the raw infrared images.<img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191224090111350.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191224090111350.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191224090111350.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191224090111350.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191224090111350.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191224090111350.png"/></li>
</ul>
<h4 id="methods-1">Methods</h4>
<ul>
<li><strong>system overview</strong>:</li>
</ul>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191224084552045.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191224084552045.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191224084552045.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191224084552045.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191224084552045.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191224084552045.png"/></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191224090211496.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191224090211496.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191224090211496.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191224090211496.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191224090211496.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191224090211496.png"/></p>
<ol>
<li>a temporal sequence of 3D coordinates of the skeleton joints of !ngers, palms and forearms</li>
<li>the key characteristics of ASL signs including hand shape, hand movement and relative location of two hands    spatio-temporal trajectories of ASL characteristics</li>
<li>models the spatial structure and temporal dynamics of the spatio-temporal trajectories of ASL characteristics for word-level ASL translation</li>
<li>CTC-based framework that leverages the captured probabilistic dependencies between words in one complete sentence and translates the whole sentence end-to-end without requiring users to pause between adjacent signs.</li>
</ol>
<p><strong>【ASL Characteristics Extraction】</strong></p>
<ul>
<li>Savitzky-Golay flter [37] to improve the signal to noise ratio of the raw skeleton joints data</li>
</ul>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191224090953165.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191224090953165.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191224090953165.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191224090953165.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191224090953165.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191224090953165.png"/></p>
<ul>
<li>extract hand shape: <img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191224092151069.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191224092151069.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191224092151069.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191224092151069.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191224092151069.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191224092151069.png"/></li>
<li>hand movement information:<img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191224092217948.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191224092217948.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191224092217948.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191224092217948.png 2x"
    data-sizes="auto"
    alt="image-20191224092217948"
    title="image-20191224092217948"/></li>
</ul>
<p><strong>【Word-Level ASL Translation】</strong>： translation errors when different signs share very similar characteristics at the beginning of the signs</p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191224092513888.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191224092513888.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191224092513888.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191224092513888.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191224092513888.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191224092513888.png"/></p>
<ul>
<li>Hierarchical Bidirectional RNN for Single-Sign Modeling:<img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191224092932396.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191224092932396.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191224092932396.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191224092932396.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191224092932396.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191224092932396.png"/></li>
</ul>
<p><strong>【Sentence level Translation】</strong> using CTC network</p>
<h1 id="2-视频理解">2. 视频理解</h1>
<p><strong>level</strong>:  CVPR_CCFA
<strong>author</strong>:Romero Morais
<strong>date</strong>:
<strong>keyword</strong>:</p>
<ul>
<li>video analyse, anomaly detection</li>
</ul>
<hr>
<h2 id="paper-anomaly-detection">Paper: Anomaly Detection</h2>
<!-- raw HTML omitted -->
<h4 id="summary-1">Summary</h4>
<ol>
<li>model the normal patterns of human movement in surveillance video for <code>anomaly detection using dynamic skeleton features.</code></li>
<li>decompose the skeletal movements into two sub-components: global body movement and local body posture. The global body movement tracks the dynamics of the whole body in the scene, while the body posture describe the skeleton configuration in the canonical coordinate frame of the body&rsquo;s bounding box.</li>
<li>model the dynamic and interaction of the coupled features in our novel Message-Passing Encoder-Decoder Recurrent Network.</li>
<li>skeleton features are compact, strongly structured, semantically rich, and highly descriptive about human aciton and movement, which are keys to anomaly detection.</li>
</ol>
<h4 id="proble-statement-2">Proble Statement</h4>
<ul>
<li>The human behavioral irregularity can be factorized into few factors regarding body motion and posture: location, velocity, direction, pose, and action.</li>
</ul>
<h4 id="methods-2">Methods</h4>
<p>【Qustion 1】 the scales of human skelons vary largely depending on their location and actions</p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200422104952789.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200422104952789.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200422104952789.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200422104952789.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200422104952789.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200422104952789.png"/>
$$
f_t^i=f_t^g+f_t^{l,i};f^g=(x^g,y^g,w,h),f^{l,i}=(x^{l,i},y^{l,i})
$$
<img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200422105234027.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200422105234027.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200422105234027.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200422105234027.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200422105234027.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200422105234027.png"/></p>
<p>【qustion 2】how to fuse local and global features</p>
<ul>
<li>propose MPED-RNN models, consisting two recurrent encoder-decoder network branches, each of them dedicated to one of the components, each branch of them has the single-encoder-dual-decoder architecture with three RNNS: Encoder,Reconstructing Decoder and Predicting Decoder.</li>
<li>use Gated Recurrnet Units in every segment of MPED_RNN for its simplicity and similar performance to LSTM</li>
</ul>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200422105321588.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200422105321588.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200422105321588.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200422105321588.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200422105321588.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200422105321588.png"/></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200422105656331.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200422105656331.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200422105656331.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200422105656331.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200422105656331.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200422105656331.png"/></p>
<p>【qurestion 3】 how to detect video anomalies?</p>
<ol>
<li><strong>Extract segments</strong>: select the overlapping skeleton segments by using sliding window of size T and stride s on the trajectory</li>
<li><strong>Estimate segment losses</strong>: decompose the segment to two sub-component, feed all segment features to the traind MPED-RNN, output the normality loss</li>
<li><strong>Gather skeleton anomaly score</strong>:  the measure the conformity of a sequence to the model given both the past and future context, using voting scheme to gather the losses of related segments into an anomaly score of each skeleton instance:<img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200422111136464.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200422111136464.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200422111136464.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200422111136464.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200422111136464.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200422111136464.png"/></li>
<li>Calculate frame anomaly score: <img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200422111221056.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200422111221056.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200422111221056.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200422111221056.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200422111221056.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200422111221056.png"/></li>
</ol>
<h4 id="evaluation-1">Evaluation</h4>
<ul>
<li><strong>Environment</strong>:
<ul>
<li>Dataset: ShanghaiTech Campus Dataset for video anomaly detection currently available, combines footage of 13 different cameras .</li>
</ul>
</li>
</ul>
<h4 id="notes-font-colororange去加强了解font-1">Notes <!-- raw HTML omitted -->去加强了解<!-- raw HTML omitted --></h4>
<ul>
<li>video anomaly detection</li>
<li>human trajectory modeling</li>
<li>sequence 一致性</li>
</ul>
<p><strong>level</strong>:
<strong>author</strong>: waqas sultani, UCF
<strong>date</strong>:
<strong>keyword</strong>:</p>
<ul>
<li>anomaly detection, video analyse</li>
</ul>
<hr>
<h1 id="paper-real-world-detection">Paper: Real-world Detection</h1>
<!-- raw HTML omitted -->
<ol>
<li>propose to learn anomaly through the deep multiple instance ranking framework by leveraging weakly labeled training video, the training labels(anomalous or normal) are at video-level instead of clip-level.</li>
<li>introduce a new large-scale dataset of 128 hours of videos with 13 realistic anomalies such as fighting, road accident, burglary robbery.</li>
<li>propose a MIL solution to anomaly detection by leveraging only weakly labeled training videos, propose MIL ranking loss with sparsity and smoothness constraints for a deep learning network to learn anomaly scores for video segments.</li>
</ol>
<h4 id="research-objective-2">Research Objective</h4>
<ul>
<li><strong>Application Area</strong>:  traffic accidents, crimes  or illegal activities.</li>
</ul>
<h4 id="proble-statement-3">Proble Statement</h4>
<ul>
<li><strong>Anomaly detection</strong>:
<ul>
<li>considering all anomalies in one group and all normal activities in another group</li>
<li>recognise specific activities.</li>
<li>impossible to define a normal event which takes all possible normal patterns/behaviors into account.</li>
<li>detect human violence by exploiting motion and limbs orientation of people</li>
<li>employed video and audio data to detect aggressive actions in surveillance videos.</li>
<li>violent flow descriptors to detect violence in crowd videos.</li>
<li>using deep learning based autoencoders to learn the model of normal behaviors and employed reconstruction loss to detect anomalies.</li>
</ul>
</li>
<li><strong>Ranking</strong>: focus on improving relative scores of the items instead of individual scores.
<ul>
<li>deep rankinng networking: used for feature learning, highlight detection, graphics interchange format generation, face detection and verification, person re-identification, place recognition, metric learning and image retrieval.</li>
</ul>
</li>
</ul>
<h4 id="methods-3">Methods</h4>
<ul>
<li><strong>system overview</strong>:</li>
</ul>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200422150129902.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200422150129902.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200422150129902.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200422150129902.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200422150129902.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200422150129902.png"/></p>
<p>【Qustion 1】less annotation learning</p>
<ul>
<li>only video-level labels indicating the presence of an anomaly in the whole video is needed. A video containing anomalies is labeled as positive and a video without any anomaly is labeled as negative.</li>
</ul>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200422150330960.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200422150330960.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200422150330960.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200422150330960.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200422150330960.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200422150330960.png"/></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200422150529485.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200422150529485.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200422150529485.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200422150529485.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200422150529485.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200422150529485.png"/></p>
<p>【qustion 2】 how to detect anomaly activities without much precise annotation?</p>
<ul>
<li>Deep MIL Ranking model:  the scores of instances in the anomalous bag should be sparse,    the anomaly score should vary smoothly between video segments.</li>
<li><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200422150817733.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200422150817733.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200422150817733.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200422150817733.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200422150817733.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200422150817733.png"/></li>
</ul>
<h4 id="notes-font-colororange去加强了解font-2">Notes <!-- raw HTML omitted -->去加强了解<!-- raw HTML omitted --></h4>
<ul>
<li>sparse-coding based approaches.</li>
<li>deep rank</li>
<li>项目代码： <a href="https://github.com/hangxu124/MyRes3D_AnoDect"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/hangxu124/MyRes3D_AnoDect<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>
<ul>
<li><a href="https://github.com/dexXxed/abnormal-event-detection"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/dexXxed/abnormal-event-detection<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></li>
<li><a href="https://github.com/nevinbaiju/anomaly-detection"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/nevinbaiju/anomaly-detection<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></li>
</ul>
</li>
</ul>
<p><strong>level</strong>:  AAAI   CCF_A
<strong>author</strong>: Yijun Cai , Haoxin Li, Jian-Fang Hu , Wei-Shi Zheng
<strong>date</strong>: 2019
<strong>keyword</strong>:</p>
<ul>
<li></li>
</ul>
<hr>
<h2 id="paper-action-knowledge-transfer">Paper: Action Knowledge Transfer</h2>
<!-- raw HTML omitted -->
<h4 id="summary-2">Summary</h4>
<ol>
<li>通过完整的视频动作序列来指导部分视频序列的预测？</li>
</ol>
<h4 id="research-objective-3">Research Objective</h4>
<ul>
<li><strong>Application Area</strong>: in reducing computational resource, traffic system.</li>
<li><strong>Purpose</strong>:  Propose to transfer action knowledge learned from fully observed videos for improving the prediction of partially observed videos</li>
</ul>
<h4 id="proble-statement-4">Proble Statement</h4>
<ul>
<li>action prediction mainly lies in the lack of discriminative action information for the partially observed videos. partially observed videos often contain incomplete action executions thus have less action information than the fully observed ones.</li>
</ul>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191202141323929.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191202141323929.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191202141323929.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191202141323929.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191202141323929.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191202141323929.png"/></p>
<ul>
<li>the existing action recognition systems can be directly used for action prediction by treating partial videos as full videos.</li>
</ul>
<p>previous work:</p>
<ul>
<li>focus on improving the discriminative power of partial videos by developing max margin learning(Kong and Fu 2015) or soft regression  framework(Hu 2016)</li>
<li>Action prediction:
<ul>
<li>Ryoo et al.(Ryoo2012) proposed to use integral and dynamic bag-fo-words for action prediction</li>
<li>Kong and Fu 2015 a max margin learning framework was presented to learn discriminative features for prediction</li>
<li>Vondrick,Pirsiavash and Torralba2016 propose to predict the feature of future frames to learn better representations for action prediction</li>
<li>Lan,Chen developed hierarchical representations at multiple granularities to predict human action</li>
<li><!-- raw HTML omitted -->they dont seek to make use of the action knowledge learned from full sequences for prediction, we propose to mine rich action knowledge from full videos<!-- raw HTML omitted --></li>
</ul>
</li>
<li>Knowledge distillation  :(Hinto ,Vinyals and Dean 2015; Huang and Wang 2017; Yim et al.2017) the knowledge contained in a large network was distilled and transferred to a small network,by enforcing the outputs or intermediate activations of the small network to match those the large network . <!-- raw HTML omitted -->our goal is to improve the discriminative power of partially observed videos <!-- raw HTML omitted --></li>
</ul>
<h4 id="methods-4">Methods</h4>
<ul>
<li><strong>system overview</strong>:</li>
</ul>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191202143008477.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191202143008477.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191202143008477.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191202143008477.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191202143008477.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191202143008477.png"/></p>
<p>[<strong>Question one</strong>] how to Learn Action Knowledge from Full Videos</p>
<p>Given a set of full videos {x i } with corresponding features {f i } and labels {y i }, we intend to learn an embedding function G to project the original feature onto an embed-ding space, and a discriminative classifier D to project the embedding to the label space:
$$
e i = G(f i ),\
p i = D(e i ).
$$</p>
<p>To encourage large distances between embeddings from different classes:</p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191202143440633.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191202143440633.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191202143440633.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191202143440633.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191202143440633.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191202143440633.png"/></p>
<p>[<strong>Qustion Two</strong>] Transferring Action Knowledge to Partial Videos</p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191202143637577.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191202143637577.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191202143637577.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191202143637577.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191202143637577.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191202143637577.png"/></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191202143706194.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191202143706194.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191202143706194.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191202143706194.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191202143706194.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191202143706194.png"/></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191202143144128.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191202143144128.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191202143144128.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191202143144128.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191202143144128.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191202143144128.png"/></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191202143845522.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191202143845522.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191202143845522.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191202143845522.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191202143845522.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191202143845522.png"/></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191202143753309.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191202143753309.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191202143753309.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191202143753309.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191202143753309.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191202143753309.png"/></p>
<h4 id="conclusion-1">Conclusion</h4>
<ul>
<li>propose a novel knowledge transfer framework to boost the performance of action prediction with partial videos ,by transferring knowledge from feature embeddings and discriminative classifier of full videos.</li>
<li>the method shows remarkable improvement for action prediction</li>
</ul>
<h4 id="notes-font-colororange去加强了解font-3">Notes <!-- raw HTML omitted -->去加强了解<!-- raw HTML omitted --></h4>
<ul>
<li><i class="fa-regular fa-square fa-fw" aria-hidden="true"></i> Kong Tao and Fu 2017    Qin et al.2017</li>
<li><i class="fa-regular fa-square fa-fw" aria-hidden="true"></i> paper 18</li>
<li><i class="fa-regular fa-square fa-fw" aria-hidden="true"></i> Additive Margin (AM) Softmax (Wang et al. 2018)</li>
<li><i class="fa-regular fa-square fa-fw" aria-hidden="true"></i> Max-margin action predictionmachine</li>
<li><i class="fa-regular fa-square fa-fw" aria-hidden="true"></i> spartiotemporal multiplier networks for video action recognition</li>
<li><i class="fa-regular fa-square fa-fw" aria-hidden="true"></i> Distilling the knowlege in a neural network   2015 Hinton</li>
<li><i class="fa-regular fa-square fa-fw" aria-hidden="true"></i> early action prediction by soft regression</li>
<li><i class="fa-regular fa-square fa-fw" aria-hidden="true"></i> Like what you like: Knowledge distill via neuron selectivity transfer</li>
<li><i class="fa-regular fa-square fa-fw" aria-hidden="true"></i> Deep sequential context networks for action prediction</li>
<li><i class="fa-regular fa-square fa-fw" aria-hidden="true"></i> learning activity progression in lstm for activity detection and early detection</li>
<li><i class="fa-regular fa-square fa-fw" aria-hidden="true"></i> learning spatialtemporal features with 3d convolutional networks</li>
<li><i class="fa-regular fa-square fa-fw" aria-hidden="true"></i> action recognition with improved trajectories</li>
<li><i class="fa-regular fa-square fa-fw" aria-hidden="true"></i> action recognition by dense trajectories</li>
<li><i class="fa-regular fa-square fa-fw" aria-hidden="true"></i> a gift from knowledge distillation:Fast optimization network minimization and transfer learning</li>
</ul>
<p><strong>level</strong>:  CCF_A
<strong>author</strong>:  Tian Lan , Tsung-Chuan , Silvio Savarese  Standford University
<strong>date</strong>: 2014 ECCV
<strong>keyword</strong>:</p>
<ul>
<li>action prediction</li>
</ul>
<hr>
<h2 id="paper-hierarchical-representation">Paper: Hierarchical Representation</h2>
<!-- raw HTML omitted -->
<h4 id="summary-3">Summary</h4>
<ol>
<li>adop an hierarchical structure to predict action from different granularity.</li>
</ol>
<h4 id="research-objective-4">Research Objective</h4>
<ul>
<li><strong>Application Area</strong>: autonomous robots, surveillance and health care , robotic applications[24], [29]</li>
<li><strong>Purpose</strong>:  predict future action</li>
</ul>
<h4 id="problem-statement">Problem Statement</h4>
<ul>
<li>capture the subtle details inherent in human movements that may imply a future action</li>
<li>humans are highly articulated objects</li>
<li>actions can be described at different levels of semantic granularities.</li>
<li>prediction should carried out as quickly as possible</li>
<li>from recognizing simple human actions such as walking and standing in constrained settings[19] to understanding complex actions in realistic video and still images collected from movies,TV show , sport games , Internet (background clutter, occlusions, viewpoint, changes)
<ul>
<li>in video: bag-of-features representations of local space-time features [22]</li>
<li>in image : contextural information such as attributes ,objects ,poses are jointly modeled with actions.</li>
</ul>
</li>
</ul>
<p>previous work:</p>
<ul>
<li>Human ability of the visual system to predict future actions based on previous observations of interactions among humans</li>
<li>recent early event detection: expand spectrum of human action recognition to actions in future
<ul>
<li>[18] addresses the problem of early recognition of unfinished activities</li>
<li>[6] SVM framework for early event detection</li>
<li>predicting motion from still images[29]</li>
<li>prediction the future trajectories of pedestrians[15, 7]</li>
</ul>
</li>
<li><!-- raw HTML omitted -->different from previous work<!-- raw HTML omitted -->
<ul>
<li><!-- raw HTML omitted -->predict future actions from any timestamp in a video , don’t constrain the input to the “early stage of an action”<!-- raw HTML omitted --></li>
<li><!-- raw HTML omitted -->predict from a short video clip or even a static image<!-- raw HTML omitted --></li>
<li><!-- raw HTML omitted -->expand the scope of action prediction from controlled lab settings to unconstrained “in-the-wild” footage<!-- raw HTML omitted --></li>
<li><!-- raw HTML omitted --> predicting future actions from still images  or short video clips in unconstrained data<!-- raw HTML omitted --></li>
</ul>
</li>
</ul>
<h4 id="methods-5">Methods</h4>
<ul>
<li><strong>Problem Formulation</strong>:</li>
</ul>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191202132446815.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191202132446815.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191202132446815.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191202132446815.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191202132446815.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191202132446815.png"/></p>
<ul>
<li><strong>system overview</strong>:</li>
</ul>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191202134658266.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191202134658266.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191202134658266.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191202134658266.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191202134658266.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191202134658266.png"/></p>
<p>【Qustion 1】how to construct hierarchy construction?</p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191202132617020.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191202132617020.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191202132617020.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191202132617020.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191202132617020.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191202132617020.png"/></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191202134937117.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191202134937117.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191202134937117.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191202134937117.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191202134937117.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191202134937117.png"/></p>
<p>【Qustion 2】Model formulation</p>
<p>define X : person example     $Y={y_i}_{i=1}^L$   ,L the total number of levels of the hierarchy and $y_i$ is the index of the corresponding moveme at level i.</p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191202135449997.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191202135449997.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191202135449997.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191202135449997.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191202135449997.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191202135449997.png"/></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191202135543989.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191202135543989.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191202135543989.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191202135543989.png 2x"
    data-sizes="auto"
    alt="image-20191202135543989"
    title="image-20191202135543989"/></p>
<p>【Qustion 3】 optimization problem</p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191202135652819.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191202135652819.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191202135652819.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191202135652819.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191202135652819.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191202135652819.png"/></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191202135725669.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191202135725669.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191202135725669.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191202135725669.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191202135725669.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191202135725669.png"/></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191202135751580.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191202135751580.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191202135751580.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191202135751580.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191202135751580.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191202135751580.png"/></p>
<h4 id="conclusion-2">Conclusion</h4>
<ul>
<li>predict future actions from a single frame in the challenging real-word scenarios</li>
<li>a hierarchical movemes to capture multiple levels of granularities in human movements</li>
<li>develop a max-margin learning framework that jointly learns the appearance models of different movemes as well as their relations</li>
</ul>
<h4 id="notes">Notes</h4>
<ul>
<li><i class="fa-regular fa-square fa-fw" aria-hidden="true"></i> 论文【1】 moveme concept   Learning and recognizing human dynamics in video sequences</li>
<li><i class="fa-regular fa-square fa-fw" aria-hidden="true"></i> paper [22] Action recognition with improved trajectories</li>
<li><i class="fa-regular fa-square fa-fw" aria-hidden="true"></i> paper[18]  Early recognition of ongoing activities from streaming videos</li>
<li><i class="fa-regular fa-square fa-fw" aria-hidden="true"></i> paper[24]  Probabilistic modeling of human movements for intention inference</li>
<li><i class="fa-regular fa-square fa-fw" aria-hidden="true"></i> paper[29] A data-driven approach for event prediction</li>
<li><i class="fa-regular fa-square fa-fw" aria-hidden="true"></i> paper[9] anticipating future activities from RGB-D data by considering
human-object interactions   Anticipating human activities using object a↵ordances
for reactive robotic response</li>
<li><i class="fa-regular fa-square fa-fw" aria-hidden="true"></i> 有没有实现代码 运行看看预测效果，通过代码进一步加深理解，在优化问题定义那块需要加强理解</li>
</ul>
<p><strong>level</strong>: ECCV  CCF_A
<strong>author</strong>: George Papandreou ,Tyler Zhu  Google Research</p>
<hr>
<h2 id="paper-personlab">Paper: PersonLab</h2>
<!-- raw HTML omitted -->
<h4 id="summary-4">Summary</h4>
<ol>
<li>present a box-free bottom-up approach for the tasks of pose estimation and instance segmentation of people in multi-person images using an efficient single-shot model</li>
<li>tackles both semantic-level reasoning and object-part associations using part-based modeling. Empoys a convolutional network to learns to detect individual keypoints and predict their relative displacements,then group key-points into person pose instances</li>
<li>propose a part-induced geometric embedding descriptor which allows us to associate semantic person pixels with their corresponding person instance,dilevering instance-level person segmentations</li>
</ol>
<h4 id="research-objective-5">Research Objective</h4>
<ul>
<li><strong>Application Area</strong>:</li>
<li><strong>Purpose</strong>:</li>
</ul>
<h4 id="methods-6">Methods</h4>
<ul>
<li><strong>system overview</strong>:</li>
</ul>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191130152232740.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191130152232740.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191130152232740.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191130152232740.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191130152232740.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191130152232740.png"/></p>
<ul>
<li><strong>Keypoint detection</strong> : detect all visible key-points belonging to any person in the image .
<ul>
<li>具体计算heatmap 到时再细看 ， short-range offset vector is to improve the keypoint localization accuracy.  <!-- raw HTML omitted -->aggregate the heatmap and short-range offsets via Hough voting into 2-D Hough score maps</li>
</ul>
</li>
<li><strong>Grouping keypoints into person detection instances</strong>    Fast greedy decoding algorithm</li>
<li><strong>Instance-level person segmentation</strong> : Given the set of keypoint-level person instance detections, the task of our method’s egmentation stage is to identify pixels that belong to people (recognition) and associate them with the detected person instances (grouping)<img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191130154003763.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191130154003763.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191130154003763.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191130154003763.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191130154003763.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191130154003763.png"/></li>
<li><strong>Semantic person segmentation  &amp;&amp; Associating segments with instances via geometric embeddings</strong></li>
</ul>
<h4 id="notes-font-colororange去加强了解font-4">Notes <!-- raw HTML omitted -->去加强了解<!-- raw HTML omitted --></h4>
<ul>
<li>not read carefully</li>
</ul>
<h1 id="3-视频行为预测不同粒度">3. 视频行为预测(不同粒度)</h1>
<p><strong>level</strong>: CVPR<br>
<strong>author</strong>: Agrim Gupta , Li Fei-Fie
<strong>date</strong>: 2018
<strong>keyword</strong>:</p>
<ul>
<li>trajectory prediction</li>
</ul>
<hr>
<h2 id="paper-social-gan">Paper: Social GAN</h2>
<!-- raw HTML omitted -->
<h4 id="summary-5">Summary</h4>
<ol>
<li>使用LSTM 来编码用户的行为,使用SocialLSTM 池化层来表示较远距离的行为关系,使用生成模型来产生多种路径,利用判别模型从中选择最佳路径</li>
</ol>
<h4 id="research-objective-6">Research Objective</h4>
<ul>
<li><strong>Purpose</strong>:  predict the future trajectory</li>
</ul>
<h4 id="proble-statement-5">Proble Statement</h4>
<ul>
<li>InterPersonal: human have innate ability to read the behavior of others when navigating crowds</li>
<li>Socially Acceptable: social norms</li>
<li>Multimodal: multiple trajectories</li>
</ul>
<p>previous work:</p>
<ul>
<li>theymodel a local neighborhood around each person</li>
<li>they tend to learn average behavior <!-- raw HTML omitted -->we aim in learning multiple socially acceptable trajectories<!-- raw HTML omitted --></li>
</ul>
<h4 id="methods-7">Methods</h4>
<ul>
<li><strong>system overview</strong>:</li>
</ul>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191118093649383.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191118093649383.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191118093649383.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191118093649383.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191118093649383.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191118093649383.png"/></p>
<p>【Qustion 1】Using LSTM to encode the location of each person.And model human-human interaction via a Pooling Module (PM). After tobs we pool hidden states of all the people present inthe scene to get a pooled tensor Pi for each person.</p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191118094457342.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191118094457342.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191118094457342.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191118094457342.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191118094457342.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191118094457342.png"/></p>
<p>condition the generationof output trajectories by initializing the hidden state of the decoder as to produce future scenarios which are consistent
with the past<img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191118094655624.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191118094655624.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191118094655624.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191118094655624.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191118094655624.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191118094655624.png"/></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191118094908418.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191118094908418.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191118094908418.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191118094908418.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191118094908418.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191118094908418.png"/></p>
<p>Discriminator. The discriminator consists of a separate encoder. Specifically, it takes as input Treal = [Xi, Yi] or fake = [Xi, Yˆi] and classifies them as real/fake</p>
<p>[Question 2] Pooling Module Challenge: 1. Variable and large number of people in a scene,we need a compact representation whichcombines information from all the people. 2. Scattered Human-Human Interaction,the network needs to model global configuration.</p>
<ul>
<li>passing the input coordinates through a MLP followed by symmetric function(Max-Pooling).use relative coordinates for translation invariance  we augment the input to the pooling module with relative position of each person with respect to person i.</li>
</ul>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191118095339973.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191118095339973.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191118095339973.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191118095339973.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191118095339973.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191118095339973.png"/></p>
<p>[question3 ] diverse sample generation . propose a variety lossfunction that encourages the network to produce diverse sample .generate k possible out put samples and choose the best prediction in L2 sense <img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191118100138050.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191118100138050.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191118100138050.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191118100138050.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191118100138050.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191118100138050.png"/></p>
<h4 id="conclusion-3">Conclusion</h4>
<ul>
<li>introduce variety loss which encourages the generative network of GAN to spread its distribution and cover the space of possible paths while being consistent with the observed inputs.</li>
<li>a new pooling mechanism that learns a global pooling vector which encodes the subtle cues for all people involved in a scene.</li>
</ul>
<p><strong>level</strong>: CVPR
<strong>author</strong>:Chih-Yao Ma ,Min-Hung Chen
<strong>date</strong>: 30 Mar 2017
<strong>keyword</strong>:</p>
<ul>
<li>LSTM, action prediction</li>
</ul>
<hr>
<h2 id="paper-ts-lstm-and-temporal-inception">Paper: TS-LSTM and Temporal-Inception</h2>
<!-- raw HTML omitted -->
<h4 id="proble-statement-6">Proble Statement</h4>
<ul>
<li>methods extending the basic-stream ConvNet have not systematically explored possible network architectures to further exploit spatiotemporal dynamics within video sequences.The network often use different baseline two-stream networks.</li>
<li>traditional two-stream ConvNets unable to expoit the most critical component in action recognition<!-- raw HTML omitted --> visual appearance across both spatial and temporal streams and their correlations are not considered <!-- raw HTML omitted -->,</li>
<li>previous work mainly try individual methods with little analysis of whether and how they can successfully use temporal information.</li>
<li>each individual work uses different networks for the baseline two-stream approach with varied performance depending on training and testing procedure as well as the optical flow method used.</li>
</ul>
<p>previous work:</p>
<ul>
<li><!-- raw HTML omitted --> hand-craft or learned features for training<!-- raw HTML omitted -->   3D ConvNets:  [9] stacked consecutive video frames and extended the first convolutional layer to learn the spatiotemporal features while exploring different fusion approaches including early fusion and slow fusion. C3D[20] replacing all the 2D convolutional kernels with 3D kernels at the expense of GPU memory. [16] factorize the original 3D kernels into 2D spatial and 1D temporal kernels and achieve comparable performance.  <!-- raw HTML omitted --> multiple layers can extract temporal correlations at different time scales and provide better capability to distinguish different types of actions<!-- raw HTML omitted --></li>
<li>ConvNets with RNNs: directly take variable length inputs and learn long-term dependencies.</li>
<li>Two-stream ConvNets:spatial features and temporal features from optical flow images.  <!-- raw HTML omitted -->we only use the feature vector representations instead of features maps<!-- raw HTML omitted --></li>
</ul>
<h4 id="methods-8">Methods</h4>
<ul>
<li><strong>system overview</strong>:</li>
</ul>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191124134305103.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191124134305103.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191124134305103.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191124134305103.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191124134305103.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191124134305103.png"/></p>
<p>【Qustion 1】Spatial stream  $ Temporal stream</p>
<p>spatial stream: the ResNet-101 spatial-stream ConvNet is pre-trained on ImageNet and fine-tured on RGB images extracted from UCF101 datasets with classification loss for predicting activities.</p>
<p>Temporal stream: stacking 10 optical flow images for temporal stream has been considered as a standard for two-stream ConvNets[13,6,28,25,27] <!-- raw HTML omitted -->follow the same pre-train procedure shown by [25]<!-- raw HTML omitted --></p>
<p>[model 1] <strong>Temporal Segment LSTM</strong>:  using 25 to divide the sampled video frames into several segments, a temporal pooling layer is applied to extrct distinguishing features from each of the segments.and LSTM is used ot extract the embedded features from all segments.</p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191124140942805.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191124140942805.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191124140942805.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191124140942805.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191124140942805.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191124140942805.png"/></p>
<p>[model 2] Temporal-ConvNet :leveraging the temporal relation across diferent frames.</p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191124141556429.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191124141556429.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191124141556429.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191124141556429.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191124141556429.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191124141556429.png"/></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20191124141419689.png"
    data-srcset="https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20191124141419689.png, https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20191124141419689.png 1.5x, https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20191124141419689.png 2x"
    data-sizes="auto"
    alt="image-20191124141419689"
    title="image-20191124141419689"/></p>
<p>different types o faction have different temporal characteristics and different kernels in different layers essentially search for different actions by expoiting different receptive fields to encode the temporal characteristics.</p>
<h4 id="evaluation-2">Evaluation</h4>
<ul>
<li><strong>Environment</strong>:
<ul>
<li>Dataset:  experiment on spatial-stream ,temporal-stream and two-stream on three different splits in the UCF101, and HMDB51 datasets.</li>
</ul>
</li>
<li>comparison evaluation 这部分没有看,如果以后用到再来细看</li>
</ul>
<h4 id="conclusion-4">Conclusion</h4>
<ul>
<li>first demostrate a strong baseline two-stream ConvNet using ResNet-101.</li>
<li>propose and investigate two different networks to further integrate spatiotemporal information: temporal segment RNN  and Inception-style Temporal-ConvNet.  but all need propercare.</li>
</ul>
<h4 id="notes-font-colororange去加强了解font-5">Notes <!-- raw HTML omitted -->去加强了解<!-- raw HTML omitted --></h4>
<ul>
<li><i class="fa-regular fa-square fa-fw" aria-hidden="true"></i> [13] incorporate spatial and temporal information extracted from RGB and optical flow images.Two-stream convolutional networks for action recognition in videos</li>
<li><i class="fa-regular fa-square fa-fw" aria-hidden="true"></i> 了解学习 14,18 8 模型 [25][28 ] 7</li>
<li><i class="fa-regular fa-square fa-fw" aria-hidden="true"></i> code available:</li>
<li><i class="fa-regular fa-square fa-fw" aria-hidden="true"></i> [6] fusion stage   Convolutional two-stream network fusion for video action recognition</li>
<li><i class="fa-regular fa-square fa-fw" aria-hidden="true"></i> Temporal segment networks: Towards good practices for deep action recognition</li>
<li><i class="fa-regular fa-square fa-fw" aria-hidden="true"></i> optical methods Brox[2] or TV-L1[29],and the results <img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191124141955787.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191124141955787.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191124141955787.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191124141955787.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191124141955787.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191124141955787.png"/></li>
</ul>
<p><strong>level</strong>:   2019 winter conference on application of computer vision
<strong>author</strong>: Erwin Wu   Tokyo Institute of Technology
<strong>date</strong>: 2019
<strong>keyword</strong>:</p>
<ul>
<li>action prediction</li>
</ul>
<hr>
<h2 id="paper-futruepose">Paper: FutruePose</h2>
<!-- raw HTML omitted -->
<h4 id="summary-6">Summary</h4>
<ol>
<li>这篇文章将人体2D坐标和光流信息结合起来通过LSTM网络预测未来0.5s 姿态2D坐标,并使用VNect网络建立3D模型,去3D模型上的若干点通过数值分析模型判断是否碰撞.</li>
<li>shortcoming:
<ol>
<li>only experiment on boxing,there are still other activities</li>
<li>foces on inference and accuracies on different algorithms ,if using hyper-parameter like d for lattice point flow and threshhold for noise filter</li>
<li>the forcasting information is limited, if building an orientation-based 3D pose estimation by dividing the human body into different parts and learning the bone rotation ,not only to related to their mother joints but relative to the entire body part</li>
<li>the frame rate of normal high speed movement like keck form a professional martial athlete</li>
<li>this paper focus on single person ,if there are multiperson.</li>
</ol>
</li>
</ol>
<h4 id="research-objective-7">Research Objective</h4>
<ul>
<li><strong>Application Area</strong>:analyse a player&rsquo;s habit ,determinate strengths and predict next movement</li>
<li><strong>Purpose</strong>:  a novel mixed reality martial arts training system using deep learning based real time human pose forecasting.</li>
</ul>
<h4 id="proble-statement-7">Proble Statement</h4>
<ul>
<li>Recent 3D motion capture systems are based on<!-- raw HTML omitted --> fabric technology<!-- raw HTML omitted --> ,requiring to wear specific suits or sensors.</li>
<li>special cameral <!-- raw HTML omitted -->RGB-Depth,IR cameras<!-- raw HTML omitted --></li>
<li><!-- raw HTML omitted -->normal dense optical flow<!-- raw HTML omitted --> requires many computations and leads to a heavy inference time in LSTM</li>
</ul>
<p>previous work:</p>
<ul>
<li>Martial Sports in AR/VR: wear VR HMD and take a pair of controllers</li>
<li>Real-time 3D pose estimation:
<ul>
<li>VNect :provide a better accuracy for the 3D skeleton recognition with less computation and good real-time ability, can&rsquo;t be used in multi-person detection.</li>
<li>OpenPose detect multiple people in a single image,but the inference time is greater.</li>
<li>[20] 3D pose Recovery using a simple and deep neural network with only two linear layers and two residual blocks ,demostrate 3D pose could be created by 2D joint positions.</li>
</ul>
</li>
<li>Pose forecasting:
<ul>
<li>3D-PFNet :the first to forecasting human dynamics from single RGB images. forcasting 2D skeletal poses and converting them into 3D space   87.6mm error.  <!-- raw HTML omitted -->offline network require large computation<!-- raw HTML omitted --></li>
<li>[12] forecast human body motion 0.5s advance using five layered neural network   ,7.9cm  <!-- raw HTML omitted -->IR sensor is not suitable to use in an outdoor environment or a large area.  not for more complicated athletic movemnet such as boxing<!-- raw HTML omitted --></li>
</ul>
</li>
</ul>
<h4 id="methods-9">Methods</h4>
<ul>
<li><strong>Problem Formulation</strong>:forecasting of 3D pose from a single image and the model fitting and collision detection.</li>
<li><strong>system overview</strong>:<img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191116143629118.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191116143629118.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191116143629118.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191116143629118.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191116143629118.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191116143629118.png"/></li>
<li><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191116152552658.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191116152552658.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191116152552658.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191116152552658.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191116152552658.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191116152552658.png"/></li>
</ul>
<p>【Qustion 1】<!-- raw HTML omitted -->how to estmate 2D pose?<!-- raw HTML omitted --> cropped using bounding box tracker.</p>
<p>use ResNet50[10] to allow the convolutional layer to regress the 2D joint data</p>
<p>【Qustion 2】 <!-- raw HTML omitted -->2D pose forecasting?<!-- raw HTML omitted --></p>
<p>using optical flow and joint positions data to do a regression on the LSTMs.  developed a sparse optical flow called Keypoint Lattice-Optical Flow ,creates several lattice points and only calculates the optical flows of the lattice points which close to keypoint.</p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191116153025285.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191116153025285.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191116153025285.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191116153025285.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191116153025285.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191116153025285.png"/></p>
<p>【Qustion 3】 <!-- raw HTML omitted -->3D pose recovery?<!-- raw HTML omitted -->  [20] an effective 3D pose recovery  using VNect network</p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191116153558928.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191116153558928.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191116153558928.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191116153558928.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191116153558928.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191116153558928.png"/></p>
<p>【Qustion 4】<!-- raw HTML omitted -->How to understand person&rsquo;s position and detect the collision in virtual environment ?<!-- raw HTML omitted --></p>
<p>using 3D model to represent user and have a surface to collide with one another.</p>
<p>using makehumanAPI to gernerate 3D model<img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191116154051111.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191116154051111.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191116154051111.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191116154051111.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191116154051111.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191116154051111.png"/></p>
<p>divided model into more than 200 segments,called &lsquo;hulls&rsquo;,each of these hulls contains a convex(凸) collider .detect a collision between two hulls using basic convex polytopes.</p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191116154239879.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191116154239879.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191116154239879.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191116154239879.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191116154239879.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191116154239879.png"/></p>
<h4 id="evaluation-3">Evaluation</h4>
<ul>
<li>
<p><strong>Environment</strong>:</p>
<ul>
<li>Hardware: using TensorFlow on the TSUBAME3.0( Xeon E5-2680  v4 CPU<em>2,Nvidia SXM2 P100 GPU</em>4),tensorflow1.4.1,cuda 8.0 cudnn 5.1lib    HTC VIve(VR HMD),Sony DSCQX10 camera,LogitechC270 webcamera.</li>
<li>Dataset: MPI-INF-3D  and Human36M datasets for pre-training and validation. ratio of 6:2:2 for
training, testing, and validation</li>
</ul>
</li>
<li>
<p><strong>result</strong></p>
<p><strong>RealTimePerformance</strong>:</p>
</li>
</ul>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191116155736081.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191116155736081.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191116155736081.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191116155736081.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191116155736081.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191116155736081.png"/></p>
<p><strong>Pose forecasting accuracy:</strong></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191116155725964.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191116155725964.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191116155725964.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191116155725964.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191116155725964.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191116155725964.png"/></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191116155800489.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191116155800489.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191116155800489.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191116155800489.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191116155800489.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191116155800489.png"/></p>
<p><strong>User case study:</strong></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191116161933715.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191116161933715.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191116161933715.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191116161933715.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191116161933715.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191116161933715.png"/></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191116162001653.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191116162001653.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191116162001653.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191116162001653.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191116162001653.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191116162001653.png"/></p>
<h4 id="contribution">Contribution</h4>
<ul>
<li>the first to realize real-time 3D human pose forecasting based on normal video frames and apply it to mixed reality martial arts use</li>
<li>a customized residual network[10]to obtain 2d human joints ,uses recurrent networks to learn the temporal features of the human motion.</li>
<li>use a lattice optical flow algorithm to calculate the joint movement with less computation</li>
</ul>
<h4 id="notes-font-colororange去加强了解font-6">Notes <!-- raw HTML omitted -->去加强了解<!-- raw HTML omitted --></h4>
<ul>
<li><i class="fa-regular fa-square fa-fw" aria-hidden="true"></i> paper[20],[12],[22],[6],19],[18]</li>
<li><i class="fa-regular fa-check-square fa-fw" aria-hidden="true"></i> 3D-PFNet  12</li>
<li><i class="fa-regular fa-check-square fa-fw" aria-hidden="true"></i> <a href="mailto:PCKh@0.05">PCKh@0.05</a> evaluation [1] measure which calculates the percentage of correct key point that uses a matching threshold of 50% of the head segment length.</li>
<li><i class="fa-regular fa-check-square fa-fw" aria-hidden="true"></i> RMSE: The root-mean-squared error (RMSE) was also calculated to show the deviation of the predicted data</li>
</ul>
<hr>
<p><strong>level</strong>:  ACM<br>
<strong>author</strong>: Yuuki Horiuchi , Yasutoshi Makino
<strong>date</strong>: 2017 .10
<strong>keyword</strong>:</p>
<ul>
<li>Machine learning , Motion estimation,Human-centered computing ,computing methodologies</li>
</ul>
<hr>
<h2 id="paper-computational-foresight">Paper: Computational Foresight</h2>
<!-- raw HTML omitted -->
<h4 id="research-objective-8">Research Objective</h4>
<ul>
<li><strong>Application Area</strong>:<!-- raw HTML omitted -->instruct sports actions ,prevent elderly form falling to the ground, prevent accident in advance. reducing delay in remote interactive system<!-- raw HTML omitted --></li>
<li><strong>Purpose</strong>:  forecast human body 0.5s before the actual motion in real-time  with accuracy of 7.9cm</li>
</ul>
<h4 id="proble-statement-8">Proble Statement</h4>
<ul>
<li>diverse communication with remote areas has become possible, <!-- raw HTML omitted -->information transmission delay<!-- raw HTML omitted --></li>
</ul>
<p>previous work:</p>
<ul>
<li>Holoportation system[1] communicate with remote people using HMD.</li>
<li>TELESAR V system[2] feel object through remotely connected robot with haptic sensing and feedback.</li>
<li>pattern categorized or estimation using DNN,  Predicting trajectory of movie in realtime.  <!-- raw HTML omitted -->there is no research that forecast body motions which is not repetitive and personalized but universal in realtime and visualize it to user.<!-- raw HTML omitted --></li>
</ul>
<h4 id="methods-10">Methods</h4>
<ul>
<li><strong>system overview</strong>:</li>
</ul>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191122164724372.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191122164724372.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191122164724372.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191122164724372.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191122164724372.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191122164724372.png"/></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191122164824683.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191122164824683.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191122164824683.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191122164824683.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191122164824683.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191122164824683.png"/></p>
<p>【Qustion 1】how to extract 25 body point and COG?    论文[14] 需要学一下</p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191122165225765.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191122165225765.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191122165225765.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191122165225765.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191122165225765.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191122165225765.png"/></p>
<p>[解决问题2]Neural Network design ?</p>
<p>combine past 10 frames of 26 data as a one learning dataset .(joints+COG position)<em>3 demensions(x,y,z)</em> * 10 frames=780</p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191122165657463.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191122165657463.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191122165657463.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191122165657463.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191122165657463.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191122165657463.png"/></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191122165302517.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191122165302517.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191122165302517.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191122165302517.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191122165302517.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191122165302517.png"/></p>
<p>[解决问题3] 损失函数 和 优化器选择</p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191122165849130.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191122165849130.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191122165849130.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191122165849130.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191122165849130.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191122165849130.png"/></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20191122170046854.png"
    data-srcset="https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20191122170046854.png, https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20191122170046854.png 1.5x, https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20191122170046854.png 2x"
    data-sizes="auto"
    alt="image-20191122170046854"
    title="image-20191122170046854"/></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191122171352648.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191122171352648.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191122171352648.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191122171352648.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191122171352648.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191122171352648.png"/></p>
<h4 id="evaluation-4">Evaluation</h4>
<ul>
<li><strong>Environment</strong>:
<ul>
<li>Dataset:   Kinect V2  eleven subject to jump as many time as they could ,one duration for one minute. they allow to jump either ways in random order and the distance less than 2.5m.</li>
<li>laptop(CPU: intel core-i7-7820HK 2.9-3.9Ghz,GPU: Nvidia Geforece GTX 1080)  3.32ms for COG NN  matrix operation, 5.75ms for redering bone image less than 33ms for measuring depth map and 3D position of 25 body joints and COG</li>
<li><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191122171323598.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191122171323598.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191122171323598.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191122171323598.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191122171323598.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191122171323598.png"/></li>
<li><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191122170458248.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191122170458248.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191122170458248.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191122170458248.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191122170458248.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191122170458248.png"/></li>
</ul>
</li>
<li></li>
</ul>
<h4 id="conclusion-5">Conclusion</h4>
<ul>
<li>均方误差MSE   注:RMSE（即MSE的平方根)<img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191122171538058.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191122171538058.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191122171538058.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191122171538058.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191122171538058.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191122171538058.png"/></li>
<li>平均绝对误差（MAE）<img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191122171610156.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191122171610156.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191122171610156.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191122171610156.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191122171610156.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191122171610156.png"/></li>
</ul>
<h4 id="notes-font-colororange去加强了解font-7">Notes <!-- raw HTML omitted -->去加强了解<!-- raw HTML omitted --></h4>
<ul>
<li>human gesture recognition by using depth map [10]  Neural network for
dynamic human motion prediction</li>
<li>论文14 计算重心</li>
</ul>
<hr>
<p><strong>level</strong>: CVPR   CCF A
<strong>author</strong>: Junwei Liang ,Li Fei-Fei
<strong>date</strong>: &lsquo;2019-05-31&rsquo;
<strong>keyword</strong>:</p>
<ul>
<li>LSTM, activity prediction</li>
</ul>
<hr>
<h2 id="paper-peeking-into-the-future">Paper: Peeking into the future</h2>
<!-- raw HTML omitted -->
<h4 id="summary-7">Summary</h4>
<p>这篇文章通过 分析 人的位置，行为，与周围事务的距离，周围的环境信息来预测未来轨迹何未来动作，并通过位置预测算法来减少 人位置的累计误差。</p>
<ol>
<li>在编码用户与周围的事务互动时，能不能编码用户的之间的动作联系对应起来，而不是简单的距离</li>
</ol>
<h4 id="research-objective-9">Research Objective</h4>
<ul>
<li><strong>Application Area</strong>:Future person path/trajectory activity prediction (accident avoidance , smart personal assistance , self-driving car , socially-aware robots , anticipating pedestrian movement at traffic intersections or a road)</li>
<li><strong>Purpose</strong>:  deciphering human behaviors to predict pedestrian&rsquo;s future path jointly with future activities.</li>
</ul>
<h4 id="proble-statement-9">Proble Statement</h4>
<ul>
<li>Humans navigate through public spaces often with specific purposes in mind.</li>
</ul>
<p>previous work:</p>
<ul>
<li>Person-person models for trajectory prediction.
<ul>
<li>[32,34] predict person path by considering human social interactions and behaviors in crowded scene.</li>
<li>[36]learned human hehavior in crowds by imitating a decision-making process</li>
<li>Social-LSTM[1] added social pooling to model nearby pedestrian trajectory patterns.</li>
<li>Social-GAN[7] added advertisarial training on social LSTM to improve perfomance.</li>
<li><!-- raw HTML omitted -->they simply consider a person as points ,we use geometric ralation to explicitly model the person-scene interaction and the person-object relatoinsj<!-- raw HTML omitted --></li>
</ul>
</li>
<li>Person-scene models for trajectory prediction :learning the effect of the physical scene
<ul>
<li>[13] using Inverse Reinforcement learning to forecast human trajectory</li>
<li>Scene-LSTM divided the static scene into Manhattan Grid and predict pedestrian&rsquo;s location using LSTM</li>
<li>CAR-Net proposed an attention network on top of scene semantic CNN to predict person trajectory</li>
<li>SoPhie vombined deep neural network features form scene semantic segmentation model and generatice adbersarial network using attention to model person trajectory</li>
<li><!-- raw HTML omitted -->we explicitly pool scene semantic features around each person at each time instant ,the model directly learn from such interactions<!-- raw HTML omitted --></li>
</ul>
</li>
<li>Person visual features for trajectory prediction  :using individual&rsquo;s visual features instead of considering them as points in the scene.
<ul>
<li>[14] looked at pedestrian &rsquo;s faces to model their awareness to  predict whether they wil corss the road using Dynamic Bayesian Network .</li>
<li>[33] person keypoint features with a convolutional neural network to predict future path .</li>
<li><!-- raw HTML omitted -->we consider both person behavior and their interactions with soundings<!-- raw HTML omitted --></li>
</ul>
</li>
<li>Activity prediction /early recognition
<ul>
<li>[29] utilized unsupervised learning with LSTM to reconstruct and predict video representations.</li>
</ul>
</li>
<li>Multiple cues for tracking/group activity recognition:
<ul>
<li>previous works take into account multiple cues in video for tracking ,group activity recognition</li>
<li><!-- raw HTML omitted -->rich vision features focal attention,  location prediction to bridge the two taks<!-- raw HTML omitted --></li>
</ul>
</li>
<li>most existing work [31,1,7,26,21,31] which oversimplifies a person as a point in space,we encode a person through <!-- raw HTML omitted -->rich semantic features about visual appearance ,body movement ,and interaction with the surroundings ,<!-- raw HTML omitted --></li>
</ul>
<h4 id="methods-11">Methods</h4>
<ul>
<li>
<p><strong>Problem Formulation</strong>:<img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191109103802813.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191109103802813.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191109103802813.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191109103802813.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191109103802813.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191109103802813.png"/></p>
</li>
<li>
<p><strong>system overview</strong>:</p>
</li>
</ul>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191109103550662.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191109103550662.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191109103550662.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191109103550662.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191109103550662.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191109103550662.png"/></p>
<p>【Qustion 1】<!-- raw HTML omitted -->how to model the person&rsquo;s appearance and body movement about every individual in a scene<!-- raw HTML omitted -->?</p>
<ul>
<li>utilize a pre-trained object detection model with &ldquo;<strong>RoIAlign[8</strong>]&rdquo; to extrace fixed size CNN features for each person bounding box. for every person ,average the feature along the spatial dimentions and feed them into LSTM encoder   -&gt; obtain T*d ,where d is the hidden size of the LSTM.</li>
<li>utilize a person key-point detection model trained on MSCOCO dataset[6] to extract preson keypoint information.we apply <!-- raw HTML omitted -->the linear transformation to embed the keypoint coordinates ,这里的线性处理不理解<!-- raw HTML omitted -->before feed into LSTM.-&gt;obtain T*d ,where d is the hidden size of the LSTM<img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191109111014991.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191109111014991.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191109111014991.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191109111014991.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191109111014991.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191109111014991.png"/></li>
</ul>
<p>【Qustion 2】<!-- raw HTML omitted -->how to model the interaction between a person and their surroundings,person-scene and person-object<!-- raw HTML omitted --></p>
<ul>
<li>
<p>Person-scene: <!-- raw HTML omitted -->whether the person is near the sidewolk or grass<!-- raw HTML omitted --> use a <strong>pre-trained scene segmentation model[4]</strong> to extract pixel-level scene semantic classes(10 class eg.roads , sidewalks&hellip;) for each frame. the scene   the semantic features are integeres of the size T * h * w ,Given a person&rsquo;s xy coordinate ,we pool the scene features at the person;s current location from the convolution feature map.  <img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191109111844802.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191109111844802.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191109111844802.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191109111844802.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191109111844802.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191109111844802.png"/></p>
</li>
<li>
<p>Person-object: <!-- raw HTML omitted -->how far away the person is to the other person or object<!-- raw HTML omitted -->models the <strong>geometric relation</strong> and <strong>the object type</strong> of all objects/persons in the scene.在论文[9]中证明了这个方法的高效性<img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191109112732633.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191109112732633.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191109112732633.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191109112732633.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191109112732633.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191109112732633.png"/></p>
<ul>
<li>geometric relation:</li>
</ul>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191109112036904.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191109112036904.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191109112036904.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191109112036904.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191109112036904.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191109112036904.png"/></p>
<ul>
<li>object type:</li>
</ul>
</li>
</ul>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191109112647171.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191109112647171.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191109112647171.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191109112647171.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191109112647171.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191109112647171.png"/></p>
<p>【Qustion 3】<!-- raw HTML omitted -->how to predict the trajectory ?<!-- raw HTML omitted --> using effective focal attention[17]  ,原始模型见[7]</p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191109113310088.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191109113310088.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191109113310088.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191109113310088.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191109113310088.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191109113310088.png"/></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20191109113735888.png"
    data-srcset="https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20191109113735888.png, https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20191109113735888.png 1.5x, https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20191109113735888.png 2x"
    data-sizes="auto"
    alt="image-20191109113735888"
    title="image-20191109113735888"/></p>
<p>【Qustion 4】<!-- raw HTML omitted -->how to predict activity ?<!-- raw HTML omitted --> introduce an auxiliary task :activity location prediction in addition to predicting the future activity label of the person .<img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191109114434310.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191109114434310.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191109114434310.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191109114434310.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191109114434310.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191109114434310.png"/></p>
<ul>
<li>activity location prediction with Manhattan Grid  (location classification(to predict correct grid block in which the  final location coordinates reside),location regression(to predict the deviation of the grid block center to final location coordinate))   <!-- raw HTML omitted -->how to accurate localization using multi-scale features in a cost-effective way<!-- raw HTML omitted --></li>
<li>activity label prediction: <img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191109142619975.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191109142619975.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191109142619975.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191109142619975.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191109142619975.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191109142619975.png"/></li>
</ul>
<h4 id="evaluation-5">Evaluation</h4>
<ul>
<li><strong>Environment</strong>:
<ul>
<li>Dataset: ActEV/ViRAT</li>
</ul>
</li>
<li>model the intention in terms of a predefined set of 29 activities provided by NIST .</li>
</ul>
<h4 id="conclusion-6">Conclusion</h4>
<ul>
<li>propose an end-to-end multi-task learnig system<!-- raw HTML omitted --> utilizing rich visual features about human behavioral information and interaction with their surroundings<!-- raw HTML omitted --> .</li>
<li>the first empirical evidence that joint medeling of paths and activities benefits future path prediction.
<ul>
<li>learning activity together with the path may benefit the future path prediction</li>
<li>joint model advances the capability of understanding not only the future path but also the future activity by taking into account the rich semantic context in videos.</li>
<li>introduce an auxiliary task for future activity prediction,activity location.</li>
</ul>
</li>
<li>propose multi-task learning framework with new techniques to tackle the challenge of joint future path and activity prediction.</li>
<li>validate the model on two benchmarks: ETH&amp;UCY , and ActEV/VIRAT.</li>
</ul>
<h4 id="notes-font-colororange去加强了解font-8">Notes <!-- raw HTML omitted -->去加强了解<!-- raw HTML omitted --></h4>
<ul>
<li><i class="fa-regular fa-square fa-fw" aria-hidden="true"></i> Effective focal attention was originally proposed to carry out multimode inference over a sequence of images for visual question answering. key idea is project multiple features into a space of correlation where discriminative features can be easier to capture by attention mechanism.</li>
<li><i class="fa-regular fa-square fa-fw" aria-hidden="true"></i> Attention mechanism   ???</li>
<li><i class="fa-regular fa-square fa-fw" aria-hidden="true"></i> 论文37 decision-making process 方法是什么？</li>
<li><i class="fa-regular fa-square fa-fw" aria-hidden="true"></i> [13] using Inverse Reinforcement  方法是什么</li>
<li><i class="fa-regular fa-square fa-fw" aria-hidden="true"></i> [33] person keypoint features to predict trajectory ?</li>
<li><i class="fa-regular fa-square fa-fw" aria-hidden="true"></i> <strong>RoIAlign[8</strong>]   学习使用这个网络</li>
<li><i class="fa-regular fa-square fa-fw" aria-hidden="true"></i> <strong>pre-trained scene segmentation model[4]</strong>  学习了解下场景分割技术</li>
<li><i class="fa-regular fa-square fa-fw" aria-hidden="true"></i> Code 学习使用： <a href="https://github.com/google/next-prediction"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/google/next-prediction<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></li>
</ul>
<hr>
<p><strong>level</strong>: CVPR ccf A
<strong>author</strong>:  alexandre Alahi , Kratarth Goel     stanford.edu
<strong>date</strong>:
<strong>keyword</strong>:</p>
<ul>
<li></li>
</ul>
<hr>
<h2 id="paper-social-lstm">Paper: Social LSTM</h2>
<!-- raw HTML omitted -->
<h4 id="research-objective-10">Research Objective</h4>
<ul>
<li><strong>Application Area</strong>: social aware roots[41], intelligent tracking system[43]</li>
<li><strong>Purpose</strong>:  predict the motion dynamics in crowded scenes.</li>
</ul>
<h4 id="proble-statement-10">Proble Statement</h4>
<p>previous work:</p>
<ul>
<li>they use hand-craft functions(人工特征) to model interactions for specific settings rather than inferring them in data driven fashion.</li>
<li>they focus on modeling interactions among people in close proximity to each other(to avoid immediate collisions), don‘t anticipate interactions that could occur in the more distant future.</li>
<li>RNN model for sequence prediction (speech recognition , caption generation , machine translation , image/vedio classification, human dynamic)
<ul>
<li>Model  and <!-- raw HTML omitted -->Gated Recurrent Units[12]<!-- raw HTML omitted -->  most common methods.</li>
<li>[20] predict isolated handwriting sequence</li>
</ul>
</li>
</ul>
<h4 id="methods-12">Methods</h4>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191112092925006.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191112092925006.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191112092925006.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191112092925006.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191112092925006.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191112092925006.png"/></p>
<p>【定义问题1】every person has different motion pattern,they move with different velocities ,acceleration and have different gaits ,how to model person-specific motion properties from a limited set of initial observation corrosponding to the person</p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191112092949875.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191112092949875.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191112092949875.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191112092949875.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191112092949875.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191112092949875.png"/></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20191112093015584.png"
    data-srcset="https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20191112093015584.png, https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20191112093015584.png 1.5x, https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20191112093015584.png 2x"
    data-sizes="auto"
    alt="image-20191112093015584"
    title="image-20191112093015584"/></p>
<p>【定义问题2】 every person has a different number of neighbors and in very dese crowds,the number could prohibitively high?</p>
<p>a compact representaion &ldquo;Social &quot; pooling layers ,and preserve the spatial information through grid based pooling .</p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191112094414250.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191112094414250.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191112094414250.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191112094414250.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191112094414250.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191112094414250.png"/></p>
<p>【定义问题3】 how to estimate the Position?</p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20191112100947577.png"
    data-srcset="https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20191112100947577.png, https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20191112100947577.png 1.5x, https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20191112100947577.png 2x"
    data-sizes="auto"
    alt="image-20191112100947577"
    title="image-20191112100947577"/></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191112101003868.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191112101003868.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191112101003868.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191112101003868.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191112101003868.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191112101003868.png"/></p>
<p>【定义问题4】 how to deal with occupancy map pooling?</p>
<p>the Social LSTM model can be used to pool any set of features from neighboring trajectory ,and learn to reposition a trajectory to avoid immediate collision with neighbors.<!-- raw HTML omitted -->这一部分不太明白<!-- raw HTML omitted --></p>
<h4 id="evaluation-6">Evaluation</h4>
<ul>
<li><strong>Environment</strong>: ETH  ,UCY</li>
<li><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191112101541086.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191112101541086.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191112101541086.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191112101541086.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191112101541086.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191112101541086.png"/></li>
</ul>
<h4 id="conclusion-7">Conclusion</h4>
<ul>
<li>introduce the Social pooling layer which allows the LSTMs of partially proximal sequences to share their hidden-states with each other.</li>
<li>analyze the trajectory patterns generated by our model to understant the social constrains learned from the trajectory datasets.</li>
<li>predicting the trajectories of pedestrians much  more accurately than state-of-the-art models on ETH,UCY</li>
</ul>
<h4 id="notes--font-coloryellow去加强了解font">Notes  <!-- raw HTML omitted -->去加强了解<!-- raw HTML omitted --></h4>
<ul>
<li>
<p><i class="fa-regular fa-square fa-fw" aria-hidden="true"></i> Generating sequences with recurrent neural networks</p>
</li>
<li>
<p><i class="fa-regular fa-square fa-fw" aria-hidden="true"></i> LSTM speech generation【21】demo  去github上找代码</p>
</li>
<li>
<p><i class="fa-regular fa-square fa-fw" aria-hidden="true"></i> [32] 学习和了解 Inverse Reinforcement Learning to predict human paths in static scenes.</p>
</li>
<li>
<p><i class="fa-regular fa-square fa-fw" aria-hidden="true"></i> Theano: A cpu and gpu math compiler in python</p>
</li>
<li>
<p><i class="fa-regular fa-check-square fa-fw" aria-hidden="true"></i> bivariate Gaussian distribution多元正态分布<img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191121101725026.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191121101725026.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191121101725026.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191121101725026.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191121101725026.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191121101725026.png"/></p>
</li>
</ul>
<hr>
<p><strong>level</strong>: IEEE Access   CCF B 类
<strong>author</strong>: 10.25.2019
<strong>date</strong>: &lsquo;2019-10-25&rsquo;
<strong>keyword</strong>:</p>
<ul>
<li>Action recognition,deep learning ,pedestrian detection ,time-to-cross estimation</li>
</ul>
<hr>
<h2 id="paper-multi-task-pedestrian">Paper: Multi-Task Pedestrian</h2>
<!-- raw HTML omitted -->
<h4 id="summary-8">Summary</h4>
<p>这篇文章解决了如何去检测行人，识别行人的动作（利用JAAD数据库和现有的方法），并且预测了在行人过马路的状态下穿过时间预测。使用了RetinaNet 网络检测，LSTM网络去预测。</p>
<p>问题：</p>
<ol>
<li>如何去检测多个人的，以及多个人的相应的动作  引用文章的没有细说？需要看下</li>
<li>在预测的时候LSTM网络仅仅是BB坐标，每个人的步速步伐大小可能不一样这里应该怎么解决？</li>
<li>现有RF实现骨架检测技术，以及手势识别技术，人体骨架各部分运动检测，能否用LSTM预测下一个动作，这是预测行人过马路，如果在室内可能需要检测或预测哪些动作</li>
</ol>
<h4 id="research-objective-11">Research Objective</h4>
<ul>
<li><strong>Application Area</strong>:understand the intention of road users involved to ensure their safety and secure the traffic flow.</li>
<li><strong>Purpose</strong>:  estimate TTC.</li>
<li><strong>System_Design</strong>:</li>
<li><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191106112647294.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191106112647294.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191106112647294.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191106112647294.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191106112647294.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191106112647294.png"/></li>
<li><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191106111040275.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191106111040275.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191106111040275.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191106111040275.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191106111040275.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191106111040275.png"/></li>
<li><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191106111058015.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191106111058015.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191106111058015.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191106111058015.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191106111058015.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191106111058015.png"/></li>
<li><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191106111116174.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191106111116174.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191106111116174.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191106111116174.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191106111116174.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191106111116174.png"/></li>
<li><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191106111520160.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191106111520160.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191106111520160.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191106111520160.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191106111520160.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191106111520160.png"/></li>
</ul>
<h4 id="proble-statement-11">Proble Statement</h4>
<ul>
<li><strong>pedestrian detection problem</strong>: progress in pedestrian detection is hindered by the difficulty of detecting all(partially)occluded pedestrians and the problem of operating efficiently in severe weather conditions.</li>
<li><strong>ADAS need to solve three problems</strong>: 1. a detection model for localizing and recognizing the pedestrians among other road users 2. a prediction model to estimate the pedestrian actions over next frames(short,medium,long-time prediction)</li>
<li><strong>Datashortcoming:</strong> there are no public databases annotated with pedestrian time to cross while there are several interesting huge pedestrian detection databases(Kitti,caltech,among others),and some databases don&rsquo;t provide any pedestrian action labels</li>
<li>Estimation of the pedestrian intention and especcially of the pedestrian  actions is even more challenging because of the <!-- raw HTML omitted -->ambiguities<!-- raw HTML omitted --> in pedestrian motions.</li>
</ul>
<p>previous work:</p>
<ul>
<li>pedestrian movement and pedestrian behaviors[13],[14],interacctions between pedestrians[15] [16] ,pedestrian tracking paths[9] ,[10],  a review of the predicting pedestrian behavior[12],<!-- raw HTML omitted -->pedestrian intention requires to use pedestrian specific dynamic information and contextual road environment<!-- raw HTML omitted --> ,in [17]  present a pedestrian action recognition based on <!-- raw HTML omitted -->AlexNet handling JAAD dataset and use temporal and spatial-temporal contextual information to increase the prediction perfomance<!-- raw HTML omitted --></li>
<li>[9] A pedestrian position estimation based on the<!-- raw HTML omitted --> Extended Kalman Filter and Interacting Multiple Model algorithm using Constant Velocity<!-- raw HTML omitted --></li>
<li>[18] combination of the Gaussian Precess Dynamic Models ,Probablistic Hierarchical Trajectory Machine with Kalman Filter and interacting Multiple Model-based on the Daimler Data.</li>
<li>[10] A short-term prediction of pedestian behaviors <!-- raw HTML omitted -->using Daimler datasets,to predict the pedestrian trajectory and its final destination using CNN base on LSTM and path planning<!-- raw HTML omitted -->.</li>
<li>[13] mixture of CNN based pedestrian detection tracking and pose estimation to predict the pedestrian crossing actions based on the JAAD dataset</li>
<li><strong>Summary</strong>:previous only discriminates between the pedetrian from the non-pedestrian among other road users and estimates the pedestrian action or its final destination for the next frames（short medium and long term) <!-- raw HTML omitted -->the Time to cross estimation of pedestrians is more challengin than predicting the pedestrian action since it requires contextual spatial-termprary:a fine analysis  of the pedestrian motion and the whole scene<!-- raw HTML omitted --></li>
</ul>
<h4 id="methods---------219">Methods         [2],[19]</h4>
<p>【定义问题0】no public databases anntated with pedestrian time to cross,the databases don&rsquo;t provide any pedestrian action labels?</p>
<p>we select some cues from the JAAD [1] public data set in order to solve this issue and then we made our pedestrian TTC annotation for all videos.  <!-- raw HTML omitted -->这个cue是指什么<!-- raw HTML omitted -->    JAAD 数据已经包括了 pedestrian bounding boxes for pedestrian detection and pedestrian attributes.</p>
<p>【定义问题1】how to detect pedestrian ?</p>
<p>Applying a generic object detector based on the public RetineNet[2], the author handled the Resnet50[19]CNN architecture for the classification task with the Keras public open-source implementation described in [2],all the training process is based on the JAAD dataset,which provides an annotation of pedestrians with behavioral tags and pedestrians without behaciors tags.</p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20191106112333433.png"
    data-srcset="https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20191106112333433.png, https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20191106112333433.png 1.5x, https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20191106112333433.png 2x"
    data-sizes="auto"
    alt="image-20191106112333433"
    title="image-20191106112333433"/></p>
<p>【定义问题2】how to split the pedestrian Joint Attention for Autonomous Driving into four class?   previous work</p>
<p>【定义问题3】how to estimate Time to cross?</p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20191106112423122.png"
    data-srcset="https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20191106112423122.png, https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20191106112423122.png 1.5x, https://gitee.com/github-25970295/blogpictureV2/raw/master/image-20191106112423122.png 2x"
    data-sizes="auto"
    alt="image-20191106112423122"
    title="image-20191106112423122"/></p>
<h4 id="evaluation-7">Evaluation</h4>
<ul>
<li><strong>Environment</strong>: dataset: JAAD dataset[17] provides pedestrian bounding boxes for pedestrian detection,pedestrian attributes for estimating the pedestrian behavior and traffic scene elements.</li>
</ul>
<h4 id="conclusion-8">Conclusion</h4>
<ul>
<li>Train all pedestrian Bounding Boxes samples with the <!-- raw HTML omitted -->RetinaNet<!-- raw HTML omitted --> for pedestrian detection purpose</li>
<li>Split the pedestrian Joint Attention for Autonomous Driving(<!-- raw HTML omitted -->JAAD<!-- raw HTML omitted -->) data set into four classes for pedestrian action functionality :pedestrian is preparing to cross the street ,pedestrian is crossing the street ,pedestrian is about to cross the street and pedestrian intention is ambiguous</li>
<li>Train <!-- raw HTML omitted -->LSTM<!-- raw HTML omitted --> model using only BB coordinates in order to estimate the time to cross of each pedestrian.</li>
</ul>
<h4 id="notes--font-colororange去加强了解font">Notes  <!-- raw HTML omitted -->去加强了解<!-- raw HTML omitted --></h4>
<ul>
<li><i class="fa-regular fa-square fa-fw" aria-hidden="true"></i> 了解 JAAD 数据集什么格式</li>
<li><i class="fa-regular fa-square fa-fw" aria-hidden="true"></i> 学习和运行RetinaNet网络</li>
<li><i class="fa-regular fa-square fa-fw" aria-hidden="true"></i> 学习何使用AlexNet 网洛</li>
<li><i class="fa-regular fa-check-square fa-fw" aria-hidden="true"></i> 了解LSTM网络</li>
<li><i class="fa-regular fa-square fa-fw" aria-hidden="true"></i> LSTM 网络运行使用</li>
<li><i class="fa-regular fa-square fa-fw" aria-hidden="true"></i> 论文 9，18，10中的方法</li>
<li><i class="fa-regular fa-square fa-fw" aria-hidden="true"></i> SSD 网络学习使用</li>
<li><i class="fa-regular fa-square fa-fw" aria-hidden="true"></i> Faster-RCNN网络学习使用</li>
<li><i class="fa-regular fa-square fa-fw" aria-hidden="true"></i> Yolo3网络学习使用</li>
</ul>
<h1 id="4感知系统">4.感知系统</h1>
<p><strong>level</strong>: Pro.ACM Interact.Mob   Wearable Ubiquitous Technol
<strong>author</strong>:  	karan ahuja Carnegie Mello University<br>
<strong>date</strong>: 2019 9
<strong>keyword</strong>:</p>
<ul>
<li>Human-centered computing ,Interactive systems and tools, Classroom sensing,Compute Vision,Speech</li>
</ul>
<hr>
<h2 id="paper-edusense">Paper: EduSense</h2>
<!-- raw HTML omitted -->
<h4 id="proble-statement-12">Proble Statement</h4>
<ul>
<li>need an expert to instructs   expensive</li>
<li>lack of sufficient feedback opportunities on pedagogical skill</li>
</ul>
<p>previous work:</p>
<ul>
<li>Instrumented Classrooms
<ul>
<li>instrumented with <!-- raw HTML omitted --> pressure sensors<!-- raw HTML omitted --> to characterize varying levels of interest and engagement,such as slumped back sitting upright.</li>
<li>Affectiva&rsquo;s wrist-worn Q sensor[62] which senses the wearer&rsquo;s skin conducance ,temperature and motion to infer engegement level.</li>
<li>EngageMeter[32] used electroencephalography headsets to detect shifts in student engagement,alertness and workload</li>
</ul>
</li>
<li>Non-Invasive Class Sensing
<ul>
<li>[19] an omnidirectional room microphone and head-mounted teacher microphone to automatically segment teacher and student speech events, intervals of silence.</li>
<li>Oral presentation practice systems AwareMe[11],Presentation Sensei[46],PoboCOP[75] compute speeck quality metrics (pitch variety,pauses fillers speaking rate)</li>
<li>Equally versatile using cameras ,detect hand rises,skin tone ,edge detection</li>
<li>Robust Face detection find and count student,estimate their  head orientation,coarsely signaling their area of focus,facial landmarks to analyse engagement,frustration,off-task behavior.</li>
</ul>
</li>
</ul>
<h4 id="methods-13">Methods</h4>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191112104005467.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191112104005467.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191112104005467.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191112104005467.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191112104005467.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191112104005467.png"/></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191112114633868.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191112114633868.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191112114633868.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191112114633868.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191112114633868.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191112114633868.png"/></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191112114906902.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191112114906902.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191112114906902.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191112114906902.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191112114906902.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191112114906902.png"/></p>
<p>【定义问题1】Featurization Modules</p>
<ul>
<li>
<p>Sit &amp; Stand detection: using body keypoints hips , knees , feet  and ratio of distances between chest and foot, the chest and knee both legs.</p>
</li>
<li>
<p>Hand Raise detection: neck ,chest, shoulder elbow wrist and compute the direction unit vectors btween all pairs of these points,and compute the distance between all pairs of points ,normalized by the distance between all pairs of these points.</p>
</li>
<li>
<p>Upper Body detection: utilize the same eight upper body keypoints  to predict arms at rest, arms at closed and hands on face.<img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191112115847153.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191112115847153.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191112115847153.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191112115847153.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191112115847153.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191112115847153.png"/></p>
</li>
<li>
<p>Smile Detection: ten mouth landmarks on the outer lip and ten landmarks on the inner lip. compute direction unit vectors from the left lip corner to all other points  SVM to binary classifaction.</p>
</li>
<li>
<p>Mouse Open Detection: estimate if a mouse is open,to produce the talking confidence.use a Binary SVM and two highly descriptive features adapted from [71 predict eys open &amp; closed],the height of the mouth to the left and right of center ,divided by the width of the mouth.</p>
</li>
<li>
<p>Head Oriention &amp; Class Gaze : Using a perspective-n-point algorithm[50] in combination with anthropometric face data[53],produces a coarse 3D orientation of the head for each body.</p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191112121139650.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191112121139650.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191112121139650.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191112121139650.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191112121139650.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191112121139650.png"/></p>
</li>
<li>
<p>Body Position &amp; Classroom Topology: perspective-n-point produces the orientation for each body founded in a scene ,and estimate 3D position in real world coordinates. to reveal the classroom topologies and help illuminate spatial patterns in the class.</p>
</li>
<li>
<p>Synthetic Accelerometer: uuse the 3D head position produced during scene parsing and claculate a delta X/Y/Z .</p>
</li>
<li>
<p>Student &amp; Instructor Speech: the RMS of the student-facing camera&rsquo;s microphone,the RMS of the instructor facing camera&rsquo;s microphone ,the ratio between the latter two values. uisng random forest classifier to predict the current speech is coming from the instructor or students.</p>
</li>
<li>
<p>Speech Act delimiting:</p>
</li>
</ul>
<h4 id="evaluation-8">Evaluation</h4>
<ul>
<li><strong>Environment</strong>:</li>
<li><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191112121150907.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191112121150907.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191112121150907.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191112121150907.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191112121150907.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191112121150907.png"/></li>
</ul>
<h4 id="conclusion-9">Conclusion</h4>
<ul>
<li>a comprehensive sensing system that produces a plethora of theoretically-motivated visual and audio features correlated with effective instruction.</li>
<li>the first to unify them into a cohesive real-time,in-the-wild evaluated and practically-deployable system.</li>
</ul>
<h4 id="notes-1">Notes</h4>
<ul>
<li><i class="fa-regular fa-square fa-fw" aria-hidden="true"></i> Classroom Discourse Analyzer [15] 了解这个系统</li>
<li><i class="fa-regular fa-square fa-fw" aria-hidden="true"></i> [19]了解下这篇文章</li>
<li><i class="fa-regular fa-square fa-fw" aria-hidden="true"></i> 了解下  Equally versatile 系统</li>
<li><i class="fa-regular fa-square fa-fw" aria-hidden="true"></i> 了解 CERT 技术</li>
<li><i class="fa-regular fa-square fa-fw" aria-hidden="true"></i> 了解下FFMPEG</li>
<li><i class="fa-regular fa-square fa-fw" aria-hidden="true"></i> NVIDIA Visual Profiler 技术是什么</li>
<li><i class="fa-regular fa-square fa-fw" aria-hidden="true"></i> 学习使用OpenPose   dlib64-point face landmarks【44】</li>
<li><i class="fa-regular fa-square fa-fw" aria-hidden="true"></i> adaptive background noise filter  to remove background noises how??</li>
<li><i class="fa-regular fa-square fa-fw" aria-hidden="true"></i> open source system  <a href="http://www.EduSense.io"target="_blank" rel="external nofollow noopener noreferrer">http://www.EduSense.io<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></li>
</ul>
<h1 id="5-骨骼提取">5. 骨骼提取</h1>
<h2 id="51-40个骨骼提取开源项目httpsmpweixinqqcoms__bizmzi5mduymdixnamid2247507854idx2sne02294c31e6867bf2e4270178c2a75e8chksmec1c3277db6bbb612de6d5b3edaa2a4fb8da19e7b6e62b10feb6c21d8fddc748e570ef05c050scene126sessionid1600264884key3542bed875d644de951ff14ae71a83001ab1e1812ce7aa66a998b68fd92197d07eb6ed465593d3aac71554ab7ccfb47f11533fe51eec433dba65046ba6244a25e8050051445366bb86635b10cd4bcf9f1c9220c9515042c7795e056f147b995b4688cde692c65c611ca97ef9d78c191310ee8ebb1c30e3cd4a12b77aa5d24fe7ascene1uinmze0odmxotqzmq3d3ddevicetypewindows10x64version62090529langzh_cnexportkeyax1vpaqmptdpfiw823ezgry3dpass_tickettfc86xzy4b6esrk2fasnypqs4p0qrnxfr4rzkdh4co2fpl3pb2ehbommndjmdtvipdwx_header0">5.1. <a href="https://mp.weixin.qq.com/s?__biz=MzI5MDUyMDIxNA==&amp;mid=2247507854&amp;idx=2&amp;sn=e02294c31e6867bf2e4270178c2a75e8&amp;chksm=ec1c3277db6bbb612de6d5b3edaa2a4fb8da19e7b6e62b10feb6c21d8fddc748e570ef05c050&amp;scene=126&amp;sessionid=1600264884&amp;key=3542bed875d644de951ff14ae71a83001ab1e1812ce7aa66a998b68fd92197d07eb6ed465593d3aac71554ab7ccfb47f11533fe51eec433dba65046ba6244a25e8050051445366bb86635b10cd4bcf9f1c9220c9515042c7795e056f147b995b4688cde692c65c611ca97ef9d78c191310ee8ebb1c30e3cd4a12b77aa5d24fe7&amp;ascene=1&amp;uin=MzE0ODMxOTQzMQ%3D%3D&amp;devicetype=Windows&#43;10&#43;x64&amp;version=62090529&amp;lang=zh_CN&amp;exportkey=Ax1vPAqMPtdpFIw823EzgRY%3D&amp;pass_ticket=TfC86Xzy4b6ESRk%2FasnYpQs4p0qrNXFR4RzKdh4co%2FPl3pb2EHboMmNDJmdTviPd&amp;wx_header=0"target="_blank" rel="external nofollow noopener noreferrer">40个骨骼提取开源项目<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></h2>
<p><strong>level</strong>: CVPR  CCF A
<strong>author</strong>: Zhe Cao
<strong>date</strong>: &lsquo;2019-5-30&rsquo;
<strong>keyword</strong>:</p>
<ul>
<li>2D human pose estimate ，2D foot keypoint estimate，real time，multiple person，part affinity fields</li>
</ul>
<hr>
<h2 id="paper-openpose">Paper: OpenPose</h2>
<!-- raw HTML omitted -->
<h4 id="summary-9">Summary</h4>
<ul>
<li>prove PAF refinement is critical and sufficient for high accuracy ,removing the body part confidence map refinement while increasing the network depth.</li>
<li>using body and foot key-point detector.</li>
<li>Open-Pose library</li>
</ul>
<h4 id="research-objective-12">Research Objective</h4>
<ul>
<li><strong>Application Area</strong>: key-point detect eg: body skeleton ,hand skeleton ,face skeleton,hand skeleton ,the body part location for futher explored like prediction</li>
<li><strong>Purpose</strong>:  using part affinity fields to real-time calculate multiperson 2D pose.</li>
</ul>
<h4 id="problem-statement-1">Problem Statement</h4>
<ul>
<li>each image may contain an <!-- raw HTML omitted -->unknown number<!-- raw HTML omitted --> of people that can appear at any <!-- raw HTML omitted -->position or scale<!-- raw HTML omitted --></li>
<li><!-- raw HTML omitted -->interactions<!-- raw HTML omitted --> between people induce <!-- raw HTML omitted --> complex spatial interference due to contact occlusion or limb articulations,making association of parts difficult<!-- raw HTML omitted --></li>
<li>runtime complexity tends to grow with the number of people</li>
</ul>
<p>previous work:</p>
<ul>
<li>
<p><strong>Single Person Pose Estimation</strong>: perform inference over a combination of local observations on body parts and the spatial dependencies . the spatial model for articulated pose is either based on <!-- raw HTML omitted -->tree-structured graphical models<!-- raw HTML omitted -->, which parametrically encode the spatial relationship between adjacent parts following a kinematic chain or not tree models that <!-- raw HTML omitted -->augment the tree structure with additional edges <!-- raw HTML omitted -->to capture occlusion symmetry and long range relationship.</p>
<ul>
<li>[34] used a multi-stage architecture based on a sequential prediction framework, incorporating global context to refine part confidence maps and preserving multi-modal uncertainty form previous iterations.</li>
<li><!-- raw HTML omitted -->all this methods assume a single person ,where the location and scale of the person of interest is given.<!-- raw HTML omitted --></li>
</ul>
</li>
<li>
<p><strong>Multi-person Pose estimation</strong>: <!-- raw HTML omitted -->Top-down<!-- raw HTML omitted --> strategy that first detects people and then have estimated the pose of each person independently on  detected region.<!-- raw HTML omitted -->suffers form early commitment  on person detection,fail to capture the spatial dependencies across different people<!-- raw HTML omitted --> .<!-- raw HTML omitted -->bottom up<!-- raw HTML omitted --> approach that jointly labels part detection candidates and associated them to individual people,with pairwise scores regressed from spatial offsets of detections</p>
<ul>
<li>[47] further simplified their body-part relationship graph for faster inference in single-frame model and formulated articulated human tracking as spatial-temporal grouping of part proposals.</li>
<li>[49] detect individual key-points and predict their relative displacements,using greedy decoding method.</li>
</ul>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191111163031878.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191111163031878.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191111163031878.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191111163031878.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191111163031878.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191111163031878.png"/></p>
</li>
</ul>
<h4 id="methods-14">Methods</h4>
<p><strong>system overview</strong>: given a color image of size w*h ,produces the 2D positions of anatomical key-points for each person in the image.</p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191111163106168.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191111163106168.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191111163106168.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191111163106168.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191111163106168.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191111163106168.png"/></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191111182334970.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191111182334970.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191111182334970.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191111182334970.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191111182334970.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191111182334970.png"/></p>
<p>【Question1】how to detect limbs and bodypart?</p>
<p>define <!-- raw HTML omitted --> body part location S<!-- raw HTML omitted -->:<br>
$$
S=(S_1,S_2,&hellip;,S_j)  ,S_j\varepsilon R^{w<em>h</em>2}
$$
define <!-- raw HTML omitted -->vetor field of PAFS L<!-- raw HTML omitted -->:
$$
L=(L_1,L_2,&hellip;,L_j) , L_c represent a limb,L_C\varepsilon R^{w<em>h</em>2}
$$</p>
<p>$$
Image-&gt;\frac{VGG-19}{CNN}-&gt;asetofeature mapsF\frac{stage\quad t,(t&lt;T_p)}{limbs}&gt;\begin{cases} L^1=\phi^1(F),t=1\ L^t=\phi ^t(F,L^{t-1}),\forall 2\leq t\leq T_p \end{cases}\quad \frac{}{bodyPartLocation}&gt;\begin{cases} S^T_p=\rho(F,L^T_p),\forall t\varepsilon T_p\ S^t=\rho(F,L^{T_p},S^{t-1}),\forall T_p\le t\leq T_p+c \end{cases}
$$</p>
<p><!-- raw HTML omitted -->define Loss functions<!-- raw HTML omitted -->:  w is a binary mask with  W(p)=0 when annotation is missing.
$$
L_{stage}: f_L^{t_i}=\sum_{c=1}^{C}\sum_Pw(p)<em>||L_c^{ti}(p)-L_c^</em>(p)||^2 \
S_{stage}: f_S^{t_i}=\sum_{j=1}^{J}\sum_Pw(p)<em>||S_j^{ti}(p)-S_j^</em>(p)||^2
$$
<!-- raw HTML omitted -->For vanishing gradient <!-- raw HTML omitted -->:
$$
f=\sum_{i=1}^{T_p}f_L^t+\sum_{t=T_p+1}^{T_p+T_c}f_S^t
$$
confidence map: maximum   ,belief a particularly body part can be located at any given pixel</p>
<p>obtain body part conditions: Non-maximum suppression</p>
<p><!-- raw HTML omitted -->confidence mpas:<!-- raw HTML omitted -->  $S_{j,k}^* $   :  individual maps for each person k ,  $X_{j,k}\epsilon R^2$ ground truth,body part j for person k</p>
<p>for calculate the confidence body part: the value at location $p\epsilon R^2 \quad S_{j,k}^<em>(p)=exp(-\frac{||p-x_{j,k}||^2}{\rho^2})$  ,$S_j^</em>(p)=max_kS_{j,k}^*(p)$</p>
<p>【Question2】Given a set of detected body parts ,how do we assemble them to form the full-body poses o fan unknown number of people？<!-- raw HTML omitted --> ecodes both the location and orientation, don’t reduce the region of support of a limbs to a single point<!-- raw HTML omitted --></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191129113237565.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191129113237565.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191129113237565.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191129113237565.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191129113237565.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191129113237565.png"/></p>
<p>$X_{j_1,k} ,X_{j_2,k}$  : the groundtruth positions of body parts j1,j2 from the limb c.
$$
L_{c,k}^<em>(p)=\begin{cases} v \quad if \quad p \quad on \quad limb c,k \0 \quad otherwise\end{cases}, \frac {v=(x_{j2,k}-x_{j1,k})/||x_{j_2,k}-x_{j_1,k}||^2}{0\leq v</em>(p-x_{j_1,k})\leq l_{c,k} and |v\bot <em>(p-x_{j_1,k)})|\leq \rho_l}
$$
ground-truth part affinity field : $L_c^</em>(p)=\frac{1}{n_c(p)}\sum_kL_{c,k}^*(p)\quad n_c(p)$:is the number of non-zero vectors at point p across all k people.</p>
<p>eg: for two candidate part locations $d_{j_1}\quad d_{j2}$,we sample the predicted part affinity field, Lc alone the line segment to measure the confidence in their association:$E=\int_{u=0}^{u=1}{L_c(p(u))*\frac {d_{j2}-d{j1}}{||d_j2-d_j1||^2}du}dx$     where p(u) interpolates the position of two body parts dj1,dj2 ,$p(u)=(1-u)d_{j1}+ud_{j2}$</p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191125091725286-1577068664572.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191125091725286-1577068664572.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191125091725286-1577068664572.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191125091725286-1577068664572.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191125091725286-1577068664572.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191125091725286-1577068664572.png"/></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191129115349521.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191129115349521.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191129115349521.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191129115349521.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191129115349521.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191129115349521.png"/></p>
<p>a set of body part detection candidates Dj for multiple people,where $D_j={d_j^m :for\quad j\epsilon {1&hellip;J},m\epsilon {1,&hellip;,N_j}} $ where Nj is the number of candidates of part j, and $d_j^m\epsilon R^2$ is the location of m-th detection candidate of body part</p>
<p>define $z_{j_1,j_2}^{mn}\epsilon (0,1)$ to indicate wether two detection candidates $d_{j_1}^m ,d_{j_2}^n $ are connected ,the goal is to find optimal assignment for the rest set of all possible connections ,
$$
Z={z_{j_1j_2}^{mn}:for \quad j_1,j_2\epsilon (1,&hellip;,J),m\epsilon(1,&hellip;,N_{j_1}),n\epsilon(1,&hellip;,N_{j_2})}​
$$
<img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191129121106421.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191129121106421.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191129121106421.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191129121106421.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191129121106421.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191129121106421.png"/></p>
<ol>
<li>choose the minimal number of edges to obtain a spanning tree skeleton of human pose rather than using the complete graph</li>
<li>decompose the matching problem into a set of bipartite matching subproblems and determine the matching in adjacent tree nodes independently.</li>
</ol>
<h4 id="evaluation-9">Evaluation</h4>
<ul>
<li><strong>Environment</strong>:
<ul>
<li>datasets : MPII human multi-person dataset[66] consisting of 3844 training and 1758 testing groups of multiple interacting individuals in highly articulated pose with 14 body parts, COCO keypoint challenge dataset  requires simultaneously detecting people and localizing 17 key-points</li>
</ul>
</li>
</ul>
<h4 id="conclusion-10">Conclusion</h4>
<ul>
<li>present an explicit nonparametric representation of the key-point association that encodes <!-- raw HTML omitted -->both position and orientation of human limbs<!-- raw HTML omitted --></li>
<li>design an architecture that jointly <!-- raw HTML omitted -->learns part detection and association <!-- raw HTML omitted --></li>
<li>demonstrate that <!-- raw HTML omitted --> a greedy parsing algorithm is sufficient to produce high-quality parses of body poses and preserves efficiency regardless of the number of people <!-- raw HTML omitted --></li>
<li>prove that PAF and body part location refinement is far more important that combined PAF and body part location refinement</li>
<li>combining body and foot estimation into a single model boosts thee accuracy of each component individually and reduces the inference time of running them sequentially</li>
<li>open-sourced OpenPose system and included in OpenCV library.</li>
</ul>
<h4 id="notes---font-colororange去加强了解font--记录下以下论文">Notes   <!-- raw HTML omitted -->去加强了解<!-- raw HTML omitted -->  记录下以下论文</h4>
<ul>
<li><i class="fa-regular fa-square fa-fw" aria-hidden="true"></i> 学习何使用OpenPose system  ,进行到一般,模型文件需要下载</li>
<li><i class="fa-regular fa-check-square fa-fw" aria-hidden="true"></i> [20] Convolutional pose machines,   DenseNet[52]Densely connected convolutional networks, [3] Realtime multi-person 2d pose estimation using part affinity fields,  <!-- raw HTML omitted --> 大致了解了网络,需要几个代码了解下运行效果<!-- raw HTML omitted --></li>
<li><i class="fa-regular fa-check-square fa-fw" aria-hidden="true"></i> 学习和使用 Mask R-CNN[5]  <!-- raw HTML omitted -->need to practice with code<!-- raw HTML omitted --></li>
<li><i class="fa-regular fa-check-square fa-fw" aria-hidden="true"></i> 学习和使用Alpha-Pose[6]</li>
<li><i class="fa-regular fa-check-square fa-fw" aria-hidden="true"></i> ResNet[46] 学习使用    <!-- raw HTML omitted -->need to practice with code<!-- raw HTML omitted --></li>
<li><i class="fa-regular fa-square fa-fw" aria-hidden="true"></i> [2] pairwise representations   了解下是什么</li>
<li><i class="fa-regular fa-square fa-fw" aria-hidden="true"></i> 论文34 ,47,49 ,[50]需要学习了解</li>
<li><i class="fa-regular fa-square fa-fw" aria-hidden="true"></i> [49] detect individual key-points and predict their relative displacement allowing a greedy decoding process to group keypoints.</li>
<li><i class="fa-regular fa-square fa-fw" aria-hidden="true"></i> 学习和了解VGG-19【53】模型并学会使用fine-tuned方法</li>
<li><i class="fa-regular fa-check-square fa-fw" aria-hidden="true"></i> Hungrian ALgorithm 用于解决二分图匹配问题</li>
</ul>
<p>level**: CCF_A   CVPR</p>
<p><strong>author</strong>: ChaoLi,QiaoyongZhong,DiXie,ShiliangPu HikvisionResearchInstitute</p>
<p><strong>date</strong>: 2018 4.17</p>
<p><strong>keyword</strong>:</p>
<ul>
<li>skeleton based action recognition</li>
</ul>
<hr>
<h2 id="paper-co-occurence-feature">Paper: Co-occurence Feature</h2>
<!-- raw HTML omitted -->
<ol>
<li>point-level information of each joint is encoded independently,and then assembled into semantic representation  in both spatial and temporal domains</li>
<li>independent point-level features learning  and cross joint co-occurrence feature learning</li>
</ol>
<h4 id="research-objective-13">Research Objective</h4>
<ul>
<li><strong>Application Area</strong>:  intelligent surveillance system, human-computer interaction, game-control and robotics.
<ul>
<li>skeleton provides good representation for describing human actions</li>
<li>skeleton data are inherently robust against background noise and provide abstract information and high-level features of human action</li>
<li>compared with RGB data, skeleton data are extremely small in size</li>
</ul>
</li>
<li><strong>Purpose</strong>:</li>
</ul>
<h4 id="proble-statement-13">Proble Statement</h4>
<p>previous work:</p>
<ul>
<li>design and extract co-occurrence features form skeleton sequences
<ul>
<li>pairwise relative position of each joint</li>
<li>spatial orientation of pair wise joints</li>
<li>statistics-based feature like Conv3Dj   HOJ3D</li>
</ul>
</li>
<li>RNN with LSTM to model the time series of skeleton prevalently</li>
<li>CNN models to learn spatial-temporal features from skeletons
<ul>
<li>cast the frame, joint, and coordinate dimensions of skeleton sequence into width, height, and channel of an image respectively  [Du et al., 2016]</li>
<li>3D coordinates are separated into three gray-scale images [Ke et al.,2017]</li>
<li>a new skeleton transformer module to incorporate skeleton motion features [Li et al., 2017b]</li>
<li>Shortcoming:   the co-occurrence feature are aggregated locally, which may not be able to capture the long-range joint interactions involved in actions like wearing ..</li>
</ul>
</li>
</ul>
<h4 id="methods-15">Methods</h4>
<ul>
<li><strong>system overview</strong>:</li>
</ul>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200229112040352.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200229112040352.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200229112040352.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200229112040352.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200229112040352.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200229112040352.png"/></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200229115749562.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200229115749562.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200229115749562.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200229115749562.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200229115749562.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200229115749562.png"/></p>
<p><strong>[Input Define]</strong></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200229114736296.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200229114736296.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200229114736296.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200229114736296.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200229114736296.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200229114736296.png"/></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200229114716246.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200229114716246.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200229114716246.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200229114716246.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200229114716246.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200229114716246.png"/></p>
<p><strong>[Multiple Persons]</strong></p>
<ul>
<li>early fusion: all joints from multiple persons are stacked as input of the network, zero padding if less than pre-defined maximal number</li>
<li>Late fusion:  input of multiple persons go through the same subnetwork and their conv6 features maps are merged with either concatenation along channels or element-wise maximum/mean operation</li>
</ul>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200229114756753.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200229114756753.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200229114756753.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200229114756753.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200229114756753.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200229114756753.png"/></p>
<p><strong>[Loss Function Define]</strong></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200229115411034.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200229115411034.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200229115411034.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200229115411034.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200229115411034.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200229115411034.png"/></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200229115422855.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200229115422855.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200229115422855.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200229115422855.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200229115422855.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200229115422855.png"/></p>
<h4 id="evaluation-10">Evaluation</h4>
<ul>
<li><strong>Environment</strong>:
<ul>
<li>Dataset:</li>
</ul>
</li>
<li><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200229115851023.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200229115851023.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200229115851023.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200229115851023.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200229115851023.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200229115851023.png"/></li>
<li><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200229115907932.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200229115907932.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200229115907932.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200229115907932.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200229115907932.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200229115907932.png"/></li>
</ul>
<h4 id="conclusion-11">Conclusion</h4>
<ul>
<li>CNN model for learning global co-occurrences from skeleton data</li>
<li>end-to-end hierarchical feature learning network, where features are aggregated gradually from point level features to global co-occurrence features</li>
<li>exploit multi-person feature fusion strategies</li>
</ul>
<h4 id="notes-font-colororange去加强了解font-9">Notes <!-- raw HTML omitted -->去加强了解<!-- raw HTML omitted --></h4>
<ul>
<li><i class="fa-regular fa-square fa-fw" aria-hidden="true"></i> recognition and detection benchmarks <!-- raw HTML omitted -->NTU RGB+D,SBU kinect Interaction and PKU-MMD<!-- raw HTML omitted --></li>
<li><i class="fa-regular fa-square fa-fw" aria-hidden="true"></i> Learning actionlet ensemble for 3d human actionrecognition  2014</li>
<li><i class="fa-regular fa-square fa-fw" aria-hidden="true"></i> Essential body-joint and atomic action detection for human activity recognition using longest common subsequence algorithm   2012</li>
<li><i class="fa-regular fa-square fa-fw" aria-hidden="true"></i> A NewRepresentationofSkeletonSequencesfor3DAction Recognition   2017</li>
<li><i class="fa-regular fa-square fa-fw" aria-hidden="true"></i> Co-occurrence feature learning for skeleton based action recognition using regularized deep lstm networks   2016</li>
<li><i class="fa-regular fa-square fa-fw" aria-hidden="true"></i> PKU-MMD: A large scale benchmark for continuous multi-modal human action understanding.  2017</li>
<li><i class="fa-regular fa-square fa-fw" aria-hidden="true"></i> Two-stream convolutional networks for action recognition in videos   2014</li>
<li><i class="fa-regular fa-square fa-fw" aria-hidden="true"></i> window regression   Girshick et al., 2014</li>
<li><i class="fa-regular fa-square fa-fw" aria-hidden="true"></i> Cascade region proposal and global context for deep object detection  2016</li>
<li><i class="fa-regular fa-square fa-fw" aria-hidden="true"></i> An end-to-end spatiotemporal attention model for human action recognition fromskeletondata   2017 AAAi</li>
</ul>
<p><strong>level</strong>:
<strong>author</strong>:
<strong>date</strong>:  2016
<strong>keyword</strong>:</p>
<ul>
<li>point detection; heatmap</li>
</ul>
<hr>
<h1 id="paper-stacked-hourglass">Paper: Stacked Hourglass</h1>
<!-- raw HTML omitted -->
<h4 id="summary-10">Summary</h4>
<ol>
<li>On MPII there is over a 2% average accuracy improvement across all joints, with as much as a 4-5% improvement on more difficult joints like the knees and ankles.</li>
<li>propose  a stacked hourglass networks for human pose estimation;</li>
</ol>
<h4 id="methods-16">Methods</h4>
<ul>
<li><strong>system overview</strong>:</li>
</ul>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201019210140895.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201019210140895.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201019210140895.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201019210140895.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201019210140895.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201019210140895.png"/></p>
<p>【**Single Hourglass module **】</p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201019210700629.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201019210700629.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201019210700629.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201019210700629.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201019210700629.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201019210700629.png"/></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201019210520022.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201019210520022.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201019210520022.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201019210520022.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201019210520022.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201019210520022.png"/></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201020134658441.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201020134658441.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201020134658441.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201020134658441.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201020134658441.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201020134658441.png"/></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e">#coding=utf-8</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> torch
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> torch.nn <span style="color:#66d9ef">as</span> nn
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> torch.nn <span style="color:#f92672">import</span> Upsample
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> torch.autograd <span style="color:#f92672">import</span> Variable
</span></span><span style="display:flex;"><span><span style="color:#75715e">#https://sourcegraph.com/github.com/raymon-tian/hourglass-facekeypoints-detection/-/blob/models.py</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">HourGlass</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;&#34;&#34;不改变特征图的高宽&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self,n<span style="color:#f92672">=</span><span style="color:#ae81ff">4</span>,f<span style="color:#f92672">=</span><span style="color:#ae81ff">128</span>):
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        :param n: hourglass模块的层级数目
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        :param f: hourglass模块中的特征图数量
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        :return:
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>        super(HourGlass,self)<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>_n <span style="color:#f92672">=</span> n
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>_f <span style="color:#f92672">=</span> f
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>_init_layers(self<span style="color:#f92672">.</span>_n,self<span style="color:#f92672">.</span>_f)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">_init_layers</span>(self,n,f):
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># 上分支</span>
</span></span><span style="display:flex;"><span>        setattr(self,<span style="color:#e6db74">&#39;res&#39;</span><span style="color:#f92672">+</span>str(n)<span style="color:#f92672">+</span><span style="color:#e6db74">&#39;_1&#39;</span>,Residual(f,f))
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># 下分支</span>
</span></span><span style="display:flex;"><span>        setattr(self,<span style="color:#e6db74">&#39;pool&#39;</span><span style="color:#f92672">+</span>str(n)<span style="color:#f92672">+</span><span style="color:#e6db74">&#39;_1&#39;</span>,nn<span style="color:#f92672">.</span>MaxPool2d(<span style="color:#ae81ff">2</span>,<span style="color:#ae81ff">2</span>))
</span></span><span style="display:flex;"><span>        setattr(self,<span style="color:#e6db74">&#39;res&#39;</span><span style="color:#f92672">+</span>str(n)<span style="color:#f92672">+</span><span style="color:#e6db74">&#39;_2&#39;</span>,Residual(f,f))
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> n <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">1</span>:
</span></span><span style="display:flex;"><span>            self<span style="color:#f92672">.</span>_init_layers(n<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,f)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>            self<span style="color:#f92672">.</span>res_center <span style="color:#f92672">=</span> Residual(f,f)
</span></span><span style="display:flex;"><span>        setattr(self,<span style="color:#e6db74">&#39;res&#39;</span><span style="color:#f92672">+</span>str(n)<span style="color:#f92672">+</span><span style="color:#e6db74">&#39;_3&#39;</span>,Residual(f,f))
</span></span><span style="display:flex;"><span>        setattr(self,<span style="color:#e6db74">&#39;unsample&#39;</span><span style="color:#f92672">+</span>str(n),Upsample(scale_factor<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">_forward</span>(self,x,n,f):
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># 上分支</span>
</span></span><span style="display:flex;"><span>        up1 <span style="color:#f92672">=</span> x
</span></span><span style="display:flex;"><span>        up1 <span style="color:#f92672">=</span> eval(<span style="color:#e6db74">&#39;self.res&#39;</span><span style="color:#f92672">+</span>str(n)<span style="color:#f92672">+</span><span style="color:#e6db74">&#39;_1&#39;</span>)(up1)
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># 下分支</span>
</span></span><span style="display:flex;"><span>        low1 <span style="color:#f92672">=</span> eval(<span style="color:#e6db74">&#39;self.pool&#39;</span><span style="color:#f92672">+</span>str(n)<span style="color:#f92672">+</span><span style="color:#e6db74">&#39;_1&#39;</span>)(x)
</span></span><span style="display:flex;"><span>        low1 <span style="color:#f92672">=</span> eval(<span style="color:#e6db74">&#39;self.res&#39;</span><span style="color:#f92672">+</span>str(n)<span style="color:#f92672">+</span><span style="color:#e6db74">&#39;_2&#39;</span>)(low1)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> n <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">1</span>:
</span></span><span style="display:flex;"><span>            low2 <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>_forward(low1,n<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,f)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>            low2 <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>res_center(low1)
</span></span><span style="display:flex;"><span>        low3 <span style="color:#f92672">=</span> low2
</span></span><span style="display:flex;"><span>        low3 <span style="color:#f92672">=</span> eval(<span style="color:#e6db74">&#39;self.&#39;</span><span style="color:#f92672">+</span><span style="color:#e6db74">&#39;res&#39;</span><span style="color:#f92672">+</span>str(n)<span style="color:#f92672">+</span><span style="color:#e6db74">&#39;_3&#39;</span>)(low3)
</span></span><span style="display:flex;"><span>        up2 <span style="color:#f92672">=</span> eval(<span style="color:#e6db74">&#39;self.&#39;</span><span style="color:#f92672">+</span><span style="color:#e6db74">&#39;unsample&#39;</span><span style="color:#f92672">+</span>str(n))<span style="color:#f92672">.</span>forward(low3)
</span></span><span style="display:flex;"><span>        print(up1<span style="color:#f92672">.</span>shape,up2<span style="color:#f92672">.</span>shape)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> up1<span style="color:#f92672">+</span>up2
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self,x):
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> self<span style="color:#f92672">.</span>_forward(x,self<span style="color:#f92672">.</span>_n,self<span style="color:#f92672">.</span>_f)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">Residual</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    残差模块，并不改变特征图的宽高
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self,ins,outs):
</span></span><span style="display:flex;"><span>        super(Residual,self)<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># 卷积模块</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>convBlock <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Sequential(
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>BatchNorm2d(ins),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>ReLU(inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Conv2d(ins,outs<span style="color:#f92672">//</span><span style="color:#ae81ff">2</span>,<span style="color:#ae81ff">1</span>),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>BatchNorm2d(outs<span style="color:#f92672">//</span><span style="color:#ae81ff">2</span>),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>ReLU(inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Conv2d(outs<span style="color:#f92672">//</span><span style="color:#ae81ff">2</span>,outs<span style="color:#f92672">//</span><span style="color:#ae81ff">2</span>,<span style="color:#ae81ff">3</span>,<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>BatchNorm2d(outs<span style="color:#f92672">//</span><span style="color:#ae81ff">2</span>),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>ReLU(inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Conv2d(outs<span style="color:#f92672">//</span><span style="color:#ae81ff">2</span>,outs,<span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># 跳层</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> ins <span style="color:#f92672">!=</span> outs:
</span></span><span style="display:flex;"><span>            self<span style="color:#f92672">.</span>skipConv <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Conv2d(ins,outs,<span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>ins <span style="color:#f92672">=</span> ins
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>outs <span style="color:#f92672">=</span> outs
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self,x):
</span></span><span style="display:flex;"><span>        residual <span style="color:#f92672">=</span> x
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>convBlock(x)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> self<span style="color:#f92672">.</span>ins <span style="color:#f92672">!=</span> self<span style="color:#f92672">.</span>outs:
</span></span><span style="display:flex;"><span>            residual <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>skipConv(residual)
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">+=</span> residual
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> x
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">Lin</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self,numIn<span style="color:#f92672">=</span><span style="color:#ae81ff">128</span>,numout<span style="color:#f92672">=</span><span style="color:#ae81ff">4</span>):
</span></span><span style="display:flex;"><span>        super(Lin,self)<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>conv <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Conv2d(numIn,numout,<span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>bn <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>BatchNorm2d(numout)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>relu <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>ReLU(inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self,x):
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> self<span style="color:#f92672">.</span>relu(self<span style="color:#f92672">.</span>bn(self<span style="color:#f92672">.</span>conv(x)))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">KFSGNet</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self):
</span></span><span style="display:flex;"><span>        super(KFSGNet,self)<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>__conv1 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Conv2d(<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">64</span>,<span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>__relu1 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>ReLU(inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>__conv2 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Conv2d(<span style="color:#ae81ff">64</span>,<span style="color:#ae81ff">128</span>,<span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>__relu2 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>ReLU(inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>__hg <span style="color:#f92672">=</span> HourGlass()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>__lin <span style="color:#f92672">=</span> Lin()
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self,x):
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>__relu1(self<span style="color:#f92672">.</span>__conv1(x))
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>__relu2(self<span style="color:#f92672">.</span>__conv2(x))
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>__hg(x)
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>__lin(x)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> x
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> torch.utils.data <span style="color:#f92672">import</span> Dataset,DataLoader
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> numpy <span style="color:#66d9ef">as</span> np
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> torch.optim <span style="color:#66d9ef">as</span> optim
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">tempDataset</span>(Dataset):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self):
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>X <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>randn(<span style="color:#ae81ff">16</span>,<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">512</span>, <span style="color:#ae81ff">512</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>Y <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>randn(<span style="color:#ae81ff">16</span>,<span style="color:#ae81ff">4</span>,<span style="color:#ae81ff">512</span>, <span style="color:#ae81ff">512</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __len__(self):
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> len(self<span style="color:#f92672">.</span>X)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __getitem__(self, item):
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># 这里返回的时候不要设置batch_size</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> self<span style="color:#f92672">.</span>X[item],self<span style="color:#f92672">.</span>Y[item]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">if</span> __name__ <span style="color:#f92672">==</span> <span style="color:#e6db74">&#39;__main__&#39;</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">from</span> torch.nn <span style="color:#f92672">import</span> MSELoss
</span></span><span style="display:flex;"><span>    critical <span style="color:#f92672">=</span> MSELoss()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    dataset <span style="color:#f92672">=</span> tempDataset()
</span></span><span style="display:flex;"><span>    dataLoader <span style="color:#f92672">=</span> DataLoader(dataset<span style="color:#f92672">=</span>dataset,batch_size<span style="color:#f92672">=</span><span style="color:#ae81ff">64</span>)
</span></span><span style="display:flex;"><span>    shg <span style="color:#f92672">=</span> KFSGNet()<span style="color:#f92672">.</span>cuda()
</span></span><span style="display:flex;"><span>    optimizer <span style="color:#f92672">=</span> optim<span style="color:#f92672">.</span>SGD(shg<span style="color:#f92672">.</span>parameters(), lr<span style="color:#f92672">=</span><span style="color:#ae81ff">0.001</span>, momentum<span style="color:#f92672">=</span><span style="color:#ae81ff">0.9</span>,weight_decay<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-4</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> e <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">200</span>):
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> i,(x,y) <span style="color:#f92672">in</span> enumerate(dataLoader):
</span></span><span style="display:flex;"><span>            x <span style="color:#f92672">=</span> Variable(x,requires_grad<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)<span style="color:#f92672">.</span>float()<span style="color:#f92672">.</span>cuda()
</span></span><span style="display:flex;"><span>            y <span style="color:#f92672">=</span> Variable(y)<span style="color:#f92672">.</span>float()<span style="color:#f92672">.</span>cuda()
</span></span><span style="display:flex;"><span>            y_pred <span style="color:#f92672">=</span> shg<span style="color:#f92672">.</span>forward(x)
</span></span><span style="display:flex;"><span>            <span style="color:#75715e">#print(y_pred.shape,y.shape)</span>
</span></span><span style="display:flex;"><span>            loss <span style="color:#f92672">=</span> critical(y_pred[<span style="color:#ae81ff">0</span>],y[<span style="color:#ae81ff">0</span>])
</span></span><span style="display:flex;"><span>            <span style="color:#75715e">#print(&#39;loss : {}&#39;.format(loss.data))</span>
</span></span><span style="display:flex;"><span>            optimizer<span style="color:#f92672">.</span>zero_grad()
</span></span><span style="display:flex;"><span>            loss<span style="color:#f92672">.</span>backward()
</span></span><span style="display:flex;"><span>            optimizer<span style="color:#f92672">.</span>step()
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">if</span> i<span style="color:#f92672">&gt;</span><span style="color:#ae81ff">2</span>:
</span></span><span style="display:flex;"><span>                <span style="color:#66d9ef">break</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">break</span>
</span></span></code></pre></div><p><strong>level</strong>: CVPR
<strong>author</strong>: Shih-En Wei
<strong>date</strong>:
<strong>keyword</strong>:</p>
<ul>
<li>skeleton extract</li>
</ul>
<h2 id="paper-cpm">Paper: CPM</h2>
<!-- raw HTML omitted -->
<h4 id="summary-11">Summary</h4>
<ol>
<li>show a systematic design for how convolutional networks can be incorporated into the pose machine frame-work for learning image features and image-dependent spatial models for the task of pose estimation.</li>
<li><!-- raw HTML omitted -->CPM: consists of a sequence of convolutional networks that repeatly produce 2D belief maps for the location of each part, at each stage in a CPM, image features and the belief maps produced by the previous stage are used as input<!-- raw HTML omitted -->
<ol>
<li>learn feature representations for both image and spatial context directly from data</li>
<li>a different architecture that allows for globally joint trainning with back propagation</li>
<li>efficiently handle large training datasets</li>
</ol>
</li>
<li><!-- raw HTML omitted -->large receptive fields on the belief maps are crucial for learning long range spatial relationships and result in improved accuracy.<!-- raw HTML omitted --></li>
</ol>
<h4 id="research-object">Research Object</h4>
<p>previous work:</p>
<ul>
<li>
<p>classic approach:</p>
<ul>
<li><!-- raw HTML omitted -->pictorial structures:<!-- raw HTML omitted -->  spatial correlations between parts of the body are expressed as a tree-structured graphical model with kinectmatic priors that couple connected limbs.</li>
<li><!-- raw HTML omitted -->Hierarchical models:<!-- raw HTML omitted --> represent relationships between parts at different scales and size in a hierarchical tree structure.</li>
<li><!-- raw HTML omitted -->Non-tree models:<!-- raw HTML omitted --> incorporate interactions that introduce loops to augment the tree structure with additional edges that capture symmetry, occlusion and long-range relationship.<!-- raw HTML omitted -->rely on approximate inference<!-- raw HTML omitted --></li>
<li><!-- raw HTML omitted -->sequential prediction:<!-- raw HTML omitted --> learn an implicit model with potentially complex interactions between variables by directly training an inference procedure.</li>
</ul>
</li>
</ul>
<h4 id="methods-17">Methods</h4>
<ul>
<li><strong>system overview</strong>:</li>
</ul>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200309105620041.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200309105620041.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200309105620041.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200309105620041.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200309105620041.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200309105620041.png"/></p>
<p>【<strong>Pose Machines</strong>】</p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200309105711936.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200309105711936.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200309105711936.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200309105711936.png 2x"
    data-sizes="auto"
    alt="image-20200309105711936"
    title="image-20200309105711936"/></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="../../../../MEGA/MEGAsync/actionPrediction/skeleton.assets/image-20200309105732538.png"
    data-srcset="../../../../MEGA/MEGAsync/actionPrediction/skeleton.assets/image-20200309105732538.png, ../../../../MEGA/MEGAsync/actionPrediction/skeleton.assets/image-20200309105732538.png 1.5x, ../../../../MEGA/MEGAsync/actionPrediction/skeleton.assets/image-20200309105732538.png 2x"
    data-sizes="auto"
    alt="image-20200309105732538"
    title="image-20200309105732538"/></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200309105745261.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200309105745261.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200309105745261.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200309105745261.png 2x"
    data-sizes="auto"
    alt="image-20200309105745261"
    title="image-20200309105745261"/></p>
<p><strong>【Convolutional Pose Machines】</strong></p>
<ul>
<li>Keypoint Localization Using Local Image Evidence:</li>
</ul>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200309105837745.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200309105837745.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200309105837745.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200309105837745.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200309105837745.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200309105837745.png"/></p>
<ul>
<li>Sequential Prediction with Learned Spatial Context Features:</li>
</ul>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200309105933784.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200309105933784.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200309105933784.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200309105933784.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200309105933784.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200309105933784.png"/></p>
<ul>
<li>Learning in Convolutional Pose Machines</li>
</ul>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200309110012428.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200309110012428.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200309110012428.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200309110012428.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200309110012428.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200309110012428.png"/></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200309110025606.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200309110025606.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200309110025606.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200309110025606.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200309110025606.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200309110025606.png"/></p>
<h4 id="conclusion-12">Conclusion</h4>
<ul>
<li>learning implicit spatial models via a sequential composition of convolutional architectures</li>
<li>a systematic approach to designing and training such an architecture to learn both image features and image-dependent spatial models for structured presiction tasks, without the need for any graphical model style inference.</li>
</ul>
<h4 id="notes-font-colororange去加强了解font-10">Notes <!-- raw HTML omitted -->去加强了解<!-- raw HTML omitted --></h4>
<ul>
<li>code available: <a href="https://github.com/CMU-Perceptual-Computing-Lab/convolutional-pose-machines-release"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/CMU-Perceptual-Computing-Lab/convolutional-pose-machines-release<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></li>
</ul>
<p><strong>level</strong>:  CVPR  CCF_A
<strong>author</strong>: Charles R.Qi  Stanford University
<strong>date</strong>:<br>
<strong>keyword</strong>:</p>
<ul>
<li>3D object detection, Point Cloud</li>
</ul>
<hr>
<h2 id="paper-frustum-pointnets">Paper: Frustum PointNets</h2>
<!-- raw HTML omitted -->
<h4 id="summary-12">Summary</h4>
<ol>
<li>3D sensor data is often in the form of point clouds.</li>
</ol>
<h4 id="research-objective-14">Research Objective</h4>
<ul>
<li><strong>Application Area</strong>: autonomous driving, augmented reality</li>
<li><strong>Purpose</strong>:  how to effective localize objects in point clouds of large-scale scenes.</li>
</ul>
<h4 id="proble-statement-14">Proble Statement</h4>
<ul>
<li>study 3D object detection from RGB-D data in both indoor and outdoor scenes.</li>
<li>previous work focus on images or 3D voxels, often obscuring natural 3D patterns and invariances of 3D voxels.</li>
</ul>
<p>previous work:</p>
<ul>
<li>object detection and instance segmentation based on 2D image.</li>
<li>most existing works convert 3D point clouds to images by projection, or to volumetric grids by quantization and then apply convolutional networks.</li>
<li>3D object detection from RGB-D data
<ul>
<li>front view image based methods:
<ul>
<li>take monocular RGB images and shape priors or occlusion patterns to infer 3D bounding boxes</li>
<li>represent depth data as 2D maps.</li>
</ul>
</li>
<li>Bird&rsquo;s eye view based methods:
<ul>
<li>projects Li-DAR point cloud to bird&rsquo;s eye view and trains RPN</li>
</ul>
</li>
<li>3D based methods:
<ul>
<li>train 3D object classifiers by SVMs on hand-designed geometry features extracted from point cloud and localize objects using window search,</li>
</ul>
</li>
</ul>
</li>
<li>Deep Learning on Point Clouds:
<ul>
<li>convert point clouds to images or volumetric forms before feature learning</li>
<li><!-- raw HTML omitted -->require quantitization of point clouds with certain voxel resolution<!-- raw HTML omitted --></li>
</ul>
</li>
</ul>
<h4 id="methods-18">Methods</h4>
<ul>
<li><strong>Problem Formulation</strong>:</li>
</ul>
<p>input RGB-D data, classify and localize objects in 3D space.</p>
<ul>
<li><strong>system overview</strong>:</li>
</ul>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200307115907354.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200307115907354.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200307115907354.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200307115907354.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200307115907354.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200307115907354.png"/></p>
<p><strong>【Model 1】Frustum Proposal</strong></p>
<ul>
<li>with known camera projection matrix, a 2D bounding box can be lifted to a frustum(with near and far planes specified by depth sensor range) that defines a 3D search space for the object.</li>
<li>using FPN ,pre-train the model weights on ImageNet classification and COCO object detection datasets and further fine-tune it on KITTI 2D object detection to classify and predict amodal 2D boxes.</li>
</ul>
<p><strong>【Model 2】3D Instance Segmentation</strong></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200307120512402.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200307120512402.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200307120512402.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200307120512402.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200307120512402.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200307120512402.png"/></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200307120608061.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200307120608061.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200307120608061.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200307120608061.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200307120608061.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200307120608061.png"/></p>
<p><strong>【Model 3】3D Instance Segmentation PointNet</strong></p>
<ul>
<li>Similar to the case in 2D instance segmentation, depending on the position of the frustum, object points in on frustum may become cluttered or occlude points in another.</li>
<li>transform the point cloud into a local coordinate by subtracting XYZ values by its centroid. considering the bounding sphere size of a partial point cloud can be greatly affected by viewpoints and the real size of the point clouds helps the box size estimation.</li>
</ul>
<p><strong>【Model 4】Amodal 3D Box Estimation</strong></p>
<ul>
<li>learning-based 3D alignment by T-Net</li>
<li>amodal 3D box estimation pointnet</li>
</ul>
<h4 id="evaluation-11">Evaluation</h4>
<ul>
<li><strong>Environment</strong>:
<ul>
<li>Dataset: KITTI  ,</li>
</ul>
</li>
</ul>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200307121349449.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200307121349449.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200307121349449.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200307121349449.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200307121349449.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200307121349449.png"/></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200307121605990.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200307121605990.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200307121605990.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200307121605990.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200307121605990.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200307121605990.png"/></p>
<h4 id="conclusion-13">Conclusion</h4>
<ul>
<li>propose a novel framework Frustum PointNets for RGB-D data based 3D object detection</li>
<li>provide extensive quantitative evaluations to validate the design.</li>
</ul>
<h4 id="notes-font-colororange去加强了解font-11">Notes <!-- raw HTML omitted -->去加强了解<!-- raw HTML omitted --></h4>
<ul>
<li>KITTI 3D object detection:  <a href="http://www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmark=3d"target="_blank" rel="external nofollow noopener noreferrer">http://www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmark=3d<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>   contain datasets, many methods and compare.<!-- raw HTML omitted -->如果后面学习和使用到3D object detection 可以从这里学习<!-- raw HTML omitted --></li>
<li>bird&rsquo;s eye view detection benchmarks</li>
</ul>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200307122059591.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200307122059591.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200307122059591.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200307122059591.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200307122059591.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200307122059591.png"/></p>
<p><strong>level</strong>: ICCV   CCF_B
<strong>author</strong>:         National Institute of Advanced Industrial Science and Technology
<strong>date</strong>: 2017
<strong>keyword</strong>:</p>
<ul>
<li>Spatio-Temporal ,action recognision</li>
</ul>
<hr>
<h2 id="paper-3d-resnet">Paper: 3D-Resnet</h2>
<!-- raw HTML omitted -->
<h4 id="summary-13">Summary</h4>
<ol>
<li>exploring the effectiveness of ResNets with 3D convolutional kernels</li>
<li>学会使用作为基本的视频特征提取方法</li>
</ol>
<h4 id="research-objective-15">Research Objective</h4>
<ul>
<li><strong>Application Area</strong>: surveillance systems, video indexing, and human computer  interaction</li>
<li><strong>Purpose</strong>:  propose a 3D CNNs based on ResNets toward a better action representation</li>
</ul>
<h4 id="proble-statement-15">Proble Statement</h4>
<ul>
<li>
<p><strong>Action Recognition Database</strong>:</p>
<ul>
<li>HMDB51 [13]</li>
<li>UCF101 [16]</li>
<li>Kinetics human action video dataset [12]</li>
</ul>
</li>
<li>
<p>Residual block</p>
</li>
</ul>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191223101841019.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191223101841019.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191223101841019.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191223101841019.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191223101841019.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191223101841019.png"/></p>
<h4 id="methods-19">Methods</h4>
<ul>
<li><strong>network design</strong>:</li>
</ul>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191223101025170.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191223101025170.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191223101025170.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191223101025170.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191223101025170.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191223101025170.png"/></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191223101347554.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191223101347554.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191223101347554.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191223101347554.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191223101347554.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191223101347554.png"/></p>
<h4 id="notes-font-colororange去加强了解font-12">Notes <!-- raw HTML omitted -->去加强了解<!-- raw HTML omitted --></h4>
<ul>
<li><i class="fa-regular fa-square fa-fw" aria-hidden="true"></i> code available: <a href="https://github.com/kenshohara/3D-ResNets"target="_blank" rel="external nofollow noopener noreferrer">https://github.com/kenshohara/3D-ResNets<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a>.   需要做实验</li>
</ul>
<p><strong>level</strong>:  CVPR    CCF_A
<strong>author</strong>: DUSHYANT
<strong>date</strong>:  2017
<strong>keyword</strong>:</p>
<ul>
<li>3D human pose estimation</li>
</ul>
<hr>
<h2 id="paper-vnect">Paper: VNect</h2>
<!-- raw HTML omitted -->
<h4 id="research-objective-16">Research Objective</h4>
<ul>
<li>
<p><strong>Application Area</strong>:realtime motion-driven 3D game character control,self-immersion in 3D virtual and augmented reality and human-computer interaction.</p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191118101846038.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191118101846038.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191118101846038.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191118101846038.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191118101846038.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191118101846038.png"/></p>
</li>
<li>
<p><strong>Purpose</strong>:  <!-- raw HTML omitted -->stable 3D skeletal motion capture from a single camera in real-time<!-- raw HTML omitted --></p>
</li>
</ul>
<h4 id="proble-statement-16">Proble Statement</h4>
<p>previous work:</p>
<ul>
<li>Multi-view: using multi-view setups markerless motion-capture solutions attain high accuracy. <!-- raw HTML omitted -->we combine discriminative pose estimation with kinematic fitting to succeed in our underconstrained setting<!-- raw HTML omitted --></li>
<li>Monocular Depth-based: RGB-D sensors overcomes forward backward ambiguities in monocular pose estimation.</li>
<li>Monocular RGB:structure-from-motion techniques exploit motion cues in a batch of frames and also been applied to human motion estimation.</li>
<li>Given 2D joint locations ,existing approaches  use bone length and depth ordering constraints ,sparsity asumptions,joint limits,inter-penetration constraints,temporal dependencies and regression to create 3D  pose.<!-- raw HTML omitted -->sparse set of 2D locations losesimage evidence-&gt; discriminative methods<!-- raw HTML omitted --></li>
<li>previous work only obtain temporally unstable coarse pose not directly usable in applications.</li>
</ul>
<h4 id="methods-20">Methods</h4>
<ul>
<li><strong>system overview</strong>:</li>
</ul>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191118103828661.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191118103828661.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191118103828661.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191118103828661.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191118103828661.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191118103828661.png"/></p>
<p>【Qustion 1】how to use CNN to regress Pose by single RGB image?</p>
<p>extending the 2D heatmap formulation to 3D using three additional location-maps(x,yz,) per joint j , captureing the root-relative locations (x,y,z) respectively.<img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191118110049931.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191118110049931.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191118110049931.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191118110049931.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191118110049931.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191118110049931.png"/></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191121100234719.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191121100234719.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191121100234719.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191121100234719.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191121100234719.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191121100234719.png"/></p>
<h4 id="contribution-1">Contribution</h4>
<ul>
<li>the first real-time method to capture the full global 3D skeletal pose of a human in a stable ,temporally consistent manner using a single RGB camera.</li>
<li>novel fully convolutional pose formulation regresses 2D and 3D joint positions jointly in real time and doesn&rsquo;t require tightly cropped input frames, and forgoes the need to perform expensive bounding box computations.</li>
<li>model-based kinematic skeleton fitting against the 2D/3D pose predictions to produce temporally stable joint angles of a metric global 3D skeleton in rea time.</li>
<li>more applicable for outdoor scenes ,community video and low quality commodity RGB cameras.</li>
</ul>
<h4 id="notes-2">Notes</h4>
<ul>
<li>
<p><i class="fa-regular fa-check-square fa-fw" aria-hidden="true"></i> heatmap based bodyjoint detection formulation  ,heatmap is the distribution of confidence probability of body part.</p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191121094023973.png"
    data-srcset="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191121094023973.png, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191121094023973.png 1.5x, https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191121094023973.png 2x"
    data-sizes="auto"
    alt="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191121094023973.png"
    title="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191121094023973.png"/></p>
</li>
</ul>
<h1 id="heading"></h1>
</div>
<div class="post-footer" id="post-footer">
  <div class="post-info">
    <div class="post-info-line">
      <div class="post-info-mod">
        <span title=2023-09-28&#32;23:52:41>更新于 2023-09-28&nbsp;</span>
      </div><div class="post-info-license">
          <span><a rel="license external nofollow noopener noreferrer" href="https://creativecommons.org/licenses/by-nc/4.0/" target="_blank">CC BY-NC 4.0</a></span>
        </div></div>
    <div class="post-info-line">
      <div class="post-info-md"><span><a href="/video_understand/index.md" title="阅读原始文档" class="link-to-markdown">阅读原始文档</a></span><span><a href="https://liudongdong1.github.io/edit/master/content/posts%5c%e8%a7%86%e8%a7%89%e8%bf%90%e5%8a%a8%5cvideo_understand%5cVideo_Understand.md" title="编辑此页"target="_blank" rel="external nofollow noopener noreferrer" class="link-to-edit">编辑此页</a></span></div>
      <div class="post-info-share">
        <span><a href="javascript:void(0);" title="分享到 Twitter" data-sharer="twitter" data-url="https://liudongdong1.github.io/video_understand/" data-title="Video_Undertand" data-hashtags="CV"><i class="fa-brands fa-twitter fa-fw" aria-hidden="true"></i></a>
  <a href="javascript:void(0);" title="分享到 Facebook" data-sharer="facebook" data-url="https://liudongdong1.github.io/video_understand/" data-hashtag="CV"><i class="fa-brands fa-facebook-square fa-fw" aria-hidden="true"></i></a>
  <a href="javascript:void(0);" title="分享到 微博" data-sharer="weibo" data-url="https://liudongdong1.github.io/video_understand/" data-title="Video_Undertand" data-image="https://gitee.com/github-25970295/blogImage/raw/master/img/voice-recognition-speech-detect-deep-260nw-694633963.webp"><i class="fa-brands fa-weibo fa-fw" aria-hidden="true"></i></a>
  </span>
      </div>
    </div>
  </div>

  <div class="post-info-more">
    <section class="post-tags"><i class="fa-solid fa-tags fa-fw" aria-hidden="true"></i>&nbsp;<a href="/tags/cv/">CV</a></section>
    <section>
      <span><a href="javascript:void(0);" onclick="window.history.back();">返回</a></span>&nbsp;|&nbsp;<span><a href="/">主页</a></span>
    </section>
  </div>

  <div class="post-nav"><a href="/data-glove-record/" class="prev" rel="prev" title="Data Glove Record"><i class="fa-solid fa-angle-left fa-fw" aria-hidden="true"></i>Data Glove Record</a>
      <a href="/object-tracking/" class="next" rel="next" title="Object Tracking">Object Tracking<i class="fa-solid fa-angle-right fa-fw" aria-hidden="true"></i></a></div>
</div>
</article></main><footer class="footer">
    <div class="footer-container"><div class="footer-line powered">由 <a href="https://gohugo.io/" target="_blank" rel="external nofollow noopener noreferrer" title="Hugo 0.118.2">Hugo</a> 强力驱动 | 主题 - <a href="https://github.com/hugo-fixit/FixIt" target="_blank" rel="external" title="FixIt v0.2.17-RC"><img class="fixit-icon" src="/fixit.min.svg" alt="FixIt logo" />&nbsp;FixIt</a>
        </div><div class="footer-line copyright" itemscope itemtype="http://schema.org/CreativeWork"><i class="fa-regular fa-copyright fa-fw" aria-hidden="true"></i>
            <span itemprop="copyrightYear">2020 - 2023</span><span class="author" itemprop="copyrightHolder">
              <a href="https://liudongdong1.github.io/"target="_blank" rel="external nofollow noopener noreferrer">LiuDongdong</a></span><span class="license footer-divider"><a rel="license external nofollow noopener noreferrer" href="https://creativecommons.org/licenses/by-nc/4.0/" target="_blank">CC BY-NC 4.0</a></span></div><div class="footer-line statistics"><span class="site-time" title='网站运行中 ...'><i class="fa-solid fa-heartbeat fa-fw animate-icon" aria-hidden="true"></i>&nbsp;<span class="run-times">网站运行中 ...</span></span></div><div class="footer-line ibruce">
          <span id="busuanzi_container_site_uv" title='总访客数'><i class="fa-regular fa-user fa-fw" aria-hidden="true"></i>&nbsp;<span id="busuanzi_value_site_uv"><i class="fa-solid fa-spinner fa-spin fa-fw" aria-hidden="true"></i></span></span><span id="busuanzi_container_site_pv" class="footer-divider" title='总访问量'><i class="fa-regular fa-eye fa-fw" aria-hidden="true"></i>&nbsp;<span id="busuanzi_value_site_pv"><i class="fa-solid fa-spinner fa-spin fa-fw" aria-hidden="true"></i></span></span>
        </div></div>
  </footer></div><div class="widgets"><div class="fixed-buttons animate__faster d-none"><div class="fixed-button back-to-top" role="button" aria-label="回到顶部"><i class="fa-solid fa-arrow-up fa-fw" aria-hidden="true"></i><span class="variant-numeric">0%</span>
        </div></div><a href="https://liudongdong1.github.io/" title="在 GitHub 上查看源代码"target="_blank" rel="external nofollow" class="github-corner right d-none-mobile"><svg viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><div id="mask"></div><div class="reading-progress-bar" style="left: 0;top: 0;--bg-progress: #0076ff;--bg-progress-dark: #fff;"></div><noscript>
    <div class="noscript-warning">FixIt 主题在启用 JavaScript 的情况下效果最佳。</div>
  </noscript>
</div><link rel="stylesheet" href="/lib/katex/katex.min.css"><link rel="stylesheet" href="/lib/cookieconsent/cookieconsent.min.css"><script src="/lib/autocomplete/autocomplete.min.js" defer></script><script src="/lib/algoliasearch/algoliasearch-lite.umd.min.js" defer></script><script src="/lib/lazysizes/lazysizes.min.js" async defer></script><script src="/lib/sharer/sharer.min.js" async defer></script><script src="/lib/typeit/index.umd.js" defer></script><script src="/lib/katex/katex.min.js" defer></script><script src="/lib/katex/auto-render.min.js" defer></script><script src="/lib/katex/copy-tex.min.js" defer></script><script src="/lib/katex/mhchem.min.js" defer></script><script src="/lib/cookieconsent/cookieconsent.min.js" defer></script><script src="/lib/pangu/pangu.min.js" defer></script><script src="/lib/cell-watermark/watermark.min.js" defer></script><script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async defer></script><script>window.config={"code":{"copyTitle":"复制到剪贴板","editLockTitle":"锁定可编辑代码块","editUnLockTitle":"解锁可编辑代码块","editable":true,"maxShownLines":10},"comment":{"enable":false},"cookieconsent":{"content":{"dismiss":"同意","link":"了解更多","message":"本网站使用 Cookies 来改善您的浏览体验。"},"enable":true,"palette":{"button":{"background":"#f0f0f0"},"popup":{"background":"#1aa3ff"}},"theme":"edgeless"},"data":{"typeit-header-subtitle-desktop":"\u003cspan style='font-family: MMT,\"沐目体\";'\u003e吾日三省吾身\u003c/span\u003e","typeit-header-subtitle-mobile":"\u003cspan style='font-family: MMT,\"沐目体\";'\u003e吾日三省吾身\u003c/span\u003e"},"enablePWA":true,"enablePangu":true,"math":{"delimiters":[{"display":true,"left":"$$","right":"$$"},{"display":true,"left":"\\[","right":"\\]"},{"display":true,"left":"\\begin{equation}","right":"\\end{equation}"},{"display":true,"left":"\\begin{equation*}","right":"\\end{equation*}"},{"display":true,"left":"\\begin{align}","right":"\\end{align}"},{"display":true,"left":"\\begin{align*}","right":"\\end{align*}"},{"display":true,"left":"\\begin{alignat}","right":"\\end{alignat}"},{"display":true,"left":"\\begin{alignat*}","right":"\\end{alignat*}"},{"display":true,"left":"\\begin{gather}","right":"\\end{gather}"},{"display":true,"left":"\\begin{CD}","right":"\\end{CD}"},{"display":false,"left":"$","right":"$"},{"display":false,"left":"\\(","right":"\\)"}],"strict":false},"search":{"algoliaAppID":"2R1K9SKLQZ","algoliaIndex":"index.zh-cn","algoliaSearchKey":"4a226aa1c5c98d6859e4d1386adb2bc7","highlightTag":"em","maxResultLength":10,"noResultsFound":"没有找到结果","snippetLength":50,"type":"algolia"},"siteTime":"2020-12-18T16:15:22+08:00","typeit":{"cursorChar":"|","cursorSpeed":1000,"data":{"typeit-header-subtitle-desktop":["typeit-header-subtitle-desktop"],"typeit-header-subtitle-mobile":["typeit-header-subtitle-mobile"]},"duration":-1,"speed":100},"watermark":{"appendto":".wrapper\u003emain","colspacing":30,"content":"\u003cimg class=\"fixit-icon\" src=\"/fixit.min.svg\" alt=\"FixIt logo\" /\u003e FixIt 主题","enable":true,"fontfamily":"inherit","fontsize":0.85,"height":21,"opacity":0.0125,"rotate":15,"rowspacing":60,"width":150}};</script><script src="/js/theme.min.js" defer></script><script src="/js/custom.min.js" defer></script></body>
</html>
