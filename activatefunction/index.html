<!DOCTYPE html>
<html itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">
  <head>
    
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
    <meta name="robots" content="noodp" />
    <title>ActivateFunction - DAY By DAY</title><meta name="author" content="LiuDongdong">
<meta name="author-link" content="https://liudongdong1.github.io/">
<meta name="description" content="神经网络中的每个神经元节点接受上一层神经元的输出值作为本神经元的输入值，并将输入值传递给下一层，输入层神经元节点会将输入属性值直接传递给下一" /><meta name="keywords" content='model' /><meta itemprop="name" content="ActivateFunction">
<meta itemprop="description" content="神经网络中的每个神经元节点接受上一层神经元的输出值作为本神经元的输入值，并将输入值传递给下一层，输入层神经元节点会将输入属性值直接传递给下一"><meta itemprop="datePublished" content="2022-10-20T15:56:09+00:00" />
<meta itemprop="dateModified" content="2023-12-31T15:49:02+08:00" />
<meta itemprop="wordCount" content="4603"><meta itemprop="image" content="https://liudongdong1.github.io/logo.png"/>
<meta itemprop="keywords" content="model," /><meta property="og:title" content="ActivateFunction" />
<meta property="og:description" content="神经网络中的每个神经元节点接受上一层神经元的输出值作为本神经元的输入值，并将输入值传递给下一层，输入层神经元节点会将输入属性值直接传递给下一" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://liudongdong1.github.io/activatefunction/" /><meta property="og:image" content="https://liudongdong1.github.io/logo.png"/><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-10-20T15:56:09+00:00" />
<meta property="article:modified_time" content="2023-12-31T15:49:02+08:00" />
<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://liudongdong1.github.io/logo.png"/>

<meta name="twitter:title" content="ActivateFunction"/>
<meta name="twitter:description" content="神经网络中的每个神经元节点接受上一层神经元的输出值作为本神经元的输入值，并将输入值传递给下一层，输入层神经元节点会将输入属性值直接传递给下一"/>
<meta name="application-name" content="DAY By DAY">
<meta name="apple-mobile-web-app-title" content="DAY By DAY"><meta name="theme-color" data-light="#f8f8f8" data-dark="#252627" content="#f8f8f8"><meta name="msapplication-TileColor" content="#da532c"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" />
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"><link rel="manifest" href="/site.webmanifest"><link rel="canonical" href="https://liudongdong1.github.io/activatefunction/" /><link rel="prev" href="https://liudongdong1.github.io/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E9%9D%A2%E7%BB%8F/" /><link rel="next" href="https://liudongdong1.github.io/3dkeypointpaper/" /><link rel="stylesheet" href="/css/style.min.css"><link rel="stylesheet" href="/lib/fontawesome-free/all.min.css"><link rel="stylesheet" href="/lib/animate/animate.min.css"><script type="application/ld+json">
  {
    "@context": "http://schema.org",
    "@type": "BlogPosting",
    "headline": "ActivateFunction",
    "inLanguage": "zh-CN",
    "mainEntityOfPage": {
      "@type": "WebPage",
      "@id": "https:\/\/liudongdong1.github.io\/activatefunction\/"
    },"genre": "posts","keywords": "model","wordcount":  4603 ,
    "url": "https:\/\/liudongdong1.github.io\/activatefunction\/","datePublished": "2022-10-20T15:56:09+00:00","dateModified": "2023-12-31T15:49:02+08:00","license": "This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.","publisher": {
      "@type": "Organization",
      "name": "LiuDongdong","logo": "https:\/\/liudongdong1.github.io\/images\/person.png"},"author": {
        "@type": "Person",
        "name": "liudongdong1"
      },"description": ""
  }
  </script></head>
  <body data-header-desktop="auto" data-header-mobile="auto"><script>(window.localStorage?.getItem('theme') ? localStorage.getItem('theme') === 'dark' : ('auto' === 'auto' ? window.matchMedia('(prefers-color-scheme: dark)').matches : 'auto' === 'dark')) && document.body.setAttribute('data-theme', 'dark');</script><div class="wrapper"><script type="text/javascript"
        async
        src="https://cdnjs.cloudflare.com/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$']],
    displayMath: [['$$','$$']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
});

MathJax.Hub.Queue(function() {
    
    
    
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>

<style>
code.has-jax {
    font: inherit;
    font-size: 100%;
    background: inherit;
    border: inherit;
    color: #515151;
}
</style>
<header class="desktop animate__faster" id="header-desktop">
  <div class="header-wrapper" data-github-corner="right">
    <div class="header-title">
      <a href="/" title="DAY By DAY"><img
    class="lazyload logo"
    src="/svg/loading.min.svg"
    data-src="/fixit.min.svg"
    data-srcset="/fixit.min.svg, /fixit.min.svg 1.5x, /fixit.min.svg 2x"
    data-sizes="auto"
    alt="DAY By DAY"
    title="DAY By DAY"/><span class="header-title-text"></span></a><span id="typeit-header-subtitle-desktop" class="typeit header-subtitle"></span></div>
    <nav>
      <ul class="menu"><li class="menu-item">
              <a
                class="menu-link"
                href="/posts/"
                
                
              ><i class="fa-solid fa-archive fa-fw fa-sm" aria-hidden="true"></i> 所有文章</a></li><li class="menu-item">
              <a
                class="menu-link"
                href="/categories/"
                
                
              ><i class="fa-solid fa-th fa-fw fa-sm" aria-hidden="true"></i> 分类</a></li><li class="menu-item">
              <a
                class="menu-link"
                href="/tags/"
                
                
              ><i class="fa-solid fa-tags fa-fw fa-sm" aria-hidden="true"></i> 标签</a></li><li class="menu-item">
              <a
                class="menu-link"
                href="/friends/"
                title="友情链接"
                
              ><i class="fa-solid fa-users fa-fw fa-sm" aria-hidden="true"></i> 友链</a></li><li class="menu-item">
              <a
                class="menu-link"
                href="/about/"
                
                
              ><i class="fa-solid fa-info-circle fa-fw fa-sm" aria-hidden="true"></i> 关于</a></li><li class="menu-item delimiter"></li><li class="menu-item language">
            <span role="button" aria-label="选择语言" title="选择语言">简体中文<i class="dropdown-icon fa-solid fa-chevron-down" aria-hidden="true"></i>
            </span>
            <ul class="sub-menu"><li class="menu-item">没有更多翻译</li></ul>
          </li><li class="menu-item search" id="search-desktop">
            <input type="text" placeholder="搜索文章标题或内容 ..." id="search-input-desktop">
            <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-desktop" title="搜索">
              <i class="fa-solid fa-search fa-fw" aria-hidden="true"></i>
            </a>
            <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-desktop" title="清空">
              <i class="fa-solid fa-times-circle fa-fw" aria-hidden="true"></i>
            </a>
            <span class="search-button search-loading" id="search-loading-desktop">
              <i class="fa-solid fa-spinner fa-fw fa-spin" aria-hidden="true"></i>
            </span>
          </li><li class="menu-item theme-switch" title="切换主题">
          <i class="fa-solid fa-adjust fa-fw" aria-hidden="true"></i>
        </li>
      </ul>
    </nav>
  </div>
</header><header class="mobile animate__faster" id="header-mobile">
  <div class="header-container">
    <div class="header-wrapper">
      <div class="header-title">
        <a href="/" title="DAY By DAY"><img
    class="lazyload logo"
    src="/svg/loading.min.svg"
    data-src="/fixit.min.svg"
    data-srcset="/fixit.min.svg, /fixit.min.svg 1.5x, /fixit.min.svg 2x"
    data-sizes="auto"
    alt="/fixit.min.svg"
    title="/fixit.min.svg"/><span class="header-title-text"></span></a><span id="typeit-header-subtitle-mobile" class="typeit header-subtitle"></span></div>
      <div class="menu-toggle" id="menu-toggle-mobile">
        <span></span><span></span><span></span>
      </div>
    </div>
    <nav>
      <ul class="menu" id="menu-mobile"><li class="search-wrapper">
            <div class="search mobile" id="search-mobile">
              <input type="text" placeholder="搜索文章标题或内容 ..." id="search-input-mobile">
              <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-mobile" title="搜索">
                <i class="fa-solid fa-search fa-fw" aria-hidden="true"></i>
              </a>
              <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-mobile" title="清空">
                <i class="fa-solid fa-times-circle fa-fw" aria-hidden="true"></i>
              </a>
              <span class="search-button search-loading" id="search-loading-mobile">
                <i class="fa-solid fa-spinner fa-fw fa-spin" aria-hidden="true"></i>
              </span>
            </div>
            <a href="javascript:void(0);" class="search-cancel" id="search-cancel-mobile">
              取消
            </a>
          </li><li
              class="menu-item"
            ><a
                  class="menu-link"
                  href="/posts/"
                  
                  
                ><i class="fa-solid fa-archive fa-fw fa-sm" aria-hidden="true"></i> 所有文章</a></li><li
              class="menu-item"
            ><a
                  class="menu-link"
                  href="/categories/"
                  
                  
                ><i class="fa-solid fa-th fa-fw fa-sm" aria-hidden="true"></i> 分类</a></li><li
              class="menu-item"
            ><a
                  class="menu-link"
                  href="/tags/"
                  
                  
                ><i class="fa-solid fa-tags fa-fw fa-sm" aria-hidden="true"></i> 标签</a></li><li
              class="menu-item"
            ><a
                  class="menu-link"
                  href="/friends/"
                  title="友情链接"
                  
                ><i class="fa-solid fa-users fa-fw fa-sm" aria-hidden="true"></i> 友链</a></li><li
              class="menu-item"
            ><a
                  class="menu-link"
                  href="/about/"
                  
                  
                ><i class="fa-solid fa-info-circle fa-fw fa-sm" aria-hidden="true"></i> 关于</a></li><li
              class="menu-item text-center"
            ><a
                  class="menu-link"
                  href="/"
                  title="GitHub"
                  
                ><i class='fa-brands fa-github fa-fw' aria-hidden='true'></i> </a></li><li class="menu-item theme-switch" title="切换主题">
          <i class="fa-solid fa-adjust fa-fw" aria-hidden="true"></i>
        </li><li class="menu-item language">
            <span role="button" aria-label="选择语言" title="选择语言">简体中文<i class="dropdown-icon fa-solid fa-chevron-down" aria-hidden="true"></i>
            </span>
            <select class="language-select" onchange="location = this.value;"><option disabled>没有更多翻译</option></select>
          </li></ul>
    </nav>
  </div>
</header><div class="search-dropdown desktop">
    <div id="search-dropdown-desktop"></div>
  </div>
  <div class="search-dropdown mobile">
    <div id="search-dropdown-mobile"></div>
  </div><main class="container" data-page-style="normal"><aside class="toc" id="toc-auto"><h2 class="toc-title">目录 <i class="toc-icon fa-solid fa-angle-down fa-fw"></i></h2>
      <div class="toc-content" id="toc-content-auto"></div></aside>

  <aside class="aside-custom" id="aside-sakana">
    

<div class="sakana-widget">
  <div class="sakana-item" id="takina-widget"></div>
  <div class="sakana-item" id="chisato-widget"></div>
</div>
<script>
  function initSakanaWidget() {
    const takina = SakanaWidget.getCharacter('takina')
    SakanaWidget.registerCharacter('takina-slow', takina);
    new SakanaWidget({
      character: 'takina-slow',
      controls: false,
      autoFit: true,
      stroke: {
        color: "#b4b4b4",
        width: 2
      }
    }).mount('#takina-widget');

    const chisato = SakanaWidget.getCharacter('chisato')
    SakanaWidget.registerCharacter('chisato-slow', chisato);
    new SakanaWidget({
      character: 'chisato-slow',
      controls: false,
      autoFit: true,
      stroke: {
        color: "#b4b4b4",
        width: 2
      }
    }).mount('#chisato-widget');
  }
</script>
<script async onload="initSakanaWidget()" src="https://cdn.jsdelivr.net/npm/sakana-widget@2.3.0/lib/sakana.min.js">
</script></aside>

  <article class="page single">
    <div class="header"><h1 class="single-title animate__animated animate__flipInX">
        <span>ActivateFunction</span>
      </h1></div><div class="post-meta">
      <div class="post-meta-line"><span class="post-author"><span class="author"><i class="fa-solid fa-user-circle" aria-hidden="true"></i>
      liudongdong1</span></span>
          <span class="post-category">收录于 <a href="/categories/demo/"><i class="fa-regular fa-folder fa-fw"></i>&nbsp;Demo</a></span></div>
      <div class="post-meta-line"><span title=2022-10-20&#32;15:56:09>
            <i class="fa-regular fa-calendar-alt fa-fw"></i>&nbsp;<time datetime="2022-10-20" >2022-10-20</time>
          </span>&nbsp;<i class="fa-solid fa-pencil-alt fa-fw"></i>&nbsp;约 4603 字&nbsp;
        <i class="fa-regular fa-clock fa-fw"></i>&nbsp;预计阅读 10 分钟&nbsp;<span id="busuanzi_container_page_pv" class="busuanzi_visitors comment-visitors" data-flag-title="ActivateFunction">
            <i class="fa-regular fa-eye fa-fw"></i>&nbsp;<span id="busuanzi_value_page_pv">-</span>&nbsp;次阅读
          </span>&nbsp;</div>
    </div><div class="featured-image"><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://cdn.stocksnap.io/img-thumbs/280h/5NLKT00MVB.jpg"
    data-srcset="https://cdn.stocksnap.io/img-thumbs/280h/5NLKT00MVB.jpg, https://cdn.stocksnap.io/img-thumbs/280h/5NLKT00MVB.jpg 1.5x, https://cdn.stocksnap.io/img-thumbs/280h/5NLKT00MVB.jpg 2x"
    data-sizes="auto"
    alt="https://cdn.stocksnap.io/img-thumbs/280h/5NLKT00MVB.jpg"
    title="https://cdn.stocksnap.io/img-thumbs/280h/5NLKT00MVB.jpg"/></div><div class="details toc" id="toc-static" kept="true">
        <div class="details-summary toc-title">
          <span>目录</span>
          <span><i class="details-icon fa-solid fa-angle-right"></i></span>
        </div>
        <div class="details-content toc-content" id="toc-content-static"><nav id="TableOfContents">
  <ul>
    <li>
      <ul>
        <li><a href="#0-softmax">0. Softmax</a></li>
        <li><a href="#1-sigmoidlogistic">1. Sigmoid&amp;Logistic</a></li>
        <li><a href="#2-sinc">2. Sinc</a></li>
        <li><a href="#3-symmetrical-sigmoid">3. Symmetrical Sigmoid</a></li>
        <li><a href="#4-loglog">4. LogLog</a></li>
        <li><a href="#5-softmoid">5. softmoid</a></li>
        <li><a href="#6-tanh">6. tanh</a></li>
        <li><a href="#7-hard-tanh">7. Hard Tanh</a></li>
        <li><a href="#8-lecun-tanh">8. LeCun Tanh</a></li>
        <li><a href="#9-arctan">9. ArcTan</a></li>
        <li><a href="#11-softplus">11. SoftPlus</a></li>
        <li><a href="#12-relu">12. Relu</a></li>
        <li><a href="#13-leaky-relu">13. Leaky Relu</a></li>
        <li><a href="#14-prelu">14. PReLU</a></li>
        <li><a href="#15-rrelu">15. RReLu</a></li>
        <li><a href="#16-srelu">16. SReLU</a></li>
        <li><a href="#17-eluexponential-linear-units">17. ELU(Exponential Linear Units)</a></li>
        <li><a href="#18-selu">18. SELU</a></li>
        <li><a href="#19-hard-swish-或-h-swish">19. <strong>Hard-Swish 或 H-Swish</strong></a></li>
        <li><a href="#20-maxout">20. MaxOut</a></li>
        <li><a href="#21-step">21. Step</a></li>
        <li><a href="#22-identity">22. Identity</a></li>
        <li><a href="#23-bent-identity">23. Bent Identity</a></li>
        <li><a href="#24-gaussian">24. Gaussian</a></li>
        <li><a href="#25-geluhttpspytorchorgdocsstable_modulestorchnnmodulesactivationhtmlgelu">25. <a href="https://pytorch.org/docs/stable/_modules/torch/nn/modules/activation.html#GELU">GELU</a></a></li>
        <li><a href="#选择建议">选择建议</a></li>
        <li><a href="#resouces">Resouces</a></li>
      </ul>
    </li>
  </ul>
</nav></div>
      </div><div
      class="content"
      id="content"
      
      
    ><blockquote>
<p>神经网络中的每个神经元节点接受上一层神经元的输出值作为本神经元的输入值，并将输入值传递给下一层，输入层神经元节点会将输入属性值直接传递给下一层（隐层或输出层）。在多层神经网络中，上层节点的输出和下层节点的输入之间具有一个函数关系，这个函数称为激活函数（又称激励函数）。如果<code>不用激励函数</code>（其实相当于激励函数是f(x) = x），在这种情况下你<code>每一层节点的输入都是上层输出的线性函数</code>，很容易验证，无论你神经网络有多少层，<code>输出都是输入的线性组合，与没有隐藏层效果相当</code>，这种情况就是最原始的感知机（Perceptron）了，那么网络的逼近能力就相当有限。</p>
</blockquote>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524085500389.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524085500389.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524085500389.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524085500389.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524085500389.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524085500389.png"/></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/v2-506351bd86c341e4bb52ebab3b1a3f66_r.jpg"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/v2-506351bd86c341e4bb52ebab3b1a3f66_r.jpg, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/v2-506351bd86c341e4bb52ebab3b1a3f66_r.jpg 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/v2-506351bd86c341e4bb52ebab3b1a3f66_r.jpg 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/v2-506351bd86c341e4bb52ebab3b1a3f66_r.jpg"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/v2-506351bd86c341e4bb52ebab3b1a3f66_r.jpg"/></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/v2-e6d254150ab20084be23756e49b52118_r.jpg"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/v2-e6d254150ab20084be23756e49b52118_r.jpg, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/v2-e6d254150ab20084be23756e49b52118_r.jpg 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/v2-e6d254150ab20084be23756e49b52118_r.jpg 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/v2-e6d254150ab20084be23756e49b52118_r.jpg"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/v2-e6d254150ab20084be23756e49b52118_r.jpg"/></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/v2-1cb1c6307cba97557a20b617e38e2ef4_r.jpg"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/v2-1cb1c6307cba97557a20b617e38e2ef4_r.jpg, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/v2-1cb1c6307cba97557a20b617e38e2ef4_r.jpg 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/v2-1cb1c6307cba97557a20b617e38e2ef4_r.jpg 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/v2-1cb1c6307cba97557a20b617e38e2ef4_r.jpg"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/v2-1cb1c6307cba97557a20b617e38e2ef4_r.jpg"/></p>
<h3 id="0-softmax">0. Softmax</h3>
<blockquote>
<p>softmax函数的本质就是将一个K维的任意实数向量压缩（映射）成另一个K维的实数向量，其中向量中的每个元素取值都介于（0，1）之间。经常用在神经网络的最后一层，作为输出层，进行多分类。</p>
<ul>
<li>softmax建模使用的分布是多项式分布，而logistic则基于伯努利分布</li>
<li>softmax回归进行的多分类，类与类之间是互斥的，即一个输入只能被归为一类; 多个logistic回归进行多分类，输出的类别并不是互斥的，即&quot;苹果&quot;这个词语既属于&quot;水果&quot;类也属于&quot;3C&quot;类别。</li>
</ul>
</blockquote>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524111833629.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524111833629.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524111833629.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524111833629.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524111833629.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524111833629.png"/></p>
<h3 id="1-sigmoidlogistic">1. Sigmoid&amp;Logistic</h3>
<blockquote>
<p>在分类任务中，sigmoid 正逐渐被 Tanh 函数取代作为标准的激活函数，因为后者为奇函数（关于原点对称）。</p>
</blockquote>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524090921933.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524090921933.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524090921933.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524090921933.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524090921933.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524090921933.png"/></p>
<p>###2. Sinusoid</p>
<blockquote>
<p>如同余弦函数，<code>Sinusoid（或简单正弦函数）激活函数为神经网络引入了周期性</code>。该函数的<code>值域为 [-1,1]</code>，且<code>导数处处连续</code>。此外，Sinusoid 激活函数为零点对称的奇函数。</p>
</blockquote>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524092309234.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524092309234.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524092309234.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524092309234.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524092309234.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524092309234.png"/></p>
<h3 id="2-sinc">2. Sinc</h3>
<blockquote>
<p>Sinc 函数（全称是 Cardinal Sine）在信号处理中尤为重要，因为它<code>表征了矩形函数的傅立叶变换（Fourier transform）</code>。作为一种激活函数，它的优势在于<code>处处可微和对称的特性</code>，不过它比较<code>容易产生梯度消失的问题</code>。</p>
</blockquote>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524092412368.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524092412368.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524092412368.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524092412368.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524092412368.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524092412368.png"/></p>
<h3 id="3-symmetrical-sigmoid">3. Symmetrical Sigmoid</h3>
<blockquote>
<p>Symmetrical Sigmoid 是另一个 Tanh 激活函数的变种（实际上，它<code>相当于输入减半的 Tanh</code>）。和 Tanh 一样，它是反对称的、零中心、可微分的，值域在 -1 到 1 之间``。它更<code>平坦的形状和更慢的下降派生表明它可以更有效地进行学习</code>。</p>
</blockquote>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524092014307.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524092014307.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524092014307.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524092014307.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524092014307.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524092014307.png"/></p>
<h3 id="4-loglog">4. LogLog</h3>
<blockquote>
<p>Log Log 激活函数（由上图 f(x) 可知该函数为以 e 为底的嵌套指数函数）的值域为 [0,1]，Complementary Log Log 激活函数<code>有潜力替代经典的 Sigmoid 激活函数</code>。该函数<code>饱和地更快，且零点值要高于 0.5</code>。</p>
</blockquote>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524092109602.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524092109602.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524092109602.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524092109602.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524092109602.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524092109602.png"/></p>
<h3 id="5-softmoid">5. softmoid</h3>
<blockquote>
<p>特点：它能够把输入的连续实值变换为<code>0和1之间的输出</code>，特别的，如果是非常大的负数，那么输出就是0；如果是非常大的正数，输出就是1.</p>
<ul>
<li>缺点：sigmoid函数曾经被使用的很多，不过近年来，用它的人越来越少了。
<ul>
<li>在深度神经网络中梯<code>度反向传递时导致梯度爆炸和梯度消失</code>，其中梯度爆炸发生的概率非常小，而<code>梯度消失发生的概率比较大</code>。梯度从后向前传播时，<code>每传递一层梯度值都会减小为原来的0.25倍</code>，如果神经网络隐层特别多，那么梯度在穿过多层后将变得非常小接近于0，即出现梯度消失现象；当网络<code>权值初始化为 ( 1 , + ∞ )区间内的值</code>，则会出现梯度爆炸情况。</li>
<li>其解析式中含有幂运算，计算机求解时相对来讲比较耗时。对于规模比较大的深度网络，这会较大地增加训练时间。</li>
<li>Sigmoid 的 output 不是0均值（即zero-centered）.如x &gt; 0 ,   f = w T x + b x&amp;gt 0,那么对w求局部梯度则都为正，这样在反向传播的过程中w要么都往正方向更新，要么都往负方向更新，导致有一种捆绑的效果，使得收敛缓慢。</li>
</ul>
</li>
</ul>
</blockquote>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524084008547.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524084008547.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524084008547.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524084008547.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524084008547.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524084008547.png"/></p>
<h3 id="6-tanh">6. tanh</h3>
<blockquote>
<p>解决了Sigmoid函数的不是zero-centered输出问题，然而，<code>梯度消失（gradient vanishing）的问题和幂运算的问题仍然存在</code>。</p>
</blockquote>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524084539502.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524084539502.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524084539502.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524084539502.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524084539502.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524084539502.png"/></p>
<h3 id="7-hard-tanh">7. Hard Tanh</h3>
<blockquote>
<p>Hard Tanh 是 Tanh 激活函数的线性分段近似。相较而言，<code>它更易计算</code>，这使得学习计算的速度更快，尽管<code>首次派生值为零可能导致静默神经元/过慢的学习速率</code></p>
</blockquote>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524091427502.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524091427502.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524091427502.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524091427502.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524091427502.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524091427502.png"/></p>
<h3 id="8-lecun-tanh">8. LeCun Tanh</h3>
<blockquote>
<p>是 Tanh 激活函数的扩展版本。它具有以下几个可以改善学习的属性：f(± 1) = ±1；二阶导数在 x=1 最大化；且有效增益接近 1。</p>
</blockquote>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524091510721.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524091510721.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524091510721.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524091510721.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524091510721.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524091510721.png"/></p>
<h3 id="9-arctan">9. ArcTan</h3>
<blockquote>
<p>ArcTan 激活函数更加平坦，这让它比其他双曲线更加清晰。在默认情况下，<code>其输出范围在-π/2 和π/2 之间</code>。其<code>导数趋向于零的速度也更慢</code>，这意味着学习的效率更高。但这也意味着，<code>导数的计算比 Tanh 更加昂贵</code>。</p>
</blockquote>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524091542870.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524091542870.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524091542870.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524091542870.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524091542870.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524091542870.png"/></p>
<p>###10.  SoftSign</p>
<blockquote>
<p>Softsign 是 Tanh 激活函数的另一个替代选择。就像 Tanh 一样，Softsign 是反对称、去中心、可微分，并返回-1 和 1 之间的值。其更平坦的曲线与更慢的下降导数表明它可以更<code>高效地学习</code>。另一方面，<code>导数的计算比 Tanh 更麻烦。</code></p>
</blockquote>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524091708235.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524091708235.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524091708235.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524091708235.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524091708235.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524091708235.png"/></p>
<h3 id="11-softplus">11. SoftPlus</h3>
<blockquote>
<p>作为<code> ReLU 的一个不错的替代选择</code>，SoftPlus 能够<code>返回任何大于 0 的值</code>。与 ReLU 不同，SoftPlus 的导数是连续的、非零的，无处不在，从而<code>防止出现静默神经元</code>。然而，SoftPlus 另一个不同于 ReLU 的地方在于其<code>不对称性，不以零为中心，这兴许会妨碍学习</code>。此外，<code>由于导数常常小于 1，也可能出现梯度消失的问题</code>。</p>
</blockquote>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524091816118.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524091816118.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524091816118.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524091816118.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524091816118.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524091816118.png"/></p>
<h3 id="12-relu">12. Relu</h3>
<blockquote>
<ul>
<li>优点：
<ul>
<li>解决了gradient vanishing 问题（在正区间）</li>
<li>计算速度非常快，只需要判读输入是否大于0</li>
<li>收敛速度远快于sigmoid和tanh</li>
</ul>
</li>
<li>缺点：
<ul>
<li>ReLU 输出的不是zero-centered</li>
<li>当<code>输入为负值的时候</code>，ReLU 的学习速度可能会变得很慢，甚至使神经元直接无效，因为此时输入小于零而梯度为零，从而其权重无法得到更新，在剩下的训练过程中会一直保持静默。</li>
<li>Dead ReLU Problem，指的是某些神经元可能永远不会被激活，导致相应的参数永远不能被更新。
<ul>
<li>非常不幸的参数初始化，这种情况比较少见</li>
<li>learning rate太高导致在训练过程中参数更新太大，不幸使网络进入这种状态</li>
<li>可以采用<code>Xavier初始化方法</code>，以及<code>避免将learning rate设置太大</code>或<code>使用adagrad等自动调节learning rate的算法</code>。</li>
</ul>
</li>
</ul>
</li>
</ul>
</blockquote>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524084629494.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524084629494.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524084629494.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524084629494.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524084629494.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524084629494.png"/></p>
<h3 id="13-leaky-relu">13. Leaky Relu</h3>
<blockquote>
<p>Leaky ReLU有ReLU的所有优点，外加不会有Dead ReLU问题，但是在实际操作当中，并没有完全证明Leaky ReLU总是好于ReLU。</p>
<ul>
<li>带泄露修正线性单元（Leaky ReLU）的输出对负值输入有很小的坡度。</li>
</ul>
</blockquote>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524085022288.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524085022288.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524085022288.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524085022288.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524085022288.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524085022288.png"/></p>
<h3 id="14-prelu">14. PReLU</h3>
<blockquote>
<p>参数化修正线性单元（Parameteric Rectified Linear Unit，PReLU）<code>属于 ReLU 修正类激活函数的一员</code>。它<code>和 RReLU 以及 Leaky ReLU 有一些共同点</code>，即<code>为负值输入添加了一个线性项</code>。而最关键的区别是，这个线性项的斜率实际上是在模型训练中学习到的。</p>
</blockquote>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524091056192.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524091056192.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524091056192.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524091056192.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524091056192.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524091056192.png"/></p>
<h3 id="15-rrelu">15. RReLu</h3>
<blockquote>
<p>随机带泄露的修正线性单元（Randomized Leaky Rectified Linear Unit，RReLU）也属于 ReLU 修正类激活函数的一员。和 Leaky ReLU 以及 PReLU 很相似，为负值输入添加了一个线性项。而最关键的区别是，<code>这个线性项的斜率在每一个节点上都是随机分配的（通常服从均匀分布）。</code></p>
</blockquote>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524091157083.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524091157083.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524091157083.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524091157083.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524091157083.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524091157083.png"/></p>
<h3 id="16-srelu">16. SReLU</h3>
<blockquote>
<p>S 型整流线性激活单元（S-shaped Rectified Linear Activation Unit，SReLU）属于以 ReLU 为代表的整流激活函数族。它由三个分段线性函数组成。其中<code>两种函数的斜度，以及函数相交的位置会在模型训练中被学习</code>。</p>
</blockquote>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524091318519.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524091318519.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524091318519.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524091318519.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524091318519.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524091318519.png"/></p>
<h3 id="17-eluexponential-linear-units">17. ELU(Exponential Linear Units)</h3>
<blockquote>
<p>ELU也是为解决ReLU存在的问题而提出，显然，ELU有ReLU的基本所有优点:</p>
<ul>
<li>不会有Dead ReLU问题</li>
<li>输出的均值接近0，zero-centered</li>
<li><code>计算量稍大</code>。类似于Leaky ReLU，理论上虽然好于ReLU，但在实际使用中目前并没有好的证据ELU总是优于ReLU。</li>
<li>和其它修正类激活函数不同的是，它包括一个<code>负指数项，从而防止静默神经元出现，导数收敛为零</code>，从而提高学习效率。</li>
</ul>
</blockquote>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524085125447.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524085125447.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524085125447.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524085125447.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524085125447.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524085125447.png"/></p>
<h3 id="18-selu">18. SELU</h3>
<blockquote>
<p>扩展指数线性单元（Scaled Exponential Linear Unit，SELU）是激活函数指数线性单元（ELU）的一个变种。其中<code>λ和α是固定数值（分别为 1.0507 和 1.6726）</code>。这些值背后的推论（零均值/单位方差）构成了自归一化神经网络的基础（SNN）。</p>
</blockquote>
<h3 id="19-hard-swish-或-h-swish">19. <strong>Hard-Swish 或 H-Swish</strong></h3>
<blockquote>
<p>几乎类似于 swish 函数，但计算成本却比 swish 更低，因为它用线性类型的 ReLU 函数取代了指数类型的 sigmoid 函数。</p>
<ul>
<li>在归一化时，网络层数较深，性能由于relu。</li>
</ul>
</blockquote>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524090502344.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524090502344.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524090502344.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524090502344.png 2x"
    data-sizes="auto"
    alt="image-20210524090502344"
    title="image-20210524090502344"/></p>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524090302779.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524090302779.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524090302779.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524090302779.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524090302779.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524090302779.png"/></p>
<h3 id="20-maxout">20. MaxOut</h3>
<blockquote>
<p><code>ReLu 和 Leaky ReLu 都是它的特殊形式</code>，所以它有 ReLu 的优点却没有 ReLu 的缺点。坏处是它使得<code>参数翻倍，导致总参数量非常大</code>。</p>
</blockquote>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524090431806.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524090431806.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524090431806.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524090431806.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524090431806.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524090431806.png"/></p>
<h3 id="21-step">21. Step</h3>
<blockquote>
<p>激活函数 Step 更倾向于理论而不是实际，它模仿了生物神经元要么全有要么全无的属性。它无法应用于神经网络，因为其导数是 0（除了零点导数无定义以外），这意味着基于梯度的优化方法并不可行。</p>
</blockquote>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524090635449.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524090635449.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524090635449.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524090635449.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524090635449.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524090635449.png"/></p>
<h3 id="22-identity">22. Identity</h3>
<blockquote>
<p>过激活函数 Identity，节点的输入等于输出。它完美适合于潜在行为是线性（与线性回归相似）的任务。当存在非线性，单独使用该激活函数是不够的，但它依然可以在最终输出节点上作为激活函数用于回归任务。</p>
</blockquote>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524090707832.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524090707832.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524090707832.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524090707832.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524090707832.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524090707832.png"/></p>
<h3 id="23-bent-identity">23. Bent Identity</h3>
<blockquote>
<p>激活函数 Bent Identity 是<code>介于 Identity 与 ReLU 之间的一种折衷选择</code>。它允许非线性行为，尽管其<code>非零导数有效提升了学习并克服了与 ReLU 相关的静默神经元的问题。</code>由于其导数<code>可在 1 的任意一侧返回值，因此它可能容易受到梯度爆炸和消失的影响。</code></p>
</blockquote>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524091930958.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524091930958.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524091930958.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524091930958.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524091930958.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524091930958.png"/></p>
<h3 id="24-gaussian">24. Gaussian</h3>
<blockquote>
<p>高斯激活函数（Gaussian）并不是径向基函数网络（RBFN）中常用的高斯核函数，高斯激活函数在多层感知机类的模型中并不是很流行。该函数处处可微且为偶函数，但一阶导会很快收敛到零。</p>
</blockquote>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524092218128.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524092218128.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524092218128.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524092218128.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524092218128.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210524092218128.png"/></p>
<h3 id="25-geluhttpspytorchorgdocsstable_modulestorchnnmodulesactivationhtmlgelu">25. <a href="https://pytorch.org/docs/stable/_modules/torch/nn/modules/activation.html#GELU"target="_blank" rel="external nofollow noopener noreferrer">GELU<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></h3>
<p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210529192110046.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210529192110046.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210529192110046.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210529192110046.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210529192110046.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210529192110046.png"/></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">GELU</span>(Module):
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">r</span><span style="color:#e6db74">&#34;&#34;&#34;Applies the Gaussian Error Linear Units function:
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    .. math:: \text</span><span style="color:#e6db74">{GELU}</span><span style="color:#e6db74">(x) = x * \Phi(x)
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    Examples::
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        &gt;&gt;&gt; m = nn.GELU()
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        &gt;&gt;&gt; input = torch.randn(2)
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        &gt;&gt;&gt; output = m(input)
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, input: Tensor) <span style="color:#f92672">-&gt;</span> Tensor:
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> F<span style="color:#f92672">.</span>gelu(input)
</span></span></code></pre></div><p><img
    class="lazyload"
    src="/svg/loading.min.svg"
    data-src="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210529192054909.png"
    data-srcset="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210529192054909.png, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210529192054909.png 1.5x, https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210529192054909.png 2x"
    data-sizes="auto"
    alt="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210529192054909.png"
    title="https://lddpicture.oss-cn-beijing.aliyuncs.com/picture/image-20210529192054909.png"/></p>
<h3 id="选择建议">选择建议</h3>
<ul>
<li>
<p>训练深度学习网络<code>尽量使用zero-centered数据</code> (可以经过数据预处理实现) 和<code>zero-centered输出</code>。所以要尽量选择输出具有zero-centered特点的激活函数<code>以加快模型的收敛速度</code>。</p>
</li>
<li>
<p>如果<code>使用 ReLU</code>，那么一定要<code>小心设置 learning rate</code>，而且要注意不要让网络出现很多 “dead” 神经元，如果这个问题不好解决，那么可以试试 Leaky ReLU、PReLU 或者 Maxout.</p>
</li>
<li>
<p><code>最好不要用 sigmoid，你可以试试 tanh</code>，不过可以预期它的效果会比不上 ReLU 和 Maxout.</p>
</li>
<li>
<p><code>sigmoid 只会输出正数，以及靠近0的输出变化率最大</code>，tanh和sigmoid不同的是，<code>tanh输出可以是负数</code>，<code>ReLu是输入只能大于0,如果你输入含有负数，ReLu就不适合</code>，如果你的输入是图片格式，ReLu就挺常用的。</p>
</li>
<li>
<p><code>zere-centered:</code> 越接近0为中心，SGD会越接近 natural gradient（一种二次优化技术），从而降低所需的迭代次数。</p>
</li>
<li>
<p>用于<code>分类器时</code>，<code>Sigmoid函数及其组合通常效果更好</code>。</p>
</li>
<li>
<p>由于<code>梯度消失问题</code>，有时要<code>避免使用sigmoid和tanh函数</code>。</p>
</li>
<li>
<p><code>ReLU函数是一个通用的激活函数</code>，目前在大多数情况下使用, <code>ReLU函数只能在隐藏层中使用</code>, <strong>从ReLU函数开始，如果ReLU函数没有提供最优结果，再尝试其他激活函数</strong></p>
</li>
<li>
<p>如果神经网络中<code>出现死神经元</code>，那么<code>PReLU函数</code>就是最好的选择。</p>
</li>
<li>
<p>组合使用：</p>
<ul>
<li><strong>ReLU + MSE</strong></li>
</ul>
<blockquote>
<p><code>均方误差损失函数无法处理梯度消失问题</code>，而使用Leak ReLU激活函数能够减少计算时梯度消失的问题，因此在神经网络中如果需要使用均方误差损失函数，**一 般采用Leak ReLU等可以减少梯度消失的激活函数。**另外，由于均方误差具有普遍性，一般作为衡量损失值的标准，因此使用均方误差作为损失函数表现既不会太好也不至于太差。</p>
</blockquote>
<ul>
<li><strong>Sigmoid + Logistic</strong></li>
</ul>
<blockquote>
<p>Sigmoid函数会引起梯度消失问题：根据链式求导法，Sigmoid函数求导后由多个[0,1]范围的数进行连乘，如其导数形式为当其中一个数很小时， 连成后会无限趋近于零直至最后消失。而类Logistic损失函数求导时，加上对数后 连乘操作转化为求和操作，在<code>一定程度上避免了梯度消失</code>，<strong>所以我们经常可以看到 Sigmoid激活函数+交叉摘损失函数的组合。</strong></p>
</blockquote>
<ul>
<li><strong>Softmax + Logisitc</strong></li>
</ul>
<blockquote>
<p>在数学上，<code>Softmax激活函数会返回输出类的互斥概率分布</code>，也就是能<code>把离散的 输出转换为一个同分布互斥的概率</code>，如（0.2, 0.8)。另外<code>，类Logisitc损失函数是基 于概率的最大似然估计函数而来的</code>，因此输出概率化能够更加方便优化算法进行求 导和计算，所以我们经常可以看到输出层使用Softmax激活函数+交叉熵损失函数 的组合。</p>
</blockquote>
</li>
</ul>
<h3 id="resouces">Resouces</h3>
<ul>
<li><a href="https://www.jiqizhixin.com/articles/2017-10-10-3"target="_blank" rel="external nofollow noopener noreferrer">https://www.jiqizhixin.com/articles/2017-10-10-3<i class="fa-solid fa-external-link-alt fa-fw fa-xs ms-1 text-secondary" aria-hidden="true"></i></a></li>
</ul>
</div>
<div class="post-footer" id="post-footer">
  <div class="post-info">
    <div class="post-info-line">
      <div class="post-info-mod">
        <span title=2023-12-31&#32;15:49:02>更新于 2023-12-31&nbsp;</span>
      </div><div class="post-info-license">
          <span><a rel="license external nofollow noopener noreferrer" href="https://creativecommons.org/licenses/by-nc/4.0/" target="_blank">CC BY-NC 4.0</a></span>
        </div></div>
    <div class="post-info-line">
      <div class="post-info-md"><span><a href="/activatefunction/index.md" title="阅读原始文档" class="link-to-markdown">阅读原始文档</a></span><span><a href="https://liudongdong1.github.io/edit/master/content/posts%5c%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%5cmodel%5clayer%5cActivateFunction.md" title="编辑此页"target="_blank" rel="external nofollow noopener noreferrer" class="link-to-edit">编辑此页</a></span></div>
      <div class="post-info-share">
        <span><a href="javascript:void(0);" title="分享到 Twitter" data-sharer="twitter" data-url="https://liudongdong1.github.io/activatefunction/" data-title="ActivateFunction" data-hashtags="model"><i class="fa-brands fa-twitter fa-fw" aria-hidden="true"></i></a>
  <a href="javascript:void(0);" title="分享到 Facebook" data-sharer="facebook" data-url="https://liudongdong1.github.io/activatefunction/" data-hashtag="model"><i class="fa-brands fa-facebook-square fa-fw" aria-hidden="true"></i></a>
  <a href="javascript:void(0);" title="分享到 微博" data-sharer="weibo" data-url="https://liudongdong1.github.io/activatefunction/" data-title="ActivateFunction" data-image="https://cdn.stocksnap.io/img-thumbs/280h/5NLKT00MVB.jpg"><i class="fa-brands fa-weibo fa-fw" aria-hidden="true"></i></a>
  </span>
      </div>
    </div>
  </div>

  <div class="post-info-more">
    <section class="post-tags"><i class="fa-solid fa-tags fa-fw" aria-hidden="true"></i>&nbsp;<a href="/tags/model/">model</a></section>
    <section>
      <span><a href="javascript:void(0);" onclick="window.history.back();">返回</a></span>&nbsp;|&nbsp;<span><a href="/">主页</a></span>
    </section>
  </div>

  <div class="post-nav"><a href="/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E9%9D%A2%E7%BB%8F/" class="prev" rel="prev" title="面经_消息队列"><i class="fa-solid fa-angle-left fa-fw" aria-hidden="true"></i>面经_消息队列</a>
      <a href="/3dkeypointpaper/" class="next" rel="next" title="3DkeyPointPaper">3DkeyPointPaper<i class="fa-solid fa-angle-right fa-fw" aria-hidden="true"></i></a></div>
</div>
</article></main><footer class="footer">
    <div class="footer-container"><div class="footer-line powered">由 <a href="https://gohugo.io/" target="_blank" rel="external nofollow noopener noreferrer" title="Hugo 0.118.2">Hugo</a> 强力驱动 | 主题 - <a href="https://github.com/hugo-fixit/FixIt" target="_blank" rel="external" title="FixIt v0.2.17-RC"><img class="fixit-icon" src="/fixit.min.svg" alt="FixIt logo" />&nbsp;FixIt</a>
        </div><div class="footer-line copyright" itemscope itemtype="http://schema.org/CreativeWork"><i class="fa-regular fa-copyright fa-fw" aria-hidden="true"></i>
            <span itemprop="copyrightYear">2020 - 2024</span><span class="author" itemprop="copyrightHolder">
              <a href="https://liudongdong1.github.io/"target="_blank" rel="external nofollow noopener noreferrer">LiuDongdong</a></span><span class="license footer-divider"><a rel="license external nofollow noopener noreferrer" href="https://creativecommons.org/licenses/by-nc/4.0/" target="_blank">CC BY-NC 4.0</a></span></div><div class="footer-line statistics"><span class="site-time" title='网站运行中 ...'><i class="fa-solid fa-heartbeat fa-fw animate-icon" aria-hidden="true"></i>&nbsp;<span class="run-times">网站运行中 ...</span></span></div><div class="footer-line ibruce">
          <span id="busuanzi_container_site_uv" title='总访客数'><i class="fa-regular fa-user fa-fw" aria-hidden="true"></i>&nbsp;<span id="busuanzi_value_site_uv"><i class="fa-solid fa-spinner fa-spin fa-fw" aria-hidden="true"></i></span></span><span id="busuanzi_container_site_pv" class="footer-divider" title='总访问量'><i class="fa-regular fa-eye fa-fw" aria-hidden="true"></i>&nbsp;<span id="busuanzi_value_site_pv"><i class="fa-solid fa-spinner fa-spin fa-fw" aria-hidden="true"></i></span></span>
        </div></div>
  </footer></div><div class="widgets"><div class="fixed-buttons animate__faster d-none"><div class="fixed-button back-to-top" role="button" aria-label="回到顶部"><i class="fa-solid fa-arrow-up fa-fw" aria-hidden="true"></i><span class="variant-numeric">0%</span>
        </div></div><a href="https://liudongdong1.github.io/" title="在 GitHub 上查看源代码"target="_blank" rel="external nofollow" class="github-corner right d-none-mobile"><svg viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><div id="mask"></div><div class="reading-progress-bar" style="left: 0;top: 0;--bg-progress: #0076ff;--bg-progress-dark: #fff;"></div><noscript>
    <div class="noscript-warning">FixIt 主题在启用 JavaScript 的情况下效果最佳。</div>
  </noscript>
</div><link rel="stylesheet" href="/lib/katex/katex.min.css"><link rel="stylesheet" href="/lib/cookieconsent/cookieconsent.min.css"><script src="/lib/autocomplete/autocomplete.min.js" defer></script><script src="/lib/algoliasearch/algoliasearch-lite.umd.min.js" defer></script><script src="/lib/lazysizes/lazysizes.min.js" async defer></script><script src="/lib/sharer/sharer.min.js" async defer></script><script src="/lib/typeit/index.umd.js" defer></script><script src="/lib/katex/katex.min.js" defer></script><script src="/lib/katex/auto-render.min.js" defer></script><script src="/lib/katex/copy-tex.min.js" defer></script><script src="/lib/katex/mhchem.min.js" defer></script><script src="/lib/cookieconsent/cookieconsent.min.js" defer></script><script src="/lib/pangu/pangu.min.js" defer></script><script src="/lib/cell-watermark/watermark.min.js" defer></script><script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async defer></script><script>window.config={"code":{"copyTitle":"复制到剪贴板","editLockTitle":"锁定可编辑代码块","editUnLockTitle":"解锁可编辑代码块","editable":true,"maxShownLines":10},"comment":{"enable":false},"cookieconsent":{"content":{"dismiss":"同意","link":"了解更多","message":"本网站使用 Cookies 来改善您的浏览体验。"},"enable":true,"palette":{"button":{"background":"#f0f0f0"},"popup":{"background":"#1aa3ff"}},"theme":"edgeless"},"data":{"typeit-header-subtitle-desktop":"\u003cspan style='font-family: MMT,\"沐目体\";'\u003e吾日三省吾身\u003c/span\u003e","typeit-header-subtitle-mobile":"\u003cspan style='font-family: MMT,\"沐目体\";'\u003e吾日三省吾身\u003c/span\u003e"},"enablePWA":true,"enablePangu":true,"math":{"delimiters":[{"display":true,"left":"$$","right":"$$"},{"display":true,"left":"\\[","right":"\\]"},{"display":true,"left":"\\begin{equation}","right":"\\end{equation}"},{"display":true,"left":"\\begin{equation*}","right":"\\end{equation*}"},{"display":true,"left":"\\begin{align}","right":"\\end{align}"},{"display":true,"left":"\\begin{align*}","right":"\\end{align*}"},{"display":true,"left":"\\begin{alignat}","right":"\\end{alignat}"},{"display":true,"left":"\\begin{alignat*}","right":"\\end{alignat*}"},{"display":true,"left":"\\begin{gather}","right":"\\end{gather}"},{"display":true,"left":"\\begin{CD}","right":"\\end{CD}"},{"display":false,"left":"$","right":"$"},{"display":false,"left":"\\(","right":"\\)"}],"strict":false},"search":{"algoliaAppID":"2R1K9SKLQZ","algoliaIndex":"index.zh-cn","algoliaSearchKey":"4a226aa1c5c98d6859e4d1386adb2bc7","highlightTag":"em","maxResultLength":10,"noResultsFound":"没有找到结果","snippetLength":50,"type":"algolia"},"siteTime":"2020-12-18T16:15:22+08:00","typeit":{"cursorChar":"|","cursorSpeed":1000,"data":{"typeit-header-subtitle-desktop":["typeit-header-subtitle-desktop"],"typeit-header-subtitle-mobile":["typeit-header-subtitle-mobile"]},"duration":-1,"speed":100},"watermark":{"appendto":".wrapper\u003emain","colspacing":30,"content":"\u003cimg class=\"fixit-icon\" src=\"/fixit.min.svg\" alt=\"FixIt logo\" /\u003e FixIt 主题","enable":true,"fontfamily":"inherit","fontsize":0.85,"height":21,"opacity":0.0125,"rotate":15,"rowspacing":60,"width":150}};</script><script src="/js/theme.min.js" defer></script><script src="/js/custom.min.js" defer></script></body>
</html>
